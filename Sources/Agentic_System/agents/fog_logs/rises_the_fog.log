I must go in; the fog is rising
Initializing FatherOfGod
╭────────────────────────────────────────────────────────────────────────────────────────── New run - FatherOfGeorge ──────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                              │
│ Task: Initialize Root Manager orchestration                                                                                                                                                                  │
│                                                                                                                                                                                                              │
│ Context:                                                                                                                                                                                                     │
│ - PickSection: Select a promising V8 code region to analyze                                                                                                                                                  │
│ - FatherOfGeorge: Primary orchestrator of the system, coordinates between analysis and program generation                                                                                                    │
│ - CodeAnalyzer: Analyze V8 code and knowledge bases to guide the program template building                                                                                                                   │
│ - ProgramBuilder: Generate Fuzzilli program templates for fuzzing a specific code region                                                                                                                     │
│                                                                                                                                                                                                              │
╰─ LiteLLMModel - gpt-5 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'PickSection' with arguments: {'task': 'Select a promising V8 code region to analyze for directed fuzzing with Fuzzilli ProgramTemplates. Autonomously choose a code region within or adjacent │
│ to the JIT pipeline that offers good fuzzing surface and potential for subtle bugs, without any pre-specified targets or constraints.'}                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────── New run - PickSection ────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                              │
│ You're a helpful agent named 'PickSection'.                                                                                                                                                                  │
│ You have been submitted this task by your manager.                                                                                                                                                           │
│ ---                                                                                                                                                                                                          │
│ Task:                                                                                                                                                                                                        │
│ Select a promising V8 code region to analyze for directed fuzzing with Fuzzilli ProgramTemplates. Autonomously choose a code region within or adjacent to the JIT pipeline that offers good fuzzing surface  │
│ and potential for subtle bugs, without any pre-specified targets or constraints.                                                                                                                             │
│ ---                                                                                                                                                                                                          │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.                    │
│                                                                                                                                                                                                              │
│ Your final_answer WILL HAVE to contain these parts:                                                                                                                                                          │
│ ### 1. Task outcome (short version):                                                                                                                                                                         │
│ ### 2. Task outcome (extremely detailed version):                                                                                                                                                            │
│ ### 3. Additional context (if relevant):                                                                                                                                                                     │
│                                                                                                                                                                                                              │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.                                                                                        │
│ And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.                                                              │
│                                                                                                                                                                                                              │
╰─ LiteLLMModel - gpt-5 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'get_random_entry_data' with arguments: {}                                                                                                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: this is entry data for maglev/regress/regress-1455295
{"js": "// Copyright 2023 the V8 project authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n//\n// Flags: 
--allow-natives-syntax --maglev\n\nfunction foo() {\n  let caught = 0;\n  try {\n    |].forEach(undefined);\n  } catch (e) {\n    caught += 2;\n  }\n  try {\n    |].forEach(undefined);\n  } catch (e) {\n    
caught += 40;\n  }\n  return caught;\n}\n\n%PrepareFunctionForOptimization(foo);\nprint(42, foo());\nprint(42, foo());\n%OptimizeMaglevOnNextCall(foo);\nprint(42, foo());\n", "Fuzzilli": "v0 <- 
BeginPlainFunction -> \n    v1 <- LoadInteger '0'\n    v2 <- CreateNamedVariable 'caught', 'let', v1\n    BeginTry\n        v3 <- LoadUndefined\n        v4 <- CreateArray |]\n        v5 <- CallMethod v4, 
'forEach', |v3]\n    BeginCatch -> v6\n        v7 <- LoadInteger '2'\n        Update v2, '+', v7\n    EndTryCatch\n    BeginTry\n        v8 <- LoadUndefined\n        v9 <- CreateArray |]\n        v10 <- 
CallMethod v9, 'forEach', |v8]\n    BeginCatch -> v11\n        v12 <- LoadInteger '40'\n        Update v2, '+', v12\n    EndTryCatch\n    Return v2\nEndPlainFunction\nv13 <- Eval 
'%PrepareFunctionForOptimization(%@)', |v0]\nv14 <- LoadInteger '42'\nv15 <- CallFunction v0, |]\nv16 <- CreateNamedVariable 'print', 'none'\nv17 <- CallFunction v16, |v14, v15]\nv18 <- LoadInteger '42'\nv19 
<- CallFunction v0, |]\nv20 <- CallFunction v16, |v18, v19]\nv21 <- Eval '%OptimizeMaglevOnNextCall(%@)', |v0]\nv22 <- LoadInteger '42'\nv23 <- CallFunction v0, |]\nv24 <- CallFunction v16, |v22, 
v23]\n\n\nfunction foo() {\n    let caught = 0;\n    try {\n        (|]).forEach(undefined);\n    } catch(e6) {\n        caught += 2;\n    }\n    try {\n        (|]).forEach(undefined);\n    } catch(e11) {\n 
caught += 40;\n    }\n    return caught;\n}\nconst v13 = %PrepareFunctionForOptimization(foo);\nprint(42, foo());\nprint(42, foo());\nconst v21 = %OptimizeMaglevOnNextCall(foo);\nprint(42, foo());", 
"execution_data": 
"--no-experimental\n--abort-on-contradictory-flags\n--no-exit-on-contradictory-flags\n--no-allow-overwriting-for-next-flag\n--no-use-strict\n--no-trace-temporal\n--no-harmony\n--harmony-shipping\n--no-js-stag
ing\n--js-shipping\n--no-harmony-shadow-realm\n--no-harmony-struct\n--no-js-decorators\n--no-js-source-phase-imports\n--no-harmony-temporal\n--harmony-import-attributes\n--js-regexp-duplicate-named-groups\n--
js-regexp-modifiers\n--js-promise-try\n--js-atomics-pause\n--js-error-iserror\n--js-regexp-escape\n--js-explicit-resource-management\n--js-float16array\n--js-base-64\n--builtin-subclassing\n--no-enable-shared
arraybuffer-per-context\n--no-for-of-optimization\n--no-stress-snapshot\n--no-lite-mode\n--enable-allocation-folding\n--no-disable-write-barriers\n--no-enable-unconditional-write-barriers\n--no-single-generat
ion\n--no-conservative-stack-scanning\n--no-direct-handle\n--no-scavenger-conservative-object-pinning\n--no-stress-scavenger-conservative-object-pinning\n--no-stress-scavenger-conservative-object-pinning-rand
om\n--no-scavenger-precise-object-pinning\n--no-precise-object-pinning\n--scavenger-promote-quarantined-pages\n--no-local-off-stack-check\n--no-experimental-fuzzing\n--no-future\n--no-force-emit-interrupt-bud
get-checks\n--maglev\n--no-maglev-future\n--no-optimize-on-next-call-optimizes-to-maglev\n--no-stress-maglev\n--no-maglev-as-top-tier\n--maglev-inlining\n--maglev-loop-peeling\n--maglev-optimistic-peeled-loop
s\n--maglev-loop-peeling-max-size=400\n--maglev-loop-peeling-max-size-cumulative=900\n--maglev-deopt-data-on-background\n--maglev-build-code-on-background\n--maglev-destroy-on-background\n--no-maglev-inline-a
pi-calls\n--maglev-cons-string-elision\n--maglev-pretenure-store-values\n--maglev-poly-calls\n--maglev-truncation\n--no-maglev-licm\n--concurrent-maglev-max-threads=2\n--no-concurrent-maglev-high-priority-thr
eads\n--max-maglev-inline-depth=1\n--max-maglev-hard-inline-depth=10\n--max-maglev-inlined-bytecode-size=100\n--max-maglev-inlined-bytecode-size-cumulative=920\n--max-maglev-inlined-bytecode-size-small-total=
3000\n--max-maglev-inlined-bytecode-size-small=27\n--max-maglev-inlined-bytecode-size-small-with-heapnum-in-out=75\n--min-maglev-inlining-frequency=0.95\n--maglev-reuse-stack-slots\n--maglev-untagged-phis\n--
maglev-hoist-osr-value-phi-untagging\n--no-maglev-speculative-hoist-phi-untagging\n--maglev-cse\n--no-maglev-non-eager-inlining\n--no-turbolev-non-eager-inlining\n--maglev-filter=\"*\"\n--maglev-print-filter=
\"*\"\n--no-maglev-assert\n--maglev-assert-stack-size\n--no-maglev-break-on-entry\n--maglev-print-bytecode\n--maglev-print-feedback\n--maglev-print-inlined\n--print-maglev-code\n--no-trace-maglev-graph-buildi
ng\n--no-trace-maglev-loop-speeling\n--no-trace-maglev-inlining\n--no-trace-maglev-kna-processor\n--no-print-maglev-deopt-verbose\n--print-maglev-graph\n--print-maglev-graphs\n--no-trace-maglev-phi-untagging\
n--no-trace-maglev-regalloc\n--no-maglev-stats\n--no-maglev-stats-nvp\n--maglev-function-context-specialization\n--no-specialize-code-for-one-byte-seq-strings\n--additive-safe-int-feedback\n--enable-enumerate
d-keyed-access-bytecode\n--no-dict-property-const-tracking\n--script-context-cells\n--function-context-cells\n--function-context-cells-max-size=1\n--empty-context-extension-dep\n--json-stringify-fast-path\n--
cache-property-key-string-adds\n--smi-string-cache-size=8192\n--double-string-cache-size=4096\n--no-trace-number-string-cache\n--maglev-inline-date-accessors\n--turbofan-inline-date-accessors\n--inline-date-a
ccessors\n--extensible-ro-snapshot\n--max-opt=999\n--efficiency-mode=unset\n--battery-saver-mode=unset\n--memory-saver-mode=unset\n--no-efficiency-mode-disable-turbofan\n--efficiency-mode-delay-turbofan-multi
ply=3\n--wasm-trace-native=\"\"\n--no-jitless\n--no-disable-optimizing-compilers\n--memory-protection-keys\n--no-force-memory-protection-keys\n--no-assert-types\n--no-turboshaft-assert-types\n--no-verify-simp
lified-lowering\n--no-trace-compilation-dependencies\n--no-unmap-holes\n--no-assert-hole-checked-by-value\n--allocation-site-tracking\n--allocation-site-pretenuring\n--page-promotion\n--page-promotion-thresho
ld=70\n--minor-ms-page-promotion-threshold=50\n--minor-ms-page-promotion-max-lab-threshold=30\n--minor-ms-max-page-age=4\n--no-trace-page-promotions\n--trace-pretenuring\n--no-trace-pretenuring-statistics\n--
no-trace-resize-large-object\n--track-field-types\n--no-trace-block-coverage\n--trace-protector-invalidation\n--decommit-pooled-pages\n--zero-unused-memory\n--no-high-end-android\n--high-end-android-physical-
memory-threshold=4294967295\n--no-minor-ms-shortcut-strings\n--no-feedback-normalization\n--internalize-on-the-fly\n--unbox-double-arrays\n--string-slices\n--invocation-count-for-feedback-allocation=8\n--invo
cation-count-for-maglev=400\n--invocation-count-for-maglev-osr=100\n--osr-from-maglev\n--osr-from-maglev-interrupt-scale-factor=0.8\n--no-always-osr-from-maglev\n--invocation-count-for-turbofan=16000\n--invoc
ation-count-for-osr=800\n--osr-to-tierup=1\n--minimum-invocations-after-ic-update=500\n--minimum-invocations-before-optimization=2\n--no-jit-fuzzing\n--use-std-math-pow\n--use-ic\n--lazy-feedback-allocation\n
--no-stress-ic\n--ignition-elide-noneffectful-bytecodes\n--ignition-reo\n--ignition-filter-expression-positions\n--ignition-share-named-property-feedback\n--ignition-elide-redundant-tdz-checks\n--print-byteco
de\n--enable-lazy-source-positions\n--no-stress-lazy-source-positions\n--print-bytecode-filter=\"*\"\n--omit-default-ctors\n--no-trace-ignition-codegen\n--trace-ignition-dispatches-output-file=\"\"\n--no-trac
e-track-allocation-sites\n--trace-migration\n--trace-generalization\n--reuse-scope-infos\n--sparkplug\n--no-always-sparkplug\n--baseline-batch-compilation\n--concurrent-sparkplug\n--concurrent-sparkplug-max-t
hreads=1\n--no-concurrent-sparkplug-high-priority-threads\n--sparkplug-filter=\"*\"\n--no-sparkplug-needs-short-builtins\n--baseline-batch-compilation-threshold=4096\n--no-trace-baseline\n--no-trace-baseline-
batch-compilation\n--no-shared-string-table\n--no-always-use-string-forwarding-table\n--no-transition-strings-during-gc-with-stack\n--initial-shared-heap-size=0\n--max-shared-heap-size=0\n--no-concurrent-buil
tin-generation\n--concurrent-recompilation\n--no-trace-concurrent-recompilation\n--concurrent-recompilation-queue-length=8\n--concurrent-recompilation-delay=0\n--concurrent-recompilation-front-running\n--conc
urrent-turbofan-max-threads=4\n--no-stress-concurrent-inlining\n--maglev-overwrite-budget\n--maglev-overwrite-osr-budget\n--no-stress-concurrent-inlining-attach-code\n--max-serializer-nesting=25\n--no-trace-h
eap-broker-verbose\n--no-trace-heap-broker\n--stress-runs=0\n--deopt-every-n-times=0\n--no-print-deopt-stress\n--turbofan\n--opt\n--no-stress-turbo-late-spilling\n--turbo-wasm-address-reassociation\n--no-conc
urrent-turbo-tracing\n--no-optimize-maglev-optimizes-to-turbofan\n--turbo-filter=\"*\"\n--no-trace-turbo\n--trace-turbo-path=\"\"\n--trace-turbo-filter=\"*\"\n--no-trace-turbo-graph\n--no-trace-turbo-schedule
d\n--trace-turbo-file-prefix=\"turbo\"\n--trace-turbo-cfg-file=\"turbo.cfg\"\n--trace-turbo-types\n--no-trace-turbo-scheduler\n--no-trace-turbo-reduction\n--no-trace-turbo-bailouts\n--no-trace-turbo-trimming\
n--no-trace-turbo-jt\n--no-trace-turbo-ceq\n--no-trace-turbo-loop\n--no-trace-turbo-alloc\n--no-trace-all-uses\n--no-trace-representation\n--no-trace-turbo-stack-accesses\n--no-fuzzing-and-concurrent-recompil
ation\n--turbo-verify\n--turbo-verify-machine-graph=\"\"\n--no-verify-csa\n--no-trace-verify-csa\n--csa-trap-on-node=\"\"\n--builtins-effects-src=\"\"\n--fixed-array-bounds-checks\n--no-turbo-stats\n--no-turb
o-stats-nvp\n--no-turbo-stats-wasm\n--turbo-splitting\n--turbo-inlining\n--turbo-elide-frames\n--max-inlined-bytecode-size=460\n--max-inlined-bytecode-size-cumulative=920\n--max-turbolev-inlined-bytecode-size
-cumulative=1840\n--max-inlined-bytecode-size-absolute=4600\n--max-inlined-bytecode-size-small-total=30000\n--reserve-inline-budget-scale-factor=1.2\n--max-inlined-bytecode-size-small=27\n--max-inlined-byteco
de-size-small-with-heapnum-in-out=75\n--max-optimized-bytecode-size=61440\n--min-inlining-frequency=0.05\n--no-stress-inline\n--no-trace-turbo-inlining\n--turbo-inline-array-builtins\n--use-osr\n--maglev-osr\
n--concurrent-osr\n--maglev-allocation-folding=2\n--maglev-escape-analysis\n--no-trace-maglev-escape-analysis\n--no-maglev-object-tracking\n--trace-maglev-object-tracking\n--no-turbo-string-builder\n--trace-o
sr\n--log-or-trace-osr\n--analyze-environment-liveness\n--no-trace-environment-liveness\n--turbo-load-elimination\n--trace-turbo-load-elimination\n--no-turbo-profiling\n--no-turbo-profiling-verbose\n--turbo-p
rofiling-output=\"\"\n--no-reorder-builtins\n--no-abort-on-bad-builtin-profile-data\n--no-warn-about-builtin-profile-data\n--dump-builtins-hashes-to-file=\"\"\n--turbo-verify-allocation\n--turbo-move-optimiza
tion\n--turbo-jt\n--turbo-loop-peeling\n--turbo-loop-variable\n--turbo-loop-rotation\n--turbo-cf-optimization\n--turbo-escape\n--turbo-allocation-folding\n--no-turbo-instruction-scheduling\n--no-turbo-stress-
instruction-scheduling\n--turbo-store-elimination\n--trace-store-elimination\n--turbo-typer-hardening\n--turbo-rewrite-far-jumps\n--no-stress-gc-during-compilation\n--turbo-fast-api-calls\n--no-fast-api-allow
-float-in-sim\n--no-turbo-compress-frame-translations\n--turbo-inline-js-wasm-calls\n--no-turbo-optimize-inlined-js-wasm-wrappers\n--turbo-optimize-apply\n--turbo-optimize-math-minmax\n--turboshaft\n--no-turb
oshaft-enable-debug-features\n--turboshaft-wasm-load-elimination\n--no-turboshaft-wasm-in-js-inlining\n--no-turbolev-inline-js-wasm-wrappers\n--turboshaft-load-elimination\n--turboshaft-loop-unrolling\n--turb
oshaft-string-concat-escape-analysis\n--no-turboshaft-typed-optimizations\n--no-turbolev\n--no-print-turbolev-frontend\n--no-turbolev-future\n--typed-array-length-loading\n--no-deduplicate-heap-number-request
s\n--no-turboshaft-verify-load-elimination\n--turboshaft-opt-bisect-limit=18446744073709551615\n--turboshaft-opt-bisect-break=18446744073709551615\n--no-turboshaft-verify-reductions\n--turboshaft-trace-typing
\n--turboshaft-trace-reduction\n--no-turboshaft-trace-intermediate-reductions\n--turboshaft-trace-emitted\n--no-turboshaft-trace-unrolling\n--no-turboshaft-trace-peeling\n--no-turboshaft-trace-load-eliminatio
n\n--no-turboshaft-trace-if-else-to-switch\n--trace-turbolev-graph-building\n--profile-guided-optimization\n--profile-guided-optimization-for-empty-feedback-vector\n--invocation-count-for-early-optimization=3
0\n--invocation-count-for-maglev-with-delay=600\n--no-optimize-for-size\n--reopt-after-lazy-deopts\n--no-verify-get-js-builtin-state\n--wasm-generic-wrapper\n--wasm-num-compilation-tasks=128\n--no-trace-wasm-
native-heap\n--no-trace-wasm-offheap-memory\n--no-print-wasm-offheap-memory-size\n--no-trace-wasm-serialization\n--wasm-async-compilation\n--no-wasm-test-streaming\n--wasm-native-module-cache\n--wasm-max-mem-
pages=4294967295\n--wasm-max-table-size=10000000\n--wasm-max-committed-code-mb=4095\n--wasm-max-code-space-size-mb=1024\n--wasm-tier-up\n--wasm-dynamic-tiering\n--no-wasm-sync-tier-up\n--wasm-tiering-budget=1
3000000\n--wasm-wrapper-tiering-budget=1000\n--max-wasm-functions=1000000\n--wasm-caching-threshold=1000\n--wasm-caching-hard-threshold=1000000\n--wasm-caching-timeout-ms=2000\n--no-trace-wasm-compilation-tim
es\n--wasm-tier-up-filter=-1\n--wasm-eager-tier-up-function=-1\n--no-trace-wasm-decoder\n--no-trace-wasm-compiler\n--no-trace-wasm-streaming\n--no-trace-wasm-stack-switching\n--no-stress-wasm-stack-switching\
n--wasm-stack-switching-stack-size=984\n--liftoff\n--no-liftoff-only\n--no-enable-testing-opcode-in-wasm\n--no-trace-liftoff\n--no-trace-wasm-memory\n--no-trace-wasm-globals\n--wasm-tier-mask-for-testing=0\n-
-wasm-debug-mask-for-testing=0\n--no-experimental-wasm-pgo-to-file\n--no-experimental-wasm-pgo-from-file\n--validate-asm\n--no-suppress-asm-messages\n--no-trace-asm-time\n--no-trace-asm-scanner\n--no-trace-as
m-parser\n--no-stress-validate-asm\n--no-dump-wasm-module\n--dump-wasm-module-path=\"\"\n--no-wasm-fast-api\n--wasm-deopt\n--wasm-deopts-per-function-limit=10\n--no-experimental-wasm-type-reflection\n--no-exp
erimental-wasm-instruction-tracing\n--no-experimental-wasm-custom-descriptors\n--no-experimental-wasm-shared\n--no-experimental-wasm-fp16\n--no-experimental-wasm-memory-control\n--no-experimental-wasm-wasmfx\
n--no-experimental-wasm-rab-integration\n--no-experimental-wasm-compilation-hints\n--no-experimental-wasm-stringref\n--no-experimental-wasm-imported-strings-utf8\n--no-experimental-wasm-growable-stacks\n--exp
erimental-wasm-legacy-eh\n--experimental-wasm-branch-hinting\n--experimental-wasm-exnref\n--no-experimental-wasm-assume-ref-cast-succeeds\n--no-experimental-wasm-ref-cast-nop\n--no-experimental-wasm-skip-null
-checks\n--no-experimental-wasm-skip-bounds-checks\n--no-experimental-wasm-js-interop\n--no-wasm-staging\n--wasm-opt\n--wasm-bounds-checks\n--wasm-stack-checks\n--no-wasm-enforce-bounds-checks\n--wasm-math-in
trinsics\n--no-wasm-inlining\n--wasm-inlining-budget=5000\n--wasm-inlining-max-size=500\n--wasm-inlining-factor=3\n--wasm-inlining-min-budget=50\n--no-wasm-inlining-ignore-call-counts\n--no-trace-wasm-inlinin
g\n--no-trace-wasm-typer\n--no-wasm-inlining-call-indirect\n--wasm-bulkmem-inlining\n--wasm-loop-unrolling\n--wasm-loop-peeling\n--wasm-loop-peeling-max-size=1000\n--no-trace-wasm-loop-peeling\n--no-wasm-fuzz
er-gen-test\n--no-print-wasm-code\n--print-wasm-code-function-index=-1\n--no-print-wasm-stub-code\n--asm-wasm-lazy-compilation\n--wasm-lazy-compilation\n--no-trace-wasm-lazy-compilation\n--no-wasm-lazy-valida
tion\n--no-wasm-simd-ssse3-codegen\n--wasm-code-gc\n--no-trace-wasm-code-gc\n--no-stress-wasm-code-gc\n--wasm-max-initial-code-space-reservation=0\n--no-stress-wasm-memory-moving\n--flush-liftoff-code\n--no-s
tress-branch-hinting\n--wasm-max-module-size=1073741824\n--wasm-disassembly-max-mb=1000\n--wasm-capi-thread-pool-size=0\n--trace-wasm\n--no-trace-wasm-instances\n--no-experimental-wasm-revectorize\n--no-trace
-wasm-revectorize\n--wasm-memory64-trap-handling\n--no-wasm-jitless\n--no-wasm-jitless-if-available-for-testing\n--no-wasm-allow-mixed-eh-for-testing\n--no-wasm-code-coverage\n--stress-sampling-allocation-pro
filer=0\n--no-lazy-new-space-shrinking\n--min-semi-space-size=0\n--max-semi-space-size=0\n--semi-space-growth-factor=2\n--max-old-space-size=0\n--max-heap-size=0\n--initial-heap-size=0\n--initial-old-space-si
ze=0\n--preconfigured-old-space-size=0\n--no-gc-global\n--random-gc-interval=0\n--gc-interval=-1\n--cppgc-random-gc-interval=0\n--retain-maps-for-n-gc=2\n--trace-gc\n--no-trace-gc-nvp\n--no-trace-gc-ignore-sc
avenger\n--no-trace-memory-reducer\n--trace-gc-verbose\n--no-trace-gc-freelists\n--no-trace-gc-freelists-verbose\n--no-trace-gc-heap-layout\n--trace-gc-heap-layout-ignore-minor-gc\n--no-trace-evacuation-candi
dates\n--no-trace-pending-allocations\n--trace-allocation-stack-interval=-1\n--trace-duplicate-threshold-kb=0\n--no-trace-fragmentation\n--no-trace-fragmentation-verbose\n--no-minor-ms-trace-fragmentation\n--
no-trace-evacuation\n--no-trace-mutator-utilization\n--incremental-marking\n--incremental-marking-task\n--incremental-marking-start-user-visible\n--no-incremental-marking-always-user-visible\n--incremental-ma
rking-soft-trigger=0\n--incremental-marking-hard-trigger=0\n--no-incremental-marking-unified-schedule\n--no-trace-unmapper\n--parallel-scavenge\n--minor-gc-task\n--minor-gc-task-trigger=80\n--minor-gc-task-wi
th-lower-priority\n--no-trace-parallel-scavenge\n--no-cppgc-young-generation\n--no-optimize-gc-for-battery\n--concurrent-marking\n--concurrent-marking-max-worker-num=7\n--concurrent-array-buffer-sweeping\n--n
o-stress-concurrent-allocation\n--parallel-marking\n--ephemeron-fixpoint-iterations=10\n--no-trace-concurrent-marking\n--concurrent-sweeping\n--parallel-compaction\n--parallel-pointer-update\n--parallel-weak-
ref-clearing\n--detect-ineffective-gcs-near-heap-limit\n--no-ineffective-gcs-forces-last-resort\n--ineffective-gc-size-threshold=0.8\n--ineffective-gc-mutator-utilization-threshold=0.4\n--no-trace-incremental
-marking\n--no-trace-stress-marking\n--no-trace-stress-scavenge\n--no-track-gc-object-stats\n--no-trace-gc-object-stats\n--no-trace-zone-stats\n--zone-stats-tolerance=1048576\n--no-trace-zone-type-stats\n--no
-trace-backing-store\n--gc-stats=0\n--track-detached-contexts\n--no-trace-detached-contexts\n--no-verify-heap\n--no-verify-heap-skip-remembered-set\n--verify-write-barriers\n--no-memory-reducer-respects-froze
n-state\n--move-object-start\n--memory-reducer\n--memory-reducer-favors-memory\n--memory-reducer-for-small-heaps\n--memory-reducer-gc-count=2\n--no-external-memory-accounted-in-global-limit\n--external-memory
-max-growing-factor=1.3\n--no-gc-speed-uses-counters\n--heap-growing-percent=0\n--v8-os-page-size=0\n--allocation-buffer-parking\n--compact\n--compact-code-space\n--no-compact-on-every-full-gc\n--compact-with
-stack\n--shortcut-strings-with-stack\n--no-stress-compaction\n--resize-large-object\n--no-stress-compaction-random\n--no-flush-baseline-code\n--flush-bytecode\n--bytecode-old-age=6\n--no-flush-code-based-on-
time\n--no-flush-code-based-on-tab-visibility\n--bytecode-old-time=30\n--no-stress-flush-code\n--no-trace-flush-code\n--use-marking-progress-bar\n--no-stress-per-context-marking-worklist\n--no-stress-incremen
tal-marking\n--memory-pool\n--memory-pool-share-memory-on-teardown\n--memory-pool-release-before-memory-pressure-gcs\n--no-memory-pool-release-on-malloc-failures\n--large-page-pool\n--max-large-page-pool-size
=32\n--large-page-pool-timeout=3\n--no-managed-zone-memory\n--no-fuzzer-gc-analysis\n--stress-marking=0\n--stress-scavenge=0\n--reclaim-unmodified-wrappers\n--parallel-reclaim-unmodified-wrappers\n--no-gc-exp
eriment-less-compaction\n--gc-memory-reducer-start-delay-ms=8000\n--no-concurrent-marking-high-priority-threads\n--no-disable-abortjs\n--no-randomize-all-allocations\n--no-manual-evacuation-candidates-selecti
on\n--no-clear-free-memory\n--idle-gc-on-context-disposal\n--no-trace-context-disposal\n--cppheap-incremental-marking\n--cppheap-concurrent-marking\n--no-memory-balancer\n--memory-balancer-c-value=3e-10\n--no
-trace-memory-balancer\n--late-heap-limit-check\n--reserve-contiguous-compressed-read-only-space\n--debug-code\n--no-trap-on-abort\n--no-slow-debug-code\n--no-code-comments\n--enable-sse3\n--enable-ssse3\n--e
nable-sse4-1\n--enable-sse4-2\n--enable-sahf\n--enable-avx\n--enable-avx2\n--enable-avx-vnni\n--enable-avx-vnni-int8\n--enable-fma3\n--enable-f16c\n--enable-bmi1\n--enable-bmi2\n--enable-lzcnt\n--enable-popcn
t\n--arm-arch=\"armv8\"\n--no-force-long-branches\n--mcpu=\"auto\"\n--partial-constant-pool\n--sim-arm64-optional-features=\"none\"\n--intel-jcc-erratum-mitigation\n--no-enable-source-at-csa-bind\n--enable-ar
mv7=unset\n--enable-vfp3=unset\n--enable-32dregs=unset\n--enable-neon=unset\n--enable-sudiv=unset\n--enable-armv8=unset\n--enable-regexp-unaligned-accesses\n--script-streaming\n--no-stress-background-compile\
n--concurrent-cache-deserialization\n--merge-background-deserialized-script-with-compilation-cache\n--no-verify-code-merge\n--no-experimental-embedder-instance-types\n--no-expose-gc\n--expose-gc-as=\"\"\n--no
-expose-externalize-string\n--no-expose-statistics\n--no-expose-trigger-failure\n--no-expose-ignition-statistics\n--stack-trace-limit=10\n--no-builtins-in-stack-traces\n--no-experimental-stack-trace-frames\n-
-no-disallow-code-generation-from-strings\n--no-expose-async-hooks\n--expose-cputracemark-as=\"\"\n--experimental-report-exceptions-from-callbacks\n--no-allow-unsafe-function-constructor\n--no-force-slow-path
\n--no-test-small-max-function-context-stub-size\n--inline-new\n--switch-table-spread-threshold=3\n--switch-table-min-cases=6\n--stress-lazy-compilation=0\n--no-trace\n--lazy\n--lazy-eval\n--lazy-streaming\n-
-no-max-lazy\n--trace-opt\n--trace-opt-status\n--trace-opt-verbose\n--no-trace-opt-stats\n--trace-deopt\n--no-log-deopt\n--no-trace-deopt-verbose\n--no-trace-file-names\n--no-always-osr\n--no-trace-serializer
\n--no-external-reference-stats\n--compilation-cache\n--cache-prototype-transitions\n--no-lazy-compile-dispatcher\n--lazy-compile-dispatcher-max-threads=0\n--no-trace-compiler-dispatcher\n--no-parallel-compil
e-tasks-for-eager-toplevel\n--no-parallel-compile-tasks-for-lazy\n--cpu-profiler-sampling-interval=1000\n--no-trace-side-effect-free-debug-evaluate\n--hard-abort\n--log-colour\n--no-expose-inspector-scripts\n
--inspector-live-edit\n--stack-size=984\n--max-stack-trace-source-length=300\n--no-clear-exceptions-on-js-entry\n--use-original-message-for-stack-trace\n--histogram-interval=600000\n--no-heap-profiler-trace-o
bjects\n--heap-profiler-use-embedder-graph\n--no-heap-snapshot-on-oom\n--heap-snapshot-on-gc=-1\n--heap-snapshot-string-limit=1024\n--no-heap-profiler-show-hidden-objects\n--no-profile-heap-snapshot\n--no-hea
p-snapshot-verify\n--no-sampling-heap-profiler-suppress-randomness\n--no-log-ic\n--no-fast-map-update\n--max-valid-polymorphic-map-count=4\n--clone-object-sidestep-transitions\n--fast-properties-soft-limit=12
\n--max-fast-properties=128\n--native-code-counters\n--super-ic\n--no-mega-dom-ic\n--no-trace-prototype-users\n--no-trace-for-in-enumerate\n--no-log-maps\n--log-maps-details\n--move-prototype-transitions-firs
t\n--allow-natives-syntax\n--no-allow-natives-for-differential-fuzzing\n--no-parse-only\n--no-enable-parser-ablation\n--no-enable-preparser-ablation\n--no-enable-bytecode-compiler-ablation\n--parser-ablation-
amount=0.8\n--preparser-ablation-amount=0.8\n--bytecode-compiler-ablation-amount=0.8\n--async-stack-traces\n--no-stack-trace-on-illegal\n--no-abort-on-uncaught-exception\n--no-correctness-fuzzer-suppressions\
n--no-rehash-snapshot\n--hash-seed=0\n--random-seed=0\n--fuzzer-random-seed=0\n--no-hashes-collide\n--no-trace-rail\n--no-print-all-exceptions\n--no-detailed-error-stack-trace\n--adjust-os-scheduling-paramete
rs\n--experimental-flush-embedded-blob-icache\n--allow-allocation-in-fast-api-call\n--short-builtin-calls\n--no-trace-code-range-allocation\n--better-code-range-allocation\n--no-abort-on-far-code-range\n--no-
runtime-call-stats\n--no-rcs\n--no-rcs-cpu-time\n--verify-snapshot-checksum\n--no-profile-deserialization\n--no-trace-deserialization\n--no-serialization-statistics\n--regexp-optimization\n--no-regexp-interpr
et-all\n--regexp-tier-up\n--regexp-tier-up-ticks=1\n--regexp-peephole-optimization\n--regexp-results-cache\n--no-regexp-assemble-from-bytecode\n--no-trace-regexp-peephole-optimization\n--no-trace-regexp-bytec
odes\n--no-trace-regexp-assembler\n--no-trace-regexp-parser\n--no-trace-regexp-tier-up\n--no-trace-regexp-graph\n--no-enable-experimental-regexp-engine\n--no-default-to-experimental-regexp-engine\n--no-experi
mental-regexp-engine-capture-group-opt\n--experimental-regexp-engine-capture-group-opt-max-memory-usage=1024\n--no-trace-experimental-regexp-engine\n--no-enable-experimental-regexp-engine-on-excessive-backtra
cks\n--regexp-backtracks-before-fallback=50000\n--regexp-simd\n--no-trace-read-only-promotion\n--no-trace-read-only-promotion-verbose\n--testing-bool-flag\n--testing-maybe-bool-flag=unset\n--testing-int-flag=
13\n--testing-float-flag=2.5\n--testing-string-flag=\"Hello, 
world!\"\n--testing-prng-seed=42\n--no-testing-d8-test-runner\n--no-strict-termination-checks\n--no-fuzzing\n--no-hole-fuzzing\n--no-sandbox-testing\n--no-sandbox-fuzzing\n--no-expose-memory-corruption-api\n-
-freeze-flags-after-init\n--no-cet-compatible\n--embedded-src=\"\"\n--embedded-variant=\"\"\n--startup-src=\"\"\n--startup-blob=\"\"\n--target-arch=\"\"\n--target-os=\"\"\n--no-target-is-simulator\n--turbo-pr
ofiling-input=\"\"\n--turbo-log-builtins-count-input=\"\"\n--text-is-readable\n--no-trace-minor-ms-parallel-marking\n--no-minor-ms\n--concurrent-minor-ms-marking\n--black-allocated-pages\n--no-sticky-mark-bit
s\n--minor-ms-min-new-space-capacity-for-concurrent-marking-mb=0\n--minor-ms-concurrent-marking-trigger=90\n--minor-ms-min-lab-size-kb=0\n--no-handle-weak-ref-weakly-in-minor-gc\n--no-help\n--print-flag-value
s\n--no-print-feature-flags-json\n--no-slow-histograms\n--no-use-external-strings\n--map-counters=\"\"\n--no-mock-arraybuffer-allocator\n--mock-arraybuffer-allocator-limit=0\n--no-multi-mapped-mock-allocator\
n--no-gdbjit\n--no-gdbjit-full\n--no-gdbjit-dump\n--gdbjit-dump-filter=\"\"\n--no-enable-slow-asserts\n--no-print-ast\n--print-scopes\n--no-trace-contexts\n--no-gc-verbose\n--no-code-stats\n--no-print-handles
\n--no-check-handle-count\n--no-print-global-handles\n--trace-turbo-escape\n--no-trace-module-status\n--no-trace-normalization\n--no-trace-lazy\n--no-trace-isolates\n--no-regexp-possessive-quantifier\n--no-pr
int-break-location\n--logfile=\"v8.log\"\n--no-logfile-per-isolate\n--no-log\n--no-log-all\n--no-log-internal-timer-events\n--no-log-timer-events\n--no-log-source-code\n--no-log-source-position\n--no-log-code
\n--no-log-feedback-vector\n--no-log-code-disassemble\n--no-log-function-events\n--no-detailed-line-info\n--no-prof-include-idle\n--no-perfetto-code-logger\n--prof-sampling-interval=1000\n--no-prof-cpp\n--pro
f-browser-mode\n--no-prof\n--no-ll-prof\n--no-perf-basic-prof\n--perf-basic-prof-path=\"/tmp\"\n--no-perf-basic-prof-only-functions\n--no-perf-prof\n--perf-prof-path=\".\"\n--no-perf-prof-annotate-wasm\n--no-
perf-prof-delete-file\n--no-perf-prof-unwinding-info\n--gc-fake-mmap=\"/tmp/__v8_gc__\"\n--no-redirect-code-traces\n--redirect-code-traces-to=\"code.asm\"\n--print-opt-source\n--no-vtune-prof-annotate-wasm\n-
-win64-unwinding-info\n--no-interpreted-frames-native-stack\n--no-enable-etw-stack-walking\n--no-etw-trace-debug\n--no-enable-etw-by-custom-filter-only\n--no-print-builtin-size\n--no-trace-elements-transition
s\n--no-trace-creation-allocation-sites\n--no-print-code\n--no-print-opt-code\n--print-opt-code-filter=\"*\"\n--no-print-code-verbose\n--no-print-builtin-code\n--print-builtin-code-filter=\"*\"\n--no-print-re
gexp-code\n--no-print-regexp-bytecode\n--no-predictable\n--no-predictable-and-random-seed-is-0\n--no-predictable-gc-schedule\n--no-single-threaded\n--no-single-threaded-gc\n--single-threaded-gc-in-background\
n--parallel-pause-for-gc-in-background\n--incremental-marking-for-gc-in-background\n--no-shared-heap\n--no-proto-assign-seq-opt\n--use-libm-trig-functions\n--no-verify-predictable\n--dump-allocations-digest-a
t-alloc=-1\nConcurrent maglev has been disabled for tracing.\n|2427045:0xe9c00138000] Shrinking page 0xe9c00004f80: end 0x38d600340000 -> 0x38d600309000\nGlobal scope:\nglobal { // (0xe9c00644030) (0, 27)\n  
// will be compiled\n  // NormalFunction\n  // 1 stack slots\n  // temporary vars:\n  TEMPORARY .result;  // (0xe9c00644338) local|0]\n  // local vars:\n  VAR fuzzilli;  // (0xe9c00644268) \n  // dynamic 
vars:\n  DYNAMIC_GLOBAL native;  // (0xe9c006443f8) never assigned\n}\n|generated bytecode for function:  (0x38d600827475 <SharedFunctionInfo>)]\nBytecode length: 25\nParameter count 1\nRegister count 
3\nFrame size 24\n         0x125000800074 @    0 : 13 00             LdaConstant |0]\n         0x125000800076 @    2 : d1                Star1\n         0x125000800077 @    3 : 1b fe f7          Mov 
<closure>, r2\n    0 E> 0x12500080007a @    6 : 6e 6b 01 f8 02    CallRuntime |DeclareGlobals], r1-r2\n    7 S> 0x12500080007f @   11 : 8b 01 00 00       CreateClosure |1], |0], #0\n         0x125000800083 @ 
15 : d1                Star1\n         0x125000800084 @   16 : 25 02 00          StaGlobal |2], |0]\n         0x125000800087 @   19 : 1b f8 f9          Mov r1, r0\n         0x12500080008a @   22 : 0b f9      
Ldar r0\n   27 S> 0x12500080008c @   24 : b7                Return\nConstant pool (size = 3)\nHandler Table (size = 0)\nSource Position Table (size = 8)\nInner function scope:\nfunction foo () { // 
(0xe9c00638220) (225, 411)\n  // NormalFunction\n  // 2 heap slots\n  // local vars:\n  LET caught;  // (0xe9c0063d248) never assigned\n\n  catch { // (0xe9c0063d958) (365, 392)\n    // 2 heap slots\n  }\n\n 
catch { // (0xe9c0063d3d8) (293, 319)\n    // 2 heap slots\n  }\n}\nGlobal scope:\nglobal { // (0xe9c00638030) (0, 537)\n  // will be compiled\n  // NormalFunction\n  // 1 stack slots\n  // temporary vars:\n 
TEMPORARY .result;  // (0xe9c00638710) local|0]\n  // local vars:\n  VAR foo;  // (0xe9c006383e0) \n  // dynamic vars:\n  DYNAMIC_GLOBAL print;  // (0xe9c00638810) never assigned\n\n  function foo () { // 
(0xe9c00638220) (225, 411)\n    // lazily parsed\n    // NormalFunction\n    // 2 heap slots\n  }\n}\n|generated bytecode for function:  (0x38d600827c9d <SharedFunctionInfo>)]\nBytecode length: 91\nParameter 
count 1\nRegister count 4\nFrame size 32\n         0x12500080010c @    0 : 13 00             LdaConstant |0]\n         0x12500080010e @    2 : d1                Star1\n         0x12500080010f @    3 : 1b fe 
f7          Mov <closure>, r2\n    0 E> 0x125000800112 @    6 : 6e 6b 01 f8 02    CallRuntime |DeclareGlobals], r1-r2\n  413 S> 0x125000800117 @   11 : 23 01 00          LdaGlobal |1], |0]\n         
0x12500080011a @   14 : d1                Star1\n         0x12500080011b @   15 : 6e 06 02 f8 01    CallRuntime |PrepareFunctionForOptimization], r1-r1\n  451 S> 0x125000800120 @   20 : 23 02 02          
LdaGlobal |2], |2]\n         0x125000800123 @   23 : d1                Star1\n         0x125000800124 @   24 : 0d 2a             LdaSmi |42]\n         0x125000800126 @   26 : d0                Star2\n        
0x125000800127 @   27 : 23 01 00          LdaGlobal |1], |0]\n         0x12500080012a @   30 : cf                Star3\n  461 E> 0x12500080012b @   31 : 6a f6 04          CallUndefinedReceiver0 r3, |4]\n     
0x12500080012e @   34 : cf                Star3\n  451 E> 0x12500080012f @   35 : 6c f8 f7 f6 06    CallUndefinedReceiver2 r1, r2, r3, |6]\n  469 S> 0x125000800134 @   40 : 23 02 02          LdaGlobal |2], 
|2]\n         0x125000800137 @   43 : d1                Star1\n         0x125000800138 @   44 : 0d 2a             LdaSmi |42]\n         0x12500080013a @   46 : d0                Star2\n         0x12500080013b
@   47 : 23 01 00          LdaGlobal |1], |0]\n         0x12500080013e @   50 : cf                Star3\n  479 E> 0x12500080013f @   51 : 6a f6 08          CallUndefinedReceiver0 r3, |8]\n         
0x125000800142 @   54 : cf                Star3\n  469 E> 0x125000800143 @   55 : 6c f8 f7 f6 0a    CallUndefinedReceiver2 r1, r2, r3, |10]\n  487 S> 0x125000800148 @   60 : 23 01 00          LdaGlobal |1], 
|0]\n         0x12500080014b @   63 : d1                Star1\n         0x12500080014c @   64 : 6e 03 02 f8 01    CallRuntime |OptimizeMaglevOnNextCall], r1-r1\n  519 S> 0x125000800151 @   69 : 23 02 02      
LdaGlobal |2], |2]\n         0x125000800154 @   72 : d1                Star1\n         0x125000800155 @   73 : 0d 2a             LdaSmi |42]\n         0x125000800157 @   75 : d0                Star2\n        
0x125000800158 @   76 : 23 01 00          LdaGlobal |1], |0]\n         0x12500080015b @   79 : cf                Star3\n  529 E> 0x12500080015c @   80 : 6a f6 0c          CallUndefinedReceiver0 r3, |12]\n    
0x12500080015f @   83 : cf                Star3\n  519 E> 0x125000800160 @   84 : 6c f8 f7 f6 0e    CallUndefinedReceiver2 r1, r2, r3, |14]\n         0x125000800165 @   89 : d2                Star0\n  537 S> 
0x125000800166 @   90 : b7                Return\nConstant pool (size = 3)\nHandler Table (size = 0)\nSource Position Table (size = 31)\nGlobal scope:\nfunction foo () { // (0xe9c00638220) (225, 411)\n  // 
will be compiled\n  // NormalFunction\n  // 1 stack slots\n  // local vars:\n  LET caught;  // (0xe9c00638470) local|0], hole initialization elided\n\n  catch { // (0xe9c00638e30) (365, 392)\n    // 3 heap 
slots\n    // local vars:\n    VAR e;  // (0xe9c006390c0) context|2], never assigned\n  }\n\n  catch { // (0xe9c00638720) (293, 319)\n    // 3 heap slots\n    // local vars:\n    VAR e;  // (0xe9c006389b0) 
context|2], never assigned\n  }\n}\n|generated bytecode for function: foo (0x38d600827cfd <SharedFunctionInfo foo>)]\nBytecode length: 83\nParameter count 1\nRegister count 5\nFrame size 40\n  245 S> 
0x1250008001f4 @    0 : 0c                LdaZero\n         0x1250008001f5 @    1 : d2                Star0\n         0x1250008001f6 @    2 : 1b ff f8          Mov <context>, r1\n  260 S> 0x1250008001f9 @    
5 : 86 00             CreateEmptyArrayLiteral |0]\n         0x1250008001fb @    7 : cf                Star3\n  263 E> 0x1250008001fc @    8 : 33 f6 00 01       GetNamedProperty r3, |0], |1]\n         
0x125000800200 @   12 : d0                Star2\n         0x125000800201 @   13 : 0e                LdaUndefined\n         0x125000800202 @   14 : ce                Star4\n  263 E> 0x125000800203 @   15 : 67 
f7 f6 f5 03    CallProperty1 r2, r3, r4, |3]\n         0x125000800208 @   20 : 96 15             Jump |21] (0x12500080021d @ 41)\n         0x12500080020a @   22 : d0                Star2\n         
0x12500080020b @   23 : 8d f7 01          CreateCatchContext r2, |1]\n         0x12500080020e @   26 : d1                Star1\n         0x12500080020f @   27 : 10                LdaTheHole\n         
0x125000800210 @   28 : b4                SetPendingMessage\n         0x125000800211 @   29 : 0b f8             Ldar r1\n         0x125000800213 @   31 : 1c f7             PushContext r2\n  303 S> 
0x125000800215 @   33 : 0b f9             Ldar r0\n         0x125000800217 @   35 : 4d 02 05          AddSmi |2], |5]\n         0x12500080021a @   38 : d2                Star0\n         0x12500080021b @   39 
: 1d f7             PopContext r2\n         0x12500080021d @   41 : 1b ff f8          Mov <context>, r1\n  332 S> 0x125000800220 @   44 : 86 06             CreateEmptyArrayLiteral |6]\n         0x125000800222
@   46 : cf                Star3\n  335 E> 0x125000800223 @   47 : 33 f6 00 07       GetNamedProperty r3, |0], |7]\n         0x125000800227 @   51 : d0                Star2\n         0x125000800228 @   52 : 
0e                LdaUndefined\n         0x125000800229 @   53 : ce                Star4\n  335 E> 0x12500080022a @   54 : 67 f7 f6 f5 09    CallProperty1 r2, r3, r4, |9]\n         0x12500080022f @   59 : 96 
15             Jump |21] (0x125000800244 @ 80)\n         0x125000800231 @   61 : d0                Star2\n         0x125000800232 @   62 : 8d f7 02          CreateCatchContext r2, |2]\n         0x125000800235
@   65 : d1                Star1\n         0x125000800236 @   66 : 10                LdaTheHole\n         0x125000800237 @   67 : b4                SetPendingMessage\n         0x125000800238 @   68 : 0b f8   
Ldar r1\n         0x12500080023a @   70 : 1c f7             PushContext r2\n  375 S> 0x12500080023c @   72 : 0b f9             Ldar r0\n         0x12500080023e @   74 : 4d 28 0b          AddSmi |40], |11]\n  
0x125000800241 @   77 : d2                Star0\n         0x125000800242 @   78 : 1d f7             PopContext r2\n  395 S> 0x125000800244 @   80 : 0b f9             Ldar r0\n  409 S> 0x125000800246 @   82 : 
b7                Return\nConstant pool (size = 3)\nHandler Table (size = 32)\nSource Position Table (size = 27)\n42 42\n42 42\n|manually marking 0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)> for 
optimization to MAGLEV, ConcurrencyMode::kSynchronous]\n|optimization status (^MAGLEV) foo INTERPRETED_FUNCTION]\n|optimization status (^MAGLEV) foo INTERPRETED_FUNCTION]\n|compiling method 0x38d600827d91 
<JSFunction foo (sfi = 0x38d600827cfd)> (target MAGLEV), mode: ConcurrencyMode::kSynchronous]\nCompiling 0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)> with Maglev\nParameter count 1\nRegister count 
5\nFrame size 40\n  245 S> 0x1250008001f4 @    0 : 0c                LdaZero\n         0x1250008001f5 @    1 : d2                Star0\n         0x1250008001f6 @    2 : 1b ff f8          Mov <context>, r1\n  
260 S> 0x1250008001f9 @    5 : 86 00             CreateEmptyArrayLiteral |0]\n         0x1250008001fb @    7 : cf                Star3\n  263 E> 0x1250008001fc @    8 : 33 f6 00 01       GetNamedProperty r3, 
|0], |1]\n         0x125000800200 @   12 : d0                Star2\n         0x125000800201 @   13 : 0e                LdaUndefined\n         0x125000800202 @   14 : ce                Star4\n  263 E> 
0x125000800203 @   15 : 67 f7 f6 f5 03    CallProperty1 r2, r3, r4, |3]\n         0x125000800208 @   20 : 96 15             Jump |21] (0x12500080021d @ 41)\n         0x12500080020a @   22 : d0                
Star2\n         0x12500080020b @   23 : 8d f7 01          CreateCatchContext r2, |1]\n         0x12500080020e @   26 : d1                Star1\n         0x12500080020f @   27 : 10                LdaTheHole\n 
0x125000800210 @   28 : b4                SetPendingMessage\n         0x125000800211 @   29 : 0b f8             Ldar r1\n         0x125000800213 @   31 : 1c f7             PushContext r2\n  303 S> 
0x125000800215 @   33 : 0b f9             Ldar r0\n         0x125000800217 @   35 : 4d 02 05          AddSmi |2], |5]\n         0x12500080021a @   38 : d2                Star0\n         0x12500080021b @   39 
: 1d f7             PopContext r2\n         0x12500080021d @   41 : 1b ff f8          Mov <context>, r1\n  332 S> 0x125000800220 @   44 : 86 06             CreateEmptyArrayLiteral |6]\n         0x125000800222
@   46 : cf                Star3\n  335 E> 0x125000800223 @   47 : 33 f6 00 07       GetNamedProperty r3, |0], |7]\n         0x125000800227 @   51 : d0                Star2\n         0x125000800228 @   52 : 
0e                LdaUndefined\n         0x125000800229 @   53 : ce                Star4\n  335 E> 0x12500080022a @   54 : 67 f7 f6 f5 09    CallProperty1 r2, r3, r4, |9]\n         0x12500080022f @   59 : 96 
15             Jump |21] (0x125000800244 @ 80)\n         0x125000800231 @   61 : d0                Star2\n         0x125000800232 @   62 : 8d f7 02          CreateCatchContext r2, |2]\n         0x125000800235
@   65 : d1                Star1\n         0x125000800236 @   66 : 10                LdaTheHole\n         0x125000800237 @   67 : b4                SetPendingMessage\n         0x125000800238 @   68 : 0b f8   
Ldar r1\n         0x12500080023a @   70 : 1c f7             PushContext r2\n  375 S> 0x12500080023c @   72 : 0b f9             Ldar r0\n         0x12500080023e @   74 : 4d 28 0b          AddSmi |40], |11]\n  
0x125000800241 @   77 : d2                Star0\n         0x125000800242 @   78 : 1d f7             PopContext r2\n  395 S> 0x125000800244 @   80 : 0b f9             Ldar r0\n  409 S> 0x125000800246 @   82 : 
b7                Return\nConstant pool (size = 3)\nHandler Table (size = 32)\nSource Position Table (size = 27)\n0x38d600827e69 <FeedbackVector|12]>\nAfter graph building\nGraph\n\n  \u001b|0m   \u001b|0m3: 
Constant(0x38d6008107ad <NativeContext|300]>), 34 uses\n  \u001b|0m  \u001b|0m96: Constant(0x38d600827d71 <FeedbackCell|one closure]>), 1 uses\n  \u001b|0m   \u001b|0m2: Constant(0x38d600827d91 <JSFunction 
foo (sfi = 0x38d600827cfd)>), 12 uses\n  \u001b|0m   \u001b|0m9: Constant(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>), 0 uses \ud83e\udea6\n  \u001b|0m  \u001b|0m17: Constant(0x38d600816f21 <JSFunction 
forEach (sfi = 0x38d6002dee25)>), 6 uses\n  \u001b|0m  \u001b|0m40: Constant(0x38d600827e2d <ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m  \u001b|0m38: Constant(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>), 0 
uses \ud83e\udea6\n  \u001b|0m  \u001b|0m81: Constant(0x38d600827e0d <ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m   \u001b|0m4: RootConstant(undefined_value), 28 uses\n  \u001b|0m  \u001b|0m49: 
RootConstant(the_hole_value), 2 uses\n  \u001b|0m  \u001b|0m10: RootConstant(empty_fixed_array), 4 uses\n  \u001b|0m   \u001b|0m8: RootConstant(one_pointer_filler_map), 0 uses \ud83e\udea6\n  \u001b|0m   
\u001b|0m7: SmiConstant(0), 12 uses\n  \u001b|0m  \u001b|0m53: SmiConstant(2), 1 uses\n  \u001b|0m  \u001b|0m41: SmiConstant(3), 2 uses\n  \u001b|0m  \u001b|0m51: Int32Constant(2), 0 uses \ud83e\udea6, cannot
truncate to int32\n  \u001b|0m  \u001b|0m39: Int32Constant(3), 0 uses \ud83e\udea6, cannot truncate to int32\n  \u001b|0m  \u001b|0m90: Int32Constant(40), 1 uses, cannot truncate to int32\n   \u001b|0mBlock 
b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  \u001b|0m\u001b|0;34m   0 : 
LdaZero\n\u001b|m  \u001b|0m   \u001b|0m1: InitialValue(<this>), 12 uses\n  \u001b|0m   \u001b|0m5: FunctionEntryStackCheck\n  \u001b|0m      \u21b3 lazy @-1 (2 live vars)\n  \u001b|0m   \u001b|0m6: Jump b1\n
\u001b|0m   \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  
\u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m  \u001b|0m11: AllocationBlock(Young), 1 uses\n  \u001b|0m  \u001b|0m12: InlinedAllocation(object 0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |n11], 16 uses (3 non escaping uses)\n  \u001b|0m  \u001b|0m13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |n12]\n  \u001b|0m  \u001b|0m14: 
StoreTaggedFieldNoWriteBarrier(0x4) |n12, n10]\n  \u001b|0m  \u001b|0m15: StoreTaggedFieldNoWriteBarrier(0x8) |n12, n10]\n  \u001b|0m  \u001b|0m16: StoreTaggedFieldNoWriteBarrier(0xc) |n12, n7]\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, 
r4, |3]\n\u001b|m  \u001b|0m  \u001b|0m18: LoadTaggedFieldForProperty(0xc, compressed) |n12], 6 uses\n  \u001b|0m  \u001b|0m19: ThrowIfNotCallable |n4]\n  \u001b|0m      \u2502      @15 (4 live vars)\n  
\u001b|0m      \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m      \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n  \u001b|0m  \u001b|0m20: UnsafeSmiUntag |n18], 2 uses, cannot 
truncate to int32\n  \u001b|0m  \u001b|0m21: Jump b2\n  \u001b|0m   \u2502  with gap moves:\n  \u001b|0m   \u2502    - n7 \u2192 22: \u03c6\u1d40\u2071 r0\n  \u001b|0m   \u2502    - n18 \u2192 23: 
\u03c6\u1d40 r1\n  \u001b|0m   \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b2\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m22: \u03c6\u1d40\u2071 r0 (n7, n35), 2 uses\n\u001b|0;31m\u2502 \u001b|0m  
\u001b|0m23: \u03c6\u1d40 r1 (n18, n31), 0 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m24: UnsafeSmiUntag |n22], 4 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u001b|0m25:
BranchIfInt32Compare(LessThan) |n24, n20] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  
\u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m26: 
Int32IncrementWithOverflow |n24], 3 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m27: LoadTaggedField(0x8, compressed) |n12], 1 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m28: LoadFixedArrayElement(compressed) |n27, n24], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m29: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n28, n22, n12], 0 uses, but required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m 
\u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  
\u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m30: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n12]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m31: LoadTaggedFieldForProperty(0xc, compressed) |n12], 2 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m32: UnsafeSmiUntag |n31], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n20, n32]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m35: UnsafeSmiTagInt32 |n26], 1 
uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u001b|0m34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m      with gap moves:\n \u001b|0;32m\u2502\u001b|0m        - n35 \u2192 22: \u03c6\u1d40\u2071 r0\n 
\u001b|0;32m\u2502\u001b|0m        - n31 \u2192 23: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u001b|0m36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m      with gap moves:\n\u001b|0;31m\u2502 \u001b|0m        - n7 \u2192 54: \u03c6\u1d40 
r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m37: \u03c6\u1d40\u2091 <accumulator>, 1 uses\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  23 : CreateCatchContext r2, |1]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m42: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m43: InlinedAllocation(object 
0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n42], 5 uses (5 non escaping uses)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m44: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) 
|n43]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m45: StoreTaggedFieldNoWriteBarrier(0x4) |n43, n41]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m46: StoreTaggedFieldWithWriteBarrier(0x8) |n43, 
n40]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m47: StoreTaggedFieldWithWriteBarrier(0xc) |n43, n3]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m48: StoreTaggedFieldWithWriteBarrier(0x10) |n43, 
n37]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m50: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov 
<context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m52: Jump b6\n\u001b|0;31m\u2502 \u001b|0m   \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m   \u2502    - n53 \u2192 54: \u03c6\u1d40 
r0\n\u001b|0;31m\u2502 \u001b|0m   \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m  \u001b|0m54: \u03c6\u1d40 r0 (n7, n53), 8 uses\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m  
\u001b|0m55: AllocationBlock(Young), 1 uses\n  \u001b|0m  \u001b|0m56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n55], 16 uses (3 non escaping uses)\n  \u001b|0m  \u001b|0m57: 
StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |n56]\n  \u001b|0m  \u001b|0m58: StoreTaggedFieldNoWriteBarrier(0x4) |n56, n10]\n  \u001b|0m  \u001b|0m59: 
StoreTaggedFieldNoWriteBarrier(0x8) |n56, n10]\n  \u001b|0m  \u001b|0m60: StoreTaggedFieldNoWriteBarrier(0xc) |n56, n7]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  \u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m  \u001b|0m61: LoadTaggedFieldForProperty(0xc, 
compressed) |n56], 6 uses\n  \u001b|0m  \u001b|0m62: ThrowIfNotCallable |n4]\n  \u001b|0m      \u2502      @54 (4 live vars)\n  \u001b|0m      \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m  
\u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m  \u001b|0m63: UnsafeSmiUntag |n61], 2 uses, cannot truncate to int32\n  \u001b|0m  \u001b|0m64: Jump b7\n  \u001b|0m   \u2502  with gap 
moves:\n  \u001b|0m   \u2502    - n7 \u2192 65: \u03c6\u1d40\u2071 r0\n  \u001b|0m   \u2502    - n61 \u2192 66: \u03c6\u1d40 r1\n  \u001b|0m   \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock 
b7\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m65: \u03c6\u1d40\u2071 r0 (n7, n78), 2 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m66: \u03c6\u1d40 r1 (n61, n74), 0 uses\n\u001b|0;31m\u2502 \u001b|0m  
\u001b|0m67: UnsafeSmiUntag |n65], 4 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u001b|0m68: BranchIfInt32Compare(LessThan) |n67, n63] b8 
b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1 eager @54 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m69: Int32IncrementWithOverflow |n67], 3 
uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m70: LoadTaggedField(0x8, compressed) |n56], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m71: 
LoadFixedArrayElement(compressed) |n70, n67], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m72: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n71, n65, n56], 0 uses, but 
required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1
eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m73: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n56]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m74: LoadTaggedFieldForProperty(0xc, compressed) |n56], 2 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m75: UnsafeSmiUntag |n74], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1 eager @54 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m76: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n63, n75]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m78: UnsafeSmiTagInt32 |n69], 1 
uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u001b|0m77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m      with gap moves:\n \u001b|0;32m\u2502\u001b|0m        - n78 \u2192 65: \u03c6\u1d40\u2071 r0\n 
\u001b|0;32m\u2502\u001b|0m        - n74 \u2192 66: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m\u001b|0;34m  59 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u001b|0m79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m      with gap moves:\n\u001b|0;31m\u2502 \u001b|0m        - n54 \u2192 95: \u03c6\u1d40 
r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m80: \u03c6\u1d40\u2091 <accumulator>, 2 uses\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  62 : CreateCatchContext r2, |2]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m82: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m83: InlinedAllocation(object 
0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n82], 6 uses (6 non escaping uses)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m84: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) 
|n83]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m85: StoreTaggedFieldNoWriteBarrier(0x4) |n83, n41]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m86: StoreTaggedFieldWithWriteBarrier(0x8) |n83, 
n81]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m87: StoreTaggedFieldWithWriteBarrier(0xc) |n83, n3]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m88: StoreTaggedFieldWithWriteBarrier(0x10) |n83, 
n80]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m89: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : 
AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m      \u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m91: CheckedSmiUntag |n54], 1 uses, cannot truncate to 
int32\n\u001b|0;31m\u2502 \u001b|0m      \u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m92: Int32AddWithOverflow |n91, n90], 1 uses, can truncate to int32 |-2147483648, 
2147483647]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m94: Int32ToNumber |n92], 1 uses\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m93: Jump b11\n\u001b|0;31m\u2502 
\u001b|0m   \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m   \u2502    - n94 \u2192 95: \u03c6\u1d40 r0\n\u001b|0;31m\u2502 \u001b|0m   \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  
\u001b|0m  \u001b|0m95: \u03c6\u1d40 r0 (n54, n94), 1 uses\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  \u001b|0m  \u001b|0m97: ReduceInterruptBudgetForReturn(82) |n96]\n  \u001b|0m  \u001b|0m98: 
Return |n95]\n  \u001b|0m\n\nAfter Phi untagging\nGraph\n\n  \u001b|0m    \u001b|0m3: Constant(0x38d6008107ad <NativeContext|300]>), 34 uses\n  \u001b|0m   \u001b|0m96: Constant(0x38d600827d71 
<FeedbackCell|one closure]>), 1 uses\n  \u001b|0m    \u001b|0m2: Constant(0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)>), 12 uses\n  \u001b|0m    \u001b|0m9: Constant(0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>), 0 uses \ud83e\udea6\n  \u001b|0m   \u001b|0m17: Constant(0x38d600816f21 <JSFunction forEach (sfi = 0x38d6002dee25)>), 6 uses\n  \u001b|0m   \u001b|0m40: 
Constant(0x38d600827e2d <ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m   \u001b|0m38: Constant(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>), 0 uses \ud83e\udea6\n  \u001b|0m   \u001b|0m81: Constant(0x38d600827e0d
<ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m    \u001b|0m4: RootConstant(undefined_value), 28 uses\n  \u001b|0m   \u001b|0m49: RootConstant(the_hole_value), 2 uses\n  \u001b|0m   \u001b|0m10: 
RootConstant(empty_fixed_array), 4 uses\n  \u001b|0m    \u001b|0m8: RootConstant(one_pointer_filler_map), 0 uses \ud83e\udea6\n  \u001b|0m    \u001b|0m7: SmiConstant(0), 11 uses\n  \u001b|0m   \u001b|0m53: 
SmiConstant(2), 0 uses \ud83e\udea6\n  \u001b|0m   \u001b|0m41: SmiConstant(3), 2 uses\n  \u001b|0m   \u001b|0m99: Int32Constant(0), 1 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m51: 
Int32Constant(2), 1 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m39: Int32Constant(3), 0 uses \ud83e\udea6, cannot truncate to int32\n  \u001b|0m   \u001b|0m90: Int32Constant(40), 1 uses, cannot 
truncate to int32\n   \u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  
\u001b|0m\u001b|0;34m   0 : LdaZero\n\u001b|m  \u001b|0m    \u001b|0m1: InitialValue(<this>), 12 uses\n  \u001b|0m    \u001b|0m5: FunctionEntryStackCheck\n  \u001b|0m       \u21b3 lazy @-1 (2 live vars)\n  
\u001b|0m    \u001b|0m6: Jump b1\n  \u001b|0m    \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m   \u001b|0m11: AllocationBlock(Young), 1 uses\n  \u001b|0m   
\u001b|0m12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n11], 16 uses (3 non escaping uses)\n  \u001b|0m   \u001b|0m13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |n12]\n  \u001b|0m   \u001b|0m14: StoreTaggedFieldNoWriteBarrier(0x4) |n12, n10]\n  \u001b|0m   \u001b|0m15: StoreTaggedFieldNoWriteBarrier(0x8) |n12, n10]\n  \u001b|0m   \u001b|0m16: 
StoreTaggedFieldNoWriteBarrier(0xc) |n12, n7]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  
\u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   \u001b|0m18: LoadTaggedFieldForProperty(0xc, compressed) |n12], 6 uses\n  \u001b|0m   \u001b|0m19: ThrowIfNotCallable |n4]\n  
\u001b|0m       \u2502      @15 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n  \u001b|0m   
\u001b|0m20: UnsafeSmiUntag |n18], 2 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m21: Jump b2\n  \u001b|0m    \u2502  with gap moves:\n  \u001b|0m    \u2502    - n7 \u2192 22: \u03c6\u1d40\u2071 
r0\n  \u001b|0m    \u2502    - n18 \u2192 23: \u03c6\u1d40 r1\n  \u001b|0m    \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m22: \u03c6\u1d40\u2071 r0 (n7, 
n35), 2 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m23: \u03c6\u1d40 r1 (n18, n31), 0 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m24: UnsafeSmiUntag |n22], 4 uses, cannot truncate to 
int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m25: BranchIfInt32Compare(LessThan) |n24, n20] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    
\u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502  
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m26: Int32IncrementWithOverflow |n24], 3 uses, cannot truncate to 
int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m27: LoadTaggedField(0x8, compressed) |n12], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28: 
LoadFixedArrayElement(compressed) |n27, n24], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n28, n22, n12], 0 uses, but 
required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1
eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n12]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31: LoadTaggedFieldForProperty(0xc, compressed) |n12], 2 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32: UnsafeSmiUntag |n31], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n20, n32]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35: UnsafeSmiTagInt32 |n26], 1 
uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m       with gap moves:\n \u001b|0;32m\u2502\u001b|0m         - n35 \u2192 22: \u03c6\u1d40\u2071 r0\n 
\u001b|0;32m\u2502\u001b|0m         - n31 \u2192 23: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - n99 \u2192 54: \u03c6\u1d35 
r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m37: \u03c6\u1d40\u2091 <accumulator>, 1 uses\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  23 : CreateCatchContext r2, |1]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m42: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m43: InlinedAllocation(object
0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n42], 5 uses (5 non escaping uses)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m44: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) 
|n43]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m45: StoreTaggedFieldNoWriteBarrier(0x4) |n43, n41]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m46: StoreTaggedFieldWithWriteBarrier(0x8) |n43, 
n40]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m47: StoreTaggedFieldWithWriteBarrier(0xc) |n43, n3]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m48: StoreTaggedFieldWithWriteBarrier(0x10) |n43, 
n37]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m50: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov 
<context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m52: Jump b6\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - n51 \u2192 54: \u03c6\u1d35 
r0\n\u001b|0;31m\u2502 \u001b|0m    \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m54: \u03c6\u1d35 r0 (n99, n51), 9 uses\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   
\u001b|0m55: AllocationBlock(Young), 1 uses\n  \u001b|0m   \u001b|0m56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n55], 16 uses (3 non escaping uses)\n  \u001b|0m   \u001b|0m57:
StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |n56]\n  \u001b|0m   \u001b|0m58: StoreTaggedFieldNoWriteBarrier(0x4) |n56, n10]\n  \u001b|0m   \u001b|0m59: 
StoreTaggedFieldNoWriteBarrier(0x8) |n56, n10]\n  \u001b|0m   \u001b|0m60: StoreTaggedFieldNoWriteBarrier(0xc) |n56, n7]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  \u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m61: LoadTaggedFieldForProperty(0xc, 
compressed) |n56], 6 uses\n  \u001b|0m   \u001b|0m62: ThrowIfNotCallable |n4]\n  \u001b|0m       \u2502      @54 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m63: UnsafeSmiUntag |n61], 2 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m64: Jump b7\n  \u001b|0m  
\u2502  with gap moves:\n  \u001b|0m    \u2502    - n7 \u2192 65: \u03c6\u1d40\u2071 r0\n  \u001b|0m    \u2502    - n61 \u2192 66: \u03c6\u1d40 r1\n  \u001b|0m    
\u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m65: \u03c6\u1d40\u2071 r0 (n7, n78), 2 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m66: \u03c6\u1d40 r1 (n61, 
n74), 0 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m67: UnsafeSmiUntag |n65], 4 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m68: 
BranchIfInt32Compare(LessThan) |n67, n63] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m 
\u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m69: 
Int32IncrementWithOverflow |n67], 3 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m70: LoadTaggedField(0x8, compressed) |n56], 1 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m71: LoadFixedArrayElement(compressed) |n70, n67], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m72: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n71, n65, n56], 0 uses, but required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m
\u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, 
r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m73: CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|n56]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m74: LoadTaggedFieldForProperty(0xc, compressed) |n56], 2 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m75: UnsafeSmiUntag 
|n74], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m76: CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n63, 
n75]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m78: UnsafeSmiTagInt32 |n69], 1 uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m       with 
gap moves:\n \u001b|0;32m\u2502\u001b|0m         - n78 \u2192 65: \u03c6\u1d40\u2071 r0\n \u001b|0;32m\u2502\u001b|0m         - n74 \u2192 66: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n 
\u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m  \u001b|0m100: Int32ToNumber |n54], 1 uses\n  \u001b|0m\u001b|0;34m  59 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m79: Jump 
b11\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - n100 \u2192 95: \u03c6\u1d40 r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception
handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m80: \u03c6\u1d40\u2091 <accumulator>, 2 uses\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  62 : CreateCatchContext r2, |2]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m 
\u001b|0m82: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m83: InlinedAllocation(object 0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n82], 6 uses (6 non escaping 
uses)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m84: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) |n83]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m85: StoreTaggedFieldNoWriteBarrier(0x4) 
|n83, n41]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m86: StoreTaggedFieldWithWriteBarrier(0x8) |n83, n81]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m87: StoreTaggedFieldWithWriteBarrier(0xc) |n83, 
n3]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m88: StoreTaggedFieldWithWriteBarrier(0x10) |n83, n80]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m89: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m       \u21b1 eager @74 (4 live 
vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m92: Int32AddWithOverflow |n54, n90], 1 uses, can truncate to int32 |-2147483648, 2147483647]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m94: Int32ToNumber |n92], 1 
uses\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m93: Jump b11\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - n94 
\u2192 95: \u03c6\u1d40 r0\n\u001b|0;31m\u2502 \u001b|0m    \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m95: \u03c6\u1d40 r0 (n100, n94), 1 uses\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  
\u001b|0m   \u001b|0m97: ReduceInterruptBudgetForReturn(82) |n96]\n  \u001b|0m   \u001b|0m98: Return |n95]\n  \u001b|0m\n\nAfter use marking\nGraph\n\n  \u001b|0m    \u001b|0m3: Constant(0x38d6008107ad 
<NativeContext|300]>) \u2192 (x)\n  \u001b|0m   \u001b|0m96: Constant(0x38d600827d71 <FeedbackCell|one closure]>) \u2192 (x)\n  \u001b|0m    \u001b|0m2: Constant(0x38d600827d91 <JSFunction foo (sfi = 
0x38d600827cfd)>) \u2192 (x)\n  \u001b|0m   \u001b|0m17: Constant(0x38d600816f21 <JSFunction forEach (sfi = 0x38d6002dee25)>) \u2192 (x)\n  \u001b|0m   \u001b|0m40: Constant(0x38d600827e2d <ScopeInfo 
CATCH_SCOPE>) \u2192 (x)\n  \u001b|0m   \u001b|0m81: Constant(0x38d600827e0d <ScopeInfo CATCH_SCOPE>) \u2192 (x)\n  \u001b|0m    \u001b|0m4: RootConstant(undefined_value) \u2192 (x)\n  \u001b|0m   
\u001b|0m49: RootConstant(the_hole_value) \u2192 (x)\n  \u001b|0m   \u001b|0m10: RootConstant(empty_fixed_array) \u2192 (x)\n  \u001b|0m    \u001b|0m7: SmiConstant(0) \u2192 (x)\n  \u001b|0m   \u001b|0m41: 
SmiConstant(3) \u2192 (x)\n  \u001b|0m   \u001b|0m99: Int32Constant(0) \u2192 (x)\n  \u001b|0m   \u001b|0m51: Int32Constant(2) \u2192 (x)\n  \u001b|0m   \u001b|0m90: Int32Constant(40) \u2192 (x)\n   
\u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  \u001b|0m\u001b|0;34m   0 : 
LdaZero\n\u001b|m  \u001b|0m    \u001b|0m1: InitialValue(<this>) \u2192 (x)\n  \u001b|0m    \u001b|0m5: FunctionEntryStackCheck\n  \u001b|0m       \u21b3 lazy @-1 (2 live vars)\n  \u001b|0m    \u001b|0m6: 
Jump b1\n  \u001b|0m    \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m   \u001b|0m11: AllocationBlock(Young) \u2192 (x)\n  \u001b|0m   
\u001b|0m12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n11:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |v0/n12:(x)]\n  \u001b|0m   \u001b|0m14: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n12:(x), v0/n10:(x)]\n  \u001b|0m   \u001b|0m15: StoreTaggedFieldNoWriteBarrier(0x8) |v0/n12:(x), 
v0/n10:(x)]\n  \u001b|0m   \u001b|0m16: StoreTaggedFieldNoWriteBarrier(0xc) |v0/n12:(x), v0/n7:(x)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   \u001b|0m18: LoadTaggedFieldForProperty(0xc, compressed) 
|v0/n12:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m19: ThrowIfNotCallable |v0/n4:(x)]\n  \u001b|0m       \u2502      @15 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n  \u001b|0m   \u001b|0m20: UnsafeSmiUntag |v0/n18:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m21: Jump b2\n  \u001b|0m    \u2502  with gap
moves:\n  \u001b|0m    \u2502    - v0/n7:(x) \u2192 22: \u03c6\u1d40\u2071 r0 (x)\n  \u001b|0m    \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m22: 
\u03c6\u1d40\u2071 r0 (n7, n35) (compressed) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m24: UnsafeSmiUntag |v0/n22:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m25: 
BranchIfInt32Compare(LessThan) |v0/n24:(x), v0/n20:(x)] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock 
b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m26: Int32IncrementWithOverflow |v0/n24:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   
\u001b|0m27: LoadTaggedField(0x8, compressed) |v0/n12:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28: LoadFixedArrayElement(compressed) |v0/n27:(x), v0/n24:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v0/n4:(x), v0/n3:(x), v0/n4:(x), v0/n28:(x), v0/n22:(x), v0/n12:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1
eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n12:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31: LoadTaggedFieldForProperty(0xc, compressed) |v0/n12:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32: UnsafeSmiUntag |v0/n31:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v0/n20:(x), v0/n32:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35: UnsafeSmiTagInt32 |v0/n26:(x)] \u2192 
(x)\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m       with gap moves:\n \u001b|0;32m\u2502\u001b|0m         - v0/n35:(x) \u2192 22: \u03c6\u1d40\u2071 r0 
(x)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m36: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - v0/n99:(x) \u2192 54: \u03c6\u1d35 r0 (x)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 
(exception handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m37: \u03c6\u1d40\u2091 <accumulator> (compressed) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  23 : CreateCatchContext r2, 
|1]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m42: AllocationBlock(Young) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m43: InlinedAllocation(object 0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) 
|v0/n42:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m44: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) |v0/n43:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m45: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n43:(x), v0/n41:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m46: StoreTaggedFieldWithWriteBarrier(0x8) |v0/n43:(x), v0/n40:(x)] 
\ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m47: StoreTaggedFieldWithWriteBarrier(0xc) |v0/n43:(x), v0/n3:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m48: 
StoreTaggedFieldWithWriteBarrier(0x10) |v0/n43:(x), v0/n37:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m50: 
SetPendingMessage |v0/n49:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov <context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m52: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - v0/n51:(x) \u2192 54: \u03c6\u1d35 r0 (x)\n\u001b|0;31m\u2502 \u001b|0m    
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m54: \u03c6\u1d35 r0 (n99, n51) \u2192 (x)\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   \u001b|0m55: AllocationBlock(Young) \u2192 (x)\n
\u001b|0m   \u001b|0m56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n55:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m57: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |v0/n56:(x)]\n  \u001b|0m   \u001b|0m58: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n56:(x), v0/n10:(x)]\n  \u001b|0m   \u001b|0m59: StoreTaggedFieldNoWriteBarrier(0x8) |v0/n56:(x), 
v0/n10:(x)]\n  \u001b|0m   \u001b|0m60: StoreTaggedFieldNoWriteBarrier(0xc) |v0/n56:(x), v0/n7:(x)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  \u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m61: LoadTaggedFieldForProperty(0xc, compressed) 
|v0/n56:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m62: ThrowIfNotCallable |v0/n4:(x)]\n  \u001b|0m       \u2502      @54 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m63: UnsafeSmiUntag |v0/n61:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m64: Jump b7\n  \u001b|0m    \u2502  with 
gap moves:\n  \u001b|0m    \u2502    - v0/n7:(x) \u2192 65: \u03c6\u1d40\u2071 r0 (x)\n  \u001b|0m    \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m65: 
\u03c6\u1d40\u2071 r0 (n7, n78) (compressed) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m67: UnsafeSmiUntag |v0/n65:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m68: 
BranchIfInt32Compare(LessThan) |v0/n67:(x), v0/n63:(x)] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock 
b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m69: Int32IncrementWithOverflow |v0/n67:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   
\u001b|0m70: LoadTaggedField(0x8, compressed) |v0/n56:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m71: LoadFixedArrayElement(compressed) |v0/n70:(x), v0/n67:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m72: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v0/n4:(x), v0/n3:(x), v0/n4:(x), v0/n71:(x), v0/n65:(x), v0/n56:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       
\u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m73: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n56:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m74: LoadTaggedFieldForProperty(0xc, compressed) |v0/n56:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m75: UnsafeSmiUntag |v0/n74:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m76: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v0/n63:(x), v0/n75:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m78: UnsafeSmiTagInt32 |v0/n69:(x)] \u2192 
(x)\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m       with gap moves:\n \u001b|0;32m\u2502\u001b|0m         - v0/n78:(x) \u2192 65: \u03c6\u1d40\u2071 r0 
(x)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m  \u001b|0m100: Int32ToNumber |v0/n54:(x)] \u2192 (x)\n  \u001b|0m\u001b|0;34m  59 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - v0/n100:(x) \u2192 95: \u03c6\u1d40 r0
(x)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m80: \u03c6\u1d40\u2091 <accumulator> (compressed) \u2192 
(x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  62 : CreateCatchContext r2, |2]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m82: AllocationBlock(Young) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m83: InlinedAllocation(object 0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |v0/n82:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m84: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, 
InlinedAllocation) |v0/n83:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m85: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n83:(x), v0/n41:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m86:
StoreTaggedFieldWithWriteBarrier(0x8) |v0/n83:(x), v0/n81:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m87: StoreTaggedFieldWithWriteBarrier(0xc) |v0/n83:(x), v0/n3:(x)] 
\ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m88: StoreTaggedFieldWithWriteBarrier(0x10) |v0/n83:(x), v0/n80:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : 
SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m89: SetPendingMessage |v0/n49:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> 
(0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m       
\u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m92: Int32AddWithOverflow |v0/n54:(x), v0/n90:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m94: Int32ToNumber |v0/n92:(x)] 
\u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m93: Jump b11\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - 
v0/n94:(x) \u2192 95: \u03c6\u1d40 r0 (x)\n\u001b|0;31m\u2502 \u001b|0m    \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m95: \u03c6\u1d40 r0 (n100, n94) (compressed) \u2192 
(x)\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : 
Return\n\u001b|m  \u001b|0m   \u001b|0m97: ReduceInterruptBudgetForReturn(82) |v0/n96:(x)]\n  \u001b|0m   \u001b|0m98: Return |v0/n95:(x)]\n  \u001b|0m\n\nAfter register allocation pre-processing\nGraph\n\n  
\u001b|0m     \u001b|0m1/3: Constant(0x38d6008107ad <NativeContext|300]>) \u2192 v-1, live range: |1-69]\n  \u001b|0m    \u001b|0m2/96: Constant(0x38d600827d71 <FeedbackCell|one closure]>) \u2192 v-1, live 
range: |2-73]\n  \u001b|0m     \u001b|0m3/2: Constant(0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)>) \u2192 v-1, live range: |3-69]\n  \u001b|0m    \u001b|0m4/17: Constant(0x38d600816f21 <JSFunction 
forEach (sfi = 0x38d6002dee25)>) \u2192 v-1, live range: |4-64]\n  \u001b|0m     \u001b|0m5/4: RootConstant(undefined_value) \u2192 v-1, live range: |5-64]\n  \u001b|0m    \u001b|0m6/49: 
RootConstant(the_hole_value) \u2192 v-1, live range: |6-68]\n  \u001b|0m    \u001b|0m7/10: RootConstant(empty_fixed_array) \u2192 v-1, live range: |7-46]\n  \u001b|0m     \u001b|0m8/7: SmiConstant(0) \u2192 
v-1, live range: |8-51]\n  \u001b|0m    \u001b|0m9/99: Int32Constant(0) \u2192 v-1, live range: |9-38]\n  \u001b|0m   \u001b|0m10/51: Int32Constant(2) \u2192 v-1, live range: |10-40]\n  \u001b|0m   
\u001b|0m11/90: Int32Constant(40) \u2192 v-1, live range: |11-69]\n   \u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  \u001b|0m\u001b|0;34m   0 : LdaZero\n\u001b|m  \u001b|0m    \u001b|0m12/1: InitialValue(<this>) \u2192 v-1(=-6S), live range: |12-69]\n  
\u001b|0m    \u001b|0m13/5: FunctionEntryStackCheck\n  \u001b|0m          \u21b3 lazy @-1 (2 live vars)\n  \u001b|0m    \u001b|0m14/6: Jump b1\n  \u001b|0m       \u2193\n   \u001b|0mBlock b1\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral
|0]\n\u001b|m  \u001b|0m   \u001b|0m15/11: AllocationBlock(Young) \u2192 v-1(R), live range: |15-16]\n  \u001b|0m   \u001b|0m16/12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|v15/n11:v-1(R)] \u2192 v-1(0), live range: |16-37]\n  \u001b|0m   \u001b|0m17/13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |v16/n12:v-1(=rdi)]\n  \u001b|0m   \u001b|0m18/14:
StoreTaggedFieldNoWriteBarrier(0x4) |v16/n12:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m19/15: StoreTaggedFieldNoWriteBarrier(0x8) |v16/n12:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m20/16: 
StoreTaggedFieldNoWriteBarrier(0xc) |v16/n12:v-1(R), v8/n7:v-1(R)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   \u001b|0m21/18: LoadTaggedFieldForProperty(0xc, compressed) 
|v16/n12:v-1(R)] \u2192 v-1(R), live range: |21-37]\n  \u001b|0m   \u001b|0m22/19: ThrowIfNotCallable |v5/n4:v-1(R)]\n  \u001b|0m          \u2502      @15 (4 live vars)\n  \u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m          \u21b3 throw (b5)\n  \u001b|0m   \u001b|0m23/20: UnsafeSmiUntag |v21/n18:v-1(R)] \u2192 v-1(0), live range: |23-37]\n  \u001b|0m   \u001b|0m24/21: 
Jump b2\n  \u001b|0m       \u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:v-1(*) \u2192 22: \u03c6\u1d40\u2071 r0 v-1(*)\n  \u001b|0m       \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock 
b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m25/22: \u03c6\u1d40\u2071 r0 (n7, n35) \u2192 v-1(*), live range: |25-31]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m26/24: UnsafeSmiUntag |v25/n22:v-1(R)] \u2192 
v-1(0), live range: |26-30]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m27/25: BranchIfInt32Compare(LessThan) |v26/n24:v-1(R), v23/n20:v-1(R)] b3 
b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28/26: Int32IncrementWithOverflow 
|v26/n24:v-1(R)] \u2192 v-1(0), live range: |28-36]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29/27: LoadTaggedField(0x8, decompressed) |v16/n12:v-1(R)] \u2192 v-1(R), live range: 
|29-30]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30/28: LoadFixedArrayElement(decompressed) |v29/n27:v-1(R), v26/n24:v-1(R)] \u2192 v-1(R), live range: 
|30-31]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31/29: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v5/n4:v-1(=rdi), v1/n3:v-1(=rsi), v5/n4:v-1(*), v30/n28:v-1(*), v25/n22:v-1(*), 
v16/n12:v-1(*)] \u2192 v-1(=rax)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw (b5)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32/30: CheckMaps(0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |v16/n12:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33/31: LoadTaggedFieldForProperty(0xc, compressed) |v16/n12:v-1(R)] \u2192 v-1(R), live range: 
|33-34]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m34/32: UnsafeSmiUntag |v33/n31:v-1(R)] \u2192 v-1(0), live range: |34-35]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager
@15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35/33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v23/n20:v-1(R), v34/n32:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m36/35: UnsafeSmiTagInt32 |v28/n26:v-1(R)] \u2192 
v-1(0), live range: |36-37]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m37/34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n \u001b|0;32m\u2502\u001b|0m            - 
v36/n35:v-1(*) \u2192 22: \u03c6\u1d40\u2071 r0 v-1(*)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m38/36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m          with gap moves:\n\u001b|0;31m\u2502 \u001b|0m            - v9/n99:v-1(*) \u2192 54: 
\u03c6\u1d35 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 
\u001b|0m   \u001b|0m39/50: SetPendingMessage |v6/n49:v-1(R)] \u2192 v-1(R)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov <context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m40/52: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v10/n51:v-1(*) \u2192 54: \u03c6\u1d35 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m       
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m41/54: \u03c6\u1d35 r0 (n99, n51) \u2192 v-1(*), live range: |41-69]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo 
foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   \u001b|0m42/55: 
AllocationBlock(Young) \u2192 v-1(R), live range: |42-43]\n  \u001b|0m   \u001b|0m43/56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v42/n55:v-1(R)] \u2192 v-1(0), live range: 
|43-64]\n  \u001b|0m   \u001b|0m44/57: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |v43/n56:v-1(=rdi)]\n  \u001b|0m   \u001b|0m45/58: StoreTaggedFieldNoWriteBarrier(0x4) 
|v43/n56:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m46/59: StoreTaggedFieldNoWriteBarrier(0x8) |v43/n56:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m47/60: StoreTaggedFieldNoWriteBarrier(0xc) 
|v43/n56:v-1(R), v8/n7:v-1(R)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  
\u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m48/61: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:v-1(R)] \u2192 v-1(R), live range: |48-64]\n  \u001b|0m   
\u001b|0m49/62: ThrowIfNotCallable |v5/n4:v-1(R)]\n  \u001b|0m          \u2502      @54 (4 live vars)\n  \u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m          \u21b3 
throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m50/63: UnsafeSmiUntag |v48/n61:v-1(R)] \u2192 v-1(0), live range: |50-64]\n  \u001b|0m   \u001b|0m51/64: Jump b7\n  \u001b|0m       
\u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:v-1(*) \u2192 65: \u03c6\u1d40\u2071 r0 v-1(*)\n  \u001b|0m       \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 
\u001b|0m   \u001b|0m52/65: \u03c6\u1d40\u2071 r0 (n7, n78) \u2192 v-1(*), live range: |52-58]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m53/67: UnsafeSmiUntag |v52/n65:v-1(R)] \u2192 v-1(0), live range: 
|53-57]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m54/68: BranchIfInt32Compare(LessThan) |v53/n67:v-1(R), v50/n63:v-1(R)] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       
\u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          
\u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m55/69: Int32IncrementWithOverflow |v53/n67:v-1(R)] \u2192 v-1(0), live range: 
|55-63]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m56/70: LoadTaggedField(0x8, decompressed) |v43/n56:v-1(R)] \u2192 v-1(R), live range: |56-57]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  
\u001b|0m57/71: LoadFixedArrayElement(decompressed) |v56/n70:v-1(R), v53/n67:v-1(R)] \u2192 v-1(R), live range: |57-58]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m58/72: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |v5/n4:v-1(=rdi), v1/n3:v-1(=rsi), v5/n4:v-1(*), v57/n71:v-1(*), v52/n65:v-1(*), v43/n56:v-1(*)] \u2192 v-1(=rax)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502   
@54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw @61 (b10) : 
{<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m59/73: CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|v43/n56:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m60/74: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:v-1(R)] \u2192 v-1(R), live range: 
|60-61]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m61/75: UnsafeSmiUntag |v60/n74:v-1(R)] \u2192 v-1(0), live range: |61-62]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager
@54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m62/76: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v50/n63:v-1(R), v61/n75:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m63/78: UnsafeSmiTagInt32 |v55/n69:v-1(R)] \u2192 
v-1(0), live range: |63-64]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m64/77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n \u001b|0;32m\u2502\u001b|0m            - 
v63/n78:v-1(*) \u2192 65: \u03c6\u1d40\u2071 r0 v-1(*)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m  \u001b|0m65/100: Int32ToNumber |v41/n54:v-1(R)] \u2192 v-1(R), 
live range: |65-66]\n  \u001b|0m\u001b|0;34m  59 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m66/79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m          with gap 
moves:\n\u001b|0;31m\u2502 \u001b|0m            - v65/n100:v-1(*) \u2192 95: \u03c6\u1d40 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception 
handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m67/80: \u03c6\u1d40\u2091 <accumulator> \u2192 v-1(*), live range: |67-69]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : 
SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m68/89: SetPendingMessage |v6/n49:v-1(R)] \u2192 v-1(R)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> 
(0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m          
\u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m69/92: Int32AddWithOverflow |v41/n54:v-1(R), v11/n90:v-1(*)] \u2192 v-1(0), live range: |69-70]\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m70/94: Int32ToNumber |v69/n92:v-1(R)] \u2192 v-1(R), live range: |70-71]\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m71/93: Jump b11\n\u001b|0;31m\u2502 
\u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v70/n94:v-1(*) \u2192 95: \u03c6\u1d40 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m       
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m72/95: \u03c6\u1d40 r0 (n100, n94) \u2192 v-1(*), live range: |72-74]\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  \u001b|0m   \u001b|0m73/97: 
ReduceInterruptBudgetForReturn(82) |v2/n96:v-1(R)]\n  \u001b|0m   \u001b|0m74/98: Return |v72/n95:v-1(=rax)]\n  \u001b|0m\n\nAfter register allocation\nGraph\n\n  \u001b|0m     \u001b|0m1/3: 
Constant(0x38d6008107ad <NativeContext|300]>) \u2192 v-1, live range: |1-69]\n  \u001b|0m    \u001b|0m2/96: Constant(0x38d600827d71 <FeedbackCell|one closure]>) \u2192 v-1, live range: |2-73]\n  \u001b|0m    
\u001b|0m3/2: Constant(0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)>) \u2192 v-1, live range: |3-69]\n  \u001b|0m    \u001b|0m4/17: Constant(0x38d600816f21 <JSFunction forEach (sfi = 
0x38d6002dee25)>) \u2192 v-1, live range: |4-64]\n  \u001b|0m     \u001b|0m5/4: RootConstant(undefined_value) \u2192 v-1, live range: |5-64]\n  \u001b|0m    \u001b|0m6/49: RootConstant(the_hole_value) \u2192 
v-1, live range: |6-68]\n  \u001b|0m    \u001b|0m7/10: RootConstant(empty_fixed_array) \u2192 v-1, live range: |7-46]\n  \u001b|0m     \u001b|0m8/7: SmiConstant(0) \u2192 v-1, live range: |8-51]\n  \u001b|0m 
\u001b|0m9/99: Int32Constant(0) \u2192 v-1, live range: |9-38]\n  \u001b|0m   \u001b|0m10/51: Int32Constant(2) \u2192 v-1, live range: |10-40]\n  \u001b|0m   \u001b|0m11/90: Int32Constant(40) \u2192 v-1, live
range: |11-69]\n   \u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  
\u001b|0m\u001b|0;34m   0 : LdaZero\n\u001b|m  \u001b|0m    \u001b|0m12/1: InitialValue(<this>) \u2192 |stack:-6|t], live range: |12-69]\n  \u001b|0m    \u001b|0m13/5: FunctionEntryStackCheck\n  \u001b|0m    
\u21b3 lazy @-1 (2 live vars)\n  \u001b|0m    \u001b|0m14/6: Jump b1\n  \u001b|0m       \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m   \u001b|0m15/11: AllocationBlock(Young) \u2192 
|rdi|R|t], live range: |15-16]\n  \u001b|0m   \u001b|0m16/12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v15/n11:|rdi|R|t]] \u2192 |rdi|R|t] (spilled: |stack:0|t]), live range: 
|16-37]\n  \u001b|0m   \u001b|0m17/13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |v16/n12:|rdi|R|t]]\n  \u001b|0m     \u001b|0m101: ConstantGapMove(n10 \u2192 |rax|R|t])\n  
\u001b|0m   \u001b|0m18/14: StoreTaggedFieldNoWriteBarrier(0x4) |v16/n12:|rdi|R|t], v7/n10:|rax|R|t]]\n  \u001b|0m   \u001b|0m19/15: StoreTaggedFieldNoWriteBarrier(0x8) |v16/n12:|rdi|R|t], v7/n10:|rax|R|t]]\n
\u001b|0m     \u001b|0m102: ConstantGapMove(n7 \u2192 |rcx|R|t])\n  \u001b|0m   \u001b|0m20/16: StoreTaggedFieldNoWriteBarrier(0xc) |v16/n12:|rdi|R|t], v8/n7:|rcx|R|t]]\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   
\u001b|0m21/18: LoadTaggedFieldForProperty(0xc, compressed) |v16/n12:|rdi|R|t]] \u2192 |rdx|R|t] (spilled: |stack:1|t]), live range: |21-37]\n  \u001b|0m     \u001b|0m103: ConstantGapMove(n4 \u2192 
|rbx|R|t])\n  \u001b|0m   \u001b|0m22/19: ThrowIfNotCallable |v5/n4:|rbx|R|t]]\n  \u001b|0m          \u2502      @15 (4 live vars)\n  \u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m          \u21b3 throw (b5)\n  \u001b|0m   \u001b|0m23/20: UnsafeSmiUntag |v21/n18:|rdx|R|t]] \u2192 |rdx|R|w32] (spilled: |stack:1|w32]), live range: |23-37]\n  \u001b|0m   \u001b|0m24/21: Jump b2\n
\u001b|0m       \u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:|rcx|R|t] \u2192 22: \u03c6\u1d40\u2071 r0 |rcx|R|t]\n  \u001b|0m       \u2502  with register merges:\n  \u001b|0m       \u2502    
- |rax|R|t] \u2192 rax\n  \u001b|0m       \u2502    - |rbx|R|t] \u2192 rbx\n  \u001b|0m       \u2502    - |rdi|R|t] \u2192 rdi\n  \u001b|0m       \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock 
b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m25/22: \u03c6\u1d40\u2071 r0 (n7, n35) (decompressed) \u2192 |rcx|R|t], live range: |25-31]\n\u001b|0;31m\u2502 \u001b|0m     \u001b|0m104: GapMove(|rcx|R|t] \u2192
|rsi|R|t])\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m26/24: UnsafeSmiUntag |v25/n22:|rcx|R|t]] \u2192 |rcx|R|w32], live range: |26-30]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m27/25: 
BranchIfInt32Compare(LessThan) |v26/n24:|rcx|R|w32], v23/n20:|rdx|R|w32]] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock 
b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m105: GapMove(|rcx|R|w32] \u2192 |r8|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28/26: Int32IncrementWithOverflow 
|v26/n24:|rcx|R|w32]] \u2192 |rcx|R|w32] (spilled: |stack:0|w32]), live range: |28-36]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29/27: LoadTaggedField(0x8, decompressed) |v16/n12:|rdi|R|t]] 
\u2192 |r9|R|t], live range: |29-30]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30/28: LoadFixedArrayElement(decompressed) |v29/n27:|r9|R|t], v26/n24:|r8|R|w32]] \u2192 |r11|R|t], live range: 
|30-31]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m106: GapMove(|rbx|R|t] \u2192 |rdi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m107: GapMove(|rsi|R|t] \u2192 
|r8|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m108: ConstantGapMove(n3 \u2192 |rsi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31/29: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |v5/n4:|rdi|R|t], v1/n3:|rsi|R|t], v5/n4:|rbx|R|t], v30/n28:|r11|R|t], v25/n22:|r8|R|t], v16/n12:|stack:0|t]] \u2192 |rax|R|t]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      
\u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw 
(b5)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m109: GapMove(|stack:0|t] \u2192 |rax|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32/30: CheckMaps(0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |v16/n12:|rax|R|t]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33/31: LoadTaggedFieldForProperty(0xc, compressed) |v16/n12:|rax|R|t]] \u2192 |rcx|R|t], live 
range: |33-34]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m34/32: UnsafeSmiUntag |v33/n31:|rcx|R|t]] \u2192 |rcx|R|w32], live range: |34-35]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     
\u001b|0m110: GapMove(|stack:1|w32] \u2192 |rdx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502   
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35/33: CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v23/n20:|rdx|R|w32], 
v34/n32:|rcx|R|w32]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m111: GapMove(|stack:0|w32] \u2192 |rcx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m36/35: 
UnsafeSmiTagInt32 |v28/n26:|rcx|R|w32]] \u2192 |rcx|R|t], live range: |36-37]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m37/34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n 
\u001b|0;32m\u2502\u001b|0m            - v36/n35:|rcx|R|t] \u2192 22: \u03c6\u1d40\u2071 r0 |rcx|R|t]\n \u001b|0;32m\u2502\u001b|0m          with register merges:\n \u001b|0;32m\u2502\u001b|0m            - 
|constant:v-1] \u2192 rax\n \u001b|0;32m\u2502\u001b|0m            - |constant:v-1] \u2192 rbx\n \u001b|0;32m\u2502\u001b|0m            - |rax|R|t] \u2192 rdi\n \u001b|0;32m\u2502\u001b|0m\n 
\u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m38/36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m          with gap 
moves:\n\u001b|0;31m\u2502 \u001b|0m            - v9/n99:|constant:v-1] \u2192 54: \u03c6\u1d35 r0 |rcx|R|w32]\n\u001b|0;31m\u2502 \u001b|0m          with register merges:\n\u001b|0;31m\u2502 \u001b|0m       
- |rax|R|t] \u2192 rax\n\u001b|0;31m\u2502 \u001b|0m            - |rbx|R|t] \u2192 rbx\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m112: ConstantGapMove(n49 \u2192 |rax|R|t])\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m39/50: SetPendingMessage 
|v6/n49:|rax|R|t]] \u2192 |rcx|R|t]\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov <context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m40/52: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v10/n51:|constant:v-1] \u2192 54: \u03c6\u1d35 r0 |rcx|R|w32]\n\u001b|0;31m\u2502 \u001b|0m      
\u2502  with register merges:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - |constant:v-1] \u2192 rax\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - |constant:v-1] \u2192 rbx\n\u001b|0;31m\u2502 \u001b|0m 
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m41/54: \u03c6\u1d35 r0 (n99, n51) \u2192 |rcx|R|w32] (spilled: |stack:2|w32]), live range: |41-69]\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : 
CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   \u001b|0m42/55: AllocationBlock(Young) \u2192 |rdi|R|t], live range: |42-43]\n  \u001b|0m   \u001b|0m43/56: InlinedAllocation(object 0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |v42/n55:|rdi|R|t]] \u2192 |rdi|R|t] (spilled: |stack:2|t]), live range: |43-64]\n  \u001b|0m   \u001b|0m44/57: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |v43/n56:|rdi|R|t]]\n  \u001b|0m   \u001b|0m45/58: StoreTaggedFieldNoWriteBarrier(0x4) |v43/n56:|rdi|R|t], v7/n10:|rax|R|t]]\n  \u001b|0m   \u001b|0m46/59: 
StoreTaggedFieldNoWriteBarrier(0x8) |v43/n56:|rdi|R|t], v7/n10:|rax|R|t]]\n  \u001b|0m     \u001b|0m113: ConstantGapMove(n7 \u2192 |rax|R|t])\n  \u001b|0m   \u001b|0m47/60: StoreTaggedFieldNoWriteBarrier(0xc)
|v43/n56:|rdi|R|t], v8/n7:|rax|R|t]]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  
\u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m48/61: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:|rdi|R|t]] \u2192 |rdx|R|t] (spilled: |stack:3|t]), live 
range: |48-64]\n  \u001b|0m   \u001b|0m49/62: ThrowIfNotCallable |v5/n4:|rbx|R|t]]\n  \u001b|0m          \u2502      @54 (4 live vars)\n  \u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m          \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m50/63: UnsafeSmiUntag |v48/n61:|rdx|R|t]] \u2192 |rdx|R|w32] 
(spilled: |stack:4|w32]), live range: |50-64]\n  \u001b|0m   \u001b|0m51/64: Jump b7\n  \u001b|0m       \u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:|rax|R|t] \u2192 65: \u03c6\u1d40\u2071 r0 
|rax|R|t]\n  \u001b|0m       \u2502  with register merges:\n  \u001b|0m       \u2502    - |rbx|R|t] \u2192 rbx\n  \u001b|0m       \u2502    - |rdi|R|t] \u2192 rdi\n  \u001b|0m       
\u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m52/65: \u03c6\u1d40\u2071 r0 (n7, n78) (decompressed) \u2192 |rax|R|t], live range: |52-58]\n\u001b|0;31m\u2502
\u001b|0m     \u001b|0m114: GapMove(|rax|R|t] \u2192 |rcx|R|t])\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m53/67: UnsafeSmiUntag |v52/n65:|rax|R|t]] \u2192 |rax|R|w32], live range: 
|53-57]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m54/68: BranchIfInt32Compare(LessThan) |v53/n67:|rax|R|w32], v50/n63:|rdx|R|w32]] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   
\u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m115: GapMove(|rax|R|w32] \u2192 
|rsi|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m55/69: Int32IncrementWithOverflow |v53/n67:|rax|R|w32]] \u2192 |rax|R|w32] (spilled: |stack:3|w32]), live 
range: |55-63]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m56/70: LoadTaggedField(0x8, decompressed) |v43/n56:|rdi|R|t]] \u2192 |r8|R|t], live range: 
|56-57]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m57/71: LoadFixedArrayElement(decompressed) |v56/n70:|r8|R|t], v53/n67:|rsi|R|w32]] \u2192 |r9|R|t], live range: 
|57-58]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m116: GapMove(|rbx|R|t] \u2192 |rdi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m117: ConstantGapMove(n3 \u2192 
|rsi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m58/72: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v5/n4:|rdi|R|t], v1/n3:|rsi|R|t], v5/n4:|rbx|R|t], v57/n71:|r9|R|t], v52/n65:|rcx|R|t], 
v43/n56:|stack:2|t]] \u2192 |rax|R|t]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     
\u001b|0m118: GapMove(|stack:2|t] \u2192 |rax|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m59/73: CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|v43/n56:|rax|R|t]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m60/74: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:|rax|R|t]] \u2192 |rcx|R|t], live range: 
|60-61]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m61/75: UnsafeSmiUntag |v60/n74:|rcx|R|t]] \u2192 |rcx|R|w32], live range: |61-62]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     
\u001b|0m119: GapMove(|stack:4|w32] \u2192 |rdx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502   
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m62/76: CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v50/n63:|rdx|R|w32], 
v61/n75:|rcx|R|w32]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m120: GapMove(|stack:3|w32] \u2192 |rcx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m63/78: 
UnsafeSmiTagInt32 |v55/n69:|rcx|R|w32]] \u2192 |rcx|R|t], live range: |63-64]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m64/77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n 
\u001b|0;32m\u2502\u001b|0m            - v63/n78:|rcx|R|t] \u2192 65: \u03c6\u1d40\u2071 r0 |rax|R|t]\n \u001b|0;32m\u2502\u001b|0m          with register merges:\n \u001b|0;32m\u2502\u001b|0m            - 
|constant:v-1] \u2192 rbx\n \u001b|0;32m\u2502\u001b|0m            - |rax|R|t] \u2192 rdi\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m     \u001b|0m121: 
GapMove(|stack:2|w32] \u2192 |rax|R|w32])\n  \u001b|0m  \u001b|0m65/100: Int32ToNumber |v41/n54:|rax|R|w32]] \u2192 |rcx|R|t], live range: |65-66]\n  \u001b|0m\u001b|0;34m  59 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m66/79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m          with gap moves:\n\u001b|0;31m\u2502 \u001b|0m            - v65/n100:|rcx|R|t] \u2192 95:
\u03c6\u1d40 r0 |rcx|R|t]\n\u001b|0;31m\u2502 \u001b|0m          with register merges:\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m  
\u001b|0m67/80: \u03c6\u1d40\u2091 <accumulator> (compressed) \u2192 |rax|R|t], live range: |67-69]\n\u001b|0;31m\u2502 \u001b|0m     \u001b|0m122: ConstantGapMove(n49 \u2192 |rcx|R|t])\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  67 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m68/89: SetPendingMessage |v6/n49:|rcx|R|t]] \u2192 |rdx|R|t]\n\u001b|0;31m\u2502 \u001b|0m     \u001b|0m123: 
GapMove(|stack:2|w32] \u2192 |rcx|R|w32])\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m          \u21b1 eager @74 (4 live 
vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m69/92: Int32AddWithOverflow |v41/n54:|rcx|R|w32], v11/n90:|constant:v-1]] \u2192 |rcx|R|w32], live range: |69-70]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m70/94:
Int32ToNumber |v69/n92:|rcx|R|w32]] \u2192 |rax|R|t], live range: |70-71]\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m71/93: Jump b11\n\u001b|0;31m\u2502 
\u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v70/n94:|rax|R|t] \u2192 95: \u03c6\u1d40 r0 |rcx|R|t]\n\u001b|0;31m\u2502 \u001b|0m       \u2502  with register 
merges:\n\u001b|0;31m\u2502 \u001b|0m       \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m72/95: \u03c6\u1d40 r0 (n100, n94) (decompressed) \u2192 |rcx|R|t], live range: 
|72-74]\n  \u001b|0m     \u001b|0m124: ConstantGapMove(n96 \u2192 |rax|R|t])\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  \u001b|0m   \u001b|0m73/97: ReduceInterruptBudgetForReturn(82) |v2/n96:|rax|R|t]]\n  
\u001b|0m     \u001b|0m125: GapMove(|rcx|R|t] \u2192 |rax|R|t])\n  \u001b|0m   \u001b|0m74/98: Return |v72/n95:|rax|R|t]]\n  \u001b|0m\n0x125000800795 <Code MAGLEV>|completed compiling 0x38d600827d91 
<JSFunction foo (sfi = 0x38d600827cfd)> (target MAGLEV) - took 0.001, 8.466, 0.041 ms]\n42 42\n"}
[Step 1: Duration 13.06 seconds| Input tokens: 1,404 | Output tokens: 341]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'regress-1455295'}                                                                                                                │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: maglev/regress/regress-1455295
[Step 2: Duration 14.93 seconds| Input tokens: 57,863 | Output tokens: 629]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'get_js_entry_data_by_name' with arguments: {'file_name': 'maglev/regress/regress-1455295'}                                                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: {"js": "// Copyright 2023 the V8 project authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n//\n// Flags: 
--allow-natives-syntax --maglev\n\nfunction foo() {\n  let caught = 0;\n  try {\n    |].forEach(undefined);\n  } catch (e) {\n    caught += 2;\n  }\n  try {\n    |].forEach(undefined);\n  } catch (e) {\n    
caught += 40;\n  }\n  return caught;\n}\n\n%PrepareFunctionForOptimization(foo);\nprint(42, foo());\nprint(42, foo());\n%OptimizeMaglevOnNextCall(foo);\nprint(42, foo());\n", "Fuzzilli": "v0 <- 
BeginPlainFunction -> \n    v1 <- LoadInteger '0'\n    v2 <- CreateNamedVariable 'caught', 'let', v1\n    BeginTry\n        v3 <- LoadUndefined\n        v4 <- CreateArray |]\n        v5 <- CallMethod v4, 
'forEach', |v3]\n    BeginCatch -> v6\n        v7 <- LoadInteger '2'\n        Update v2, '+', v7\n    EndTryCatch\n    BeginTry\n        v8 <- LoadUndefined\n        v9 <- CreateArray |]\n        v10 <- 
CallMethod v9, 'forEach', |v8]\n    BeginCatch -> v11\n        v12 <- LoadInteger '40'\n        Update v2, '+', v12\n    EndTryCatch\n    Return v2\nEndPlainFunction\nv13 <- Eval 
'%PrepareFunctionForOptimization(%@)', |v0]\nv14 <- LoadInteger '42'\nv15 <- CallFunction v0, |]\nv16 <- CreateNamedVariable 'print', 'none'\nv17 <- CallFunction v16, |v14, v15]\nv18 <- LoadInteger '42'\nv19 
<- CallFunction v0, |]\nv20 <- CallFunction v16, |v18, v19]\nv21 <- Eval '%OptimizeMaglevOnNextCall(%@)', |v0]\nv22 <- LoadInteger '42'\nv23 <- CallFunction v0, |]\nv24 <- CallFunction v16, |v22, 
v23]\n\n\nfunction foo() {\n    let caught = 0;\n    try {\n        (|]).forEach(undefined);\n    } catch(e6) {\n        caught += 2;\n    }\n    try {\n        (|]).forEach(undefined);\n    } catch(e11) {\n 
caught += 40;\n    }\n    return caught;\n}\nconst v13 = %PrepareFunctionForOptimization(foo);\nprint(42, foo());\nprint(42, foo());\nconst v21 = %OptimizeMaglevOnNextCall(foo);\nprint(42, foo());", 
"execution_data": 
"--no-experimental\n--abort-on-contradictory-flags\n--no-exit-on-contradictory-flags\n--no-allow-overwriting-for-next-flag\n--no-use-strict\n--no-trace-temporal\n--no-harmony\n--harmony-shipping\n--no-js-stag
ing\n--js-shipping\n--no-harmony-shadow-realm\n--no-harmony-struct\n--no-js-decorators\n--no-js-source-phase-imports\n--no-harmony-temporal\n--harmony-import-attributes\n--js-regexp-duplicate-named-groups\n--
js-regexp-modifiers\n--js-promise-try\n--js-atomics-pause\n--js-error-iserror\n--js-regexp-escape\n--js-explicit-resource-management\n--js-float16array\n--js-base-64\n--builtin-subclassing\n--no-enable-shared
arraybuffer-per-context\n--no-for-of-optimization\n--no-stress-snapshot\n--no-lite-mode\n--enable-allocation-folding\n--no-disable-write-barriers\n--no-enable-unconditional-write-barriers\n--no-single-generat
ion\n--no-conservative-stack-scanning\n--no-direct-handle\n--no-scavenger-conservative-object-pinning\n--no-stress-scavenger-conservative-object-pinning\n--no-stress-scavenger-conservative-object-pinning-rand
om\n--no-scavenger-precise-object-pinning\n--no-precise-object-pinning\n--scavenger-promote-quarantined-pages\n--no-local-off-stack-check\n--no-experimental-fuzzing\n--no-future\n--no-force-emit-interrupt-bud
get-checks\n--maglev\n--no-maglev-future\n--no-optimize-on-next-call-optimizes-to-maglev\n--no-stress-maglev\n--no-maglev-as-top-tier\n--maglev-inlining\n--maglev-loop-peeling\n--maglev-optimistic-peeled-loop
s\n--maglev-loop-peeling-max-size=400\n--maglev-loop-peeling-max-size-cumulative=900\n--maglev-deopt-data-on-background\n--maglev-build-code-on-background\n--maglev-destroy-on-background\n--no-maglev-inline-a
pi-calls\n--maglev-cons-string-elision\n--maglev-pretenure-store-values\n--maglev-poly-calls\n--maglev-truncation\n--no-maglev-licm\n--concurrent-maglev-max-threads=2\n--no-concurrent-maglev-high-priority-thr
eads\n--max-maglev-inline-depth=1\n--max-maglev-hard-inline-depth=10\n--max-maglev-inlined-bytecode-size=100\n--max-maglev-inlined-bytecode-size-cumulative=920\n--max-maglev-inlined-bytecode-size-small-total=
3000\n--max-maglev-inlined-bytecode-size-small=27\n--max-maglev-inlined-bytecode-size-small-with-heapnum-in-out=75\n--min-maglev-inlining-frequency=0.95\n--maglev-reuse-stack-slots\n--maglev-untagged-phis\n--
maglev-hoist-osr-value-phi-untagging\n--no-maglev-speculative-hoist-phi-untagging\n--maglev-cse\n--no-maglev-non-eager-inlining\n--no-turbolev-non-eager-inlining\n--maglev-filter=\"*\"\n--maglev-print-filter=
\"*\"\n--no-maglev-assert\n--maglev-assert-stack-size\n--no-maglev-break-on-entry\n--maglev-print-bytecode\n--maglev-print-feedback\n--maglev-print-inlined\n--print-maglev-code\n--no-trace-maglev-graph-buildi
ng\n--no-trace-maglev-loop-speeling\n--no-trace-maglev-inlining\n--no-trace-maglev-kna-processor\n--no-print-maglev-deopt-verbose\n--print-maglev-graph\n--print-maglev-graphs\n--no-trace-maglev-phi-untagging\
n--no-trace-maglev-regalloc\n--no-maglev-stats\n--no-maglev-stats-nvp\n--maglev-function-context-specialization\n--no-specialize-code-for-one-byte-seq-strings\n--additive-safe-int-feedback\n--enable-enumerate
d-keyed-access-bytecode\n--no-dict-property-const-tracking\n--script-context-cells\n--function-context-cells\n--function-context-cells-max-size=1\n--empty-context-extension-dep\n--json-stringify-fast-path\n--
cache-property-key-string-adds\n--smi-string-cache-size=8192\n--double-string-cache-size=4096\n--no-trace-number-string-cache\n--maglev-inline-date-accessors\n--turbofan-inline-date-accessors\n--inline-date-a
ccessors\n--extensible-ro-snapshot\n--max-opt=999\n--efficiency-mode=unset\n--battery-saver-mode=unset\n--memory-saver-mode=unset\n--no-efficiency-mode-disable-turbofan\n--efficiency-mode-delay-turbofan-multi
ply=3\n--wasm-trace-native=\"\"\n--no-jitless\n--no-disable-optimizing-compilers\n--memory-protection-keys\n--no-force-memory-protection-keys\n--no-assert-types\n--no-turboshaft-assert-types\n--no-verify-simp
lified-lowering\n--no-trace-compilation-dependencies\n--no-unmap-holes\n--no-assert-hole-checked-by-value\n--allocation-site-tracking\n--allocation-site-pretenuring\n--page-promotion\n--page-promotion-thresho
ld=70\n--minor-ms-page-promotion-threshold=50\n--minor-ms-page-promotion-max-lab-threshold=30\n--minor-ms-max-page-age=4\n--no-trace-page-promotions\n--trace-pretenuring\n--no-trace-pretenuring-statistics\n--
no-trace-resize-large-object\n--track-field-types\n--no-trace-block-coverage\n--trace-protector-invalidation\n--decommit-pooled-pages\n--zero-unused-memory\n--no-high-end-android\n--high-end-android-physical-
memory-threshold=4294967295\n--no-minor-ms-shortcut-strings\n--no-feedback-normalization\n--internalize-on-the-fly\n--unbox-double-arrays\n--string-slices\n--invocation-count-for-feedback-allocation=8\n--invo
cation-count-for-maglev=400\n--invocation-count-for-maglev-osr=100\n--osr-from-maglev\n--osr-from-maglev-interrupt-scale-factor=0.8\n--no-always-osr-from-maglev\n--invocation-count-for-turbofan=16000\n--invoc
ation-count-for-osr=800\n--osr-to-tierup=1\n--minimum-invocations-after-ic-update=500\n--minimum-invocations-before-optimization=2\n--no-jit-fuzzing\n--use-std-math-pow\n--use-ic\n--lazy-feedback-allocation\n
--no-stress-ic\n--ignition-elide-noneffectful-bytecodes\n--ignition-reo\n--ignition-filter-expression-positions\n--ignition-share-named-property-feedback\n--ignition-elide-redundant-tdz-checks\n--print-byteco
de\n--enable-lazy-source-positions\n--no-stress-lazy-source-positions\n--print-bytecode-filter=\"*\"\n--omit-default-ctors\n--no-trace-ignition-codegen\n--trace-ignition-dispatches-output-file=\"\"\n--no-trac
e-track-allocation-sites\n--trace-migration\n--trace-generalization\n--reuse-scope-infos\n--sparkplug\n--no-always-sparkplug\n--baseline-batch-compilation\n--concurrent-sparkplug\n--concurrent-sparkplug-max-t
hreads=1\n--no-concurrent-sparkplug-high-priority-threads\n--sparkplug-filter=\"*\"\n--no-sparkplug-needs-short-builtins\n--baseline-batch-compilation-threshold=4096\n--no-trace-baseline\n--no-trace-baseline-
batch-compilation\n--no-shared-string-table\n--no-always-use-string-forwarding-table\n--no-transition-strings-during-gc-with-stack\n--initial-shared-heap-size=0\n--max-shared-heap-size=0\n--no-concurrent-buil
tin-generation\n--concurrent-recompilation\n--no-trace-concurrent-recompilation\n--concurrent-recompilation-queue-length=8\n--concurrent-recompilation-delay=0\n--concurrent-recompilation-front-running\n--conc
urrent-turbofan-max-threads=4\n--no-stress-concurrent-inlining\n--maglev-overwrite-budget\n--maglev-overwrite-osr-budget\n--no-stress-concurrent-inlining-attach-code\n--max-serializer-nesting=25\n--no-trace-h
eap-broker-verbose\n--no-trace-heap-broker\n--stress-runs=0\n--deopt-every-n-times=0\n--no-print-deopt-stress\n--turbofan\n--opt\n--no-stress-turbo-late-spilling\n--turbo-wasm-address-reassociation\n--no-conc
urrent-turbo-tracing\n--no-optimize-maglev-optimizes-to-turbofan\n--turbo-filter=\"*\"\n--no-trace-turbo\n--trace-turbo-path=\"\"\n--trace-turbo-filter=\"*\"\n--no-trace-turbo-graph\n--no-trace-turbo-schedule
d\n--trace-turbo-file-prefix=\"turbo\"\n--trace-turbo-cfg-file=\"turbo.cfg\"\n--trace-turbo-types\n--no-trace-turbo-scheduler\n--no-trace-turbo-reduction\n--no-trace-turbo-bailouts\n--no-trace-turbo-trimming\
n--no-trace-turbo-jt\n--no-trace-turbo-ceq\n--no-trace-turbo-loop\n--no-trace-turbo-alloc\n--no-trace-all-uses\n--no-trace-representation\n--no-trace-turbo-stack-accesses\n--no-fuzzing-and-concurrent-recompil
ation\n--turbo-verify\n--turbo-verify-machine-graph=\"\"\n--no-verify-csa\n--no-trace-verify-csa\n--csa-trap-on-node=\"\"\n--builtins-effects-src=\"\"\n--fixed-array-bounds-checks\n--no-turbo-stats\n--no-turb
o-stats-nvp\n--no-turbo-stats-wasm\n--turbo-splitting\n--turbo-inlining\n--turbo-elide-frames\n--max-inlined-bytecode-size=460\n--max-inlined-bytecode-size-cumulative=920\n--max-turbolev-inlined-bytecode-size
-cumulative=1840\n--max-inlined-bytecode-size-absolute=4600\n--max-inlined-bytecode-size-small-total=30000\n--reserve-inline-budget-scale-factor=1.2\n--max-inlined-bytecode-size-small=27\n--max-inlined-byteco
de-size-small-with-heapnum-in-out=75\n--max-optimized-bytecode-size=61440\n--min-inlining-frequency=0.05\n--no-stress-inline\n--no-trace-turbo-inlining\n--turbo-inline-array-builtins\n--use-osr\n--maglev-osr\
n--concurrent-osr\n--maglev-allocation-folding=2\n--maglev-escape-analysis\n--no-trace-maglev-escape-analysis\n--no-maglev-object-tracking\n--trace-maglev-object-tracking\n--no-turbo-string-builder\n--trace-o
sr\n--log-or-trace-osr\n--analyze-environment-liveness\n--no-trace-environment-liveness\n--turbo-load-elimination\n--trace-turbo-load-elimination\n--no-turbo-profiling\n--no-turbo-profiling-verbose\n--turbo-p
rofiling-output=\"\"\n--no-reorder-builtins\n--no-abort-on-bad-builtin-profile-data\n--no-warn-about-builtin-profile-data\n--dump-builtins-hashes-to-file=\"\"\n--turbo-verify-allocation\n--turbo-move-optimiza
tion\n--turbo-jt\n--turbo-loop-peeling\n--turbo-loop-variable\n--turbo-loop-rotation\n--turbo-cf-optimization\n--turbo-escape\n--turbo-allocation-folding\n--no-turbo-instruction-scheduling\n--no-turbo-stress-
instruction-scheduling\n--turbo-store-elimination\n--trace-store-elimination\n--turbo-typer-hardening\n--turbo-rewrite-far-jumps\n--no-stress-gc-during-compilation\n--turbo-fast-api-calls\n--no-fast-api-allow
-float-in-sim\n--no-turbo-compress-frame-translations\n--turbo-inline-js-wasm-calls\n--no-turbo-optimize-inlined-js-wasm-wrappers\n--turbo-optimize-apply\n--turbo-optimize-math-minmax\n--turboshaft\n--no-turb
oshaft-enable-debug-features\n--turboshaft-wasm-load-elimination\n--no-turboshaft-wasm-in-js-inlining\n--no-turbolev-inline-js-wasm-wrappers\n--turboshaft-load-elimination\n--turboshaft-loop-unrolling\n--turb
oshaft-string-concat-escape-analysis\n--no-turboshaft-typed-optimizations\n--no-turbolev\n--no-print-turbolev-frontend\n--no-turbolev-future\n--typed-array-length-loading\n--no-deduplicate-heap-number-request
s\n--no-turboshaft-verify-load-elimination\n--turboshaft-opt-bisect-limit=18446744073709551615\n--turboshaft-opt-bisect-break=18446744073709551615\n--no-turboshaft-verify-reductions\n--turboshaft-trace-typing
\n--turboshaft-trace-reduction\n--no-turboshaft-trace-intermediate-reductions\n--turboshaft-trace-emitted\n--no-turboshaft-trace-unrolling\n--no-turboshaft-trace-peeling\n--no-turboshaft-trace-load-eliminatio
n\n--no-turboshaft-trace-if-else-to-switch\n--trace-turbolev-graph-building\n--profile-guided-optimization\n--profile-guided-optimization-for-empty-feedback-vector\n--invocation-count-for-early-optimization=3
0\n--invocation-count-for-maglev-with-delay=600\n--no-optimize-for-size\n--reopt-after-lazy-deopts\n--no-verify-get-js-builtin-state\n--wasm-generic-wrapper\n--wasm-num-compilation-tasks=128\n--no-trace-wasm-
native-heap\n--no-trace-wasm-offheap-memory\n--no-print-wasm-offheap-memory-size\n--no-trace-wasm-serialization\n--wasm-async-compilation\n--no-wasm-test-streaming\n--wasm-native-module-cache\n--wasm-max-mem-
pages=4294967295\n--wasm-max-table-size=10000000\n--wasm-max-committed-code-mb=4095\n--wasm-max-code-space-size-mb=1024\n--wasm-tier-up\n--wasm-dynamic-tiering\n--no-wasm-sync-tier-up\n--wasm-tiering-budget=1
3000000\n--wasm-wrapper-tiering-budget=1000\n--max-wasm-functions=1000000\n--wasm-caching-threshold=1000\n--wasm-caching-hard-threshold=1000000\n--wasm-caching-timeout-ms=2000\n--no-trace-wasm-compilation-tim
es\n--wasm-tier-up-filter=-1\n--wasm-eager-tier-up-function=-1\n--no-trace-wasm-decoder\n--no-trace-wasm-compiler\n--no-trace-wasm-streaming\n--no-trace-wasm-stack-switching\n--no-stress-wasm-stack-switching\
n--wasm-stack-switching-stack-size=984\n--liftoff\n--no-liftoff-only\n--no-enable-testing-opcode-in-wasm\n--no-trace-liftoff\n--no-trace-wasm-memory\n--no-trace-wasm-globals\n--wasm-tier-mask-for-testing=0\n-
-wasm-debug-mask-for-testing=0\n--no-experimental-wasm-pgo-to-file\n--no-experimental-wasm-pgo-from-file\n--validate-asm\n--no-suppress-asm-messages\n--no-trace-asm-time\n--no-trace-asm-scanner\n--no-trace-as
m-parser\n--no-stress-validate-asm\n--no-dump-wasm-module\n--dump-wasm-module-path=\"\"\n--no-wasm-fast-api\n--wasm-deopt\n--wasm-deopts-per-function-limit=10\n--no-experimental-wasm-type-reflection\n--no-exp
erimental-wasm-instruction-tracing\n--no-experimental-wasm-custom-descriptors\n--no-experimental-wasm-shared\n--no-experimental-wasm-fp16\n--no-experimental-wasm-memory-control\n--no-experimental-wasm-wasmfx\
n--no-experimental-wasm-rab-integration\n--no-experimental-wasm-compilation-hints\n--no-experimental-wasm-stringref\n--no-experimental-wasm-imported-strings-utf8\n--no-experimental-wasm-growable-stacks\n--exp
erimental-wasm-legacy-eh\n--experimental-wasm-branch-hinting\n--experimental-wasm-exnref\n--no-experimental-wasm-assume-ref-cast-succeeds\n--no-experimental-wasm-ref-cast-nop\n--no-experimental-wasm-skip-null
-checks\n--no-experimental-wasm-skip-bounds-checks\n--no-experimental-wasm-js-interop\n--no-wasm-staging\n--wasm-opt\n--wasm-bounds-checks\n--wasm-stack-checks\n--no-wasm-enforce-bounds-checks\n--wasm-math-in
trinsics\n--no-wasm-inlining\n--wasm-inlining-budget=5000\n--wasm-inlining-max-size=500\n--wasm-inlining-factor=3\n--wasm-inlining-min-budget=50\n--no-wasm-inlining-ignore-call-counts\n--no-trace-wasm-inlinin
g\n--no-trace-wasm-typer\n--no-wasm-inlining-call-indirect\n--wasm-bulkmem-inlining\n--wasm-loop-unrolling\n--wasm-loop-peeling\n--wasm-loop-peeling-max-size=1000\n--no-trace-wasm-loop-peeling\n--no-wasm-fuzz
er-gen-test\n--no-print-wasm-code\n--print-wasm-code-function-index=-1\n--no-print-wasm-stub-code\n--asm-wasm-lazy-compilation\n--wasm-lazy-compilation\n--no-trace-wasm-lazy-compilation\n--no-wasm-lazy-valida
tion\n--no-wasm-simd-ssse3-codegen\n--wasm-code-gc\n--no-trace-wasm-code-gc\n--no-stress-wasm-code-gc\n--wasm-max-initial-code-space-reservation=0\n--no-stress-wasm-memory-moving\n--flush-liftoff-code\n--no-s
tress-branch-hinting\n--wasm-max-module-size=1073741824\n--wasm-disassembly-max-mb=1000\n--wasm-capi-thread-pool-size=0\n--trace-wasm\n--no-trace-wasm-instances\n--no-experimental-wasm-revectorize\n--no-trace
-wasm-revectorize\n--wasm-memory64-trap-handling\n--no-wasm-jitless\n--no-wasm-jitless-if-available-for-testing\n--no-wasm-allow-mixed-eh-for-testing\n--no-wasm-code-coverage\n--stress-sampling-allocation-pro
filer=0\n--no-lazy-new-space-shrinking\n--min-semi-space-size=0\n--max-semi-space-size=0\n--semi-space-growth-factor=2\n--max-old-space-size=0\n--max-heap-size=0\n--initial-heap-size=0\n--initial-old-space-si
ze=0\n--preconfigured-old-space-size=0\n--no-gc-global\n--random-gc-interval=0\n--gc-interval=-1\n--cppgc-random-gc-interval=0\n--retain-maps-for-n-gc=2\n--trace-gc\n--no-trace-gc-nvp\n--no-trace-gc-ignore-sc
avenger\n--no-trace-memory-reducer\n--trace-gc-verbose\n--no-trace-gc-freelists\n--no-trace-gc-freelists-verbose\n--no-trace-gc-heap-layout\n--trace-gc-heap-layout-ignore-minor-gc\n--no-trace-evacuation-candi
dates\n--no-trace-pending-allocations\n--trace-allocation-stack-interval=-1\n--trace-duplicate-threshold-kb=0\n--no-trace-fragmentation\n--no-trace-fragmentation-verbose\n--no-minor-ms-trace-fragmentation\n--
no-trace-evacuation\n--no-trace-mutator-utilization\n--incremental-marking\n--incremental-marking-task\n--incremental-marking-start-user-visible\n--no-incremental-marking-always-user-visible\n--incremental-ma
rking-soft-trigger=0\n--incremental-marking-hard-trigger=0\n--no-incremental-marking-unified-schedule\n--no-trace-unmapper\n--parallel-scavenge\n--minor-gc-task\n--minor-gc-task-trigger=80\n--minor-gc-task-wi
th-lower-priority\n--no-trace-parallel-scavenge\n--no-cppgc-young-generation\n--no-optimize-gc-for-battery\n--concurrent-marking\n--concurrent-marking-max-worker-num=7\n--concurrent-array-buffer-sweeping\n--n
o-stress-concurrent-allocation\n--parallel-marking\n--ephemeron-fixpoint-iterations=10\n--no-trace-concurrent-marking\n--concurrent-sweeping\n--parallel-compaction\n--parallel-pointer-update\n--parallel-weak-
ref-clearing\n--detect-ineffective-gcs-near-heap-limit\n--no-ineffective-gcs-forces-last-resort\n--ineffective-gc-size-threshold=0.8\n--ineffective-gc-mutator-utilization-threshold=0.4\n--no-trace-incremental
-marking\n--no-trace-stress-marking\n--no-trace-stress-scavenge\n--no-track-gc-object-stats\n--no-trace-gc-object-stats\n--no-trace-zone-stats\n--zone-stats-tolerance=1048576\n--no-trace-zone-type-stats\n--no
-trace-backing-store\n--gc-stats=0\n--track-detached-contexts\n--no-trace-detached-contexts\n--no-verify-heap\n--no-verify-heap-skip-remembered-set\n--verify-write-barriers\n--no-memory-reducer-respects-froze
n-state\n--move-object-start\n--memory-reducer\n--memory-reducer-favors-memory\n--memory-reducer-for-small-heaps\n--memory-reducer-gc-count=2\n--no-external-memory-accounted-in-global-limit\n--external-memory
-max-growing-factor=1.3\n--no-gc-speed-uses-counters\n--heap-growing-percent=0\n--v8-os-page-size=0\n--allocation-buffer-parking\n--compact\n--compact-code-space\n--no-compact-on-every-full-gc\n--compact-with
-stack\n--shortcut-strings-with-stack\n--no-stress-compaction\n--resize-large-object\n--no-stress-compaction-random\n--no-flush-baseline-code\n--flush-bytecode\n--bytecode-old-age=6\n--no-flush-code-based-on-
time\n--no-flush-code-based-on-tab-visibility\n--bytecode-old-time=30\n--no-stress-flush-code\n--no-trace-flush-code\n--use-marking-progress-bar\n--no-stress-per-context-marking-worklist\n--no-stress-incremen
tal-marking\n--memory-pool\n--memory-pool-share-memory-on-teardown\n--memory-pool-release-before-memory-pressure-gcs\n--no-memory-pool-release-on-malloc-failures\n--large-page-pool\n--max-large-page-pool-size
=32\n--large-page-pool-timeout=3\n--no-managed-zone-memory\n--no-fuzzer-gc-analysis\n--stress-marking=0\n--stress-scavenge=0\n--reclaim-unmodified-wrappers\n--parallel-reclaim-unmodified-wrappers\n--no-gc-exp
eriment-less-compaction\n--gc-memory-reducer-start-delay-ms=8000\n--no-concurrent-marking-high-priority-threads\n--no-disable-abortjs\n--no-randomize-all-allocations\n--no-manual-evacuation-candidates-selecti
on\n--no-clear-free-memory\n--idle-gc-on-context-disposal\n--no-trace-context-disposal\n--cppheap-incremental-marking\n--cppheap-concurrent-marking\n--no-memory-balancer\n--memory-balancer-c-value=3e-10\n--no
-trace-memory-balancer\n--late-heap-limit-check\n--reserve-contiguous-compressed-read-only-space\n--debug-code\n--no-trap-on-abort\n--no-slow-debug-code\n--no-code-comments\n--enable-sse3\n--enable-ssse3\n--e
nable-sse4-1\n--enable-sse4-2\n--enable-sahf\n--enable-avx\n--enable-avx2\n--enable-avx-vnni\n--enable-avx-vnni-int8\n--enable-fma3\n--enable-f16c\n--enable-bmi1\n--enable-bmi2\n--enable-lzcnt\n--enable-popcn
t\n--arm-arch=\"armv8\"\n--no-force-long-branches\n--mcpu=\"auto\"\n--partial-constant-pool\n--sim-arm64-optional-features=\"none\"\n--intel-jcc-erratum-mitigation\n--no-enable-source-at-csa-bind\n--enable-ar
mv7=unset\n--enable-vfp3=unset\n--enable-32dregs=unset\n--enable-neon=unset\n--enable-sudiv=unset\n--enable-armv8=unset\n--enable-regexp-unaligned-accesses\n--script-streaming\n--no-stress-background-compile\
n--concurrent-cache-deserialization\n--merge-background-deserialized-script-with-compilation-cache\n--no-verify-code-merge\n--no-experimental-embedder-instance-types\n--no-expose-gc\n--expose-gc-as=\"\"\n--no
-expose-externalize-string\n--no-expose-statistics\n--no-expose-trigger-failure\n--no-expose-ignition-statistics\n--stack-trace-limit=10\n--no-builtins-in-stack-traces\n--no-experimental-stack-trace-frames\n-
-no-disallow-code-generation-from-strings\n--no-expose-async-hooks\n--expose-cputracemark-as=\"\"\n--experimental-report-exceptions-from-callbacks\n--no-allow-unsafe-function-constructor\n--no-force-slow-path
\n--no-test-small-max-function-context-stub-size\n--inline-new\n--switch-table-spread-threshold=3\n--switch-table-min-cases=6\n--stress-lazy-compilation=0\n--no-trace\n--lazy\n--lazy-eval\n--lazy-streaming\n-
-no-max-lazy\n--trace-opt\n--trace-opt-status\n--trace-opt-verbose\n--no-trace-opt-stats\n--trace-deopt\n--no-log-deopt\n--no-trace-deopt-verbose\n--no-trace-file-names\n--no-always-osr\n--no-trace-serializer
\n--no-external-reference-stats\n--compilation-cache\n--cache-prototype-transitions\n--no-lazy-compile-dispatcher\n--lazy-compile-dispatcher-max-threads=0\n--no-trace-compiler-dispatcher\n--no-parallel-compil
e-tasks-for-eager-toplevel\n--no-parallel-compile-tasks-for-lazy\n--cpu-profiler-sampling-interval=1000\n--no-trace-side-effect-free-debug-evaluate\n--hard-abort\n--log-colour\n--no-expose-inspector-scripts\n
--inspector-live-edit\n--stack-size=984\n--max-stack-trace-source-length=300\n--no-clear-exceptions-on-js-entry\n--use-original-message-for-stack-trace\n--histogram-interval=600000\n--no-heap-profiler-trace-o
bjects\n--heap-profiler-use-embedder-graph\n--no-heap-snapshot-on-oom\n--heap-snapshot-on-gc=-1\n--heap-snapshot-string-limit=1024\n--no-heap-profiler-show-hidden-objects\n--no-profile-heap-snapshot\n--no-hea
p-snapshot-verify\n--no-sampling-heap-profiler-suppress-randomness\n--no-log-ic\n--no-fast-map-update\n--max-valid-polymorphic-map-count=4\n--clone-object-sidestep-transitions\n--fast-properties-soft-limit=12
\n--max-fast-properties=128\n--native-code-counters\n--super-ic\n--no-mega-dom-ic\n--no-trace-prototype-users\n--no-trace-for-in-enumerate\n--no-log-maps\n--log-maps-details\n--move-prototype-transitions-firs
t\n--allow-natives-syntax\n--no-allow-natives-for-differential-fuzzing\n--no-parse-only\n--no-enable-parser-ablation\n--no-enable-preparser-ablation\n--no-enable-bytecode-compiler-ablation\n--parser-ablation-
amount=0.8\n--preparser-ablation-amount=0.8\n--bytecode-compiler-ablation-amount=0.8\n--async-stack-traces\n--no-stack-trace-on-illegal\n--no-abort-on-uncaught-exception\n--no-correctness-fuzzer-suppressions\
n--no-rehash-snapshot\n--hash-seed=0\n--random-seed=0\n--fuzzer-random-seed=0\n--no-hashes-collide\n--no-trace-rail\n--no-print-all-exceptions\n--no-detailed-error-stack-trace\n--adjust-os-scheduling-paramete
rs\n--experimental-flush-embedded-blob-icache\n--allow-allocation-in-fast-api-call\n--short-builtin-calls\n--no-trace-code-range-allocation\n--better-code-range-allocation\n--no-abort-on-far-code-range\n--no-
runtime-call-stats\n--no-rcs\n--no-rcs-cpu-time\n--verify-snapshot-checksum\n--no-profile-deserialization\n--no-trace-deserialization\n--no-serialization-statistics\n--regexp-optimization\n--no-regexp-interpr
et-all\n--regexp-tier-up\n--regexp-tier-up-ticks=1\n--regexp-peephole-optimization\n--regexp-results-cache\n--no-regexp-assemble-from-bytecode\n--no-trace-regexp-peephole-optimization\n--no-trace-regexp-bytec
odes\n--no-trace-regexp-assembler\n--no-trace-regexp-parser\n--no-trace-regexp-tier-up\n--no-trace-regexp-graph\n--no-enable-experimental-regexp-engine\n--no-default-to-experimental-regexp-engine\n--no-experi
mental-regexp-engine-capture-group-opt\n--experimental-regexp-engine-capture-group-opt-max-memory-usage=1024\n--no-trace-experimental-regexp-engine\n--no-enable-experimental-regexp-engine-on-excessive-backtra
cks\n--regexp-backtracks-before-fallback=50000\n--regexp-simd\n--no-trace-read-only-promotion\n--no-trace-read-only-promotion-verbose\n--testing-bool-flag\n--testing-maybe-bool-flag=unset\n--testing-int-flag=
13\n--testing-float-flag=2.5\n--testing-string-flag=\"Hello, 
world!\"\n--testing-prng-seed=42\n--no-testing-d8-test-runner\n--no-strict-termination-checks\n--no-fuzzing\n--no-hole-fuzzing\n--no-sandbox-testing\n--no-sandbox-fuzzing\n--no-expose-memory-corruption-api\n-
-freeze-flags-after-init\n--no-cet-compatible\n--embedded-src=\"\"\n--embedded-variant=\"\"\n--startup-src=\"\"\n--startup-blob=\"\"\n--target-arch=\"\"\n--target-os=\"\"\n--no-target-is-simulator\n--turbo-pr
ofiling-input=\"\"\n--turbo-log-builtins-count-input=\"\"\n--text-is-readable\n--no-trace-minor-ms-parallel-marking\n--no-minor-ms\n--concurrent-minor-ms-marking\n--black-allocated-pages\n--no-sticky-mark-bit
s\n--minor-ms-min-new-space-capacity-for-concurrent-marking-mb=0\n--minor-ms-concurrent-marking-trigger=90\n--minor-ms-min-lab-size-kb=0\n--no-handle-weak-ref-weakly-in-minor-gc\n--no-help\n--print-flag-value
s\n--no-print-feature-flags-json\n--no-slow-histograms\n--no-use-external-strings\n--map-counters=\"\"\n--no-mock-arraybuffer-allocator\n--mock-arraybuffer-allocator-limit=0\n--no-multi-mapped-mock-allocator\
n--no-gdbjit\n--no-gdbjit-full\n--no-gdbjit-dump\n--gdbjit-dump-filter=\"\"\n--no-enable-slow-asserts\n--no-print-ast\n--print-scopes\n--no-trace-contexts\n--no-gc-verbose\n--no-code-stats\n--no-print-handles
\n--no-check-handle-count\n--no-print-global-handles\n--trace-turbo-escape\n--no-trace-module-status\n--no-trace-normalization\n--no-trace-lazy\n--no-trace-isolates\n--no-regexp-possessive-quantifier\n--no-pr
int-break-location\n--logfile=\"v8.log\"\n--no-logfile-per-isolate\n--no-log\n--no-log-all\n--no-log-internal-timer-events\n--no-log-timer-events\n--no-log-source-code\n--no-log-source-position\n--no-log-code
\n--no-log-feedback-vector\n--no-log-code-disassemble\n--no-log-function-events\n--no-detailed-line-info\n--no-prof-include-idle\n--no-perfetto-code-logger\n--prof-sampling-interval=1000\n--no-prof-cpp\n--pro
f-browser-mode\n--no-prof\n--no-ll-prof\n--no-perf-basic-prof\n--perf-basic-prof-path=\"/tmp\"\n--no-perf-basic-prof-only-functions\n--no-perf-prof\n--perf-prof-path=\".\"\n--no-perf-prof-annotate-wasm\n--no-
perf-prof-delete-file\n--no-perf-prof-unwinding-info\n--gc-fake-mmap=\"/tmp/__v8_gc__\"\n--no-redirect-code-traces\n--redirect-code-traces-to=\"code.asm\"\n--print-opt-source\n--no-vtune-prof-annotate-wasm\n-
-win64-unwinding-info\n--no-interpreted-frames-native-stack\n--no-enable-etw-stack-walking\n--no-etw-trace-debug\n--no-enable-etw-by-custom-filter-only\n--no-print-builtin-size\n--no-trace-elements-transition
s\n--no-trace-creation-allocation-sites\n--no-print-code\n--no-print-opt-code\n--print-opt-code-filter=\"*\"\n--no-print-code-verbose\n--no-print-builtin-code\n--print-builtin-code-filter=\"*\"\n--no-print-re
gexp-code\n--no-print-regexp-bytecode\n--no-predictable\n--no-predictable-and-random-seed-is-0\n--no-predictable-gc-schedule\n--no-single-threaded\n--no-single-threaded-gc\n--single-threaded-gc-in-background\
n--parallel-pause-for-gc-in-background\n--incremental-marking-for-gc-in-background\n--no-shared-heap\n--no-proto-assign-seq-opt\n--use-libm-trig-functions\n--no-verify-predictable\n--dump-allocations-digest-a
t-alloc=-1\nConcurrent maglev has been disabled for tracing.\n|2427045:0xe9c00138000] Shrinking page 0xe9c00004f80: end 0x38d600340000 -> 0x38d600309000\nGlobal scope:\nglobal { // (0xe9c00644030) (0, 27)\n  
// will be compiled\n  // NormalFunction\n  // 1 stack slots\n  // temporary vars:\n  TEMPORARY .result;  // (0xe9c00644338) local|0]\n  // local vars:\n  VAR fuzzilli;  // (0xe9c00644268) \n  // dynamic 
vars:\n  DYNAMIC_GLOBAL native;  // (0xe9c006443f8) never assigned\n}\n|generated bytecode for function:  (0x38d600827475 <SharedFunctionInfo>)]\nBytecode length: 25\nParameter count 1\nRegister count 
3\nFrame size 24\n         0x125000800074 @    0 : 13 00             LdaConstant |0]\n         0x125000800076 @    2 : d1                Star1\n         0x125000800077 @    3 : 1b fe f7          Mov 
<closure>, r2\n    0 E> 0x12500080007a @    6 : 6e 6b 01 f8 02    CallRuntime |DeclareGlobals], r1-r2\n    7 S> 0x12500080007f @   11 : 8b 01 00 00       CreateClosure |1], |0], #0\n         0x125000800083 @ 
15 : d1                Star1\n         0x125000800084 @   16 : 25 02 00          StaGlobal |2], |0]\n         0x125000800087 @   19 : 1b f8 f9          Mov r1, r0\n         0x12500080008a @   22 : 0b f9      
Ldar r0\n   27 S> 0x12500080008c @   24 : b7                Return\nConstant pool (size = 3)\nHandler Table (size = 0)\nSource Position Table (size = 8)\nInner function scope:\nfunction foo () { // 
(0xe9c00638220) (225, 411)\n  // NormalFunction\n  // 2 heap slots\n  // local vars:\n  LET caught;  // (0xe9c0063d248) never assigned\n\n  catch { // (0xe9c0063d958) (365, 392)\n    // 2 heap slots\n  }\n\n 
catch { // (0xe9c0063d3d8) (293, 319)\n    // 2 heap slots\n  }\n}\nGlobal scope:\nglobal { // (0xe9c00638030) (0, 537)\n  // will be compiled\n  // NormalFunction\n  // 1 stack slots\n  // temporary vars:\n 
TEMPORARY .result;  // (0xe9c00638710) local|0]\n  // local vars:\n  VAR foo;  // (0xe9c006383e0) \n  // dynamic vars:\n  DYNAMIC_GLOBAL print;  // (0xe9c00638810) never assigned\n\n  function foo () { // 
(0xe9c00638220) (225, 411)\n    // lazily parsed\n    // NormalFunction\n    // 2 heap slots\n  }\n}\n|generated bytecode for function:  (0x38d600827c9d <SharedFunctionInfo>)]\nBytecode length: 91\nParameter 
count 1\nRegister count 4\nFrame size 32\n         0x12500080010c @    0 : 13 00             LdaConstant |0]\n         0x12500080010e @    2 : d1                Star1\n         0x12500080010f @    3 : 1b fe 
f7          Mov <closure>, r2\n    0 E> 0x125000800112 @    6 : 6e 6b 01 f8 02    CallRuntime |DeclareGlobals], r1-r2\n  413 S> 0x125000800117 @   11 : 23 01 00          LdaGlobal |1], |0]\n         
0x12500080011a @   14 : d1                Star1\n         0x12500080011b @   15 : 6e 06 02 f8 01    CallRuntime |PrepareFunctionForOptimization], r1-r1\n  451 S> 0x125000800120 @   20 : 23 02 02          
LdaGlobal |2], |2]\n         0x125000800123 @   23 : d1                Star1\n         0x125000800124 @   24 : 0d 2a             LdaSmi |42]\n         0x125000800126 @   26 : d0                Star2\n        
0x125000800127 @   27 : 23 01 00          LdaGlobal |1], |0]\n         0x12500080012a @   30 : cf                Star3\n  461 E> 0x12500080012b @   31 : 6a f6 04          CallUndefinedReceiver0 r3, |4]\n     
0x12500080012e @   34 : cf                Star3\n  451 E> 0x12500080012f @   35 : 6c f8 f7 f6 06    CallUndefinedReceiver2 r1, r2, r3, |6]\n  469 S> 0x125000800134 @   40 : 23 02 02          LdaGlobal |2], 
|2]\n         0x125000800137 @   43 : d1                Star1\n         0x125000800138 @   44 : 0d 2a             LdaSmi |42]\n         0x12500080013a @   46 : d0                Star2\n         0x12500080013b
@   47 : 23 01 00          LdaGlobal |1], |0]\n         0x12500080013e @   50 : cf                Star3\n  479 E> 0x12500080013f @   51 : 6a f6 08          CallUndefinedReceiver0 r3, |8]\n         
0x125000800142 @   54 : cf                Star3\n  469 E> 0x125000800143 @   55 : 6c f8 f7 f6 0a    CallUndefinedReceiver2 r1, r2, r3, |10]\n  487 S> 0x125000800148 @   60 : 23 01 00          LdaGlobal |1], 
|0]\n         0x12500080014b @   63 : d1                Star1\n         0x12500080014c @   64 : 6e 03 02 f8 01    CallRuntime |OptimizeMaglevOnNextCall], r1-r1\n  519 S> 0x125000800151 @   69 : 23 02 02      
LdaGlobal |2], |2]\n         0x125000800154 @   72 : d1                Star1\n         0x125000800155 @   73 : 0d 2a             LdaSmi |42]\n         0x125000800157 @   75 : d0                Star2\n        
0x125000800158 @   76 : 23 01 00          LdaGlobal |1], |0]\n         0x12500080015b @   79 : cf                Star3\n  529 E> 0x12500080015c @   80 : 6a f6 0c          CallUndefinedReceiver0 r3, |12]\n    
0x12500080015f @   83 : cf                Star3\n  519 E> 0x125000800160 @   84 : 6c f8 f7 f6 0e    CallUndefinedReceiver2 r1, r2, r3, |14]\n         0x125000800165 @   89 : d2                Star0\n  537 S> 
0x125000800166 @   90 : b7                Return\nConstant pool (size = 3)\nHandler Table (size = 0)\nSource Position Table (size = 31)\nGlobal scope:\nfunction foo () { // (0xe9c00638220) (225, 411)\n  // 
will be compiled\n  // NormalFunction\n  // 1 stack slots\n  // local vars:\n  LET caught;  // (0xe9c00638470) local|0], hole initialization elided\n\n  catch { // (0xe9c00638e30) (365, 392)\n    // 3 heap 
slots\n    // local vars:\n    VAR e;  // (0xe9c006390c0) context|2], never assigned\n  }\n\n  catch { // (0xe9c00638720) (293, 319)\n    // 3 heap slots\n    // local vars:\n    VAR e;  // (0xe9c006389b0) 
context|2], never assigned\n  }\n}\n|generated bytecode for function: foo (0x38d600827cfd <SharedFunctionInfo foo>)]\nBytecode length: 83\nParameter count 1\nRegister count 5\nFrame size 40\n  245 S> 
0x1250008001f4 @    0 : 0c                LdaZero\n         0x1250008001f5 @    1 : d2                Star0\n         0x1250008001f6 @    2 : 1b ff f8          Mov <context>, r1\n  260 S> 0x1250008001f9 @    
5 : 86 00             CreateEmptyArrayLiteral |0]\n         0x1250008001fb @    7 : cf                Star3\n  263 E> 0x1250008001fc @    8 : 33 f6 00 01       GetNamedProperty r3, |0], |1]\n         
0x125000800200 @   12 : d0                Star2\n         0x125000800201 @   13 : 0e                LdaUndefined\n         0x125000800202 @   14 : ce                Star4\n  263 E> 0x125000800203 @   15 : 67 
f7 f6 f5 03    CallProperty1 r2, r3, r4, |3]\n         0x125000800208 @   20 : 96 15             Jump |21] (0x12500080021d @ 41)\n         0x12500080020a @   22 : d0                Star2\n         
0x12500080020b @   23 : 8d f7 01          CreateCatchContext r2, |1]\n         0x12500080020e @   26 : d1                Star1\n         0x12500080020f @   27 : 10                LdaTheHole\n         
0x125000800210 @   28 : b4                SetPendingMessage\n         0x125000800211 @   29 : 0b f8             Ldar r1\n         0x125000800213 @   31 : 1c f7             PushContext r2\n  303 S> 
0x125000800215 @   33 : 0b f9             Ldar r0\n         0x125000800217 @   35 : 4d 02 05          AddSmi |2], |5]\n         0x12500080021a @   38 : d2                Star0\n         0x12500080021b @   39 
: 1d f7             PopContext r2\n         0x12500080021d @   41 : 1b ff f8          Mov <context>, r1\n  332 S> 0x125000800220 @   44 : 86 06             CreateEmptyArrayLiteral |6]\n         0x125000800222
@   46 : cf                Star3\n  335 E> 0x125000800223 @   47 : 33 f6 00 07       GetNamedProperty r3, |0], |7]\n         0x125000800227 @   51 : d0                Star2\n         0x125000800228 @   52 : 
0e                LdaUndefined\n         0x125000800229 @   53 : ce                Star4\n  335 E> 0x12500080022a @   54 : 67 f7 f6 f5 09    CallProperty1 r2, r3, r4, |9]\n         0x12500080022f @   59 : 96 
15             Jump |21] (0x125000800244 @ 80)\n         0x125000800231 @   61 : d0                Star2\n         0x125000800232 @   62 : 8d f7 02          CreateCatchContext r2, |2]\n         0x125000800235
@   65 : d1                Star1\n         0x125000800236 @   66 : 10                LdaTheHole\n         0x125000800237 @   67 : b4                SetPendingMessage\n         0x125000800238 @   68 : 0b f8   
Ldar r1\n         0x12500080023a @   70 : 1c f7             PushContext r2\n  375 S> 0x12500080023c @   72 : 0b f9             Ldar r0\n         0x12500080023e @   74 : 4d 28 0b          AddSmi |40], |11]\n  
0x125000800241 @   77 : d2                Star0\n         0x125000800242 @   78 : 1d f7             PopContext r2\n  395 S> 0x125000800244 @   80 : 0b f9             Ldar r0\n  409 S> 0x125000800246 @   82 : 
b7                Return\nConstant pool (size = 3)\nHandler Table (size = 32)\nSource Position Table (size = 27)\n42 42\n42 42\n|manually marking 0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)> for 
optimization to MAGLEV, ConcurrencyMode::kSynchronous]\n|optimization status (^MAGLEV) foo INTERPRETED_FUNCTION]\n|optimization status (^MAGLEV) foo INTERPRETED_FUNCTION]\n|compiling method 0x38d600827d91 
<JSFunction foo (sfi = 0x38d600827cfd)> (target MAGLEV), mode: ConcurrencyMode::kSynchronous]\nCompiling 0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)> with Maglev\nParameter count 1\nRegister count 
5\nFrame size 40\n  245 S> 0x1250008001f4 @    0 : 0c                LdaZero\n         0x1250008001f5 @    1 : d2                Star0\n         0x1250008001f6 @    2 : 1b ff f8          Mov <context>, r1\n  
260 S> 0x1250008001f9 @    5 : 86 00             CreateEmptyArrayLiteral |0]\n         0x1250008001fb @    7 : cf                Star3\n  263 E> 0x1250008001fc @    8 : 33 f6 00 01       GetNamedProperty r3, 
|0], |1]\n         0x125000800200 @   12 : d0                Star2\n         0x125000800201 @   13 : 0e                LdaUndefined\n         0x125000800202 @   14 : ce                Star4\n  263 E> 
0x125000800203 @   15 : 67 f7 f6 f5 03    CallProperty1 r2, r3, r4, |3]\n         0x125000800208 @   20 : 96 15             Jump |21] (0x12500080021d @ 41)\n         0x12500080020a @   22 : d0                
Star2\n         0x12500080020b @   23 : 8d f7 01          CreateCatchContext r2, |1]\n         0x12500080020e @   26 : d1                Star1\n         0x12500080020f @   27 : 10                LdaTheHole\n 
0x125000800210 @   28 : b4                SetPendingMessage\n         0x125000800211 @   29 : 0b f8             Ldar r1\n         0x125000800213 @   31 : 1c f7             PushContext r2\n  303 S> 
0x125000800215 @   33 : 0b f9             Ldar r0\n         0x125000800217 @   35 : 4d 02 05          AddSmi |2], |5]\n         0x12500080021a @   38 : d2                Star0\n         0x12500080021b @   39 
: 1d f7             PopContext r2\n         0x12500080021d @   41 : 1b ff f8          Mov <context>, r1\n  332 S> 0x125000800220 @   44 : 86 06             CreateEmptyArrayLiteral |6]\n         0x125000800222
@   46 : cf                Star3\n  335 E> 0x125000800223 @   47 : 33 f6 00 07       GetNamedProperty r3, |0], |7]\n         0x125000800227 @   51 : d0                Star2\n         0x125000800228 @   52 : 
0e                LdaUndefined\n         0x125000800229 @   53 : ce                Star4\n  335 E> 0x12500080022a @   54 : 67 f7 f6 f5 09    CallProperty1 r2, r3, r4, |9]\n         0x12500080022f @   59 : 96 
15             Jump |21] (0x125000800244 @ 80)\n         0x125000800231 @   61 : d0                Star2\n         0x125000800232 @   62 : 8d f7 02          CreateCatchContext r2, |2]\n         0x125000800235
@   65 : d1                Star1\n         0x125000800236 @   66 : 10                LdaTheHole\n         0x125000800237 @   67 : b4                SetPendingMessage\n         0x125000800238 @   68 : 0b f8   
Ldar r1\n         0x12500080023a @   70 : 1c f7             PushContext r2\n  375 S> 0x12500080023c @   72 : 0b f9             Ldar r0\n         0x12500080023e @   74 : 4d 28 0b          AddSmi |40], |11]\n  
0x125000800241 @   77 : d2                Star0\n         0x125000800242 @   78 : 1d f7             PopContext r2\n  395 S> 0x125000800244 @   80 : 0b f9             Ldar r0\n  409 S> 0x125000800246 @   82 : 
b7                Return\nConstant pool (size = 3)\nHandler Table (size = 32)\nSource Position Table (size = 27)\n0x38d600827e69 <FeedbackVector|12]>\nAfter graph building\nGraph\n\n  \u001b|0m   \u001b|0m3: 
Constant(0x38d6008107ad <NativeContext|300]>), 34 uses\n  \u001b|0m  \u001b|0m96: Constant(0x38d600827d71 <FeedbackCell|one closure]>), 1 uses\n  \u001b|0m   \u001b|0m2: Constant(0x38d600827d91 <JSFunction 
foo (sfi = 0x38d600827cfd)>), 12 uses\n  \u001b|0m   \u001b|0m9: Constant(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>), 0 uses \ud83e\udea6\n  \u001b|0m  \u001b|0m17: Constant(0x38d600816f21 <JSFunction 
forEach (sfi = 0x38d6002dee25)>), 6 uses\n  \u001b|0m  \u001b|0m40: Constant(0x38d600827e2d <ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m  \u001b|0m38: Constant(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>), 0 
uses \ud83e\udea6\n  \u001b|0m  \u001b|0m81: Constant(0x38d600827e0d <ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m   \u001b|0m4: RootConstant(undefined_value), 28 uses\n  \u001b|0m  \u001b|0m49: 
RootConstant(the_hole_value), 2 uses\n  \u001b|0m  \u001b|0m10: RootConstant(empty_fixed_array), 4 uses\n  \u001b|0m   \u001b|0m8: RootConstant(one_pointer_filler_map), 0 uses \ud83e\udea6\n  \u001b|0m   
\u001b|0m7: SmiConstant(0), 12 uses\n  \u001b|0m  \u001b|0m53: SmiConstant(2), 1 uses\n  \u001b|0m  \u001b|0m41: SmiConstant(3), 2 uses\n  \u001b|0m  \u001b|0m51: Int32Constant(2), 0 uses \ud83e\udea6, cannot
truncate to int32\n  \u001b|0m  \u001b|0m39: Int32Constant(3), 0 uses \ud83e\udea6, cannot truncate to int32\n  \u001b|0m  \u001b|0m90: Int32Constant(40), 1 uses, cannot truncate to int32\n   \u001b|0mBlock 
b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  \u001b|0m\u001b|0;34m   0 : 
LdaZero\n\u001b|m  \u001b|0m   \u001b|0m1: InitialValue(<this>), 12 uses\n  \u001b|0m   \u001b|0m5: FunctionEntryStackCheck\n  \u001b|0m      \u21b3 lazy @-1 (2 live vars)\n  \u001b|0m   \u001b|0m6: Jump b1\n
\u001b|0m   \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  
\u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m  \u001b|0m11: AllocationBlock(Young), 1 uses\n  \u001b|0m  \u001b|0m12: InlinedAllocation(object 0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |n11], 16 uses (3 non escaping uses)\n  \u001b|0m  \u001b|0m13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |n12]\n  \u001b|0m  \u001b|0m14: 
StoreTaggedFieldNoWriteBarrier(0x4) |n12, n10]\n  \u001b|0m  \u001b|0m15: StoreTaggedFieldNoWriteBarrier(0x8) |n12, n10]\n  \u001b|0m  \u001b|0m16: StoreTaggedFieldNoWriteBarrier(0xc) |n12, n7]\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, 
r4, |3]\n\u001b|m  \u001b|0m  \u001b|0m18: LoadTaggedFieldForProperty(0xc, compressed) |n12], 6 uses\n  \u001b|0m  \u001b|0m19: ThrowIfNotCallable |n4]\n  \u001b|0m      \u2502      @15 (4 live vars)\n  
\u001b|0m      \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m      \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n  \u001b|0m  \u001b|0m20: UnsafeSmiUntag |n18], 2 uses, cannot 
truncate to int32\n  \u001b|0m  \u001b|0m21: Jump b2\n  \u001b|0m   \u2502  with gap moves:\n  \u001b|0m   \u2502    - n7 \u2192 22: \u03c6\u1d40\u2071 r0\n  \u001b|0m   \u2502    - n18 \u2192 23: 
\u03c6\u1d40 r1\n  \u001b|0m   \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b2\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m22: \u03c6\u1d40\u2071 r0 (n7, n35), 2 uses\n\u001b|0;31m\u2502 \u001b|0m  
\u001b|0m23: \u03c6\u1d40 r1 (n18, n31), 0 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m24: UnsafeSmiUntag |n22], 4 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u001b|0m25:
BranchIfInt32Compare(LessThan) |n24, n20] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  
\u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m26: 
Int32IncrementWithOverflow |n24], 3 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m27: LoadTaggedField(0x8, compressed) |n12], 1 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m28: LoadFixedArrayElement(compressed) |n27, n24], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m29: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n28, n22, n12], 0 uses, but required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m 
\u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  
\u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m30: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n12]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m31: LoadTaggedFieldForProperty(0xc, compressed) |n12], 2 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m32: UnsafeSmiUntag |n31], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n20, n32]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m35: UnsafeSmiTagInt32 |n26], 1 
uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u001b|0m34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m      with gap moves:\n \u001b|0;32m\u2502\u001b|0m        - n35 \u2192 22: \u03c6\u1d40\u2071 r0\n 
\u001b|0;32m\u2502\u001b|0m        - n31 \u2192 23: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u001b|0m36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m      with gap moves:\n\u001b|0;31m\u2502 \u001b|0m        - n7 \u2192 54: \u03c6\u1d40 
r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m37: \u03c6\u1d40\u2091 <accumulator>, 1 uses\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  23 : CreateCatchContext r2, |1]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m42: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m43: InlinedAllocation(object 
0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n42], 5 uses (5 non escaping uses)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m44: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) 
|n43]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m45: StoreTaggedFieldNoWriteBarrier(0x4) |n43, n41]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m46: StoreTaggedFieldWithWriteBarrier(0x8) |n43, 
n40]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m47: StoreTaggedFieldWithWriteBarrier(0xc) |n43, n3]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m48: StoreTaggedFieldWithWriteBarrier(0x10) |n43, 
n37]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m50: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov 
<context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m52: Jump b6\n\u001b|0;31m\u2502 \u001b|0m   \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m   \u2502    - n53 \u2192 54: \u03c6\u1d40 
r0\n\u001b|0;31m\u2502 \u001b|0m   \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m  \u001b|0m54: \u03c6\u1d40 r0 (n7, n53), 8 uses\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m  
\u001b|0m55: AllocationBlock(Young), 1 uses\n  \u001b|0m  \u001b|0m56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n55], 16 uses (3 non escaping uses)\n  \u001b|0m  \u001b|0m57: 
StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |n56]\n  \u001b|0m  \u001b|0m58: StoreTaggedFieldNoWriteBarrier(0x4) |n56, n10]\n  \u001b|0m  \u001b|0m59: 
StoreTaggedFieldNoWriteBarrier(0x8) |n56, n10]\n  \u001b|0m  \u001b|0m60: StoreTaggedFieldNoWriteBarrier(0xc) |n56, n7]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  \u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m  \u001b|0m61: LoadTaggedFieldForProperty(0xc, 
compressed) |n56], 6 uses\n  \u001b|0m  \u001b|0m62: ThrowIfNotCallable |n4]\n  \u001b|0m      \u2502      @54 (4 live vars)\n  \u001b|0m      \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m  
\u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m  \u001b|0m63: UnsafeSmiUntag |n61], 2 uses, cannot truncate to int32\n  \u001b|0m  \u001b|0m64: Jump b7\n  \u001b|0m   \u2502  with gap 
moves:\n  \u001b|0m   \u2502    - n7 \u2192 65: \u03c6\u1d40\u2071 r0\n  \u001b|0m   \u2502    - n61 \u2192 66: \u03c6\u1d40 r1\n  \u001b|0m   \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock 
b7\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m65: \u03c6\u1d40\u2071 r0 (n7, n78), 2 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m66: \u03c6\u1d40 r1 (n61, n74), 0 uses\n\u001b|0;31m\u2502 \u001b|0m  
\u001b|0m67: UnsafeSmiUntag |n65], 4 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u001b|0m68: BranchIfInt32Compare(LessThan) |n67, n63] b8 
b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1 eager @54 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m69: Int32IncrementWithOverflow |n67], 3 
uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m70: LoadTaggedField(0x8, compressed) |n56], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m71: 
LoadFixedArrayElement(compressed) |n70, n67], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m72: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n71, n65, n56], 0 uses, but 
required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1
eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m73: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n56]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m74: LoadTaggedFieldForProperty(0xc, compressed) |n56], 2 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m75: UnsafeSmiUntag |n74], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u21b1 eager @54 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m76: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n63, n75]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  \u001b|0m78: UnsafeSmiTagInt32 |n69], 1 
uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u001b|0m77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m      with gap moves:\n \u001b|0;32m\u2502\u001b|0m        - n78 \u2192 65: \u03c6\u1d40\u2071 r0\n 
\u001b|0;32m\u2502\u001b|0m        - n74 \u2192 66: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m\u001b|0;34m  59 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u001b|0m79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m      with gap moves:\n\u001b|0;31m\u2502 \u001b|0m        - n54 \u2192 95: \u03c6\u1d40 
r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m80: \u03c6\u1d40\u2091 <accumulator>, 2 uses\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  62 : CreateCatchContext r2, |2]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m82: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m83: InlinedAllocation(object 
0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n82], 6 uses (6 non escaping uses)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m84: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) 
|n83]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m85: StoreTaggedFieldNoWriteBarrier(0x4) |n83, n41]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m86: StoreTaggedFieldWithWriteBarrier(0x8) |n83, 
n81]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m87: StoreTaggedFieldWithWriteBarrier(0xc) |n83, n3]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m88: StoreTaggedFieldWithWriteBarrier(0x10) |n83, 
n80]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m89: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : 
AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m      \u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m91: CheckedSmiUntag |n54], 1 uses, cannot truncate to 
int32\n\u001b|0;31m\u2502 \u001b|0m      \u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m92: Int32AddWithOverflow |n91, n90], 1 uses, can truncate to int32 |-2147483648, 
2147483647]\n\u001b|0;31m\u2502 \u001b|0m  \u001b|0m94: Int32ToNumber |n92], 1 uses\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m  \u001b|0m93: Jump b11\n\u001b|0;31m\u2502 
\u001b|0m   \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m   \u2502    - n94 \u2192 95: \u03c6\u1d40 r0\n\u001b|0;31m\u2502 \u001b|0m   \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  
\u001b|0m  \u001b|0m95: \u03c6\u1d40 r0 (n54, n94), 1 uses\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  \u001b|0m  \u001b|0m97: ReduceInterruptBudgetForReturn(82) |n96]\n  \u001b|0m  \u001b|0m98: 
Return |n95]\n  \u001b|0m\n\nAfter Phi untagging\nGraph\n\n  \u001b|0m    \u001b|0m3: Constant(0x38d6008107ad <NativeContext|300]>), 34 uses\n  \u001b|0m   \u001b|0m96: Constant(0x38d600827d71 
<FeedbackCell|one closure]>), 1 uses\n  \u001b|0m    \u001b|0m2: Constant(0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)>), 12 uses\n  \u001b|0m    \u001b|0m9: Constant(0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>), 0 uses \ud83e\udea6\n  \u001b|0m   \u001b|0m17: Constant(0x38d600816f21 <JSFunction forEach (sfi = 0x38d6002dee25)>), 6 uses\n  \u001b|0m   \u001b|0m40: 
Constant(0x38d600827e2d <ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m   \u001b|0m38: Constant(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>), 0 uses \ud83e\udea6\n  \u001b|0m   \u001b|0m81: Constant(0x38d600827e0d
<ScopeInfo CATCH_SCOPE>), 1 uses\n  \u001b|0m    \u001b|0m4: RootConstant(undefined_value), 28 uses\n  \u001b|0m   \u001b|0m49: RootConstant(the_hole_value), 2 uses\n  \u001b|0m   \u001b|0m10: 
RootConstant(empty_fixed_array), 4 uses\n  \u001b|0m    \u001b|0m8: RootConstant(one_pointer_filler_map), 0 uses \ud83e\udea6\n  \u001b|0m    \u001b|0m7: SmiConstant(0), 11 uses\n  \u001b|0m   \u001b|0m53: 
SmiConstant(2), 0 uses \ud83e\udea6\n  \u001b|0m   \u001b|0m41: SmiConstant(3), 2 uses\n  \u001b|0m   \u001b|0m99: Int32Constant(0), 1 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m51: 
Int32Constant(2), 1 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m39: Int32Constant(3), 0 uses \ud83e\udea6, cannot truncate to int32\n  \u001b|0m   \u001b|0m90: Int32Constant(40), 1 uses, cannot 
truncate to int32\n   \u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  
\u001b|0m\u001b|0;34m   0 : LdaZero\n\u001b|m  \u001b|0m    \u001b|0m1: InitialValue(<this>), 12 uses\n  \u001b|0m    \u001b|0m5: FunctionEntryStackCheck\n  \u001b|0m       \u21b3 lazy @-1 (2 live vars)\n  
\u001b|0m    \u001b|0m6: Jump b1\n  \u001b|0m    \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m   \u001b|0m11: AllocationBlock(Young), 1 uses\n  \u001b|0m   
\u001b|0m12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n11], 16 uses (3 non escaping uses)\n  \u001b|0m   \u001b|0m13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |n12]\n  \u001b|0m   \u001b|0m14: StoreTaggedFieldNoWriteBarrier(0x4) |n12, n10]\n  \u001b|0m   \u001b|0m15: StoreTaggedFieldNoWriteBarrier(0x8) |n12, n10]\n  \u001b|0m   \u001b|0m16: 
StoreTaggedFieldNoWriteBarrier(0xc) |n12, n7]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  
\u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   \u001b|0m18: LoadTaggedFieldForProperty(0xc, compressed) |n12], 6 uses\n  \u001b|0m   \u001b|0m19: ThrowIfNotCallable |n4]\n  
\u001b|0m       \u2502      @15 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n  \u001b|0m   
\u001b|0m20: UnsafeSmiUntag |n18], 2 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m21: Jump b2\n  \u001b|0m    \u2502  with gap moves:\n  \u001b|0m    \u2502    - n7 \u2192 22: \u03c6\u1d40\u2071 
r0\n  \u001b|0m    \u2502    - n18 \u2192 23: \u03c6\u1d40 r1\n  \u001b|0m    \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m22: \u03c6\u1d40\u2071 r0 (n7, 
n35), 2 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m23: \u03c6\u1d40 r1 (n18, n31), 0 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m24: UnsafeSmiUntag |n22], 4 uses, cannot truncate to 
int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m25: BranchIfInt32Compare(LessThan) |n24, n20] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    
\u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502  
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m26: Int32IncrementWithOverflow |n24], 3 uses, cannot truncate to 
int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m27: LoadTaggedField(0x8, compressed) |n12], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28: 
LoadFixedArrayElement(compressed) |n27, n24], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n28, n22, n12], 0 uses, but 
required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1
eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n12]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31: LoadTaggedFieldForProperty(0xc, compressed) |n12], 2 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32: UnsafeSmiUntag |n31], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n20, n32]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35: UnsafeSmiTagInt32 |n26], 1 
uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m       with gap moves:\n \u001b|0;32m\u2502\u001b|0m         - n35 \u2192 22: \u03c6\u1d40\u2071 r0\n 
\u001b|0;32m\u2502\u001b|0m         - n31 \u2192 23: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - n99 \u2192 54: \u03c6\u1d35 
r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m37: \u03c6\u1d40\u2091 <accumulator>, 1 uses\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  23 : CreateCatchContext r2, |1]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m42: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m43: InlinedAllocation(object
0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n42], 5 uses (5 non escaping uses)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m44: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) 
|n43]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m45: StoreTaggedFieldNoWriteBarrier(0x4) |n43, n41]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m46: StoreTaggedFieldWithWriteBarrier(0x8) |n43, 
n40]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m47: StoreTaggedFieldWithWriteBarrier(0xc) |n43, n3]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m48: StoreTaggedFieldWithWriteBarrier(0x10) |n43, 
n37]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m50: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov 
<context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m52: Jump b6\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - n51 \u2192 54: \u03c6\u1d35 
r0\n\u001b|0;31m\u2502 \u001b|0m    \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m54: \u03c6\u1d35 r0 (n99, n51), 9 uses\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   
\u001b|0m55: AllocationBlock(Young), 1 uses\n  \u001b|0m   \u001b|0m56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |n55], 16 uses (3 non escaping uses)\n  \u001b|0m   \u001b|0m57:
StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |n56]\n  \u001b|0m   \u001b|0m58: StoreTaggedFieldNoWriteBarrier(0x4) |n56, n10]\n  \u001b|0m   \u001b|0m59: 
StoreTaggedFieldNoWriteBarrier(0x8) |n56, n10]\n  \u001b|0m   \u001b|0m60: StoreTaggedFieldNoWriteBarrier(0xc) |n56, n7]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  \u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m61: LoadTaggedFieldForProperty(0xc, 
compressed) |n56], 6 uses\n  \u001b|0m   \u001b|0m62: ThrowIfNotCallable |n4]\n  \u001b|0m       \u2502      @54 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m63: UnsafeSmiUntag |n61], 2 uses, cannot truncate to int32\n  \u001b|0m   \u001b|0m64: Jump b7\n  \u001b|0m  
\u2502  with gap moves:\n  \u001b|0m    \u2502    - n7 \u2192 65: \u03c6\u1d40\u2071 r0\n  \u001b|0m    \u2502    - n61 \u2192 66: \u03c6\u1d40 r1\n  \u001b|0m    
\u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m65: \u03c6\u1d40\u2071 r0 (n7, n78), 2 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m66: \u03c6\u1d40 r1 (n61, 
n74), 0 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m67: UnsafeSmiUntag |n65], 4 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m68: 
BranchIfInt32Compare(LessThan) |n67, n63] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m 
\u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m69: 
Int32IncrementWithOverflow |n67], 3 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m70: LoadTaggedField(0x8, compressed) |n56], 1 
uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m71: LoadFixedArrayElement(compressed) |n70, n67], 1 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m72: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |n4, n3, n4, n71, n65, n56], 0 uses, but required\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m
\u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, 
r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m73: CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|n56]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m74: LoadTaggedFieldForProperty(0xc, compressed) |n56], 2 uses\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m75: UnsafeSmiUntag 
|n74], 1 uses, cannot truncate to int32\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m76: CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |n63, 
n75]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m78: UnsafeSmiTagInt32 |n69], 1 uses\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m       with 
gap moves:\n \u001b|0;32m\u2502\u001b|0m         - n78 \u2192 65: \u03c6\u1d40\u2071 r0\n \u001b|0;32m\u2502\u001b|0m         - n74 \u2192 66: \u03c6\u1d40 r1\n \u001b|0;32m\u2502\u001b|0m\n 
\u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m  \u001b|0m100: Int32ToNumber |n54], 1 uses\n  \u001b|0m\u001b|0;34m  59 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m79: Jump 
b11\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - n100 \u2192 95: \u03c6\u1d40 r0\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception
handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m80: \u03c6\u1d40\u2091 <accumulator>, 2 uses\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  62 : CreateCatchContext r2, |2]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m 
\u001b|0m82: AllocationBlock(Young), 1 uses\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m83: InlinedAllocation(object 0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |n82], 6 uses (6 non escaping 
uses)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m84: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) |n83]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m85: StoreTaggedFieldNoWriteBarrier(0x4) 
|n83, n41]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m86: StoreTaggedFieldWithWriteBarrier(0x8) |n83, n81]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m87: StoreTaggedFieldWithWriteBarrier(0xc) |n83, 
n3]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m88: StoreTaggedFieldWithWriteBarrier(0x10) |n83, n80]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m89: SetPendingMessage |n49], 0 uses, but required\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m       \u21b1 eager @74 (4 live 
vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m92: Int32AddWithOverflow |n54, n90], 1 uses, can truncate to int32 |-2147483648, 2147483647]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m94: Int32ToNumber |n92], 1 
uses\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m93: Jump b11\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - n94 
\u2192 95: \u03c6\u1d40 r0\n\u001b|0;31m\u2502 \u001b|0m    \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m95: \u03c6\u1d40 r0 (n100, n94), 1 uses\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  
\u001b|0m   \u001b|0m97: ReduceInterruptBudgetForReturn(82) |n96]\n  \u001b|0m   \u001b|0m98: Return |n95]\n  \u001b|0m\n\nAfter use marking\nGraph\n\n  \u001b|0m    \u001b|0m3: Constant(0x38d6008107ad 
<NativeContext|300]>) \u2192 (x)\n  \u001b|0m   \u001b|0m96: Constant(0x38d600827d71 <FeedbackCell|one closure]>) \u2192 (x)\n  \u001b|0m    \u001b|0m2: Constant(0x38d600827d91 <JSFunction foo (sfi = 
0x38d600827cfd)>) \u2192 (x)\n  \u001b|0m   \u001b|0m17: Constant(0x38d600816f21 <JSFunction forEach (sfi = 0x38d6002dee25)>) \u2192 (x)\n  \u001b|0m   \u001b|0m40: Constant(0x38d600827e2d <ScopeInfo 
CATCH_SCOPE>) \u2192 (x)\n  \u001b|0m   \u001b|0m81: Constant(0x38d600827e0d <ScopeInfo CATCH_SCOPE>) \u2192 (x)\n  \u001b|0m    \u001b|0m4: RootConstant(undefined_value) \u2192 (x)\n  \u001b|0m   
\u001b|0m49: RootConstant(the_hole_value) \u2192 (x)\n  \u001b|0m   \u001b|0m10: RootConstant(empty_fixed_array) \u2192 (x)\n  \u001b|0m    \u001b|0m7: SmiConstant(0) \u2192 (x)\n  \u001b|0m   \u001b|0m41: 
SmiConstant(3) \u2192 (x)\n  \u001b|0m   \u001b|0m99: Int32Constant(0) \u2192 (x)\n  \u001b|0m   \u001b|0m51: Int32Constant(2) \u2192 (x)\n  \u001b|0m   \u001b|0m90: Int32Constant(40) \u2192 (x)\n   
\u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  \u001b|0m\u001b|0;34m   0 : 
LdaZero\n\u001b|m  \u001b|0m    \u001b|0m1: InitialValue(<this>) \u2192 (x)\n  \u001b|0m    \u001b|0m5: FunctionEntryStackCheck\n  \u001b|0m       \u21b3 lazy @-1 (2 live vars)\n  \u001b|0m    \u001b|0m6: 
Jump b1\n  \u001b|0m    \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m   \u001b|0m11: AllocationBlock(Young) \u2192 (x)\n  \u001b|0m   
\u001b|0m12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n11:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |v0/n12:(x)]\n  \u001b|0m   \u001b|0m14: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n12:(x), v0/n10:(x)]\n  \u001b|0m   \u001b|0m15: StoreTaggedFieldNoWriteBarrier(0x8) |v0/n12:(x), 
v0/n10:(x)]\n  \u001b|0m   \u001b|0m16: StoreTaggedFieldNoWriteBarrier(0xc) |v0/n12:(x), v0/n7:(x)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   \u001b|0m18: LoadTaggedFieldForProperty(0xc, compressed) 
|v0/n12:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m19: ThrowIfNotCallable |v0/n4:(x)]\n  \u001b|0m       \u2502      @15 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n  \u001b|0m   \u001b|0m20: UnsafeSmiUntag |v0/n18:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m21: Jump b2\n  \u001b|0m    \u2502  with gap
moves:\n  \u001b|0m    \u2502    - v0/n7:(x) \u2192 22: \u03c6\u1d40\u2071 r0 (x)\n  \u001b|0m    \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m22: 
\u03c6\u1d40\u2071 r0 (n7, n35) (compressed) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m24: UnsafeSmiUntag |v0/n22:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m25: 
BranchIfInt32Compare(LessThan) |v0/n24:(x), v0/n20:(x)] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock 
b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m26: Int32IncrementWithOverflow |v0/n24:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   
\u001b|0m27: LoadTaggedField(0x8, compressed) |v0/n12:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28: LoadFixedArrayElement(compressed) |v0/n27:(x), v0/n24:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v0/n4:(x), v0/n3:(x), v0/n4:(x), v0/n28:(x), v0/n22:(x), v0/n12:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @22 (b5) : {<this>:n1, <context>:n3, r0:n7}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1
eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n12:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31: LoadTaggedFieldForProperty(0xc, compressed) |v0/n12:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32: UnsafeSmiUntag |v0/n31:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v0/n20:(x), v0/n32:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35: UnsafeSmiTagInt32 |v0/n26:(x)] \u2192 
(x)\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m       with gap moves:\n \u001b|0;32m\u2502\u001b|0m         - v0/n35:(x) \u2192 22: \u03c6\u1d40\u2071 r0 
(x)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m36: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - v0/n99:(x) \u2192 54: \u03c6\u1d35 r0 (x)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 
(exception handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m37: \u03c6\u1d40\u2091 <accumulator> (compressed) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  23 : CreateCatchContext r2, 
|1]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m42: AllocationBlock(Young) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m43: InlinedAllocation(object 0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) 
|v0/n42:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m44: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, InlinedAllocation) |v0/n43:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m45: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n43:(x), v0/n41:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m46: StoreTaggedFieldWithWriteBarrier(0x8) |v0/n43:(x), v0/n40:(x)] 
\ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m47: StoreTaggedFieldWithWriteBarrier(0xc) |v0/n43:(x), v0/n3:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m48: 
StoreTaggedFieldWithWriteBarrier(0x10) |v0/n43:(x), v0/n37:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m50: 
SetPendingMessage |v0/n49:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov <context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m52: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - v0/n51:(x) \u2192 54: \u03c6\u1d35 r0 (x)\n\u001b|0;31m\u2502 \u001b|0m    
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m54: \u03c6\u1d35 r0 (n99, n51) \u2192 (x)\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   \u001b|0m55: AllocationBlock(Young) \u2192 (x)\n
\u001b|0m   \u001b|0m56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n55:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m57: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |v0/n56:(x)]\n  \u001b|0m   \u001b|0m58: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n56:(x), v0/n10:(x)]\n  \u001b|0m   \u001b|0m59: StoreTaggedFieldNoWriteBarrier(0x8) |v0/n56:(x), 
v0/n10:(x)]\n  \u001b|0m   \u001b|0m60: StoreTaggedFieldNoWriteBarrier(0xc) |v0/n56:(x), v0/n7:(x)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  \u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m61: LoadTaggedFieldForProperty(0xc, compressed) 
|v0/n56:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m62: ThrowIfNotCallable |v0/n4:(x)]\n  \u001b|0m       \u2502      @54 (4 live vars)\n  \u001b|0m       \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m63: UnsafeSmiUntag |v0/n61:(x)] \u2192 (x)\n  \u001b|0m   \u001b|0m64: Jump b7\n  \u001b|0m    \u2502  with 
gap moves:\n  \u001b|0m    \u2502    - v0/n7:(x) \u2192 65: \u03c6\u1d40\u2071 r0 (x)\n  \u001b|0m    \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m65: 
\u03c6\u1d40\u2071 r0 (n7, n78) (compressed) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m67: UnsafeSmiUntag |v0/n65:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m68: 
BranchIfInt32Compare(LessThan) |v0/n67:(x), v0/n63:(x)] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m    \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock 
b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m69: Int32IncrementWithOverflow |v0/n67:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   
\u001b|0m70: LoadTaggedField(0x8, compressed) |v0/n56:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m71: LoadFixedArrayElement(compressed) |v0/n70:(x), v0/n67:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m72: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v0/n4:(x), v0/n3:(x), v0/n4:(x), v0/n71:(x), v0/n65:(x), v0/n56:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       
\u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m73: 
CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v0/n56:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m74: LoadTaggedFieldForProperty(0xc, compressed) |v0/n56:(x)] \u2192 
(x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m75: UnsafeSmiUntag |v0/n74:(x)] \u2192 (x)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u21b1 eager @54 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m76: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v0/n63:(x), v0/n75:(x)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m78: UnsafeSmiTagInt32 |v0/n69:(x)] \u2192 
(x)\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m       with gap moves:\n \u001b|0;32m\u2502\u001b|0m         - v0/n78:(x) \u2192 65: \u03c6\u1d40\u2071 r0 
(x)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m  \u001b|0m100: Int32ToNumber |v0/n54:(x)] \u2192 (x)\n  \u001b|0m\u001b|0;34m  59 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m       with gap moves:\n\u001b|0;31m\u2502 \u001b|0m         - v0/n100:(x) \u2192 95: \u03c6\u1d40 r0
(x)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m80: \u03c6\u1d40\u2091 <accumulator> (compressed) \u2192 
(x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  62 : CreateCatchContext r2, |2]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m82: AllocationBlock(Young) \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m83: InlinedAllocation(object 0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>) |v0/n82:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m84: StoreMap(0x38d60081e809 <Map(CATCH_CONTEXT_TYPE)>, 
InlinedAllocation) |v0/n83:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m85: StoreTaggedFieldNoWriteBarrier(0x4) |v0/n83:(x), v0/n41:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m86:
StoreTaggedFieldWithWriteBarrier(0x8) |v0/n83:(x), v0/n81:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m87: StoreTaggedFieldWithWriteBarrier(0xc) |v0/n83:(x), v0/n3:(x)] 
\ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m88: StoreTaggedFieldWithWriteBarrier(0x10) |v0/n83:(x), v0/n80:(x)] \ud83e\udea6\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : 
SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m89: SetPendingMessage |v0/n49:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> 
(0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m       
\u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m92: Int32AddWithOverflow |v0/n54:(x), v0/n90:(x)] \u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m94: Int32ToNumber |v0/n92:(x)] 
\u2192 (x)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m93: Jump b11\n\u001b|0;31m\u2502 \u001b|0m    \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m    \u2502    - 
v0/n94:(x) \u2192 95: \u03c6\u1d40 r0 (x)\n\u001b|0;31m\u2502 \u001b|0m    \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m95: \u03c6\u1d40 r0 (n100, n94) (compressed) \u2192 
(x)\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : 
Return\n\u001b|m  \u001b|0m   \u001b|0m97: ReduceInterruptBudgetForReturn(82) |v0/n96:(x)]\n  \u001b|0m   \u001b|0m98: Return |v0/n95:(x)]\n  \u001b|0m\n\nAfter register allocation pre-processing\nGraph\n\n  
\u001b|0m     \u001b|0m1/3: Constant(0x38d6008107ad <NativeContext|300]>) \u2192 v-1, live range: |1-69]\n  \u001b|0m    \u001b|0m2/96: Constant(0x38d600827d71 <FeedbackCell|one closure]>) \u2192 v-1, live 
range: |2-73]\n  \u001b|0m     \u001b|0m3/2: Constant(0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)>) \u2192 v-1, live range: |3-69]\n  \u001b|0m    \u001b|0m4/17: Constant(0x38d600816f21 <JSFunction 
forEach (sfi = 0x38d6002dee25)>) \u2192 v-1, live range: |4-64]\n  \u001b|0m     \u001b|0m5/4: RootConstant(undefined_value) \u2192 v-1, live range: |5-64]\n  \u001b|0m    \u001b|0m6/49: 
RootConstant(the_hole_value) \u2192 v-1, live range: |6-68]\n  \u001b|0m    \u001b|0m7/10: RootConstant(empty_fixed_array) \u2192 v-1, live range: |7-46]\n  \u001b|0m     \u001b|0m8/7: SmiConstant(0) \u2192 
v-1, live range: |8-51]\n  \u001b|0m    \u001b|0m9/99: Int32Constant(0) \u2192 v-1, live range: |9-38]\n  \u001b|0m   \u001b|0m10/51: Int32Constant(2) \u2192 v-1, live range: |10-40]\n  \u001b|0m   
\u001b|0m11/90: Int32Constant(40) \u2192 v-1, live range: |11-69]\n   \u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  \u001b|0m\u001b|0;34m   0 : LdaZero\n\u001b|m  \u001b|0m    \u001b|0m12/1: InitialValue(<this>) \u2192 v-1(=-6S), live range: |12-69]\n  
\u001b|0m    \u001b|0m13/5: FunctionEntryStackCheck\n  \u001b|0m          \u21b3 lazy @-1 (2 live vars)\n  \u001b|0m    \u001b|0m14/6: Jump b1\n  \u001b|0m       \u2193\n   \u001b|0mBlock b1\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral
|0]\n\u001b|m  \u001b|0m   \u001b|0m15/11: AllocationBlock(Young) \u2192 v-1(R), live range: |15-16]\n  \u001b|0m   \u001b|0m16/12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|v15/n11:v-1(R)] \u2192 v-1(0), live range: |16-37]\n  \u001b|0m   \u001b|0m17/13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |v16/n12:v-1(=rdi)]\n  \u001b|0m   \u001b|0m18/14:
StoreTaggedFieldNoWriteBarrier(0x4) |v16/n12:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m19/15: StoreTaggedFieldNoWriteBarrier(0x8) |v16/n12:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m20/16: 
StoreTaggedFieldNoWriteBarrier(0xc) |v16/n12:v-1(R), v8/n7:v-1(R)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   \u001b|0m21/18: LoadTaggedFieldForProperty(0xc, compressed) 
|v16/n12:v-1(R)] \u2192 v-1(R), live range: |21-37]\n  \u001b|0m   \u001b|0m22/19: ThrowIfNotCallable |v5/n4:v-1(R)]\n  \u001b|0m          \u2502      @15 (4 live vars)\n  \u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m          \u21b3 throw (b5)\n  \u001b|0m   \u001b|0m23/20: UnsafeSmiUntag |v21/n18:v-1(R)] \u2192 v-1(0), live range: |23-37]\n  \u001b|0m   \u001b|0m24/21: 
Jump b2\n  \u001b|0m       \u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:v-1(*) \u2192 22: \u03c6\u1d40\u2071 r0 v-1(*)\n  \u001b|0m       \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock 
b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m25/22: \u03c6\u1d40\u2071 r0 (n7, n35) \u2192 v-1(*), live range: |25-31]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m26/24: UnsafeSmiUntag |v25/n22:v-1(R)] \u2192 
v-1(0), live range: |26-30]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m27/25: BranchIfInt32Compare(LessThan) |v26/n24:v-1(R), v23/n20:v-1(R)] b3 
b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28/26: Int32IncrementWithOverflow 
|v26/n24:v-1(R)] \u2192 v-1(0), live range: |28-36]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29/27: LoadTaggedField(0x8, decompressed) |v16/n12:v-1(R)] \u2192 v-1(R), live range: 
|29-30]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30/28: LoadFixedArrayElement(decompressed) |v29/n27:v-1(R), v26/n24:v-1(R)] \u2192 v-1(R), live range: 
|30-31]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31/29: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v5/n4:v-1(=rdi), v1/n3:v-1(=rsi), v5/n4:v-1(*), v30/n28:v-1(*), v25/n22:v-1(*), 
v16/n12:v-1(*)] \u2192 v-1(=rax)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw (b5)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32/30: CheckMaps(0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |v16/n12:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33/31: LoadTaggedFieldForProperty(0xc, compressed) |v16/n12:v-1(R)] \u2192 v-1(R), live range: 
|33-34]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m34/32: UnsafeSmiUntag |v33/n31:v-1(R)] \u2192 v-1(0), live range: |34-35]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager
@15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35/33: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v23/n20:v-1(R), v34/n32:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m36/35: UnsafeSmiTagInt32 |v28/n26:v-1(R)] \u2192 
v-1(0), live range: |36-37]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m37/34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n \u001b|0;32m\u2502\u001b|0m            - 
v36/n35:v-1(*) \u2192 22: \u03c6\u1d40\u2071 r0 v-1(*)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m38/36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m          with gap moves:\n\u001b|0;31m\u2502 \u001b|0m            - v9/n99:v-1(*) \u2192 54: 
\u03c6\u1d35 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 
\u001b|0m   \u001b|0m39/50: SetPendingMessage |v6/n49:v-1(R)] \u2192 v-1(R)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov <context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m40/52: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v10/n51:v-1(*) \u2192 54: \u03c6\u1d35 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m       
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m41/54: \u03c6\u1d35 r0 (n99, n51) \u2192 v-1(*), live range: |41-69]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo 
foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   \u001b|0m42/55: 
AllocationBlock(Young) \u2192 v-1(R), live range: |42-43]\n  \u001b|0m   \u001b|0m43/56: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v42/n55:v-1(R)] \u2192 v-1(0), live range: 
|43-64]\n  \u001b|0m   \u001b|0m44/57: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |v43/n56:v-1(=rdi)]\n  \u001b|0m   \u001b|0m45/58: StoreTaggedFieldNoWriteBarrier(0x4) 
|v43/n56:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m46/59: StoreTaggedFieldNoWriteBarrier(0x8) |v43/n56:v-1(R), v7/n10:v-1(R)]\n  \u001b|0m   \u001b|0m47/60: StoreTaggedFieldNoWriteBarrier(0xc) 
|v43/n56:v-1(R), v8/n7:v-1(R)]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  
\u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m48/61: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:v-1(R)] \u2192 v-1(R), live range: |48-64]\n  \u001b|0m   
\u001b|0m49/62: ThrowIfNotCallable |v5/n4:v-1(R)]\n  \u001b|0m          \u2502      @54 (4 live vars)\n  \u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m          \u21b3 
throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m50/63: UnsafeSmiUntag |v48/n61:v-1(R)] \u2192 v-1(0), live range: |50-64]\n  \u001b|0m   \u001b|0m51/64: Jump b7\n  \u001b|0m       
\u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:v-1(*) \u2192 65: \u03c6\u1d40\u2071 r0 v-1(*)\n  \u001b|0m       \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 
\u001b|0m   \u001b|0m52/65: \u03c6\u1d40\u2071 r0 (n7, n78) \u2192 v-1(*), live range: |52-58]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m53/67: UnsafeSmiUntag |v52/n65:v-1(R)] \u2192 v-1(0), live range: 
|53-57]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m54/68: BranchIfInt32Compare(LessThan) |v53/n67:v-1(R), v50/n63:v-1(R)] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       
\u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          
\u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m55/69: Int32IncrementWithOverflow |v53/n67:v-1(R)] \u2192 v-1(0), live range: 
|55-63]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m56/70: LoadTaggedField(0x8, decompressed) |v43/n56:v-1(R)] \u2192 v-1(R), live range: |56-57]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m  
\u001b|0m57/71: LoadFixedArrayElement(decompressed) |v56/n70:v-1(R), v53/n67:v-1(R)] \u2192 v-1(R), live range: |57-58]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m58/72: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |v5/n4:v-1(=rdi), v1/n3:v-1(=rsi), v5/n4:v-1(*), v57/n71:v-1(*), v52/n65:v-1(*), v43/n56:v-1(*)] \u2192 v-1(=rax)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502   
@54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw @61 (b10) : 
{<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m59/73: CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|v43/n56:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m60/74: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:v-1(R)] \u2192 v-1(R), live range: 
|60-61]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m61/75: UnsafeSmiUntag |v60/n74:v-1(R)] \u2192 v-1(0), live range: |61-62]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager
@54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m62/76: 
CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v50/n63:v-1(R), v61/n75:v-1(R)]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m63/78: UnsafeSmiTagInt32 |v55/n69:v-1(R)] \u2192 
v-1(0), live range: |63-64]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m64/77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n \u001b|0;32m\u2502\u001b|0m            - 
v63/n78:v-1(*) \u2192 65: \u03c6\u1d40\u2071 r0 v-1(*)\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m  \u001b|0m65/100: Int32ToNumber |v41/n54:v-1(R)] \u2192 v-1(R), 
live range: |65-66]\n  \u001b|0m\u001b|0;34m  59 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m66/79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m          with gap 
moves:\n\u001b|0;31m\u2502 \u001b|0m            - v65/n100:v-1(*) \u2192 95: \u03c6\u1d40 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception 
handler)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m67/80: \u03c6\u1d40\u2091 <accumulator> \u2192 v-1(*), live range: |67-69]\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  67 : 
SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m68/89: SetPendingMessage |v6/n49:v-1(R)] \u2192 v-1(R)\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> 
(0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m          
\u21b1 eager @74 (4 live vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m69/92: Int32AddWithOverflow |v41/n54:v-1(R), v11/n90:v-1(*)] \u2192 v-1(0), live range: |69-70]\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m70/94: Int32ToNumber |v69/n92:v-1(R)] \u2192 v-1(R), live range: |70-71]\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m71/93: Jump b11\n\u001b|0;31m\u2502 
\u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v70/n94:v-1(*) \u2192 95: \u03c6\u1d40 r0 v-1(*)\n\u001b|0;31m\u2502 \u001b|0m       
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m72/95: \u03c6\u1d40 r0 (n100, n94) \u2192 v-1(*), live range: |72-74]\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  \u001b|0m   \u001b|0m73/97: 
ReduceInterruptBudgetForReturn(82) |v2/n96:v-1(R)]\n  \u001b|0m   \u001b|0m74/98: Return |v72/n95:v-1(=rax)]\n  \u001b|0m\n\nAfter register allocation\nGraph\n\n  \u001b|0m     \u001b|0m1/3: 
Constant(0x38d6008107ad <NativeContext|300]>) \u2192 v-1, live range: |1-69]\n  \u001b|0m    \u001b|0m2/96: Constant(0x38d600827d71 <FeedbackCell|one closure]>) \u2192 v-1, live range: |2-73]\n  \u001b|0m    
\u001b|0m3/2: Constant(0x38d600827d91 <JSFunction foo (sfi = 0x38d600827cfd)>) \u2192 v-1, live range: |3-69]\n  \u001b|0m    \u001b|0m4/17: Constant(0x38d600816f21 <JSFunction forEach (sfi = 
0x38d6002dee25)>) \u2192 v-1, live range: |4-64]\n  \u001b|0m     \u001b|0m5/4: RootConstant(undefined_value) \u2192 v-1, live range: |5-64]\n  \u001b|0m    \u001b|0m6/49: RootConstant(the_hole_value) \u2192 
v-1, live range: |6-68]\n  \u001b|0m    \u001b|0m7/10: RootConstant(empty_fixed_array) \u2192 v-1, live range: |7-46]\n  \u001b|0m     \u001b|0m8/7: SmiConstant(0) \u2192 v-1, live range: |8-51]\n  \u001b|0m 
\u001b|0m9/99: Int32Constant(0) \u2192 v-1, live range: |9-38]\n  \u001b|0m   \u001b|0m10/51: Int32Constant(2) \u2192 v-1, live range: |10-40]\n  \u001b|0m   \u001b|0m11/90: Int32Constant(40) \u2192 v-1, live
range: |11-69]\n   \u001b|0mBlock b0\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:6:12)\n\u001b|m  
\u001b|0m\u001b|0;34m   0 : LdaZero\n\u001b|m  \u001b|0m    \u001b|0m12/1: InitialValue(<this>) \u2192 |stack:-6|t], live range: |12-69]\n  \u001b|0m    \u001b|0m13/5: FunctionEntryStackCheck\n  \u001b|0m    
\u21b3 lazy @-1 (2 live vars)\n  \u001b|0m    \u001b|0m14/6: Jump b1\n  \u001b|0m       \u2193\n   \u001b|0mBlock b1\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 
<String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:4)\n\u001b|m  \u001b|0m\u001b|0;34m   5 : CreateEmptyArrayLiteral |0]\n\u001b|m  \u001b|0m   \u001b|0m15/11: AllocationBlock(Young) \u2192 
|rdi|R|t], live range: |15-16]\n  \u001b|0m   \u001b|0m16/12: InlinedAllocation(object 0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) |v15/n11:|rdi|R|t]] \u2192 |rdi|R|t] (spilled: |stack:0|t]), live range: 
|16-37]\n  \u001b|0m   \u001b|0m17/13: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, InlinedAllocation) |v16/n12:|rdi|R|t]]\n  \u001b|0m     \u001b|0m101: ConstantGapMove(n10 \u2192 |rax|R|t])\n  
\u001b|0m   \u001b|0m18/14: StoreTaggedFieldNoWriteBarrier(0x4) |v16/n12:|rdi|R|t], v7/n10:|rax|R|t]]\n  \u001b|0m   \u001b|0m19/15: StoreTaggedFieldNoWriteBarrier(0x8) |v16/n12:|rdi|R|t], v7/n10:|rax|R|t]]\n
\u001b|0m     \u001b|0m102: ConstantGapMove(n7 \u2192 |rcx|R|t])\n  \u001b|0m   \u001b|0m20/16: StoreTaggedFieldNoWriteBarrier(0xc) |v16/n12:|rdi|R|t], v8/n7:|rcx|R|t]]\n  \u001b|0m\u001b|1;34m0x38d600827cfd 
<SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:9:7)\n\u001b|m  \u001b|0m\u001b|0;34m  15 : CallProperty1 r2, r3, r4, |3]\n\u001b|m  \u001b|0m   
\u001b|0m21/18: LoadTaggedFieldForProperty(0xc, compressed) |v16/n12:|rdi|R|t]] \u2192 |rdx|R|t] (spilled: |stack:1|t]), live range: |21-37]\n  \u001b|0m     \u001b|0m103: ConstantGapMove(n4 \u2192 
|rbx|R|t])\n  \u001b|0m   \u001b|0m22/19: ThrowIfNotCallable |v5/n4:|rbx|R|t]]\n  \u001b|0m          \u2502      @15 (4 live vars)\n  \u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n  
\u001b|0m          \u21b3 throw (b5)\n  \u001b|0m   \u001b|0m23/20: UnsafeSmiUntag |v21/n18:|rdx|R|t]] \u2192 |rdx|R|w32] (spilled: |stack:1|w32]), live range: |23-37]\n  \u001b|0m   \u001b|0m24/21: Jump b2\n
\u001b|0m       \u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:|rcx|R|t] \u2192 22: \u03c6\u1d40\u2071 r0 |rcx|R|t]\n  \u001b|0m       \u2502  with register merges:\n  \u001b|0m       \u2502    
- |rax|R|t] \u2192 rax\n  \u001b|0m       \u2502    - |rbx|R|t] \u2192 rbx\n  \u001b|0m       \u2502    - |rdi|R|t] \u2192 rdi\n  \u001b|0m       \u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock 
b2\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m25/22: \u03c6\u1d40\u2071 r0 (n7, n35) (decompressed) \u2192 |rcx|R|t], live range: |25-31]\n\u001b|0;31m\u2502 \u001b|0m     \u001b|0m104: GapMove(|rcx|R|t] \u2192
|rsi|R|t])\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m26/24: UnsafeSmiUntag |v25/n22:|rcx|R|t]] \u2192 |rcx|R|w32], live range: |26-30]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m27/25: 
BranchIfInt32Compare(LessThan) |v26/n24:|rcx|R|w32], v23/n20:|rdx|R|w32]] b3 b4\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m       \u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock 
b3\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m105: GapMove(|rcx|R|w32] \u2192 |r8|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m28/26: Int32IncrementWithOverflow 
|v26/n24:|rcx|R|w32]] \u2192 |rcx|R|w32] (spilled: |stack:0|w32]), live range: |28-36]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m29/27: LoadTaggedField(0x8, decompressed) |v16/n12:|rdi|R|t]] 
\u2192 |r9|R|t], live range: |29-30]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m30/28: LoadFixedArrayElement(decompressed) |v29/n27:|r9|R|t], v26/n24:|r8|R|w32]] \u2192 |r11|R|t], live range: 
|30-31]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m106: GapMove(|rbx|R|t] \u2192 |rdi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m107: GapMove(|rsi|R|t] \u2192 
|r8|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m108: ConstantGapMove(n3 \u2192 |rsi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m31/29: \ud83d\udc22 
Call(NULL_OR_UNDEFINED, Any) |v5/n4:|rdi|R|t], v1/n3:|rsi|R|t], v5/n4:|rbx|R|t], v30/n28:|r11|R|t], v25/n22:|r8|R|t], v16/n12:|stack:0|t]] \u2192 |rax|R|t]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m      
\u2502      @15 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy @ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw 
(b5)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m109: GapMove(|stack:0|t] \u2192 |rax|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live 
vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       @ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m32/30: CheckMaps(0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |v16/n12:|rax|R|t]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m33/31: LoadTaggedFieldForProperty(0xc, compressed) |v16/n12:|rax|R|t]] \u2192 |rcx|R|t], live 
range: |33-34]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m34/32: UnsafeSmiUntag |v33/n31:|rcx|R|t]] \u2192 |rcx|R|w32], live range: |34-35]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     
\u001b|0m110: GapMove(|stack:1|w32] \u2192 |rdx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @15 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502   
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m35/33: CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v23/n20:|rdx|R|w32], 
v34/n32:|rcx|R|w32]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m111: GapMove(|stack:0|w32] \u2192 |rcx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m36/35: 
UnsafeSmiTagInt32 |v28/n26:|rcx|R|w32]] \u2192 |rcx|R|t], live range: |36-37]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m37/34: JumpLoop b2\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n 
\u001b|0;32m\u2502\u001b|0m            - v36/n35:|rcx|R|t] \u2192 22: \u03c6\u1d40\u2071 r0 |rcx|R|t]\n \u001b|0;32m\u2502\u001b|0m          with register merges:\n \u001b|0;32m\u2502\u001b|0m            - 
|constant:v-1] \u2192 rax\n \u001b|0;32m\u2502\u001b|0m            - |constant:v-1] \u2192 rbx\n \u001b|0;32m\u2502\u001b|0m            - |rax|R|t] \u2192 rdi\n \u001b|0;32m\u2502\u001b|0m\n 
\u001b|0;32m\u2570\u25ba\u001b|0mBlock b4\n  \u001b|0m\u001b|0;34m  20 : Jump |21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m38/36: Jump b6\n\u001b|0;31m\u2502 \u001b|0m          with gap 
moves:\n\u001b|0;31m\u2502 \u001b|0m            - v9/n99:|constant:v-1] \u2192 54: \u03c6\u1d35 r0 |rcx|R|w32]\n\u001b|0;31m\u2502 \u001b|0m          with register merges:\n\u001b|0;31m\u2502 \u001b|0m       
- |rax|R|t] \u2192 rax\n\u001b|0;31m\u2502 \u001b|0m            - |rbx|R|t] \u2192 rbx\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b5 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m   
\u001b|0m112: ConstantGapMove(n49 \u2192 |rax|R|t])\n\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  28 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m39/50: SetPendingMessage 
|v6/n49:|rax|R|t]] \u2192 |rcx|R|t]\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:11:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  41 : Mov <context>, r1\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m40/52: Jump 
b6\n\u001b|0;31m\u2502 \u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v10/n51:|constant:v-1] \u2192 54: \u03c6\u1d35 r0 |rcx|R|w32]\n\u001b|0;31m\u2502 \u001b|0m      
\u2502  with register merges:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - |constant:v-1] \u2192 rax\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - |constant:v-1] \u2192 rbx\n\u001b|0;31m\u2502 \u001b|0m 
\u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b6\n  \u001b|0m   \u001b|0m41/54: \u03c6\u1d35 r0 (n99, n51) \u2192 |rcx|R|w32] (spilled: |stack:2|w32]), live range: |41-69]\n  
\u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:4)\n\u001b|m  \u001b|0m\u001b|0;34m  44 : 
CreateEmptyArrayLiteral |6]\n\u001b|m  \u001b|0m   \u001b|0m42/55: AllocationBlock(Young) \u2192 |rdi|R|t], live range: |42-43]\n  \u001b|0m   \u001b|0m43/56: InlinedAllocation(object 0x38d600816bb5 
<Map|16](PACKED_SMI_ELEMENTS)>) |v42/n55:|rdi|R|t]] \u2192 |rdi|R|t] (spilled: |stack:2|t]), live range: |43-64]\n  \u001b|0m   \u001b|0m44/57: StoreMap(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>, 
InlinedAllocation) |v43/n56:|rdi|R|t]]\n  \u001b|0m   \u001b|0m45/58: StoreTaggedFieldNoWriteBarrier(0x4) |v43/n56:|rdi|R|t], v7/n10:|rax|R|t]]\n  \u001b|0m   \u001b|0m46/59: 
StoreTaggedFieldNoWriteBarrier(0x8) |v43/n56:|rdi|R|t], v7/n10:|rax|R|t]]\n  \u001b|0m     \u001b|0m113: ConstantGapMove(n7 \u2192 |rax|R|t])\n  \u001b|0m   \u001b|0m47/60: StoreTaggedFieldNoWriteBarrier(0xc)
|v43/n56:|rdi|R|t], v8/n7:|rax|R|t]]\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: \"regressions/maglev/regress/regress-1455295.js\">:14:7)\n\u001b|m  
\u001b|0m\u001b|0;34m  54 : CallProperty1 r2, r3, r4, |9]\n\u001b|m  \u001b|0m   \u001b|0m48/61: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:|rdi|R|t]] \u2192 |rdx|R|t] (spilled: |stack:3|t]), live 
range: |48-64]\n  \u001b|0m   \u001b|0m49/62: ThrowIfNotCallable |v5/n4:|rbx|R|t]]\n  \u001b|0m          \u2502      @54 (4 live vars)\n  \u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n  \u001b|0m          \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n  \u001b|0m   \u001b|0m50/63: UnsafeSmiUntag |v48/n61:|rdx|R|t]] \u2192 |rdx|R|w32] 
(spilled: |stack:4|w32]), live range: |50-64]\n  \u001b|0m   \u001b|0m51/64: Jump b7\n  \u001b|0m       \u2502  with gap moves:\n  \u001b|0m       \u2502    - v8/n7:|rax|R|t] \u2192 65: \u03c6\u1d40\u2071 r0 
|rax|R|t]\n  \u001b|0m       \u2502  with register merges:\n  \u001b|0m       \u2502    - |rbx|R|t] \u2192 rbx\n  \u001b|0m       \u2502    - |rdi|R|t] \u2192 rdi\n  \u001b|0m       
\u25bc\n\u001b|0;31m\u256d\u2500\u25ba\u001b|0mBlock b7\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m52/65: \u03c6\u1d40\u2071 r0 (n7, n78) (decompressed) \u2192 |rax|R|t], live range: |52-58]\n\u001b|0;31m\u2502
\u001b|0m     \u001b|0m114: GapMove(|rax|R|t] \u2192 |rcx|R|t])\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m53/67: UnsafeSmiUntag |v52/n65:|rax|R|t]] \u2192 |rax|R|w32], live range: 
|53-57]\n\u001b|0;31m\u2502\u001b|0;32m\u256d\u2500\u2500\u2500\u001b|0m54/68: BranchIfInt32Compare(LessThan) |v53/n67:|rax|R|w32], v50/n63:|rdx|R|w32]] b8 b9\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   
\u2193\n\u001b|0;31m\u2502\u001b|0;32m\u2502 \u001b|0mBlock b8\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m115: GapMove(|rax|R|w32] \u2192 
|rsi|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m55/69: Int32IncrementWithOverflow |v53/n67:|rax|R|w32]] \u2192 |rax|R|w32] (spilled: |stack:3|w32]), live 
range: |55-63]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m56/70: LoadTaggedField(0x8, decompressed) |v43/n56:|rdi|R|t]] \u2192 |r8|R|t], live range: 
|56-57]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m57/71: LoadFixedArrayElement(decompressed) |v56/n70:|r8|R|t], v53/n67:|rsi|R|w32]] \u2192 |r9|R|t], live range: 
|57-58]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m116: GapMove(|rbx|R|t] \u2192 |rdi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m117: ConstantGapMove(n3 \u2192 
|rsi|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m58/72: \ud83d\udc22 Call(NULL_OR_UNDEFINED, Any) |v5/n4:|rdi|R|t], v1/n3:|rsi|R|t], v5/n4:|rbx|R|t], v57/n71:|r9|R|t], v52/n65:|rcx|R|t], 
v43/n56:|stack:2|t]] \u2192 |rax|R|t]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502      @54 (4 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 lazy 
@ArrayForEachLoopLazyDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b3 throw @61 (b10) : {<this>:n1, <context>:n3, r0:n54}\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     
\u001b|0m118: GapMove(|stack:2|t] \u2192 |rax|R|t])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502       
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m59/73: CheckMaps(0x38d600816bb5 <Map|16](PACKED_SMI_ELEMENTS)>) 
|v43/n56:|rax|R|t]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m60/74: LoadTaggedFieldForProperty(0xc, compressed) |v43/n56:|rax|R|t]] \u2192 |rcx|R|t], live range: 
|60-61]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m61/75: UnsafeSmiUntag |v60/n74:|rcx|R|t]] \u2192 |rcx|R|w32], live range: |61-62]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     
\u001b|0m119: GapMove(|stack:4|w32] \u2192 |rdx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u21b1 eager @54 (7 live vars)\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m          \u2502   
@ArrayForEachLoopEagerDeoptContinuation\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m62/76: CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) |v50/n63:|rdx|R|w32], 
v61/n75:|rcx|R|w32]]\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m     \u001b|0m120: GapMove(|stack:3|w32] \u2192 |rcx|R|w32])\n\u001b|0;31m\u2502\u001b|0;32m\u2502\u001b|0m   \u001b|0m63/78: 
UnsafeSmiTagInt32 |v55/n69:|rcx|R|w32]] \u2192 |rcx|R|t], live range: |63-64]\n\u001b|0;31m\u2570\u2500\u25c4\u2500\u2500\u001b|0m64/77: JumpLoop b7\n \u001b|0;32m\u2502\u001b|0m          with gap moves:\n 
\u001b|0;32m\u2502\u001b|0m            - v63/n78:|rcx|R|t] \u2192 65: \u03c6\u1d40\u2071 r0 |rax|R|t]\n \u001b|0;32m\u2502\u001b|0m          with register merges:\n \u001b|0;32m\u2502\u001b|0m            - 
|constant:v-1] \u2192 rbx\n \u001b|0;32m\u2502\u001b|0m            - |rax|R|t] \u2192 rdi\n \u001b|0;32m\u2502\u001b|0m\n \u001b|0;32m\u2570\u25ba\u001b|0mBlock b9\n  \u001b|0m     \u001b|0m121: 
GapMove(|stack:2|w32] \u2192 |rax|R|w32])\n  \u001b|0m  \u001b|0m65/100: Int32ToNumber |v41/n54:|rax|R|w32]] \u2192 |rcx|R|t], live range: |65-66]\n  \u001b|0m\u001b|0;34m  59 : Jump 
|21]\n\u001b|m\u001b|0;31m\u256d\u2500\u2500\u2500\u2500\u001b|0m66/79: Jump b11\n\u001b|0;31m\u2502 \u001b|0m          with gap moves:\n\u001b|0;31m\u2502 \u001b|0m            - v65/n100:|rcx|R|t] \u2192 95:
\u03c6\u1d40 r0 |rcx|R|t]\n\u001b|0;31m\u2502 \u001b|0m          with register merges:\n\u001b|0;31m\u2502 \u001b|0m\n\u001b|0;31m\u2502  \u001b|0mBlock b10 (exception handler)\n\u001b|0;31m\u2502 \u001b|0m  
\u001b|0m67/80: \u03c6\u1d40\u2091 <accumulator> (compressed) \u2192 |rax|R|t], live range: |67-69]\n\u001b|0;31m\u2502 \u001b|0m     \u001b|0m122: ConstantGapMove(n49 \u2192 |rcx|R|t])\n\u001b|0;31m\u2502 
\u001b|0m\u001b|0;34m  67 : SetPendingMessage\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m68/89: SetPendingMessage |v6/n49:|rcx|R|t]] \u2192 |rdx|R|t]\n\u001b|0;31m\u2502 \u001b|0m     \u001b|0m123: 
GapMove(|stack:2|w32] \u2192 |rcx|R|w32])\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:16:4)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  74 : AddSmi |40], |11]\n\u001b|m\u001b|0;31m\u2502 \u001b|0m          \u21b1 eager @74 (4 live 
vars)\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m69/92: Int32AddWithOverflow |v41/n54:|rcx|R|w32], v11/n90:|constant:v-1]] \u2192 |rcx|R|w32], live range: |69-70]\n\u001b|0;31m\u2502 \u001b|0m   \u001b|0m70/94:
Int32ToNumber |v69/n92:|rcx|R|w32]] \u2192 |rax|R|t], live range: |70-71]\n\u001b|0;31m\u2502 \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:2)\n\u001b|m\u001b|0;31m\u2502 \u001b|0m\u001b|0;34m  80 : Ldar r0\n\u001b|m\u001b|0;31m\u2502 \u001b|0m   \u001b|0m71/93: Jump b11\n\u001b|0;31m\u2502 
\u001b|0m       \u2502  with gap moves:\n\u001b|0;31m\u2502 \u001b|0m       \u2502    - v70/n94:|rax|R|t] \u2192 95: \u03c6\u1d40 r0 |rcx|R|t]\n\u001b|0;31m\u2502 \u001b|0m       \u2502  with register 
merges:\n\u001b|0;31m\u2502 \u001b|0m       \u25bc\n\u001b|0;31m\u2570\u2500\u25ba\u001b|0mBlock b11\n  \u001b|0m   \u001b|0m72/95: \u03c6\u1d40 r0 (n100, n94) (decompressed) \u2192 |rcx|R|t], live range: 
|72-74]\n  \u001b|0m     \u001b|0m124: ConstantGapMove(n96 \u2192 |rax|R|t])\n  \u001b|0m\u001b|1;34m0x38d600827cfd <SharedFunctionInfo foo> (0x38d600848eb9 <String|45]: 
\"regressions/maglev/regress/regress-1455295.js\">:18:16)\n\u001b|m  \u001b|0m\u001b|0;34m  82 : Return\n\u001b|m  \u001b|0m   \u001b|0m73/97: ReduceInterruptBudgetForReturn(82) |v2/n96:|rax|R|t]]\n  
\u001b|0m     \u001b|0m125: GapMove(|rcx|R|t] \u2192 |rax|R|t])\n  \u001b|0m   \u001b|0m74/98: Return |v72/n95:|rax|R|t]]\n  \u001b|0m\n0x125000800795 <Code MAGLEV>|completed compiling 0x38d600827d91 
<JSFunction foo (sfi = 0x38d600827cfd)> (target MAGLEV) - took 0.001, 8.466, 0.041 ms]\n42 42\n"}
[Step 3: Duration 5.27 seconds| Input tokens: 114,373 | Output tokens: 660]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'Maglev ArrayForEachLoop LazyDeoptContinuation forEach lowering site v8 code location', 'top_k': 5, 'topic_filter': 'v8'}                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/maglev.txt",
    "similarity": 0.477,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/maglev.txt\n\nMaglev - V8\u2019s Fastest Optimizing JIT \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
Maglev - V8\u2019s Fastest Optimizing JIT Published 05 December 2023 \u00b7 Tagged with JavaScript In Chrome M117 we introduced a new optimizing compiler: Maglev. Maglev sits between our existing Sparkplug 
and TurboFan compilers, and fills the role of a fast optimizing compiler that generates good enough code, fast enough. Background # Until 2021 V8 had two main execution tiers: Ignition, the interpreter; and 
TurboFan , V8\u2019s optimizing compiler focused on peak performance. All JavaScript code is first compiled to ignition bytecode, and executed by interpreting it. During execution V8 tracks how the program 
behaves, including tracking object shapes and types. Both the runtime execution metadata and bytecode are fed into the optimizing compiler to generate high-performance, often speculative, machine code that 
runs significantly faster than the interpreter can. These improvements are clearly visible on benchmarks like JetStream , a collection of traditional pure JavaScript benchmarks measuring startup, latency, and
peak performance. TurboFan helps V8 run the suite 4.35x as fast! JetStream has a reduced emphasis on steady state performance compared to past benchmarks (like the retired Octane benchmark ), but due to the 
simplicity of many line items, the optimized code is still where most time is spent. Speedometer is a different kind of benchmark suite than JetStream. It\u2019s designed to measure a web app\u2019s 
responsiveness by timing simulated user interactions. Instead of smaller static standalone JavaScript apps, the suite consists of full web pages, most of which are built using popular frameworks. Like during 
most web page loads, Speedometer line items spend much less time running tight JavaScript loops and much more executing a lot of code that interacts with the rest of the browser. TurboFan still has a lot of 
impact on Speedometer: it runs over 1.5x as fast! But the impact is clearly much more muted than on JetStream. Part of this difference results from the fact that full pages just spend less time in pure 
JavaScript . But in part it\u2019s due to the benchmark spending a lot of time in functions that don\u2019t get hot enough to be optimized by TurboFan. Web performance benchmarks comparing unoptimized and 
optimized execution All the benchmark scores in this post were measured with Chrome 117.0.5897.3 on a 13\u201d M2 Macbook Air. Since the difference in execution speed and compile time between Ignition and 
TurboFan is so large, in 2021 we introduced a new baseline JIT called Sparkplug . It\u2019s designed to compile bytecode to equivalent machine code almost instantaneously. On JetStream, Sparkplug improves 
performance quite a bit compared to Ignition (+45%). Even when TurboFan is also in the picture we still see a solid improvement in performance (+8%). On Speedometer we see a 41% improvement over Ignition, 
bringing it close to TurboFan performance, and a 22% improvement over Ignition + TurboFan! Since Sparkplug is so fast, we can easily deploy it very broadly and get a consistent speedup. If code doesn\u2019t 
rely solely on easily optimized, long-running, tight JavaScript loops, it\u2019s a great addition. Web performance benchmarks with added Sparkplug The simplicity of Sparkplug imposes a relatively low upper 
limit on the speedup it can provide though. This is clearly demonstrated by the large gap between Ignition + Sparkplug and Ignition + TurboFan. This is where Maglev comes in, our new optimizing JIT that 
generates code that\u2019s much faster than Sparkplug code, but is generated much faster than TurboFan can. Maglev: A Simple SSA-Based JIT compiler # When we started this project we saw two paths forward to 
cover the gap between Sparkplug and TurboFan: either try to generate better code using the single-pass approach taken by Sparkplug, or build a JIT with an intermediate representation (IR). Since we felt that 
not having an IR at all during compilation would likely severely restrict the compiler, we decided to go with a somewhat traditional static single-assignment (SSA) based approach, using a CFG (control flow 
graph) rather than TurboFan's more flexible but cache unfriendly sea-of-nodes representation. The compiler itself is designed to be fast and easy to work on. It has a minimal set of passes and a simple, 
single IR that encodes specialized JavaScript semantics. Prepass # First Maglev does a prepass over the bytecode to find branch targets, including loops, and assignments to variables in loop. This pass also 
collects liveness information, encoding which values in which variables are still needed across which expressions. This information can reduce the amount of state that needs to be tracked by the compiler 
later. SSA # A printout of the Maglev SSA graph on the command line Maglev does an abstract interpretation of the frame state, creating SSA nodes representing the results of expression evaluation. Variable 
assignments are emulated by storing those SSA nodes in the respective abstract interpreter register. In the case of branches and switches, all paths are evaluated. When multiple paths merge, values in 
abstract interpreter registers are merged by inserting so-called Phi nodes: value nodes that know which value to pick depending on which path was taken at runtime. Loops can merge variable values \u201cback 
in time\u201d, with the data flowing backwards from the loop end to the loop header, in the case when variables are assigned in the loop body. That\u2019s where the data from the prepass comes in handy: since
we already know which variables are assigned inside loops, we can pre-create loop phis before we even start processing the loop body. At the end of the loop we can populate the phi input with the correct SSA 
node. This allows the SSA graph generation to be a single forward pass, without needing to \"fix up\" loop variables, while also minimizing the amount of Phi nodes that need to be allocated. Known Node 
Information # To be as fast as possible, Maglev does as much as possible at once. Instead of building a generic JavaScript graph and then lowering that during later optimization phases, which is a 
theoretically clean but computationally expensive approach, Maglev does as much as possible immediately during graph building. During graph building Maglev will look at runtime feedback metadata collected 
during unoptimized execution, and generate specialized SSA nodes for the types observed. If Maglev sees o.x and knows from the runtime feedback that o always has one specific shape, it will generate an SSA 
node to check at runtime that o still has the expected shape, followed by a cheap LoadField node which does a simple access by offset. Additionally, Maglev will make a side node that it now knows the shape of
o , making it unnecessary to check the shape again later. If Maglev later encounters an operation on o that doesn't have feedback for some reason, this kind of information learned during compilation can be 
used as a second source of feedback. Runtime information can come in various forms. Some information needs to be checked at runtime, like the shape check previously described. Other information can be used 
without runtime checks by registering dependencies to the runtime. Globals that are de-facto constant (not changed between initialization and when their value is seen by Maglev) fall into this category: 
Maglev does not need to generate code to dynamically load and check their identity. Maglev can load the value at compile time and embed it directly into the machine code; if the runtime ever mutates that 
global, it'll also take care to invalidate and deoptimize that machine code. Some forms of information are \u201cunstable\u201d. Such information can only be used to the extent that the compiler knows for 
sure that it can\u2019t change. For example, if we just allocated an object, we know it\u2019s a new object and we can skip expensive write barriers entirely. Once there has been another potential allocation,
the garbage collector could have moved the object, and we now need to emit such checks. Others are \"stable\": if we have never seen any object transition away from having a certain shape, then we can 
register a dependency on this event (any object transitioning away from that particular shape) and don\u2019t need to recheck the shape of the object, even after a call to an unknown function with unknown 
side effects. Deoptimization # Given that Maglev can use speculative information that it checks at runtime, Maglev code needs to be able to deoptimize. To make this work, Maglev attaches abstract interpreter 
frame state to nodes that can deoptimize. This state maps interpreter registers to SSA values. This state turns into metadata during code generation, providing a mapping from optimized state to unoptimized 
state. The deoptimizer interprets this data, reading values from the interpreter frame and machine registers and putting them into the required places for interpretation. This builds on the same 
deoptimization mechanism as used by TurboFan, allowing us to share most of the logic and take advantage of the testing of the existing system. Representation Selection # JavaScript numbers represent, 
according to the spec , a 64-bit floating point value. This doesn't mean that the engine has to always store them as 64-bit floats though, especially since In practice many numbers are small integers (e.g. 
array indices). V8 tries to encode numbers as 31-bit tagged integers (internally called \u201cSmall Integers\u201d or \"Smi\"), both to save memory (32bit due to pointer compression ), and for performance 
(integer operations are faster than float operations). To make numerics-heavy JavaScript code fast, it\u2019s important that optimal representations are chosen for value nodes. Unlike the interpreter and 
Sparkplug, the optimizing compiler can unbox values once it knows their type, operating on raw numbers rather than JavaScript values representing numbers, and rebox values only if strictly necessary. Floats 
can directly be passed in floating point registers instead of allocating a heap object that contains the float. Maglev learns about the representation of SSA nodes mainly by looking at runtime feedback of 
e.g., binary operations, and propagating that information forwards through the Known Node Info mechanism. When SSA values with specific representations flow into Phis, a correct representation that supports 
all the inputs needs to be chosen. Loop phis are again tricky, since inputs from within the loop are seen after a representation should be chosen for the phi \u2014 the same \"back in time\" problem as for 
graph building. This is why Maglev has a separate phase after graph building to do representation selection on loop phis. Register Allocation # After graph building and representation selection, Maglev mostly
knows what kind of code it wants to generate, and is \"done\" from a classical optimization point of view. To be able to generate code though, we need to choose where SSA values actually live when executing 
machine code; when they're in machine registers, and when they're saved on the stack. This is done through register allocation. Each Maglev node has input and output requirements, including requirements on 
temporaries needed. The register allocator does a single forward walk over the graph, maintaining an abstract machine register state not too dissimilar from the abstract interpretation state maintained during
graph building, and will satisfy those requirements, replacing the requirements on the node with actual locations. Those locations can then be used by code generation. First, a prepass runs over the graph to 
find linear live ranges of nodes, so that we can free up registers once an SSA node isn\u2019t needed anymore. This prepass also keeps track of the chain of uses. Knowing how far in the future a value is 
needed can be useful to decide which values to prioritize, and which to drop, when we run out of registers. After the prepass, the register allocation runs. Register assignment follows some simple, local 
rules: If a value is already in a register, that register is used if possible. Nodes keep track of what registers they are stored into during the graph walk. If the node doesn\u2019t yet have a register, but 
a register is free, it\u2019s picked. The node gets updated to indicate it\u2019s in the register, and the abstract register state is updated to know it contains the node. If there\u2019s no free register, 
but a register is required, another value is pushed out of the register. Ideally, we have a node that\u2019s already in a different register, and can drop this \"for free\"; otherwise we pick a value that 
won\u2019t be needed for a long time, and spill it onto the stack. On branch merges, the abstract register states from the incoming branches are merged. We try to keep as many values in registers as possible.
This can mean we need to introduce register-to-register moves, or may need to unspill values from the stack, using moves called \u201cgap moves\u201d. If a branch merge has a phi node, register allocation 
will assign output registers to the phis. Maglev prefers to output phis to the same registers as its inputs, to minimize moves. If more SSA values are live than we have registers, we\u2019ll need to spill 
some values on the stack, and unspill them later. In the spirit of Maglev, we keep it simple: if a value needs to be spilled, it is retroactively told to immediately spill on definition (right after the value
is created), and code generation will handle emitting the spill code. The definition is guaranteed to \u2018dominate\u2019 all uses of the value (to reach the use we must have passed through the definition 
and therefore the spill code). This also means that a spilled value will have exactly one spill slot for the entire duration of the code; values with overlapping lifetimes will thus have non-overlapping 
assigned spill slots. Due to representation selection, some values in the Maglev frame will be tagged pointers, pointers that V8\u2019s GC understands and needs to consider; and some will be untagged, values 
that the GC should not look at. TurboFan handles this by precisely keeping track of which stack slots contain tagged values, and which contain untagged values, which changes during execution as slots are 
reused for different values. For Maglev we decided to keep things simpler, to reduce the memory required for tracking this: we split the stack frame into a tagged and an untagged region, and only store this 
split point. Code Generation # Once we know what expressions we want to generate code for, and where we want to put their outputs and inputs, Maglev is ready to generate code. Maglev nodes directly know how 
to generate assembly code using a \u201cmacro assembler\u201d. For example, a CheckMap node knows how to emit assembler instructions that compare the shape (internally called the \u201cmap\u201d) of an input 
object with a known value, and to deoptimize the code if the object had a wrong shape. One slightly tricky bit of code handles gap moves: The requested moves created by the register allocator know that a 
value lives somewhere and needs to go elsewhere. If there\u2019s a sequence of such moves though, a preceding move could clobber the input needed by a subsequent move. The Parallel Move Resolver computes how 
to safely perform the moves so that all values end up in the right place. Results # So the compiler we just presented is both clearly much more complex than Sparkplug, and much simpler than TurboFan. How does
it fare? In terms of compilation speed we\u2019ve managed to build a JIT that\u2019s roughly 10x slower than Sparkplug, and 10x faster than TurboFan. Compile time comparison of the compilation tiers, for all 
functions compiled in JetStream This allows us to deploy Maglev much earlier than we\u2019d want to deploy TurboFan. If the feedback it relied upon ended up not being very stable yet, there\u2019s no huge 
cost to deoptimizing and recompiling later. It also allows us to use TurboFan a little later: we\u2019re running much faster than we\u2019d run with Sparkplug. Slotting in Maglev between Sparkplug and 
TurboFan results in noticeable benchmark improvements: Web performance benchmarks with Maglev We have also validated Maglev on real-world data, and see good improvements on Core Web Vitals . Since Maglev 
compiles much faster, and since we can now afford to wait longer before we compile functions with TurboFan, this results in a secondary benefit that\u2019s not as visible on the surface. The benchmarks focus 
on main-thread latency, but Maglev also significantly reduces V8\u2019s overall resource consumption by using less off-thread CPU time. The energy consumption of a process can be measured easily on an M1- or 
M2-based Macbook using taskinfo . Benchmark Energy Consumption JetStream -3.5% Speedometer -10% Maglev isn\u2019t complete by any means. We've still got plenty more work to do, more ideas to try out, and more
low-hanging fruit to pick \u2014 as Maglev gets more complete, we\u2019ll expect to see higher scores, and more reduction in energy consumption. Maglev is now available for desktop Chrome now, and will be 
rolled out to mobile devices soon. Posted by Toon Verwaest , Leszek Swirski , Victor Gomes , Olivier Fl\u00fcckiger, Darius Mercadier, and Camillo Bruni \u2014 not enough cooks to spoil the broth. Branding 
Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the
Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/spread-elements.txt",
    "similarity": 0.448,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/spread-elements.txt\n\nSpeeding up spread elements \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
Speeding up spread elements Published 04 December 2018 \u00b7 Tagged with ECMAScript benchmarks During his three-months internship on the V8 team, Hai Dang worked on improving the performance of |...array] , 
|...string] , |...set] , |...map.keys()] , and |...map.values()] (when the spread elements are at the start of the array literal). He even made Array.from(iterable) much faster as well. This article explains 
some of the gory details of his changes, which are included in V8 starting with v7.2. Spread elements # Spread elements are components of array literals that have the form ...iterable . They were introduced 
in ES2015 as a way to create arrays from iterable objects. For example, the array literal |1, ...arr, 4, ...b] creates an array whose first element is 1 followed by the elements of the array arr , then 4 , 
and finally the elements of the array b : const a = | 2 , 3 ] ; const b = | 5 , 6 , 7 ] ; const result = | 1 , ... a , 4 , ... b ] ; // \u2192 |1, 2, 3, 4, 5, 6, 7] As another example, any string can be 
spread to create an array of its characters (Unicode code points): const str = '\u3053\u3093\u306b\u3061\u306f' ; const result = | ... str ] ; // \u2192 |'\u3053', '\u3093', '\u306b', '\u3061', '\u306f'] 
Similarly, any set can be spread to create an array of its elements, sorted by insertion order: const s = new Set ( ) ; s . add ( 'V8' ) ; s . add ( 'TurboFan' ) ; const result = | ... s ] ; // \u2192 |'V8', 
'TurboFan'] In general, the spread elements syntax ...x in an array literal assumes that x provides an iterator (accessible through x|Symbol.iterator]() ). This iterator is then used to obtain the elements to
be inserted into the resulting array. The simple use case of spreading an array arr into a new array, without adding any further elements before or behind, |...arr] , is considered a concise, idiomatic way to
shallow-clone arr in ES2015. Unfortunately, in V8, the performance of this idiom lagged far behind its ES5 counterpart. The goal of Hai\u2019s internship was to change that! Why is (or were!) spread elements 
slow? # There are many ways to shallow-clone an array arr . For instance, you can use arr.slice() , or arr.concat() , or |...arr] . Or, you can write your own clone function that employs a standard for -loop:
function clone ( arr ) { // Pre-allocate the correct number of elements, to avoid // having to grow the array. const result = new Array ( arr . length ) ; for ( let i = 0 ; i < arr . length ; i ++ ) { result 
| i ] = arr | i ] ; } return result ; } Ideally, all these options would have similar performance characteristics. Unfortunately, if you pick |...arr] in V8, it is (or was ) likely to be slower than clone ! 
The reason is that V8 essentially transpiles |...arr] into an iteration like the following: function ( arr ) { const result = | ] ; const iterator = arr | Symbol . iterator ] ( ) ; const next = iterator . 
next ; for ( ; ; ) { const iteratorResult = next . call ( iterator ) ; if ( iteratorResult . done ) break ; result . push ( iteratorResult . value ) ; } return result ; } This code is generally slower than 
clone for a few reasons: It needs to create the iterator at the beginning by loading and evaluating the Symbol.iterator property. It needs to create and query the iteratorResult object at every step. It grows
the result array at every step of the iteration by calling push , thus repeatedly reallocating the backing store. The reason for using such an implementation is that, as mentioned earlier, spreading can be 
done not only on arrays but, in fact, on arbitrary iterable objects, and must follow the iteration protocol . Nevertheless, V8 should be smart enough to recognize if the object being spread is an array such 
that it can perform the elements extraction at a lower level and thereby: avoid the creation of the iterator object, avoid the creation of the iterator result objects, and avoid continuously growing and thus 
reallocating the result array (we know the number of elements in advance). We implemented this simple idea using CSA for fast arrays, i.e. arrays with one of the six most common elements kinds . The 
optimization applies for the common real-world scenario where the spread occurs at the start of the array literal, e.g. |...foo] . As shown in the graph below, this new fast path yields roughly a 3\u00d7 
performance improvement for spreading an array of length 100,000, making it about 25% faster than the hand-written clone loop. Performance improvement of spreading a fast array Note: While not shown here, the
fast path also applies when the spread elements are followed by other components (e.g. |...arr, 1, 2, 3] ), but not when they are preceded by others (e.g. |1, 2, 3, ...arr] ). Tread carefully down that fast 
path # That\u2019s clearly an impressive speedup, but we must be very careful about when it is correct to take this fast path: JavaScript allows the programmer to modify the iteration behavior of objects 
(even arrays) in various ways. Because spread elements are specified to use the iteration protocol, we need to ensure that such modifications are respected. We do so by avoiding the fast path completely 
whenever the original iteration machinery has been mutated. For example, this includes situations like the following. Own Symbol.iterator property # Normally, an array arr does not have its own 
Symbol.iterator property, so when looking up that symbol, it will be found on the array\u2019s prototype. In the example below, the prototype is bypassed by defining the Symbol.iterator property directly on 
arr itself. After this modification, looking up Symbol.iterator on arr results in an empty iterator, and thus the spread of arr yields no elements and the array literal evaluates to an empty array. const arr 
= | 1 , 2 , 3 ] ; arr | Symbol . iterator ] = function ( ) { return { next : function ( ) { return { done : true } ; } } ; } ; const result = | ... arr ] ; // \u2192 |] Modified %ArrayIteratorPrototype% # The
next method can also be modified directly on %ArrayIteratorPrototype% , the prototype of array iterators (which affects all arrays). Object . getPrototypeOf ( | ] | Symbol . iterator ] ( ) ) . next = function
( ) { return { done : true } ; } const arr = | 1 , 2 , 3 ] ; const result = | ... arr ] ; // \u2192 |] Dealing with holey arrays # Extra care is also needed when copying arrays with holes, i.e., arrays like 
|'a', , 'c'] that are missing some elements. Spreading such an array, by virtue of adhering to the iteration protocol, does not preserve the holes but instead fills them with the values found in the 
array\u2019s prototype at the corresponding indices. By default there are no elements in an array\u2019s prototype, which means that any holes are filled with undefined . For example, |...|'a', , 'c']] 
evaluates to a new array |'a', undefined, 'c'] . Our fast path is smart enough to handle holes in this default situation. Instead of blindly copying the input array\u2019s backing store, it watches out for 
holes and takes care of converting them to undefined values. The graph below contains measurements for an input array of length 100,000 containing only (tagged) 600 integers \u2014 the rest are holes. It 
shows that spreading such a holey array is now over 4\u00d7 faster than using the clone function. (They used to be roughly on par, but this is not shown in the graph). Note that although slice is included in 
this graph, the comparison with it is unfair because slice has a different semantics for holey arrays: it preserves all the holes, so it has much less work to do. Performance improvement of spreading a holey 
array of integers ( HOLEY_SMI_ELEMENTS ) The filling of holes with undefined that our fast path has to perform is not as simple as it sounds: it may require converting the whole array to a different elements 
kind. The next graph measures such a situation. The setup is the same as above, except that this time the 600 array elements are unboxed doubles and the array has the HOLEY_DOUBLE_ELEMENTS elements kind. 
Since this elements kind cannot hold tagged values such as undefined , spreading involves a costly elements kind transition, which is why the score for |...a] is much lower than in the previous graph. 
Nevertheless, it is still much faster than clone(a) . Performance improvement of spreading a holey array of doubles ( HOLEY_DOUBLE_ELEMENTS ) Spreading strings, sets, and maps # The idea of skipping the 
iterator object and avoiding growing the result array equally applies to spreading other standard data types. Indeed, we implemented similar fast paths for primitive strings, for sets, and for maps, each time
taking care to bypass them in the presence of modified iteration behavior. Concerning sets, the fast path supports not only spreading a set directly (|...set]), but also spreading its keys iterator ( 
|...set.keys()] ) and its values iterator ( |...set.values()] ). In our micro-benchmarks, these operations are now about 18\u00d7 faster than before. The fast path for maps is similar but does not support 
spreading a map directly ( |...map] ), because we consider this an uncommon operation. For the same reason, neither fast path supports the entries() iterator. In our micro-benchmarks, these operations are now
about 14\u00d7 faster than before. For spreading strings ( |...string] ), we measured a roughly 5\u00d7 improvement, as shown in the graph below by the purple and green lines. Note that this is even faster 
than a TurboFan-optimized for-of-loop (TurboFan understands string iteration and can generate optimized code for it), represented by the blue and pink lines. The reason for having two plots in each case is 
that the micro-benchmarks operate on two different string representations (one-byte strings and two-byte strings). Performance improvement of spreading a string Performance improvement of spreading a set with
100,000 integers (magenta, about 18\u00d7), shown here in comparison with a for - of loop (red) Improving Array.from performance # Fortunately, our fast paths for spread elements can be reused for Array.from 
in the case where Array.from is called with an iterable object and without a mapping function, for example, Array.from(|1, 2, 3]) . The reuse is possible because in this case, the behavior of Array.from is 
exactly the same as that of spreading. It results in an enormous performance improvement, shown below for an array with 100 doubles. Performance improvement of Array.from(array) where array contains 100 
doubles Conclusion # V8 v7.2 / Chrome 72 greatly improves the performance of spread elements when they occur at the front of the array literal, for example |...x] or |...x, 1, 2] . The improvement applies to 
spreading arrays, primitive strings, sets, maps keys, maps values, and \u2014 by extension \u2014 to Array.from(x) . Posted by Hai Dang & Georg Neis. Retweet this article! Branding Terms Privacy Twitter Edit 
this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons 
Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/csa.txt",
    "similarity": 0.437,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/csa.txt\n\nTaming architecture complexity in V8 \u2014 the CodeStubAssembler \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm
features Research Taming architecture complexity in V8 \u2014 the CodeStubAssembler Published 16 November 2017 \u00b7 Tagged with internals In this post we\u2019d like to introduce the CodeStubAssembler 
(CSA), a component in V8 that has been a very useful tool in achieving some big performance wins over the last several V8 releases. The CSA also significantly improved the V8 team\u2019s ability to quickly 
optimize JavaScript features at a low-level with a high degree of reliability, which improved the team\u2019s development velocity. A brief history of builtins and hand-written assembly in V8 # To understand 
the CSA\u2019s role in V8, it\u2019s important to understand a little bit of the context and history that led to its development. V8 squeezes performance out of JavaScript using a combination of techniques. 
For JavaScript code that runs a long time, V8\u2019s TurboFan optimizing compiler does a great job of speeding up the entire spectrum of ES2015+ functionality for peak performance. However, V8 also needs to 
execute short-running JavaScript efficiently for good baseline performance. This is especially the case for the so-called builtin functions on the pre-defined objects that are available to all JavaScript 
programs as defined by the ECMAScript specification . Historically, many of these builtin functions were self-hosted , that is, they were authored by a V8 developer in JavaScript\u2014albeit a special 
V8-internal dialect. To achieve good performance, these self-hosted builtins rely on the same mechanisms V8 uses to optimize user-supplied JavaScript. As with user-supplied code, the self-hosted builtins 
require a warm-up phase in which type feedback is gathered and they need to be compiled by the optimizing compiler. Although this technique provides good builtin performance in some situations, it\u2019s 
possible to do better. The exact semantics of the pre-defined functions on the Array.prototype are specified in exquisite detail in the spec. For important and common special cases, V8\u2019s implementers 
know in advance exactly how these builtin functions should work by understanding the specification, and they use this knowledge to carefully craft custom, hand-tuned versions up front. These optimized 
builtins handle common cases without warm-up or the need to invoke the optimizing compiler, since by construction baseline performance is already optimal upon first invocation. To squeeze the best performance
out of hand-written built-in JavaScript functions (and from other fast-path V8 code that are also somewhat confusingly called builtins), V8 developers traditionally wrote optimized builtins in assembly 
language. By using assembly, the hand-written builtin functions were especially fast by, among other things, avoiding expensive calls to V8\u2019s C++ code via trampolines and by taking advantage of V8\u2019s
custom register-based ABI that it uses internally to call JavaScript functions. Because of the advantages of hand-written assembly, V8 accumulated literally tens of thousands of lines of hand-written assembly
code for builtins over the years\u2026 per platform . All of these hand-written assembly builtins were great for improving performance, but new language features are always being standardized, and maintaining
and extending this hand-written assembly was laborious and error-prone. Enter the CodeStubAssembler # V8 developers wrestled with a dilemma for many years: is it possible to create builtins that have the 
advantage of hand-written assembly without also being fragile and difficult to maintain? With the advent of TurboFan the answer to this question is finally \u201cyes\u201d. TurboFan\u2019s backend uses a 
cross-platform intermediate representation (IR) for low-level machine operations. This low-level machine IR is input to an instruction selector, register allocator, instruction scheduler and code generator 
that produce very good code on all platforms. The backend also knows about many of the tricks that are used in V8\u2019s hand-written assembly builtins\u2014e.g. how to use and call a custom register-based 
ABI, how to support machine-level tail calls, and how to elide the construction of stack frames in leaf functions. That knowledge makes the TurboFan backend especially well-suited for generating fast code 
that integrates well with the rest of V8. This combination of functionality made a robust and maintainable alternative to hand-written assembly builtins feasible for the first time. The team built a new V8 
component\u2014dubbed the CodeStubAssembler or CSA\u2014that defines a portable assembly language built on top of TurboFan\u2019s backend. The CSA adds an API to generate TurboFan machine-level IR directly 
without having to write and parse JavaScript or apply TurboFan\u2019s JavaScript-specific optimizations. Although this fast-path to code generation is something that only V8 developers can use to speed up the
V8 engine internally, this efficient path for generating optimized assembly code in a cross-platform way directly benefits all developers\u2019 JavaScript code in the builtins constructed with the CSA, 
including the performance-critical bytecode handlers for V8\u2019s interpreter, Ignition . The CSA and JavaScript compilation pipelines The CSA interface includes operations that are very low-level and 
familiar to anybody who has ever written assembly code. For example, it includes functionality like \u201cload this object pointer from a given address\u201d and \u201cmultiply these two 32-bit numbers\u201d.
The CSA has type verification at the IR level to catch many correctness bugs at compile time rather than runtime. For example, it can ensure that a V8 developer doesn\u2019t accidentally use an object pointer
that is loaded from memory as the input for a 32-bit multiplication. This kind of type verification is simply not possible with hand-written assembly stubs. A CSA test-drive # To get a better idea of what the
CSA offers, let\u2019s go through a quick example. We\u2019ll add a new internal builtin to V8 that returns the string length from an object if it is a String. If the input object is not a String, the builtin
will return undefined . First, we add a line to the BUILTIN_LIST_BASE macro in V8\u2019s builtin-definitions.h file that declares the new builtin called GetStringLength and specifies that it has a single 
input parameter that is identified with the constant kInputObject : TFS ( GetStringLength , kInputObject ) The TFS macro declares the builtin as a T urbo F an builtin using standard Code S tub linkage, which 
simply means that it uses the CSA to generate its code and expects parameters to be passed via registers. We can then define the contents of the builtin in builtins-string-gen.cc : TF_BUILTIN ( 
GetStringLength , CodeStubAssembler ) { Label not_string ( this ) ; // Fetch the incoming object using the constant we defined for // the first parameter. Node * const maybe_string = Parameter ( Descriptor ::
kInputObject ) ; // Check to see if input is a Smi (a special representation // of small numbers). This needs to be done before the IsString // check below, since IsString assumes its argument is an // object
pointer and not a Smi. If the argument is indeed a // Smi, jump to the label |not_string|. GotoIf ( TaggedIsSmi ( maybe_string ) , & not_string ) ; // Check to see if the input object is a string. If not, 
jump to // the label |not_string|. GotoIfNot ( IsString ( maybe_string ) , & not_string ) ; // Load the length of the string (having ended up in this code // path because we verified it was string above) and 
return it // using a CSA \"macro\" LoadStringLength. Return ( LoadStringLength ( maybe_string ) ) ; // Define the location of label that is the target of the failed // IsString check above. BIND ( & 
not_string ) ; // Input object isn't a string. Return the JavaScript undefined // constant. Return ( UndefinedConstant ( ) ) ; } Note that in the example above, there are two types of instructions used. There
are primitive CSA instructions that translate directly into one or two assembly instructions like GotoIf and Return . There are a fixed set of pre-defined CSA primitive instructions roughly corresponding to 
the most commonly used assembly instructions you would find on one of V8\u2019s supported chip architectures. Others instructions in the example are macro instructions, like LoadStringLength , TaggedIsSmi , 
and IsString , that are convenience functions to output one or more primitive or macro instructions inline. Macro instructions are used to encapsulate commonly used V8 implementation idioms for easy reuse. 
They can be arbitrarily long and new macro instructions can be easily defined by V8 developers whenever needed. After compiling V8 with the above changes, we can run mksnapshot , the tool that compiles 
builtins to prepare them for V8\u2019s snapshot, with the --print-code command-line option. This options prints the generated assembly code for each builtin. If we grep for GetStringLength in the output, we 
get the following result on x64 (the code output is cleaned up a bit to make it more readable): test al , 0x1 jz not_string movq rbx , | rax - 0x1 ] cmpb | rbx + 0xb ] , 0x80 jnc not_string movq rax , | rax +
0xf ] retl not_string: movq rax , | r13 - 0x60 ] retl On 32-bit ARM platforms, the following code is generated by mksnapshot : tst r0 , #1 beq + 28 - > not_string ldr r1 , | r0 , #-1 ] ldrb r1 , | r1 , #+7 ] 
cmp r1 , #128 bge + 12 - > not_string ldr r0 , | r0 , #+7 ] bx lr not_string: ldr r0 , | r10 , #+16 ] bx lr Even though our new builtin uses a non-standard (at least non-C++) calling convention, it\u2019s 
possible to write test cases for it. The following code can be added to test-run-stubs.cc to test the builtin on all platforms: TEST ( GetStringLength ) { HandleAndZoneScope scope ; Isolate * isolate = scope 
. main_isolate ( ) ; Heap * heap = isolate -> heap ( ) ; Zone * zone = scope . main_zone ( ) ; // Test the case where input is a string StubTester tester ( isolate , zone , Builtins :: kGetStringLength ) ; 
Handle < String > input_string ( isolate -> factory ( ) -> NewStringFromAsciiChecked ( \"Oktoberfest\" ) ) ; Handle < Object > result1 = tester . Call ( input_string ) ; CHECK_EQ ( 11 , Handle < Smi > :: cast
( result1 ) -> value ( ) ) ; // Test the case where input is not a string (e.g. undefined) Handle < Object > result2 = tester . Call ( factory -> undefined_value ( ) ) ; CHECK ( result2 -> IsUndefined ( 
isolate ) ) ; } For more details about using the CSA for different kinds of builtins and for further examples, see this wiki page . A V8 developer velocity multiplier # The CSA is more than just an universal 
assembly language that targets multiple platforms. It enables much quicker turnaround when implementing new features compared to hand-writing code for each architecture as we used to do. It does this by 
providing all of the benefits of hand-written assembly while protecting developers against its most treacherous pitfalls: With the CSA, developers can write builtin code with a cross-platform set of low-level
primitives that translate directly to assembly instructions. The CSA\u2019s instruction selector ensures that this code is optimal on all of the platforms that V8 targets without requiring V8 developers to be
experts in each of those platform\u2019s assembly languages. The CSA\u2019s interface has optional types to ensure that the values manipulated by the low-level generated assembly are of the type that the code
author expects. Register allocation between assembly instructions is done by the CSA automatically rather than explicitly by hand, including building stack frames and spilling values to the stack if a builtin
uses more registers than available or makes call. This eliminates a whole class of subtle, hard-to-find bugs that plagued hand-written assembly builtins. By making the generated code less fragile the CSA 
drastically reduces the time required to write correct low-level builtins. The CSA understands ABI calling conventions\u2014both standard C++ and internal V8 register-based ones\u2014making it possible to 
easily interoperate between CSA-generated code and other parts of V8. Since CSA code is C++, it\u2019s easy to encapsulate common code generation patterns in macros that can be easily reused in many builtins.
Because V8 uses the CSA to generate the bytecode handlers for Ignition, it is very easy to inline the functionality of CSA-based builtins directly into the handlers to improve the interpreter\u2019s 
performance. V8\u2019s testing framework supports testing CSA functionality and CSA-generated builtins from C++ without having to write assembly adapters. All in all, the CSA has been a game changer for V8 
development. It has significantly improved the team\u2019s ability to optimize V8. That means we are able to optimize more of the JavaScript language faster for V8\u2019s embedders. Posted by Daniel Clifford 
, CodeStubAssembler assembler. Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s 
BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/features/at-method.txt",
    "similarity": 0.437,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/features/at-method.txt\n\nat method for relative indexing \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research at 
method for relative indexing Published 13 July 2021 \u00b7 Tagged with ECMAScript The new at method on Array.prototype , the various TypedArray prototypes, and String.prototype makes accessing an element 
nearer to the end of the collection easier and more succinct. Accessing the Nth element from the end of a collection is a common operation. However, the usual ways to do so are verbose, like 
my_array|my_array.length - N] , or might not be performant, like my_array.slice(-N)|0] . The new at method makes this operation more ergonomic by interpreting negative indices to mean \"from the end\". The 
previous examples may be expressed as my_array.at(-N) . For uniformity, positive indices are also supported, and are equivalent to ordinary property access. This new method is small enough that its full 
semantics may be understood by this compliant polyfill implementation below: function at ( n ) { // Convert the argument to an integer n = Math . trunc ( n ) || 0 ; // Allow negative indexing from the end if 
( n < 0 ) n += this . length ; // Out-of-bounds access returns undefined if ( n < 0 || n >= this . length ) return undefined ; // Otherwise, this is just normal property access return this | n ] ; } A word 
about Strings # Since at ultimately performs ordinary indexing, calling at on String values returns code units, just as ordinary indexing would. And like ordinary indexing on Strings, code units may not be 
what you want for Unicode strings! Please consider if String.prototype.codePointAt() is more appropriate for your use case. at method support # Chrome: supported since version 92 Firefox: supported since 
version 90 Safari: no support Node.js: no support Babel: supported about this feature support listing Posted by Shu-yu Guo ( @_shu ). Branding Terms Privacy Twitter Edit this page on GitHub Except as 
otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, 
see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/lazy-deserialization.txt",
    "similarity": 0.436,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/lazy-deserialization.txt\n\nLazy deserialization \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Lazy 
deserialization Published 12 February 2018 \u00b7 Tagged with internals TL;DR: Lazy deserialization was recently enabled by default in V8 v6.4 , reducing V8\u2019s memory consumption by over 500 KB per 
browser tab on average. Read on to find out more! Introducing V8 snapshots # But first, let\u2019s take a step back and have a look at how V8 uses heap snapshots to speed up creation of new Isolates (which 
roughly correspond to a browser tab in Chrome). My colleague Yang Guo gave a good introduction on that front in his article on custom startup snapshots : The JavaScript specification includes a lot of 
built-in functionality, from math functions to a full-featured regular expression engine. Every newly-created V8 context has these functions available from the start. For this to work, the global object (for 
example, the window object in a browser) and all the built-in functionality must be set up and initialized into V8\u2019s heap at the time the context is created. It takes quite some time to do this from 
scratch. Fortunately, V8 uses a shortcut to speed things up: just like thawing a frozen pizza for a quick dinner, we deserialize a previously-prepared snapshot directly into the heap to get an initialized 
context. On a regular desktop computer, this can bring the time to create a context from 40 ms down to less than 2 ms. On an average mobile phone, this could mean a difference between 270 ms and 10 ms. To 
recap: snapshots are critical for startup performance, and they are deserialized to create the initial state of V8\u2019s heap for each Isolate. The size of the snapshot thus determines the minimum size of 
the V8 heap, and larger snapshots translate directly into higher memory consumption for each Isolate. A snapshot contains everything needed to fully initialize a new Isolate, including language constants 
(e.g., the undefined value), internal bytecode handlers used by the interpreter, built-in objects (e.g., String ), and the functions installed on built-in objects (e.g., String.prototype.replace ) together 
with their executable Code objects. Startup snapshot size in bytes from 2016-01 to 2017-09. The x-axis shows V8 revision numbers. Over the past two years, the snapshot has nearly tripled in size, going from 
roughly 600 KB in early 2016 to over 1500 KB today. The vast majority of this increase comes from serialized Code objects, which have both increased in count (e.g., through recent additions to the JavaScript 
language as the language specification evolves and grows); and in size (built-ins generated by the new CodeStubAssembler pipeline ship as native code vs. the more compact bytecode or minimized JS formats). 
This is bad news, since we\u2019d like to keep memory consumption as low as possible. Lazy deserialization # One of the major pain points was that we used to copy the entire content of the snapshot into each 
Isolate. Doing so was especially wasteful for built-in functions, which were all loaded unconditionally but may never have ended up being used. This is where lazy deserialization comes in. The concept is 
quite simple: what if we were to only deserialize built-in functions just before they were called? A quick investigation of some of the most popular websites showed this approach to be quite attractive: on 
average, only 30% of all built-in functions were used, with some sites only using 16%. This looked remarkably promising, given that most of these sites are heavy JS users and these numbers can thus be seen as
a (fuzzy) lower bound of potential memory savings for the web in general. As we began working on this direction, it turned out that lazy deserialization integrated very well with V8\u2019s architecture and 
there were only a few, mostly non-invasive design changes necessary to get up and running: Well-known positions within the snapshot. Prior to lazy deserialization, the order of objects within the serialized 
snapshot was irrelevant since we\u2019d only ever deserialize the entire heap at once. Lazy deserialization must be able to deserialize any given built-in function on its own, and therefore has to know where 
it is located within the snapshot. Deserialization of single objects. V8\u2019s snapshots were initially designed for full heap deserialization, and bolting on support for single-object deserialization 
required dealing with a few quirks such as non-contiguous snapshot layout (serialized data for one object could be interspersed with data for other objects) and so-called backreferences (which can directly 
reference objects previously deserialized within the current run). The lazy deserialization mechanism itself. At runtime, the lazy deserialization handler must be able to a) determine which code object to 
deserialize, b) perform the actual deserialization, and c) attach the serialized code object to all relevant functions. Our solution to the first two points was to add a new dedicated built-ins area to the 
snapshot, which may only contain serialized code objects. Serialization occurs in a well-defined order and the starting offset of each Code object is kept in a dedicated section within the built-ins snapshot 
area. Both back-references and interspersed object data are disallowed. Lazy built-in deserialization is handled by the aptly named DeserializeLazy built-in , which is installed on all lazy built-in functions
at deserialization time. When called at runtime, it deserializes the relevant Code object and finally installs it on both the JSFunction (representing the function object) and the SharedFunctionInfo (shared 
between functions created from the same function literal). Each built-in function is deserialized at most once. In addition to built-in functions, we have also implemented lazy deserialization for bytecode 
handlers . Bytecode handlers are code objects that contain the logic to execute each bytecode within V8\u2019s Ignition interpreter. Unlike built-ins, they neither have an attached JSFunction nor a 
SharedFunctionInfo . Instead, their code objects are stored directly in the dispatch table into which the interpreter indexes when dispatching to the next bytecode handler. Lazy deserialization is similar as 
to built-ins: the DeserializeLazy handler determines which handler to deserialize by inspecting the bytecode array, deserializes the code object, and finally stores the deserialized handler in the dispatch 
table. Again, each handler is deserialized at most once. Results # We evaluated memory savings by loading the top 1000 most popular websites using Chrome 65 on an Android device, with and without lazy 
deserialization. On average, V8\u2019s heap size decreased by 540 KB, with 25% of the tested sites saving more than 620 KB, 50% saving more than 540 KB, and 75% saving more than 420 KB. Runtime performance 
(measured on standard JS benchmarks such as Speedometer, as well as a wide selection of popular websites) has remained unaffected by lazy deserialization. Next steps # Lazy deserialization ensures that each 
Isolate only loads the built-in code objects that are actually used. That is already a big win, but we believe it is possible to go one step further and reduce the (built-in-related) cost of each Isolate to 
effectively zero. We hope to bring you updates on this front later this year. Stay tuned! Posted by Jakob Gruber ( @schuay ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub 
Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For
details, see our site policies ."
  }
]
[Step 4: Duration 24.89 seconds| Input tokens: 225,940 | Output tokens: 1,357]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'site:v8 src maglev Array forEach inlining ArrayForEachLoop Maglev codegen ThrowIfNotCallable deopt continuation', 'top_k': 5,               │
│ 'topic_filter': 'v8'}                                                                                                                                                                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/maglev.txt",
    "similarity": 0.473,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/maglev.txt\n\nMaglev - V8\u2019s Fastest Optimizing JIT \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
Maglev - V8\u2019s Fastest Optimizing JIT Published 05 December 2023 \u00b7 Tagged with JavaScript In Chrome M117 we introduced a new optimizing compiler: Maglev. Maglev sits between our existing Sparkplug 
and TurboFan compilers, and fills the role of a fast optimizing compiler that generates good enough code, fast enough. Background # Until 2021 V8 had two main execution tiers: Ignition, the interpreter; and 
TurboFan , V8\u2019s optimizing compiler focused on peak performance. All JavaScript code is first compiled to ignition bytecode, and executed by interpreting it. During execution V8 tracks how the program 
behaves, including tracking object shapes and types. Both the runtime execution metadata and bytecode are fed into the optimizing compiler to generate high-performance, often speculative, machine code that 
runs significantly faster than the interpreter can. These improvements are clearly visible on benchmarks like JetStream , a collection of traditional pure JavaScript benchmarks measuring startup, latency, and
peak performance. TurboFan helps V8 run the suite 4.35x as fast! JetStream has a reduced emphasis on steady state performance compared to past benchmarks (like the retired Octane benchmark ), but due to the 
simplicity of many line items, the optimized code is still where most time is spent. Speedometer is a different kind of benchmark suite than JetStream. It\u2019s designed to measure a web app\u2019s 
responsiveness by timing simulated user interactions. Instead of smaller static standalone JavaScript apps, the suite consists of full web pages, most of which are built using popular frameworks. Like during 
most web page loads, Speedometer line items spend much less time running tight JavaScript loops and much more executing a lot of code that interacts with the rest of the browser. TurboFan still has a lot of 
impact on Speedometer: it runs over 1.5x as fast! But the impact is clearly much more muted than on JetStream. Part of this difference results from the fact that full pages just spend less time in pure 
JavaScript . But in part it\u2019s due to the benchmark spending a lot of time in functions that don\u2019t get hot enough to be optimized by TurboFan. Web performance benchmarks comparing unoptimized and 
optimized execution All the benchmark scores in this post were measured with Chrome 117.0.5897.3 on a 13\u201d M2 Macbook Air. Since the difference in execution speed and compile time between Ignition and 
TurboFan is so large, in 2021 we introduced a new baseline JIT called Sparkplug . It\u2019s designed to compile bytecode to equivalent machine code almost instantaneously. On JetStream, Sparkplug improves 
performance quite a bit compared to Ignition (+45%). Even when TurboFan is also in the picture we still see a solid improvement in performance (+8%). On Speedometer we see a 41% improvement over Ignition, 
bringing it close to TurboFan performance, and a 22% improvement over Ignition + TurboFan! Since Sparkplug is so fast, we can easily deploy it very broadly and get a consistent speedup. If code doesn\u2019t 
rely solely on easily optimized, long-running, tight JavaScript loops, it\u2019s a great addition. Web performance benchmarks with added Sparkplug The simplicity of Sparkplug imposes a relatively low upper 
limit on the speedup it can provide though. This is clearly demonstrated by the large gap between Ignition + Sparkplug and Ignition + TurboFan. This is where Maglev comes in, our new optimizing JIT that 
generates code that\u2019s much faster than Sparkplug code, but is generated much faster than TurboFan can. Maglev: A Simple SSA-Based JIT compiler # When we started this project we saw two paths forward to 
cover the gap between Sparkplug and TurboFan: either try to generate better code using the single-pass approach taken by Sparkplug, or build a JIT with an intermediate representation (IR). Since we felt that 
not having an IR at all during compilation would likely severely restrict the compiler, we decided to go with a somewhat traditional static single-assignment (SSA) based approach, using a CFG (control flow 
graph) rather than TurboFan's more flexible but cache unfriendly sea-of-nodes representation. The compiler itself is designed to be fast and easy to work on. It has a minimal set of passes and a simple, 
single IR that encodes specialized JavaScript semantics. Prepass # First Maglev does a prepass over the bytecode to find branch targets, including loops, and assignments to variables in loop. This pass also 
collects liveness information, encoding which values in which variables are still needed across which expressions. This information can reduce the amount of state that needs to be tracked by the compiler 
later. SSA # A printout of the Maglev SSA graph on the command line Maglev does an abstract interpretation of the frame state, creating SSA nodes representing the results of expression evaluation. Variable 
assignments are emulated by storing those SSA nodes in the respective abstract interpreter register. In the case of branches and switches, all paths are evaluated. When multiple paths merge, values in 
abstract interpreter registers are merged by inserting so-called Phi nodes: value nodes that know which value to pick depending on which path was taken at runtime. Loops can merge variable values \u201cback 
in time\u201d, with the data flowing backwards from the loop end to the loop header, in the case when variables are assigned in the loop body. That\u2019s where the data from the prepass comes in handy: since
we already know which variables are assigned inside loops, we can pre-create loop phis before we even start processing the loop body. At the end of the loop we can populate the phi input with the correct SSA 
node. This allows the SSA graph generation to be a single forward pass, without needing to \"fix up\" loop variables, while also minimizing the amount of Phi nodes that need to be allocated. Known Node 
Information # To be as fast as possible, Maglev does as much as possible at once. Instead of building a generic JavaScript graph and then lowering that during later optimization phases, which is a 
theoretically clean but computationally expensive approach, Maglev does as much as possible immediately during graph building. During graph building Maglev will look at runtime feedback metadata collected 
during unoptimized execution, and generate specialized SSA nodes for the types observed. If Maglev sees o.x and knows from the runtime feedback that o always has one specific shape, it will generate an SSA 
node to check at runtime that o still has the expected shape, followed by a cheap LoadField node which does a simple access by offset. Additionally, Maglev will make a side node that it now knows the shape of
o , making it unnecessary to check the shape again later. If Maglev later encounters an operation on o that doesn't have feedback for some reason, this kind of information learned during compilation can be 
used as a second source of feedback. Runtime information can come in various forms. Some information needs to be checked at runtime, like the shape check previously described. Other information can be used 
without runtime checks by registering dependencies to the runtime. Globals that are de-facto constant (not changed between initialization and when their value is seen by Maglev) fall into this category: 
Maglev does not need to generate code to dynamically load and check their identity. Maglev can load the value at compile time and embed it directly into the machine code; if the runtime ever mutates that 
global, it'll also take care to invalidate and deoptimize that machine code. Some forms of information are \u201cunstable\u201d. Such information can only be used to the extent that the compiler knows for 
sure that it can\u2019t change. For example, if we just allocated an object, we know it\u2019s a new object and we can skip expensive write barriers entirely. Once there has been another potential allocation,
the garbage collector could have moved the object, and we now need to emit such checks. Others are \"stable\": if we have never seen any object transition away from having a certain shape, then we can 
register a dependency on this event (any object transitioning away from that particular shape) and don\u2019t need to recheck the shape of the object, even after a call to an unknown function with unknown 
side effects. Deoptimization # Given that Maglev can use speculative information that it checks at runtime, Maglev code needs to be able to deoptimize. To make this work, Maglev attaches abstract interpreter 
frame state to nodes that can deoptimize. This state maps interpreter registers to SSA values. This state turns into metadata during code generation, providing a mapping from optimized state to unoptimized 
state. The deoptimizer interprets this data, reading values from the interpreter frame and machine registers and putting them into the required places for interpretation. This builds on the same 
deoptimization mechanism as used by TurboFan, allowing us to share most of the logic and take advantage of the testing of the existing system. Representation Selection # JavaScript numbers represent, 
according to the spec , a 64-bit floating point value. This doesn't mean that the engine has to always store them as 64-bit floats though, especially since In practice many numbers are small integers (e.g. 
array indices). V8 tries to encode numbers as 31-bit tagged integers (internally called \u201cSmall Integers\u201d or \"Smi\"), both to save memory (32bit due to pointer compression ), and for performance 
(integer operations are faster than float operations). To make numerics-heavy JavaScript code fast, it\u2019s important that optimal representations are chosen for value nodes. Unlike the interpreter and 
Sparkplug, the optimizing compiler can unbox values once it knows their type, operating on raw numbers rather than JavaScript values representing numbers, and rebox values only if strictly necessary. Floats 
can directly be passed in floating point registers instead of allocating a heap object that contains the float. Maglev learns about the representation of SSA nodes mainly by looking at runtime feedback of 
e.g., binary operations, and propagating that information forwards through the Known Node Info mechanism. When SSA values with specific representations flow into Phis, a correct representation that supports 
all the inputs needs to be chosen. Loop phis are again tricky, since inputs from within the loop are seen after a representation should be chosen for the phi \u2014 the same \"back in time\" problem as for 
graph building. This is why Maglev has a separate phase after graph building to do representation selection on loop phis. Register Allocation # After graph building and representation selection, Maglev mostly
knows what kind of code it wants to generate, and is \"done\" from a classical optimization point of view. To be able to generate code though, we need to choose where SSA values actually live when executing 
machine code; when they're in machine registers, and when they're saved on the stack. This is done through register allocation. Each Maglev node has input and output requirements, including requirements on 
temporaries needed. The register allocator does a single forward walk over the graph, maintaining an abstract machine register state not too dissimilar from the abstract interpretation state maintained during
graph building, and will satisfy those requirements, replacing the requirements on the node with actual locations. Those locations can then be used by code generation. First, a prepass runs over the graph to 
find linear live ranges of nodes, so that we can free up registers once an SSA node isn\u2019t needed anymore. This prepass also keeps track of the chain of uses. Knowing how far in the future a value is 
needed can be useful to decide which values to prioritize, and which to drop, when we run out of registers. After the prepass, the register allocation runs. Register assignment follows some simple, local 
rules: If a value is already in a register, that register is used if possible. Nodes keep track of what registers they are stored into during the graph walk. If the node doesn\u2019t yet have a register, but 
a register is free, it\u2019s picked. The node gets updated to indicate it\u2019s in the register, and the abstract register state is updated to know it contains the node. If there\u2019s no free register, 
but a register is required, another value is pushed out of the register. Ideally, we have a node that\u2019s already in a different register, and can drop this \"for free\"; otherwise we pick a value that 
won\u2019t be needed for a long time, and spill it onto the stack. On branch merges, the abstract register states from the incoming branches are merged. We try to keep as many values in registers as possible.
This can mean we need to introduce register-to-register moves, or may need to unspill values from the stack, using moves called \u201cgap moves\u201d. If a branch merge has a phi node, register allocation 
will assign output registers to the phis. Maglev prefers to output phis to the same registers as its inputs, to minimize moves. If more SSA values are live than we have registers, we\u2019ll need to spill 
some values on the stack, and unspill them later. In the spirit of Maglev, we keep it simple: if a value needs to be spilled, it is retroactively told to immediately spill on definition (right after the value
is created), and code generation will handle emitting the spill code. The definition is guaranteed to \u2018dominate\u2019 all uses of the value (to reach the use we must have passed through the definition 
and therefore the spill code). This also means that a spilled value will have exactly one spill slot for the entire duration of the code; values with overlapping lifetimes will thus have non-overlapping 
assigned spill slots. Due to representation selection, some values in the Maglev frame will be tagged pointers, pointers that V8\u2019s GC understands and needs to consider; and some will be untagged, values 
that the GC should not look at. TurboFan handles this by precisely keeping track of which stack slots contain tagged values, and which contain untagged values, which changes during execution as slots are 
reused for different values. For Maglev we decided to keep things simpler, to reduce the memory required for tracking this: we split the stack frame into a tagged and an untagged region, and only store this 
split point. Code Generation # Once we know what expressions we want to generate code for, and where we want to put their outputs and inputs, Maglev is ready to generate code. Maglev nodes directly know how 
to generate assembly code using a \u201cmacro assembler\u201d. For example, a CheckMap node knows how to emit assembler instructions that compare the shape (internally called the \u201cmap\u201d) of an input 
object with a known value, and to deoptimize the code if the object had a wrong shape. One slightly tricky bit of code handles gap moves: The requested moves created by the register allocator know that a 
value lives somewhere and needs to go elsewhere. If there\u2019s a sequence of such moves though, a preceding move could clobber the input needed by a subsequent move. The Parallel Move Resolver computes how 
to safely perform the moves so that all values end up in the right place. Results # So the compiler we just presented is both clearly much more complex than Sparkplug, and much simpler than TurboFan. How does
it fare? In terms of compilation speed we\u2019ve managed to build a JIT that\u2019s roughly 10x slower than Sparkplug, and 10x faster than TurboFan. Compile time comparison of the compilation tiers, for all 
functions compiled in JetStream This allows us to deploy Maglev much earlier than we\u2019d want to deploy TurboFan. If the feedback it relied upon ended up not being very stable yet, there\u2019s no huge 
cost to deoptimizing and recompiling later. It also allows us to use TurboFan a little later: we\u2019re running much faster than we\u2019d run with Sparkplug. Slotting in Maglev between Sparkplug and 
TurboFan results in noticeable benchmark improvements: Web performance benchmarks with Maglev We have also validated Maglev on real-world data, and see good improvements on Core Web Vitals . Since Maglev 
compiles much faster, and since we can now afford to wait longer before we compile functions with TurboFan, this results in a secondary benefit that\u2019s not as visible on the surface. The benchmarks focus 
on main-thread latency, but Maglev also significantly reduces V8\u2019s overall resource consumption by using less off-thread CPU time. The energy consumption of a process can be measured easily on an M1- or 
M2-based Macbook using taskinfo . Benchmark Energy Consumption JetStream -3.5% Speedometer -10% Maglev isn\u2019t complete by any means. We've still got plenty more work to do, more ideas to try out, and more
low-hanging fruit to pick \u2014 as Maglev gets more complete, we\u2019ll expect to see higher scores, and more reduction in energy consumption. Maglev is now available for desktop Chrome now, and will be 
rolled out to mobile devices soon. Posted by Toon Verwaest , Leszek Swirski , Victor Gomes , Olivier Fl\u00fcckiger, Darius Mercadier, and Camillo Bruni \u2014 not enough cooks to spoil the broth. Branding 
Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the
Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/features/at-method.txt",
    "similarity": 0.471,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/features/at-method.txt\n\nat method for relative indexing \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research at 
method for relative indexing Published 13 July 2021 \u00b7 Tagged with ECMAScript The new at method on Array.prototype , the various TypedArray prototypes, and String.prototype makes accessing an element 
nearer to the end of the collection easier and more succinct. Accessing the Nth element from the end of a collection is a common operation. However, the usual ways to do so are verbose, like 
my_array|my_array.length - N] , or might not be performant, like my_array.slice(-N)|0] . The new at method makes this operation more ergonomic by interpreting negative indices to mean \"from the end\". The 
previous examples may be expressed as my_array.at(-N) . For uniformity, positive indices are also supported, and are equivalent to ordinary property access. This new method is small enough that its full 
semantics may be understood by this compliant polyfill implementation below: function at ( n ) { // Convert the argument to an integer n = Math . trunc ( n ) || 0 ; // Allow negative indexing from the end if 
( n < 0 ) n += this . length ; // Out-of-bounds access returns undefined if ( n < 0 || n >= this . length ) return undefined ; // Otherwise, this is just normal property access return this | n ] ; } A word 
about Strings # Since at ultimately performs ordinary indexing, calling at on String values returns code units, just as ordinary indexing would. And like ordinary indexing on Strings, code units may not be 
what you want for Unicode strings! Please consider if String.prototype.codePointAt() is more appropriate for your use case. at method support # Chrome: supported since version 92 Firefox: supported since 
version 90 Safari: no support Node.js: no support Babel: supported about this feature support listing Posted by Shu-yu Guo ( @_shu ). Branding Terms Privacy Twitter Edit this page on GitHub Except as 
otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, 
see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/features/array-flat-flatmap.txt",
    "similarity": 0.468,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/features/array-flat-flatmap.txt\n\nArray.prototype.flat and Array.prototype.flatMap \u00b7 V8 V8 Show navigation Home Blog Docs Tools 
JS/Wasm features Research Array.prototype.flat and Array.prototype.flatMap Published 11 June 2019 \u00b7 Tagged with ECMAScript ES2019 Array.prototype.flat # The array in this example is several levels deep: 
it contains an array which in turn contains another array. const array = | 1 , | 2 , | 3 ] ] ] ; // ^^^^^^^^^^^^^ outer array // ^^^^^^^^ inner array // ^^^ innermost array Array#flat returns a flattened 
version of a given array. array . flat ( ) ; // \u2192 |1, 2, |3]] // \u2026is equivalent to: array . flat ( 1 ) ; // \u2192 |1, 2, |3]] The default depth is 1 , but you can pass any number to recursively 
flatten up to that depth. To keep flattening recursively until the result contains no more nested arrays, we pass Infinity . // Flatten recursively until the array contains no more nested arrays: array . flat
( Infinity ) ; // \u2192 |1, 2, 3] Why is this method known as Array.prototype.flat and not Array.prototype.flatten ? Read our #SmooshGate write-up to find out! Array.prototype.flatMap # Here\u2019s another 
example. We have a duplicate function that takes a value, and returns an array that contains that value twice. If we apply duplicate to each value in an array, we end up with a nested array. const duplicate =
( x ) => | x , x ] ; | 2 , 3 , 4 ] . map ( duplicate ) ; // \u2192 ||2, 2], |3, 3], |4, 4]] You can then call flat on the result to flatten the array: | 2 , 3 , 4 ] . map ( duplicate ) . flat ( ) ; // 
\ud83d\udc0c // \u2192 |2, 2, 3, 3, 4, 4] Since this pattern is so common in functional programming, there\u2019s now a dedicated flatMap method for it. | 2 , 3 , 4 ] . flatMap ( duplicate ) ; // \ud83d\ude80
// \u2192 |2, 2, 3, 3, 4, 4] flatMap is a little bit more efficient compared to doing a map followed by a flat separately. Interested in use cases for flatMap ? Check out Axel Rauschmayer\u2019s explanation .
Array#{flat,flatMap} support # Chrome: supported since version 69 Firefox: supported since version 62 Safari: supported since version 12 Node.js: supported since version 11 Babel: supported about this feature
support listing Posted by Mathias Bynens ( @mathias ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are 
licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/stack-trace-api.txt",
    "similarity": 0.461,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/stack-trace-api.txt\n\nStack trace API \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Stack trace API All
internal errors thrown in V8 capture a stack trace when they are created. This stack trace can be accessed from JavaScript through the non-standard error.stack property. V8 also has various hooks for 
controlling how stack traces are collected and formatted, and for allowing custom errors to also collect stack traces. This document outlines V8\u2019s JavaScript stack trace API. Basic stack traces # By 
default, almost all errors thrown by V8 have a stack property that holds the topmost 10 stack frames, formatted as a string. Here\u2019s an example of a fully formatted stack trace: ReferenceError: FAIL is 
not defined at Constraint.execute (deltablue.js:525:2) at Constraint.recalculate (deltablue.js:424:21) at Planner.addPropagate (deltablue.js:701:6) at Constraint.satisfy (deltablue.js:184:15) at 
Planner.incrementalAdd (deltablue.js:591:21) at Constraint.addConstraint (deltablue.js:162:10) at Constraint.BinaryConstraint (deltablue.js:346:7) at Constraint.EqualityConstraint (deltablue.js:515:38) at 
chainTest (deltablue.js:807:6) at deltaBlue (deltablue.js:879:2) The stack trace is collected when the error is created and is the same regardless of where or how many times the error is thrown. We collect 10
frames because it is usually enough to be useful but not so many that it has a noticeable negative performance impact. You can control how many stack frames are collected by setting the variable Error . 
stackTraceLimit Setting it to 0 disables stack trace collection. Any finite integer value can be used as the maximum number of frames to collect. Setting it to Infinity means that all frames get collected. 
This variable only affects the current context; it has to be set explicitly for each context that needs a different value. (Note that what is known as a \u201ccontext\u201d in V8 terminology corresponds to a 
page or <iframe> in Google Chrome). To set a different default value that affects all contexts use the following V8 command-line flag: --stack-trace-limit < value > To pass this flag to V8 when running Google
Chrome, use: --js-flags = '--stack-trace-limit <value>' Async stack traces # The --async-stack-traces flag (turned on by default since V8 v7.3 ) enables the new zero-cost async stack traces , which enriches 
the stack property of Error instances with async stack frames, i.e. await locations in the code. These async frames are marked with async in the stack string: ReferenceError: FAIL is not defined at bar 
(<anonymous>) at async foo (<anonymous>) At the time of this writing, this functionality is limited to await locations, Promise.all() and Promise.any() , since for those cases the engine can reconstruct the 
necessary information without any additional overhead (that\u2019s why it\u2019s zero-cost). Stack trace collection for custom exceptions # The stack trace mechanism used for built-in errors is implemented 
using a general stack trace collection API that is also available to user scripts. The function Error . captureStackTrace ( error , constructorOpt ) adds a stack property to the given error object that yields
the stack trace at the time captureStackTrace was called. Stack traces collected through Error.captureStackTrace are immediately collected, formatted, and attached to the given error object. The optional 
constructorOpt parameter allows you to pass in a function value. When collecting the stack trace all frames above the topmost call to this function, including that call, are left out of the stack trace. This 
can be useful to hide implementation details that won\u2019t be useful to the user. The usual way of defining a custom error that captures a stack trace would be: function MyError ( ) { Error . 
captureStackTrace ( this , MyError ) ; // Any other initialization goes here. } Passing in MyError as a second argument means that the constructor call to MyError won\u2019t show up in the stack trace. 
Customizing stack traces # Unlike Java where the stack trace of an exception is a structured value that allows inspection of the stack state, the stack property in V8 just holds a flat string containing the 
formatted stack trace. This is for no other reason than compatibility with other browsers. However, this is not hardcoded but only the default behavior and can be overridden by user scripts. For efficiency 
stack traces are not formatted when they are captured but on demand, the first time the stack property is accessed. A stack trace is formatted by calling Error . prepareStackTrace ( error , 
structuredStackTrace ) and using whatever this call returns as the value of the stack property. If you assign a different function value to Error.prepareStackTrace that function is used to format stack 
traces. It gets passed the error object that it is preparing a stack trace for, as well as a structured representation of the stack. User stack trace formatters are free to format the stack trace however they
want and even return non-string values. It is safe to retain references to the structured stack trace object after a call to prepareStackTrace completes so that it is also a valid return value. Note that the 
custom prepareStackTrace function is only called once the stack property of Error object is accessed. The structured stack trace is an array of CallSite objects, each of which represents a stack frame. A 
CallSite object defines the following methods getThis : returns the value of this getTypeName : returns the type of this as a string. This is the name of the function stored in the constructor field of this ,
if available, otherwise the object\u2019s ||Class]] internal property. getFunction : returns the current function getFunctionName : returns the name of the current function, typically its name property. If a 
name property is not available an attempt is made to infer a name from the function\u2019s context. getMethodName : returns the name of the property of this or one of its prototypes that holds the current 
function getFileName : if this function was defined in a script returns the name of the script getLineNumber : if this function was defined in a script returns the current line number getColumnNumber : if 
this function was defined in a script returns the current column number getEvalOrigin : if this function was created using a call to eval returns a string representing the location where eval was called 
isToplevel : is this a top-level invocation, that is, is this the global object? isEval : does this call take place in code defined by a call to eval ? isNative : is this call in native V8 code? isConstructor
: is this a constructor call? isAsync : is this an async call (i.e. await , Promise.all() , or Promise.any() )? isPromiseAll : is this an async call to Promise.all() ? getPromiseIndex : returns the index of 
the promise element that was followed in Promise.all() or Promise.any() for async stack traces, or null if the CallSite is not an async Promise.all() or Promise.any() call. The default stack trace is created 
using the CallSite API so any information that is available there is also available through this API. To maintain restrictions imposed on strict mode functions, frames that have a strict mode function and all
frames below (its caller etc.) are not allow to access their receiver and function objects. For those frames, getFunction() and getThis() returns undefined . Compatibility # The API described here is specific
to V8 and is not supported by any other JavaScript implementations. Most implementations do provide an error.stack property but the format of the stack trace is likely to be different from the format 
described here. The recommended use of this API is: Only rely on the layout of the formatted stack trace if you know your code is running in v8. It is safe to set Error.stackTraceLimit and 
Error.prepareStackTrace regardless of which implementation is running your code but be aware that it only has an effect if your code is running in V8. Appendix: Stack trace format # The default stack trace 
format used by V8 can for each stack frame give the following information: Whether the call is a construct call. The type of the this value ( Type ). The name of the function called ( functionName ). The name
of the property of this or one of its prototypes that holds the function ( methodName ). The current location within the source ( location ) Any of these may be unavailable and different formats for stack 
frames are used depending on how much of this information is available. If all the above information is available a formatted stack frame looks like this: at Type.functionName |as methodName] (location) Or, 
in the case of a construct call: at new functionName (location) Or, in case of an async call: at async functionName (location) If only one of functionName and methodName is available, or if they are both 
available but the same, the format is: at Type.name (location) If neither is available <anonymous> is used as the name. The Type value is the name of the function stored in the constructor field of this . In 
V8, all constructor calls set this property to the constructor function so unless this field has been actively changed after the object was created it, it holds the name of the function it was created by. If 
it is unavailable the ||Class]] property of the object is used. One special case is the global object where the Type is not shown. In that case the stack frame is formatted as: at functionName |as methodName]
(location) The location itself has several possible formats. Most common is the file name, line and column number within the script that defined the current function: fileName:lineNumber:columnNumber If the 
current function was created using eval the format is: eval at position \u2026where position is the full position where the call to eval occurred. Note that this means that positions can be nested if there 
are nested calls to eval , for instance: eval at Foo.a (eval at Bar.z (myscript.js:10:3)) If a stack frame is within V8\u2019s libraries the location is: native \u2026and if is unavailable, it\u2019s: unknown
location Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is 
licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/spread-elements.txt",
    "similarity": 0.459,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/spread-elements.txt\n\nSpeeding up spread elements \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
Speeding up spread elements Published 04 December 2018 \u00b7 Tagged with ECMAScript benchmarks During his three-months internship on the V8 team, Hai Dang worked on improving the performance of |...array] , 
|...string] , |...set] , |...map.keys()] , and |...map.values()] (when the spread elements are at the start of the array literal). He even made Array.from(iterable) much faster as well. This article explains 
some of the gory details of his changes, which are included in V8 starting with v7.2. Spread elements # Spread elements are components of array literals that have the form ...iterable . They were introduced 
in ES2015 as a way to create arrays from iterable objects. For example, the array literal |1, ...arr, 4, ...b] creates an array whose first element is 1 followed by the elements of the array arr , then 4 , 
and finally the elements of the array b : const a = | 2 , 3 ] ; const b = | 5 , 6 , 7 ] ; const result = | 1 , ... a , 4 , ... b ] ; // \u2192 |1, 2, 3, 4, 5, 6, 7] As another example, any string can be 
spread to create an array of its characters (Unicode code points): const str = '\u3053\u3093\u306b\u3061\u306f' ; const result = | ... str ] ; // \u2192 |'\u3053', '\u3093', '\u306b', '\u3061', '\u306f'] 
Similarly, any set can be spread to create an array of its elements, sorted by insertion order: const s = new Set ( ) ; s . add ( 'V8' ) ; s . add ( 'TurboFan' ) ; const result = | ... s ] ; // \u2192 |'V8', 
'TurboFan'] In general, the spread elements syntax ...x in an array literal assumes that x provides an iterator (accessible through x|Symbol.iterator]() ). This iterator is then used to obtain the elements to
be inserted into the resulting array. The simple use case of spreading an array arr into a new array, without adding any further elements before or behind, |...arr] , is considered a concise, idiomatic way to
shallow-clone arr in ES2015. Unfortunately, in V8, the performance of this idiom lagged far behind its ES5 counterpart. The goal of Hai\u2019s internship was to change that! Why is (or were!) spread elements 
slow? # There are many ways to shallow-clone an array arr . For instance, you can use arr.slice() , or arr.concat() , or |...arr] . Or, you can write your own clone function that employs a standard for -loop:
function clone ( arr ) { // Pre-allocate the correct number of elements, to avoid // having to grow the array. const result = new Array ( arr . length ) ; for ( let i = 0 ; i < arr . length ; i ++ ) { result 
| i ] = arr | i ] ; } return result ; } Ideally, all these options would have similar performance characteristics. Unfortunately, if you pick |...arr] in V8, it is (or was ) likely to be slower than clone ! 
The reason is that V8 essentially transpiles |...arr] into an iteration like the following: function ( arr ) { const result = | ] ; const iterator = arr | Symbol . iterator ] ( ) ; const next = iterator . 
next ; for ( ; ; ) { const iteratorResult = next . call ( iterator ) ; if ( iteratorResult . done ) break ; result . push ( iteratorResult . value ) ; } return result ; } This code is generally slower than 
clone for a few reasons: It needs to create the iterator at the beginning by loading and evaluating the Symbol.iterator property. It needs to create and query the iteratorResult object at every step. It grows
the result array at every step of the iteration by calling push , thus repeatedly reallocating the backing store. The reason for using such an implementation is that, as mentioned earlier, spreading can be 
done not only on arrays but, in fact, on arbitrary iterable objects, and must follow the iteration protocol . Nevertheless, V8 should be smart enough to recognize if the object being spread is an array such 
that it can perform the elements extraction at a lower level and thereby: avoid the creation of the iterator object, avoid the creation of the iterator result objects, and avoid continuously growing and thus 
reallocating the result array (we know the number of elements in advance). We implemented this simple idea using CSA for fast arrays, i.e. arrays with one of the six most common elements kinds . The 
optimization applies for the common real-world scenario where the spread occurs at the start of the array literal, e.g. |...foo] . As shown in the graph below, this new fast path yields roughly a 3\u00d7 
performance improvement for spreading an array of length 100,000, making it about 25% faster than the hand-written clone loop. Performance improvement of spreading a fast array Note: While not shown here, the
fast path also applies when the spread elements are followed by other components (e.g. |...arr, 1, 2, 3] ), but not when they are preceded by others (e.g. |1, 2, 3, ...arr] ). Tread carefully down that fast 
path # That\u2019s clearly an impressive speedup, but we must be very careful about when it is correct to take this fast path: JavaScript allows the programmer to modify the iteration behavior of objects 
(even arrays) in various ways. Because spread elements are specified to use the iteration protocol, we need to ensure that such modifications are respected. We do so by avoiding the fast path completely 
whenever the original iteration machinery has been mutated. For example, this includes situations like the following. Own Symbol.iterator property # Normally, an array arr does not have its own 
Symbol.iterator property, so when looking up that symbol, it will be found on the array\u2019s prototype. In the example below, the prototype is bypassed by defining the Symbol.iterator property directly on 
arr itself. After this modification, looking up Symbol.iterator on arr results in an empty iterator, and thus the spread of arr yields no elements and the array literal evaluates to an empty array. const arr 
= | 1 , 2 , 3 ] ; arr | Symbol . iterator ] = function ( ) { return { next : function ( ) { return { done : true } ; } } ; } ; const result = | ... arr ] ; // \u2192 |] Modified %ArrayIteratorPrototype% # The
next method can also be modified directly on %ArrayIteratorPrototype% , the prototype of array iterators (which affects all arrays). Object . getPrototypeOf ( | ] | Symbol . iterator ] ( ) ) . next = function
( ) { return { done : true } ; } const arr = | 1 , 2 , 3 ] ; const result = | ... arr ] ; // \u2192 |] Dealing with holey arrays # Extra care is also needed when copying arrays with holes, i.e., arrays like 
|'a', , 'c'] that are missing some elements. Spreading such an array, by virtue of adhering to the iteration protocol, does not preserve the holes but instead fills them with the values found in the 
array\u2019s prototype at the corresponding indices. By default there are no elements in an array\u2019s prototype, which means that any holes are filled with undefined . For example, |...|'a', , 'c']] 
evaluates to a new array |'a', undefined, 'c'] . Our fast path is smart enough to handle holes in this default situation. Instead of blindly copying the input array\u2019s backing store, it watches out for 
holes and takes care of converting them to undefined values. The graph below contains measurements for an input array of length 100,000 containing only (tagged) 600 integers \u2014 the rest are holes. It 
shows that spreading such a holey array is now over 4\u00d7 faster than using the clone function. (They used to be roughly on par, but this is not shown in the graph). Note that although slice is included in 
this graph, the comparison with it is unfair because slice has a different semantics for holey arrays: it preserves all the holes, so it has much less work to do. Performance improvement of spreading a holey 
array of integers ( HOLEY_SMI_ELEMENTS ) The filling of holes with undefined that our fast path has to perform is not as simple as it sounds: it may require converting the whole array to a different elements 
kind. The next graph measures such a situation. The setup is the same as above, except that this time the 600 array elements are unboxed doubles and the array has the HOLEY_DOUBLE_ELEMENTS elements kind. 
Since this elements kind cannot hold tagged values such as undefined , spreading involves a costly elements kind transition, which is why the score for |...a] is much lower than in the previous graph. 
Nevertheless, it is still much faster than clone(a) . Performance improvement of spreading a holey array of doubles ( HOLEY_DOUBLE_ELEMENTS ) Spreading strings, sets, and maps # The idea of skipping the 
iterator object and avoiding growing the result array equally applies to spreading other standard data types. Indeed, we implemented similar fast paths for primitive strings, for sets, and for maps, each time
taking care to bypass them in the presence of modified iteration behavior. Concerning sets, the fast path supports not only spreading a set directly (|...set]), but also spreading its keys iterator ( 
|...set.keys()] ) and its values iterator ( |...set.values()] ). In our micro-benchmarks, these operations are now about 18\u00d7 faster than before. The fast path for maps is similar but does not support 
spreading a map directly ( |...map] ), because we consider this an uncommon operation. For the same reason, neither fast path supports the entries() iterator. In our micro-benchmarks, these operations are now
about 14\u00d7 faster than before. For spreading strings ( |...string] ), we measured a roughly 5\u00d7 improvement, as shown in the graph below by the purple and green lines. Note that this is even faster 
than a TurboFan-optimized for-of-loop (TurboFan understands string iteration and can generate optimized code for it), represented by the blue and pink lines. The reason for having two plots in each case is 
that the micro-benchmarks operate on two different string representations (one-byte strings and two-byte strings). Performance improvement of spreading a string Performance improvement of spreading a set with
100,000 integers (magenta, about 18\u00d7), shown here in comparison with a for - of loop (red) Improving Array.from performance # Fortunately, our fast paths for spread elements can be reused for Array.from 
in the case where Array.from is called with an iterable object and without a mapping function, for example, Array.from(|1, 2, 3]) . The reuse is possible because in this case, the behavior of Array.from is 
exactly the same as that of spreading. It results in an enormous performance improvement, shown below for an array with 100 doubles. Performance improvement of Array.from(array) where array contains 100 
doubles Conclusion # V8 v7.2 / Chrome 72 greatly improves the performance of spread elements when they occur at the front of the array literal, for example |...x] or |...x, 1, 2] . The improvement applies to 
spreading arrays, primitive strings, sets, maps keys, maps values, and \u2014 by extension \u2014 to Array.from(x) . Posted by Hai Dang & Georg Neis. Retweet this article! Branding Terms Privacy Twitter Edit 
this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons 
Attribution 3.0 License . For details, see our site policies ."
  }
]
[Step 5: Duration 32.72 seconds| Input tokens: 349,168 | Output tokens: 2,574]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 6 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'final_answer' with arguments: {'answer': '### 1. Task outcome (short version):\nSelected code region for directed fuzzing: Maglev’s Array.prototype.forEach specialization and loop lowering  │
│ in the Maglev graph builder, including its Eager/Lazy deopt continuations, ThrowIfNotCallable path, and exception/catch-context materialization.\n\n### 2. Task outcome (extremely detailed version):\n-     │
│ What I analyzed (Stage 0–2):\n  - Pulled a random entry and locked onto regressions/maglev/regress/regress-1455295.js. The JS calls [].forEach(undefined) twice inside try/catch, then optimizes to Maglev,  │
│ then calls once more. The Fuzzilli translation matches this layout.\n  - The execution_data prints the full Maglev SSA graph, Phi untagging, and register allocation for foo. Critically, the graph          │
│ explicitly shows specialized handling for Array.forEach:\n    - Nodes and continuations: ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopEagerDeoptContinuation.\n    - Loop structure with           │
│ CheckMaps(Map[16](PACKED_SMI_ELEMENTS)), LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, and a NULL_OR_UNDEFINED Call.\n    - ThrowIfNotCallable      │
│ guarding the callback (undefined in this test), causing control to flow through exception handler blocks that do CreateCatchContext and SetPendingMessage.\n    - Handler blocks show                        │
│ InlinedAllocation(Map(CATCH_CONTEXT_TYPE)) and write-barriered stores for catch context fields.\n    - Representation selection shows maglev-untagged-phis and frequent UnsafeSmiUntag/Int32ToNumber         │
│ transitions; there’s explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) to protect against concurrent array-length changes.\n\n- What I searched (Stage 3):\n  - Used the V8 knowledge  │
│ base to gather Maglev architecture context (v8.dev/blog/maglev). That doc aligns with what we see:\n    - Maglev builds a specialized IR during graph building, it attaches deopt metadata, performs         │
│ representation selection (including handling of loop Phis), and then does a single-pass register allocation and codegen.\n    - Deoptimization attachment on nodes like ThrowIfNotCallable is expected in    │
│ speculative fast paths such as builtin specializations (exactly our forEach case).\n\n- Why this region is promising for fuzzing:\n  - It is specific: we’re not targeting “Maglev” broadly; we’re focusing  │
│ on Maglev’s builtin specialization/lowering of Array.prototype.forEach and the exact code paths managing its loop body, continuations, and exception edges.\n  - Complex control/data flow:\n    - The       │
│ forEach specialization synthesizes a loop with backedges and Phis (index, length), does representation selection and phi untagging, and relies on deopt continuations to safely fall back when assumptions   │
│ break.\n    - There’s a non-trivial intersection between length checks (UnsignedLessThanEqual, ArrayLengthChanged), element-kind checks (CheckMaps), index arithmetic (Int32IncrementWithOverflow), and      │
│ speculative node forms (UnsafeSmiUntag), all under possible exception edges.\n  - Exception and catch-context materialization:\n    - The test explicitly exercises ThrowIfNotCallable followed by           │
│ try/catch, which forces the Maglev exception path to allocate a catch context and set pending messages. This path includes write barriers and potentially tricky GC interactions with inlined allocations    │
│ and pending message state.\n  - Historically fragile surfaces:\n    - JIT loop lowering around array builtins is a classic source of off-by-ones, stale length reads, holes handling, and elements-kind      │
│ transitions.\n    - Deopt continuations at control flow joins can mismap materialized values under exception edges.\n    - Phi untagging across backedges and exception edges (untagged-phis enabled) is a   │
│ known source of subtle miscompilations.\n\n- Concrete V8 code region to target:\n  - The Maglev graph builder logic that recognizes Array.prototype.forEach, constructs the ArrayForEachLoop nodes, and      │
│ wires up the Lazy/Eager deopt continuations (plus codegen that emits ThrowIfNotCallable checks and the exception/catch-context path). In the V8 tree this is typically found in:\n    -                      │
│ src/maglev/maglev-graph-builder.cc and closely-related helpers that specialize CallProperty for array iteration builtins (the bits that yield ArrayForEachLoopEagerDeoptContinuation /                       │
│ ArrayForEachLoopLazyDeoptContinuation in printing and emit ThrowIfNotCallable).\n    - Supporting code that emits exception handling and catch-context creation in Maglev (CreateCatchContext,               │
│ SetPendingMessage) when the callback is not callable.\n  - From the printed graph we see exact node classes and codegen responsibilities: CheckMaps, LoadTaggedFieldForProperty(length), UnsafeSmiUntag,     │
│ Int32IncrementWithOverflow, LoadFixedArrayElement, NULL_OR_UNDEFINED Call to the callback, and deopt metadata attached to these callsites.\n\n- How the provided regression directly maps to these code      │
│ paths:\n  - The graph dump shows:\n    - ThrowIfNotCallable [n4] at the forEach callsite with lazy deopt continuation markers.\n    - ArrayForEachLoopEagerDeoptContinuation and                             │
│ ArrayForEachLoopLazyDeoptContinuation laid out around the loop blocks (b3/b8) and exception blocks (b5/b10).\n    - Exception edge creates catch contexts via inlined allocation, stores into well-known     │
│ offsets with/without write barriers, and then SetPendingMessage.\n    - Representation selection after Phi untagging makes index an Int32 and retags as necessary; index growth uses                         │
│ Int32IncrementWithOverflow and rechecks against the (re)loaded/untagged length.\n\n- Why this offers an excellent fuzzing surface:\n  - We can stress:\n    - Callback variability (undefined, primitives,   │
│ proxies with callable traps, functions that deopt/OSR/tier-change).\n    - Array elements kinds (PACKED_SMI, PACKED_DOUBLE, HOLEY_* variants) and dynamic transitions during the loop (mutations of          │
│ length/elements in callback).\n    - Holes, sparse indices, side-effecting getters, and prototype mutations to force re-checks and deopts.\n    - Exception paths (callback throws vs ThrowIfNotCallable),   │
│ nested try/catch/finally, and pending message states across reentrant calls.\n    - Index arithmetic edge cases (large lengths, near INT32_MAX, negative coerced indices through odd receivers),             │
│ representation mismatches in Phis, and aggressive untagged-phis optimization.\n    - Cross-realm/modified Array.prototype.forEach, shadowing Symbol.iterator, and property shape changes that should be      │
│ caught by CheckMaps but might create timing windows with deopt continuations.\n\n- Deliverable for the fuzzing sub-agent:\n  - Provide a Fuzzilli ProgramTemplate that preferentially emits forEach-like     │
│ patterns and toggles array/receiver/callback shapes through feedback-warming runs before calling %OptimizeMaglevOnNextCall, then applies stressful behaviors during the optimized call, with optional        │
│ try/catch wrappers.\n  - Focus instrumentation on code regions that correspond to:\n    - ArrayForEachLoop(Eager|Lazy)DeoptContinuation.\n    - ThrowIfNotCallable guard.\n    - CreateCatchContext +        │
│ SetPendingMessage sequences.\n    - UnsafeSmiUntag/Int32IncrementWithOverflow + CheckInt32Condition(ArrayLengthChanged).\n\nHere is the requested JSON object with the selected region and rationale:\n{\n   │
│ CODE REGION: "Maglev Array.prototype.forEach specialization and loop lowering in src/maglev/maglev-graph-builder (and friends), including ArrayForEachLoopEagerDeoptContinuation /                           │
│ ArrayForEachLoopLazyDeoptContinuation, ThrowIfNotCallable handling, and Maglev exception/catch-context materialization paths."\n    SUMMARY_OF_CODE_REGION: "This region covers the Maglev JIT’s builtin     │
│ specialization for Array.prototype.forEach. During graph building, Maglev recognizes forEach on arrays and constructs a specialized loop with checks (CheckMaps for elements kind,                           │
│ LoadTaggedFieldForProperty for length, UnsafeSmiUntag for untagging Smi length/index, Int32IncrementWithOverflow for induction), and emits a call to the callback (NULL_OR_UNDEFINED when applicable). It    │
│ wires Eager and Lazy deopt continuations for safe fallback when feedback assumptions break (e.g., length changed, elements kind mismatch, non-callable callback). When the callback is not callable (e.g.,   │
│ undefined), a ThrowIfNotCallable node triggers the exception path. The exception path allocates a catch context via inlined allocation (Map(CATCH_CONTEXT_TYPE)), stores scope and exception values using    │
│ write barriers, and calls SetPendingMessage before resuming at the catch handler. Representation selection and Maglev’s untagged-phis optimization operate across the loop’s backedge and exception edges,   │
│ retagging as needed on returns. The printed graph for regressions/maglev/regress/regress-1455295.js shows these exact nodes and continuations, proving that ArrayForEachLoop lowering, ThrowIfNotCallable,   │
│ and catch-context creation paths are active under Maglev with try/catch. This region is rich in speculative checks, exception edges, GC-sensitive inlined allocations, integer untagging/retagging, and      │
│ deopt metadata—prime territory for subtle JIT miscompilation/exception-state bugs."\n    REASONING: "The selected regression explicitly exercises Array.forEach with an undefined callback inside try/catch, │
│ then optimizes to Maglev, and runs again. The Maglev graph dump shows: ThrowIfNotCallable at the callsite; Eager and Lazy deopt continuations named ArrayForEachLoop*; loop nodes (CheckMaps, UnsafeSmiUntag │
│ of length/index, Int32IncrementWithOverflow, LoadFixedArrayElement, CheckInt32Condition(ArrayLengthChanged)); and exception handler blocks with CreateCatchContext and SetPendingMessage using write         │
│ barriers. This precisely maps to Maglev’s specialized forEach lowering and exception machinery. These paths combine multiple historically fragile concerns (array length/element-kind dynamics, loop Phi     │
│ untagging, mixed tagged/untagged representations, deopt across exception edges, and GC-visible context materialization), offering a high-value, high-coverage fuzzing surface. The V8 Maglev architecture    │
│ doc further supports that Maglev embeds speculative assumptions and deopt metadata directly during graph building, making builtin specializations a natural target for exploitable                           │
│ miscompilations."\n}\n\n### 3. Additional context (if relevant):\n- Signals in the execution_data supporting this pick:\n  - "ThrowIfNotCallable" nodes surrounding the forEach call and lazy deopt          │
│ markers.\n  - Blocks labeled ArrayForEachLoop(Eager|Lazy)DeoptContinuation.\n  - Catch-context inlined allocations (Map(CATCH_CONTEXT_TYPE)) with StoreTaggedFieldWithWriteBarrier and SetPendingMessage.\n  │
│ - maglev-untagged-phis enabled; frequent UnsafeSmiUntag of Phis and length, and Int32IncrementWithOverflow.\n  - Explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) guarding concurrent │
│ array length changes.\n- Fuzzing angles to consider for templates:\n  - Vary array elements kind (SMI, DOUBLE, HOLEY variants), and mutate length/elements inside callback to force deopts.\n  - Use         │
│ non-callable receivers/callbacks (undefined, null, numbers, proxies with callable/non-callable traps) to traverse ThrowIfNotCallable vs. callback paths.\n  - Introduce exceptions inside callbacks and nest │
│ try/catch/finally to stress catch-context re-materialization.\n  - Prototype mutations, getters on length, and cross-realm Array.prototype to perturb CheckMaps/shape dependencies.\n  - Large/special       │
│ lengths (near INT32_MAX), sparse indices, and typed arrays vs fast arrays to explore index/representation edges.\n'}                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: ### 1. Task outcome (short version):
Selected code region for directed fuzzing: Maglev’s Array.prototype.forEach specialization and loop lowering in the Maglev graph builder, including its Eager/Lazy deopt continuations, ThrowIfNotCallable path,
and exception/catch-context materialization.

### 2. Task outcome (extremely detailed version):
- What I analyzed (Stage 0–2):
  - Pulled a random entry and locked onto regressions/maglev/regress/regress-1455295.js. The JS calls |].forEach(undefined) twice inside try/catch, then optimizes to Maglev, then calls once more. The Fuzzilli
translation matches this layout.
  - The execution_data prints the full Maglev SSA graph, Phi untagging, and register allocation for foo. Critically, the graph explicitly shows specialized handling for Array.forEach:
    - Nodes and continuations: ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopEagerDeoptContinuation.
    - Loop structure with CheckMaps(Map|16](PACKED_SMI_ELEMENTS)), LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, and a NULL_OR_UNDEFINED Call.
    - ThrowIfNotCallable guarding the callback (undefined in this test), causing control to flow through exception handler blocks that do CreateCatchContext and SetPendingMessage.
    - Handler blocks show InlinedAllocation(Map(CATCH_CONTEXT_TYPE)) and write-barriered stores for catch context fields.
    - Representation selection shows maglev-untagged-phis and frequent UnsafeSmiUntag/Int32ToNumber transitions; there’s explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) to protect 
against concurrent array-length changes.

- What I searched (Stage 3):
  - Used the V8 knowledge base to gather Maglev architecture context (v8.dev/blog/maglev). That doc aligns with what we see:
    - Maglev builds a specialized IR during graph building, it attaches deopt metadata, performs representation selection (including handling of loop Phis), and then does a single-pass register allocation and
codegen.
    - Deoptimization attachment on nodes like ThrowIfNotCallable is expected in speculative fast paths such as builtin specializations (exactly our forEach case).

- Why this region is promising for fuzzing:
  - It is specific: we’re not targeting “Maglev” broadly; we’re focusing on Maglev’s builtin specialization/lowering of Array.prototype.forEach and the exact code paths managing its loop body, continuations, 
and exception edges.
  - Complex control/data flow:
    - The forEach specialization synthesizes a loop with backedges and Phis (index, length), does representation selection and phi untagging, and relies on deopt continuations to safely fall back when 
assumptions break.
    - There’s a non-trivial intersection between length checks (UnsignedLessThanEqual, ArrayLengthChanged), element-kind checks (CheckMaps), index arithmetic (Int32IncrementWithOverflow), and speculative node
forms (UnsafeSmiUntag), all under possible exception edges.
  - Exception and catch-context materialization:
    - The test explicitly exercises ThrowIfNotCallable followed by try/catch, which forces the Maglev exception path to allocate a catch context and set pending messages. This path includes write barriers and
potentially tricky GC interactions with inlined allocations and pending message state.
  - Historically fragile surfaces:
    - JIT loop lowering around array builtins is a classic source of off-by-ones, stale length reads, holes handling, and elements-kind transitions.
    - Deopt continuations at control flow joins can mismap materialized values under exception edges.
    - Phi untagging across backedges and exception edges (untagged-phis enabled) is a known source of subtle miscompilations.

- Concrete V8 code region to target:
  - The Maglev graph builder logic that recognizes Array.prototype.forEach, constructs the ArrayForEachLoop nodes, and wires up the Lazy/Eager deopt continuations (plus codegen that emits ThrowIfNotCallable 
checks and the exception/catch-context path). In the V8 tree this is typically found in:
    - src/maglev/maglev-graph-builder.cc and closely-related helpers that specialize CallProperty for array iteration builtins (the bits that yield ArrayForEachLoopEagerDeoptContinuation / 
ArrayForEachLoopLazyDeoptContinuation in printing and emit ThrowIfNotCallable).
    - Supporting code that emits exception handling and catch-context creation in Maglev (CreateCatchContext, SetPendingMessage) when the callback is not callable.
  - From the printed graph we see exact node classes and codegen responsibilities: CheckMaps, LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, 
NULL_OR_UNDEFINED Call to the callback, and deopt metadata attached to these callsites.

- How the provided regression directly maps to these code paths:
  - The graph dump shows:
    - ThrowIfNotCallable |n4] at the forEach callsite with lazy deopt continuation markers.
    - ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation laid out around the loop blocks (b3/b8) and exception blocks (b5/b10).
    - Exception edge creates catch contexts via inlined allocation, stores into well-known offsets with/without write barriers, and then SetPendingMessage.
    - Representation selection after Phi untagging makes index an Int32 and retags as necessary; index growth uses Int32IncrementWithOverflow and rechecks against the (re)loaded/untagged length.

- Why this offers an excellent fuzzing surface:
  - We can stress:
    - Callback variability (undefined, primitives, proxies with callable traps, functions that deopt/OSR/tier-change).
    - Array elements kinds (PACKED_SMI, PACKED_DOUBLE, HOLEY_* variants) and dynamic transitions during the loop (mutations of length/elements in callback).
    - Holes, sparse indices, side-effecting getters, and prototype mutations to force re-checks and deopts.
    - Exception paths (callback throws vs ThrowIfNotCallable), nested try/catch/finally, and pending message states across reentrant calls.
    - Index arithmetic edge cases (large lengths, near INT32_MAX, negative coerced indices through odd receivers), representation mismatches in Phis, and aggressive untagged-phis optimization.
    - Cross-realm/modified Array.prototype.forEach, shadowing Symbol.iterator, and property shape changes that should be caught by CheckMaps but might create timing windows with deopt continuations.

- Deliverable for the fuzzing sub-agent:
  - Provide a Fuzzilli ProgramTemplate that preferentially emits forEach-like patterns and toggles array/receiver/callback shapes through feedback-warming runs before calling %OptimizeMaglevOnNextCall, then 
applies stressful behaviors during the optimized call, with optional try/catch wrappers.
  - Focus instrumentation on code regions that correspond to:
    - ArrayForEachLoop(Eager|Lazy)DeoptContinuation.
    - ThrowIfNotCallable guard.
    - CreateCatchContext + SetPendingMessage sequences.
    - UnsafeSmiUntag/Int32IncrementWithOverflow + CheckInt32Condition(ArrayLengthChanged).

Here is the requested JSON object with the selected region and rationale:
{
    CODE REGION: "Maglev Array.prototype.forEach specialization and loop lowering in src/maglev/maglev-graph-builder (and friends), including ArrayForEachLoopEagerDeoptContinuation / 
ArrayForEachLoopLazyDeoptContinuation, ThrowIfNotCallable handling, and Maglev exception/catch-context materialization paths."
    SUMMARY_OF_CODE_REGION: "This region covers the Maglev JIT’s builtin specialization for Array.prototype.forEach. During graph building, Maglev recognizes forEach on arrays and constructs a specialized 
loop with checks (CheckMaps for elements kind, LoadTaggedFieldForProperty for length, UnsafeSmiUntag for untagging Smi length/index, Int32IncrementWithOverflow for induction), and emits a call to the callback
(NULL_OR_UNDEFINED when applicable). It wires Eager and Lazy deopt continuations for safe fallback when feedback assumptions break (e.g., length changed, elements kind mismatch, non-callable callback). When 
the callback is not callable (e.g., undefined), a ThrowIfNotCallable node triggers the exception path. The exception path allocates a catch context via inlined allocation (Map(CATCH_CONTEXT_TYPE)), stores 
scope and exception values using write barriers, and calls SetPendingMessage before resuming at the catch handler. Representation selection and Maglev’s untagged-phis optimization operate across the loop’s 
backedge and exception edges, retagging as needed on returns. The printed graph for regressions/maglev/regress/regress-1455295.js shows these exact nodes and continuations, proving that ArrayForEachLoop 
lowering, ThrowIfNotCallable, and catch-context creation paths are active under Maglev with try/catch. This region is rich in speculative checks, exception edges, GC-sensitive inlined allocations, integer 
untagging/retagging, and deopt metadata—prime territory for subtle JIT miscompilation/exception-state bugs."
    REASONING: "The selected regression explicitly exercises Array.forEach with an undefined callback inside try/catch, then optimizes to Maglev, and runs again. The Maglev graph dump shows: 
ThrowIfNotCallable at the callsite; Eager and Lazy deopt continuations named ArrayForEachLoop*; loop nodes (CheckMaps, UnsafeSmiUntag of length/index, Int32IncrementWithOverflow, LoadFixedArrayElement, 
CheckInt32Condition(ArrayLengthChanged)); and exception handler blocks with CreateCatchContext and SetPendingMessage using write barriers. This precisely maps to Maglev’s specialized forEach lowering and 
exception machinery. These paths combine multiple historically fragile concerns (array length/element-kind dynamics, loop Phi untagging, mixed tagged/untagged representations, deopt across exception edges, 
and GC-visible context materialization), offering a high-value, high-coverage fuzzing surface. The V8 Maglev architecture doc further supports that Maglev embeds speculative assumptions and deopt metadata 
directly during graph building, making builtin specializations a natural target for exploitable miscompilations."
}

### 3. Additional context (if relevant):
- Signals in the execution_data supporting this pick:
  - "ThrowIfNotCallable" nodes surrounding the forEach call and lazy deopt markers.
  - Blocks labeled ArrayForEachLoop(Eager|Lazy)DeoptContinuation.
  - Catch-context inlined allocations (Map(CATCH_CONTEXT_TYPE)) with StoreTaggedFieldWithWriteBarrier and SetPendingMessage.
  - maglev-untagged-phis enabled; frequent UnsafeSmiUntag of Phis and length, and Int32IncrementWithOverflow.
  - Explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) guarding concurrent array length changes.
- Fuzzing angles to consider for templates:
  - Vary array elements kind (SMI, DOUBLE, HOLEY variants), and mutate length/elements inside callback to force deopts.
  - Use non-callable receivers/callbacks (undefined, null, numbers, proxies with callable/non-callable traps) to traverse ThrowIfNotCallable vs. callback paths.
  - Introduce exceptions inside callbacks and nest try/catch/finally to stress catch-context re-materialization.
  - Prototype mutations, getters on length, and cross-realm Array.prototype to perturb CheckMaps/shape dependencies.
  - Large/special lengths (near INT32_MAX), sparse indices, and typed arrays vs fast arrays to explore index/representation edges.
Final answer: ### 1. Task outcome (short version):
Selected code region for directed fuzzing: Maglev’s Array.prototype.forEach specialization and loop lowering in the Maglev graph builder, including its Eager/Lazy deopt continuations, ThrowIfNotCallable path,
and exception/catch-context materialization.

### 2. Task outcome (extremely detailed version):
- What I analyzed (Stage 0–2):
  - Pulled a random entry and locked onto regressions/maglev/regress/regress-1455295.js. The JS calls [].forEach(undefined) twice inside try/catch, then optimizes to Maglev, then calls once more. The Fuzzilli
translation matches this layout.
  - The execution_data prints the full Maglev SSA graph, Phi untagging, and register allocation for foo. Critically, the graph explicitly shows specialized handling for Array.forEach:
    - Nodes and continuations: ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopEagerDeoptContinuation.
    - Loop structure with CheckMaps(Map[16](PACKED_SMI_ELEMENTS)), LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, and a NULL_OR_UNDEFINED Call.
    - ThrowIfNotCallable guarding the callback (undefined in this test), causing control to flow through exception handler blocks that do CreateCatchContext and SetPendingMessage.
    - Handler blocks show InlinedAllocation(Map(CATCH_CONTEXT_TYPE)) and write-barriered stores for catch context fields.
    - Representation selection shows maglev-untagged-phis and frequent UnsafeSmiUntag/Int32ToNumber transitions; there’s explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) to protect 
against concurrent array-length changes.

- What I searched (Stage 3):
  - Used the V8 knowledge base to gather Maglev architecture context (v8.dev/blog/maglev). That doc aligns with what we see:
    - Maglev builds a specialized IR during graph building, it attaches deopt metadata, performs representation selection (including handling of loop Phis), and then does a single-pass register allocation and
codegen.
    - Deoptimization attachment on nodes like ThrowIfNotCallable is expected in speculative fast paths such as builtin specializations (exactly our forEach case).

- Why this region is promising for fuzzing:
  - It is specific: we’re not targeting “Maglev” broadly; we’re focusing on Maglev’s builtin specialization/lowering of Array.prototype.forEach and the exact code paths managing its loop body, continuations, 
and exception edges.
  - Complex control/data flow:
    - The forEach specialization synthesizes a loop with backedges and Phis (index, length), does representation selection and phi untagging, and relies on deopt continuations to safely fall back when 
assumptions break.
    - There’s a non-trivial intersection between length checks (UnsignedLessThanEqual, ArrayLengthChanged), element-kind checks (CheckMaps), index arithmetic (Int32IncrementWithOverflow), and speculative node
forms (UnsafeSmiUntag), all under possible exception edges.
  - Exception and catch-context materialization:
    - The test explicitly exercises ThrowIfNotCallable followed by try/catch, which forces the Maglev exception path to allocate a catch context and set pending messages. This path includes write barriers and
potentially tricky GC interactions with inlined allocations and pending message state.
  - Historically fragile surfaces:
    - JIT loop lowering around array builtins is a classic source of off-by-ones, stale length reads, holes handling, and elements-kind transitions.
    - Deopt continuations at control flow joins can mismap materialized values under exception edges.
    - Phi untagging across backedges and exception edges (untagged-phis enabled) is a known source of subtle miscompilations.

- Concrete V8 code region to target:
  - The Maglev graph builder logic that recognizes Array.prototype.forEach, constructs the ArrayForEachLoop nodes, and wires up the Lazy/Eager deopt continuations (plus codegen that emits ThrowIfNotCallable 
checks and the exception/catch-context path). In the V8 tree this is typically found in:
    - src/maglev/maglev-graph-builder.cc and closely-related helpers that specialize CallProperty for array iteration builtins (the bits that yield ArrayForEachLoopEagerDeoptContinuation / 
ArrayForEachLoopLazyDeoptContinuation in printing and emit ThrowIfNotCallable).
    - Supporting code that emits exception handling and catch-context creation in Maglev (CreateCatchContext, SetPendingMessage) when the callback is not callable.
  - From the printed graph we see exact node classes and codegen responsibilities: CheckMaps, LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, 
NULL_OR_UNDEFINED Call to the callback, and deopt metadata attached to these callsites.

- How the provided regression directly maps to these code paths:
  - The graph dump shows:
    - ThrowIfNotCallable [n4] at the forEach callsite with lazy deopt continuation markers.
    - ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation laid out around the loop blocks (b3/b8) and exception blocks (b5/b10).
    - Exception edge creates catch contexts via inlined allocation, stores into well-known offsets with/without write barriers, and then SetPendingMessage.
    - Representation selection after Phi untagging makes index an Int32 and retags as necessary; index growth uses Int32IncrementWithOverflow and rechecks against the (re)loaded/untagged length.

- Why this offers an excellent fuzzing surface:
  - We can stress:
    - Callback variability (undefined, primitives, proxies with callable traps, functions that deopt/OSR/tier-change).
    - Array elements kinds (PACKED_SMI, PACKED_DOUBLE, HOLEY_* variants) and dynamic transitions during the loop (mutations of length/elements in callback).
    - Holes, sparse indices, side-effecting getters, and prototype mutations to force re-checks and deopts.
    - Exception paths (callback throws vs ThrowIfNotCallable), nested try/catch/finally, and pending message states across reentrant calls.
    - Index arithmetic edge cases (large lengths, near INT32_MAX, negative coerced indices through odd receivers), representation mismatches in Phis, and aggressive untagged-phis optimization.
    - Cross-realm/modified Array.prototype.forEach, shadowing Symbol.iterator, and property shape changes that should be caught by CheckMaps but might create timing windows with deopt continuations.

- Deliverable for the fuzzing sub-agent:
  - Provide a Fuzzilli ProgramTemplate that preferentially emits forEach-like patterns and toggles array/receiver/callback shapes through feedback-warming runs before calling %OptimizeMaglevOnNextCall, then 
applies stressful behaviors during the optimized call, with optional try/catch wrappers.
  - Focus instrumentation on code regions that correspond to:
    - ArrayForEachLoop(Eager|Lazy)DeoptContinuation.
    - ThrowIfNotCallable guard.
    - CreateCatchContext + SetPendingMessage sequences.
    - UnsafeSmiUntag/Int32IncrementWithOverflow + CheckInt32Condition(ArrayLengthChanged).

Here is the requested JSON object with the selected region and rationale:
{
    CODE REGION: "Maglev Array.prototype.forEach specialization and loop lowering in src/maglev/maglev-graph-builder (and friends), including ArrayForEachLoopEagerDeoptContinuation / 
ArrayForEachLoopLazyDeoptContinuation, ThrowIfNotCallable handling, and Maglev exception/catch-context materialization paths."
    SUMMARY_OF_CODE_REGION: "This region covers the Maglev JIT’s builtin specialization for Array.prototype.forEach. During graph building, Maglev recognizes forEach on arrays and constructs a specialized 
loop with checks (CheckMaps for elements kind, LoadTaggedFieldForProperty for length, UnsafeSmiUntag for untagging Smi length/index, Int32IncrementWithOverflow for induction), and emits a call to the callback
(NULL_OR_UNDEFINED when applicable). It wires Eager and Lazy deopt continuations for safe fallback when feedback assumptions break (e.g., length changed, elements kind mismatch, non-callable callback). When 
the callback is not callable (e.g., undefined), a ThrowIfNotCallable node triggers the exception path. The exception path allocates a catch context via inlined allocation (Map(CATCH_CONTEXT_TYPE)), stores 
scope and exception values using write barriers, and calls SetPendingMessage before resuming at the catch handler. Representation selection and Maglev’s untagged-phis optimization operate across the loop’s 
backedge and exception edges, retagging as needed on returns. The printed graph for regressions/maglev/regress/regress-1455295.js shows these exact nodes and continuations, proving that ArrayForEachLoop 
lowering, ThrowIfNotCallable, and catch-context creation paths are active under Maglev with try/catch. This region is rich in speculative checks, exception edges, GC-sensitive inlined allocations, integer 
untagging/retagging, and deopt metadata—prime territory for subtle JIT miscompilation/exception-state bugs."
    REASONING: "The selected regression explicitly exercises Array.forEach with an undefined callback inside try/catch, then optimizes to Maglev, and runs again. The Maglev graph dump shows: 
ThrowIfNotCallable at the callsite; Eager and Lazy deopt continuations named ArrayForEachLoop*; loop nodes (CheckMaps, UnsafeSmiUntag of length/index, Int32IncrementWithOverflow, LoadFixedArrayElement, 
CheckInt32Condition(ArrayLengthChanged)); and exception handler blocks with CreateCatchContext and SetPendingMessage using write barriers. This precisely maps to Maglev’s specialized forEach lowering and 
exception machinery. These paths combine multiple historically fragile concerns (array length/element-kind dynamics, loop Phi untagging, mixed tagged/untagged representations, deopt across exception edges, 
and GC-visible context materialization), offering a high-value, high-coverage fuzzing surface. The V8 Maglev architecture doc further supports that Maglev embeds speculative assumptions and deopt metadata 
directly during graph building, making builtin specializations a natural target for exploitable miscompilations."
}

### 3. Additional context (if relevant):
- Signals in the execution_data supporting this pick:
  - "ThrowIfNotCallable" nodes surrounding the forEach call and lazy deopt markers.
  - Blocks labeled ArrayForEachLoop(Eager|Lazy)DeoptContinuation.
  - Catch-context inlined allocations (Map(CATCH_CONTEXT_TYPE)) with StoreTaggedFieldWithWriteBarrier and SetPendingMessage.
  - maglev-untagged-phis enabled; frequent UnsafeSmiUntag of Phis and length, and Int32IncrementWithOverflow.
  - Explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) guarding concurrent array length changes.
- Fuzzing angles to consider for templates:
  - Vary array elements kind (SMI, DOUBLE, HOLEY variants), and mutate length/elements inside callback to force deopts.
  - Use non-callable receivers/callbacks (undefined, null, numbers, proxies with callable/non-callable traps) to traverse ThrowIfNotCallable vs. callback paths.
  - Introduce exceptions inside callbacks and nest try/catch/finally to stress catch-context re-materialization.
  - Prototype mutations, getters on length, and cross-realm Array.prototype to perturb CheckMaps/shape dependencies.
  - Large/special lengths (near INT32_MAX), sparse indices, and typed arrays vs fast arrays to explore index/representation edges.

[Step 6: Duration 134.70 seconds| Input tokens: 482,475 | Output tokens: 7,162]
Observations: Here is the final answer from your managed agent 'PickSection':
### 1. Task outcome (short version):
Selected code region for directed fuzzing: Maglev’s Array.prototype.forEach specialization and loop lowering in the Maglev graph builder, including its Eager/Lazy deopt continuations, ThrowIfNotCallable path,
and exception/catch-context materialization.

### 2. Task outcome (extremely detailed version):
- What I analyzed (Stage 0–2):
  - Pulled a random entry and locked onto regressions/maglev/regress/regress-1455295.js. The JS calls |].forEach(undefined) twice inside try/catch, then optimizes to Maglev, then calls once more. The Fuzzilli
translation matches this layout.
  - The execution_data prints the full Maglev SSA graph, Phi untagging, and register allocation for foo. Critically, the graph explicitly shows specialized handling for Array.forEach:
    - Nodes and continuations: ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopEagerDeoptContinuation.
    - Loop structure with CheckMaps(Map|16](PACKED_SMI_ELEMENTS)), LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, and a NULL_OR_UNDEFINED Call.
    - ThrowIfNotCallable guarding the callback (undefined in this test), causing control to flow through exception handler blocks that do CreateCatchContext and SetPendingMessage.
    - Handler blocks show InlinedAllocation(Map(CATCH_CONTEXT_TYPE)) and write-barriered stores for catch context fields.
    - Representation selection shows maglev-untagged-phis and frequent UnsafeSmiUntag/Int32ToNumber transitions; there’s explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) to protect 
against concurrent array-length changes.

- What I searched (Stage 3):
  - Used the V8 knowledge base to gather Maglev architecture context (v8.dev/blog/maglev). That doc aligns with what we see:
    - Maglev builds a specialized IR during graph building, it attaches deopt metadata, performs representation selection (including handling of loop Phis), and then does a single-pass register allocation and
codegen.
    - Deoptimization attachment on nodes like ThrowIfNotCallable is expected in speculative fast paths such as builtin specializations (exactly our forEach case).

- Why this region is promising for fuzzing:
  - It is specific: we’re not targeting “Maglev” broadly; we’re focusing on Maglev’s builtin specialization/lowering of Array.prototype.forEach and the exact code paths managing its loop body, continuations, 
and exception edges.
  - Complex control/data flow:
    - The forEach specialization synthesizes a loop with backedges and Phis (index, length), does representation selection and phi untagging, and relies on deopt continuations to safely fall back when 
assumptions break.
    - There’s a non-trivial intersection between length checks (UnsignedLessThanEqual, ArrayLengthChanged), element-kind checks (CheckMaps), index arithmetic (Int32IncrementWithOverflow), and speculative node
forms (UnsafeSmiUntag), all under possible exception edges.
  - Exception and catch-context materialization:
    - The test explicitly exercises ThrowIfNotCallable followed by try/catch, which forces the Maglev exception path to allocate a catch context and set pending messages. This path includes write barriers and
potentially tricky GC interactions with inlined allocations and pending message state.
  - Historically fragile surfaces:
    - JIT loop lowering around array builtins is a classic source of off-by-ones, stale length reads, holes handling, and elements-kind transitions.
    - Deopt continuations at control flow joins can mismap materialized values under exception edges.
    - Phi untagging across backedges and exception edges (untagged-phis enabled) is a known source of subtle miscompilations.

- Concrete V8 code region to target:
  - The Maglev graph builder logic that recognizes Array.prototype.forEach, constructs the ArrayForEachLoop nodes, and wires up the Lazy/Eager deopt continuations (plus codegen that emits ThrowIfNotCallable 
checks and the exception/catch-context path). In the V8 tree this is typically found in:
    - src/maglev/maglev-graph-builder.cc and closely-related helpers that specialize CallProperty for array iteration builtins (the bits that yield ArrayForEachLoopEagerDeoptContinuation / 
ArrayForEachLoopLazyDeoptContinuation in printing and emit ThrowIfNotCallable).
    - Supporting code that emits exception handling and catch-context creation in Maglev (CreateCatchContext, SetPendingMessage) when the callback is not callable.
  - From the printed graph we see exact node classes and codegen responsibilities: CheckMaps, LoadTaggedFieldForProperty(length), UnsafeSmiUntag, Int32IncrementWithOverflow, LoadFixedArrayElement, 
NULL_OR_UNDEFINED Call to the callback, and deopt metadata attached to these callsites.

- How the provided regression directly maps to these code paths:
  - The graph dump shows:
    - ThrowIfNotCallable |n4] at the forEach callsite with lazy deopt continuation markers.
    - ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation laid out around the loop blocks (b3/b8) and exception blocks (b5/b10).
    - Exception edge creates catch contexts via inlined allocation, stores into well-known offsets with/without write barriers, and then SetPendingMessage.
    - Representation selection after Phi untagging makes index an Int32 and retags as necessary; index growth uses Int32IncrementWithOverflow and rechecks against the (re)loaded/untagged length.

- Why this offers an excellent fuzzing surface:
  - We can stress:
    - Callback variability (undefined, primitives, proxies with callable traps, functions that deopt/OSR/tier-change).
    - Array elements kinds (PACKED_SMI, PACKED_DOUBLE, HOLEY_* variants) and dynamic transitions during the loop (mutations of length/elements in callback).
    - Holes, sparse indices, side-effecting getters, and prototype mutations to force re-checks and deopts.
    - Exception paths (callback throws vs ThrowIfNotCallable), nested try/catch/finally, and pending message states across reentrant calls.
    - Index arithmetic edge cases (large lengths, near INT32_MAX, negative coerced indices through odd receivers), representation mismatches in Phis, and aggressive untagged-phis optimization.
    - Cross-realm/modified Array.prototype.forEach, shadowing Symbol.iterator, and property shape changes that should be caught by CheckMaps but might create timing windows with deopt continuations.

- Deliverable for the fuzzing sub-agent:
  - Provide a Fuzzilli ProgramTemplate that preferentially emits forEach-like patterns and toggles array/receiver/callback shapes through feedback-warming runs before calling %OptimizeMaglevOnNextCall, then 
applies stressful behaviors during the optimized call, with optional try/catch wrappers.
  - Focus instrumentation on code regions that correspond to:
    - ArrayForEachLoop(Eager|Lazy)DeoptContinuation.
    - ThrowIfNotCallable guard.
    - CreateCatchContext + SetPendingMessage sequences.
    - UnsafeSmiUntag/Int32IncrementWithOverflow + CheckInt32Condition(ArrayLengthChanged).

Here is the requested JSON object with the selected region and rationale:
{
    CODE REGION: "Maglev Array.prototype.forEach specialization and loop lowering in src/maglev/maglev-graph-builder (and friends), including ArrayForEachLoopEagerDeoptContinuation / 
ArrayForEachLoopLazyDeoptContinuation, ThrowIfNotCallable handling, and Maglev exception/catch-context materialization paths."
    SUMMARY_OF_CODE_REGION: "This region covers the Maglev JIT’s builtin specialization for Array.prototype.forEach. During graph building, Maglev recognizes forEach on arrays and constructs a specialized 
loop with checks (CheckMaps for elements kind, LoadTaggedFieldForProperty for length, UnsafeSmiUntag for untagging Smi length/index, Int32IncrementWithOverflow for induction), and emits a call to the callback
(NULL_OR_UNDEFINED when applicable). It wires Eager and Lazy deopt continuations for safe fallback when feedback assumptions break (e.g., length changed, elements kind mismatch, non-callable callback). When 
the callback is not callable (e.g., undefined), a ThrowIfNotCallable node triggers the exception path. The exception path allocates a catch context via inlined allocation (Map(CATCH_CONTEXT_TYPE)), stores 
scope and exception values using write barriers, and calls SetPendingMessage before resuming at the catch handler. Representation selection and Maglev’s untagged-phis optimization operate across the loop’s 
backedge and exception edges, retagging as needed on returns. The printed graph for regressions/maglev/regress/regress-1455295.js shows these exact nodes and continuations, proving that ArrayForEachLoop 
lowering, ThrowIfNotCallable, and catch-context creation paths are active under Maglev with try/catch. This region is rich in speculative checks, exception edges, GC-sensitive inlined allocations, integer 
untagging/retagging, and deopt metadata—prime territory for subtle JIT miscompilation/exception-state bugs."
    REASONING: "The selected regression explicitly exercises Array.forEach with an undefined callback inside try/catch, then optimizes to Maglev, and runs again. The Maglev graph dump shows: 
ThrowIfNotCallable at the callsite; Eager and Lazy deopt continuations named ArrayForEachLoop*; loop nodes (CheckMaps, UnsafeSmiUntag of length/index, Int32IncrementWithOverflow, LoadFixedArrayElement, 
CheckInt32Condition(ArrayLengthChanged)); and exception handler blocks with CreateCatchContext and SetPendingMessage using write barriers. This precisely maps to Maglev’s specialized forEach lowering and 
exception machinery. These paths combine multiple historically fragile concerns (array length/element-kind dynamics, loop Phi untagging, mixed tagged/untagged representations, deopt across exception edges, 
and GC-visible context materialization), offering a high-value, high-coverage fuzzing surface. The V8 Maglev architecture doc further supports that Maglev embeds speculative assumptions and deopt metadata 
directly during graph building, making builtin specializations a natural target for exploitable miscompilations."
}

### 3. Additional context (if relevant):
- Signals in the execution_data supporting this pick:
  - "ThrowIfNotCallable" nodes surrounding the forEach call and lazy deopt markers.
  - Blocks labeled ArrayForEachLoop(Eager|Lazy)DeoptContinuation.
  - Catch-context inlined allocations (Map(CATCH_CONTEXT_TYPE)) with StoreTaggedFieldWithWriteBarrier and SetPendingMessage.
  - maglev-untagged-phis enabled; frequent UnsafeSmiUntag of Phis and length, and Int32IncrementWithOverflow.
  - Explicit CheckInt32Condition(UnsignedLessThanEqual, ArrayLengthChanged) guarding concurrent array length changes.
- Fuzzing angles to consider for templates:
  - Vary array elements kind (SMI, DOUBLE, HOLEY variants), and mutate length/elements inside callback to force deopts.
  - Use non-callable receivers/callbacks (undefined, null, numbers, proxies with callable/non-callable traps) to traverse ThrowIfNotCallable vs. callback paths.
  - Introduce exceptions inside callbacks and nest try/catch/finally to stress catch-context re-materialization.
  - Prototype mutations, getters on length, and cross-realm Array.prototype to perturb CheckMaps/shape dependencies.
  - Large/special lengths (near INT32_MAX), sparse indices, and typed arrays vs fast arrays to explore index/representation edges.
[Step 1: Duration 248.05 seconds| Input tokens: 2,439 | Output tokens: 655]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'CodeAnalyzer' with arguments: {'task': 'Stage 1. Analyze the selected code region returned by PickSection: Maglev’s Array.prototype.forEach specialization and loop lowering in the Maglev    │
│ graph builder, including ArrayForEachLoopEagerDeoptContinuation / ArrayForEachLoopLazyDeoptContinuation, ThrowIfNotCallable handling, and exception/catch-context materialization (CreateCatchContext,       │
│ SetPendingMessage). Goals: 1) Map the exact files and functions in V8 (e.g., src/maglev/maglev-graph-builder.cc, related Maglev nodes and codegen) responsible for recognizing and lowering                  │
│ Array.prototype.forEach into a loop; 2) Identify where and how Eager/Lazy deopt continuations are constructed and how deopt metadata is attached to nodes; 3) Locate ThrowIfNotCallable emission and         │
│ exception-edge wiring into catch handlers; 4) Locate representation selection phases that perform untagged-phi transformations and index arithmetic (UnsafeSmiUntag, Int32IncrementWithOverflow), and any    │
│ array-length change checks (UnsignedLessThanEqual, ArrayLengthChanged); 5) Identify interactions with elements kinds (PACKED_SMI, DOUBLE, HOLEY variants), map checks, and feedback-driven specialization;   │
│ 6) Summarize potentially fragile spots: off-by-one in loop bounds, holes handling, length changes mid-iteration, mis-wiring of deopt continuations at backedges/exception joins, materialization of catch    │
│ contexts and write barriers, pending message state; 7) Provide canonical function/class names, file paths, and doc references from V8 source/knowledge base; 8) Provide a concise list of subcomponents that │
│ a ProgramTemplate should stress (e.g., callback callable/non-callable, try/catch nesting, element-kind transitions, length mutations, proxy callbacks, cross-realm forEach). Output: a structured summary    │
│ enumerating the relevant files, classes, key functions, nodes, known edge cases, and suggested fuzzing axes tied to those nodes. This is preparatory for a later ProgramBuilder request.'}                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────── New run - CodeAnalyzer ───────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                              │
│ You're a helpful agent named 'CodeAnalyzer'.                                                                                                                                                                 │
│ You have been submitted this task by your manager.                                                                                                                                                           │
│ ---                                                                                                                                                                                                          │
│ Task:                                                                                                                                                                                                        │
│ Stage 1. Analyze the selected code region returned by PickSection: Maglev’s Array.prototype.forEach specialization and loop lowering in the Maglev graph builder, including                                  │
│ ArrayForEachLoopEagerDeoptContinuation / ArrayForEachLoopLazyDeoptContinuation, ThrowIfNotCallable handling, and exception/catch-context materialization (CreateCatchContext, SetPendingMessage). Goals: 1)  │
│ Map the exact files and functions in V8 (e.g., src/maglev/maglev-graph-builder.cc, related Maglev nodes and codegen) responsible for recognizing and lowering Array.prototype.forEach into a loop; 2)        │
│ Identify where and how Eager/Lazy deopt continuations are constructed and how deopt metadata is attached to nodes; 3) Locate ThrowIfNotCallable emission and exception-edge wiring into catch handlers; 4)   │
│ Locate representation selection phases that perform untagged-phi transformations and index arithmetic (UnsafeSmiUntag, Int32IncrementWithOverflow), and any array-length change checks                       │
│ (UnsignedLessThanEqual, ArrayLengthChanged); 5) Identify interactions with elements kinds (PACKED_SMI, DOUBLE, HOLEY variants), map checks, and feedback-driven specialization; 6) Summarize potentially     │
│ fragile spots: off-by-one in loop bounds, holes handling, length changes mid-iteration, mis-wiring of deopt continuations at backedges/exception joins, materialization of catch contexts and write          │
│ barriers, pending message state; 7) Provide canonical function/class names, file paths, and doc references from V8 source/knowledge base; 8) Provide a concise list of subcomponents that a ProgramTemplate  │
│ should stress (e.g., callback callable/non-callable, try/catch nesting, element-kind transitions, length mutations, proxy callbacks, cross-realm forEach). Output: a structured summary enumerating the      │
│ relevant files, classes, key functions, nodes, known edge cases, and suggested fuzzing axes tied to those nodes. This is preparatory for a later ProgramBuilder request.                                     │
│ ---                                                                                                                                                                                                          │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.                    │
│                                                                                                                                                                                                              │
│ Your final_answer WILL HAVE to contain these parts:                                                                                                                                                          │
│ ### 1. Task outcome (short version):                                                                                                                                                                         │
│ ### 2. Task outcome (extremely detailed version):                                                                                                                                                            │
│ ### 3. Additional context (if relevant):                                                                                                                                                                     │
│                                                                                                                                                                                                              │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.                                                                                        │
│ And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.                                                              │
│                                                                                                                                                                                                              │
╰─ LiteLLMModel - gpt-5 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'V8Search' with arguments: {'task': 'Find the exact places in V8’s Maglev pipeline that implement specialization and loop-lowering for Array.prototype.forEach, including any builder methods, │
│ nodes, and codegen paths. Specifically: 1) In src/maglev/maglev-graph-builder.cc (and nearby), locate functions that recognize Array.prototype.forEach and lower it into an explicit index loop. Look for    │
│ names like TryBuildArrayForEach, BuildArrayForEachLoop, BuildArrayIteratingBuiltin, or inline-Array helpers; also search in maglev-graph-builder-inl.cc and maglev-graph-building.cc if present. 2) Find the │
│ Eager/Lazy deopt continuation types used for the forEach loop (e.g., ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation) and where the deopt metadata/continuations are attached  │
│ to nodes/loop headers. 3) Find ThrowIfNotCallable emission in Maglev for callback validation, and how exception edges are wired to catch handlers, including CreateCatchContext and SetPendingMessage calls  │
│ (likely in builtin or runtime lowering; search in maglev-graph-builder for catch blocks materialization and in maglev-exception-handler or maglev-catch). 4) Identify representation selection/untagging     │
│ during loop lowering (UnsafeSmiUntag, Int32IncrementWithOverflow, Uint32/UnsignedLessThanEqual comparisons), and any ArrayLengthChanged or length-change guards. 5) Locate interactions with elements kinds  │
│ (PACKED_SMI/DOUBLE/HOLEY), map checks, feedback-based specialization for arrays within Maglev (e.g., monomorphic array map checks, elements kind dispatch) and the corresponding nodes (e.g., LoadElement,   │
│ CheckArrayMaps, CheckFixedArrayBounds, CheckElementsKind). 6) Return the file paths, canonical class/function names, node names, and any doc/comments that mention ArrayForEach specialization and           │
│ loop-lowering inside Maglev.'}                                                                                                                                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────── New run - V8Search ─────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                              │
│ You're a helpful agent named 'V8Search'.                                                                                                                                                                     │
│ You have been submitted this task by your manager.                                                                                                                                                           │
│ ---                                                                                                                                                                                                          │
│ Task:                                                                                                                                                                                                        │
│ Find the exact places in V8’s Maglev pipeline that implement specialization and loop-lowering for Array.prototype.forEach, including any builder methods, nodes, and codegen paths. Specifically: 1) In      │
│ src/maglev/maglev-graph-builder.cc (and nearby), locate functions that recognize Array.prototype.forEach and lower it into an explicit index loop. Look for names like TryBuildArrayForEach,                 │
│ BuildArrayForEachLoop, BuildArrayIteratingBuiltin, or inline-Array helpers; also search in maglev-graph-builder-inl.cc and maglev-graph-building.cc if present. 2) Find the Eager/Lazy deopt continuation    │
│ types used for the forEach loop (e.g., ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation) and where the deopt metadata/continuations are attached to nodes/loop headers. 3) Find │
│ ThrowIfNotCallable emission in Maglev for callback validation, and how exception edges are wired to catch handlers, including CreateCatchContext and SetPendingMessage calls (likely in builtin or runtime   │
│ lowering; search in maglev-graph-builder for catch blocks materialization and in maglev-exception-handler or maglev-catch). 4) Identify representation selection/untagging during loop lowering              │
│ (UnsafeSmiUntag, Int32IncrementWithOverflow, Uint32/UnsignedLessThanEqual comparisons), and any ArrayLengthChanged or length-change guards. 5) Locate interactions with elements kinds                       │
│ (PACKED_SMI/DOUBLE/HOLEY), map checks, feedback-based specialization for arrays within Maglev (e.g., monomorphic array map checks, elements kind dispatch) and the corresponding nodes (e.g., LoadElement,   │
│ CheckArrayMaps, CheckFixedArrayBounds, CheckElementsKind). 6) Return the file paths, canonical class/function names, node names, and any doc/comments that mention ArrayForEach specialization and           │
│ loop-lowering inside Maglev.                                                                                                                                                                                 │
│ ---                                                                                                                                                                                                          │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.                    │
│                                                                                                                                                                                                              │
│ Your final_answer WILL HAVE to contain these parts:                                                                                                                                                          │
│ ### 1. Task outcome (short version):                                                                                                                                                                         │
│ ### 2. Task outcome (extremely detailed version):                                                                                                                                                            │
│ ### 3. Additional context (if relevant):                                                                                                                                                                     │
│                                                                                                                                                                                                              │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.                                                                                        │
│ And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.                                                              │
│                                                                                                                                                                                                              │
╰─ LiteLLMModel - gpt-5 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder', 'options': "--filter='maglev-graph-builder'"}                                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'forEach', 'options': "--filter='forEach'"}                                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'options': '-L 2 maglev'}                                                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='forEach' 'forEach' | head -n 1000
Observations: unknown option: forEach
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='maglev-graph-builder' 'maglev-graph-builder' | head -n 1000
Observations: unknown option: maglev-graph-builder
Command: cd /Users/tarun/v8/v8/src && tree -L 2 maglev | head -n 1000
Observations: maglev
├── arm
│   ├── maglev-assembler-arm-inl.h
│   ├── maglev-assembler-arm.cc
│   └── maglev-ir-arm.cc
├── arm64
│   ├── maglev-assembler-arm64-inl.h
│   ├── maglev-assembler-arm64.cc
│   └── maglev-ir-arm64.cc
├── DEPS
├── DIR_METADATA
├── maglev-assembler-inl.h
├── maglev-assembler.cc
├── maglev-assembler.h
├── maglev-basic-block.h
├── maglev-code-gen-state.h
├── maglev-code-generator.cc
├── maglev-code-generator.h
├── maglev-compilation-info.cc
├── maglev-compilation-info.h
├── maglev-compilation-unit.cc
├── maglev-compilation-unit.h
├── maglev-compiler.cc
├── maglev-compiler.h
├── maglev-concurrent-dispatcher.cc
├── maglev-concurrent-dispatcher.h
├── maglev-graph-builder.cc
├── maglev-graph-builder.h
├── maglev-graph-labeller.h
├── maglev-graph-printer.cc
├── maglev-graph-printer.h
├── maglev-graph-processor.h
├── maglev-graph-verifier.h
├── maglev-graph.h
├── maglev-interpreter-frame-state.cc
├── maglev-interpreter-frame-state.h
├── maglev-ir-inl.h
├── maglev-ir.cc
├── maglev-ir.h
├── maglev-phi-representation-selector.cc
├── maglev-phi-representation-selector.h
├── maglev-pipeline-statistics.cc
├── maglev-pipeline-statistics.h
├── maglev-post-hoc-optimizations-processors.h
├── maglev-pre-regalloc-codegen-processors.h
├── maglev-regalloc-data.h
├── maglev-regalloc.cc
├── maglev-regalloc.h
├── maglev-register-frame-array.h
├── maglev.cc
├── maglev.h
├── OWNERS
├── riscv
│   ├── maglev-assembler-riscv-inl.h
│   ├── maglev-assembler-riscv.cc
│   └── maglev-ir-riscv.cc
├── s390
│   ├── maglev-assembler-s390-inl.h
│   ├── maglev-assembler-s390.cc
│   └── maglev-ir-s390.cc
└── x64
    ├── maglev-assembler-x64-inl.h
    ├── maglev-assembler-x64.cc
    └── maglev-ir-x64.cc

6 directories, 58 files
[Step 1: Duration 8.78 seconds| Input tokens: 2,476 | Output tokens: 421]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '.', 'options': '--filter forEach'}                                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '.', 'options': '--filter TryBuildArrayForEach'}                                                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '.', 'options': '--filter BuildArrayForEachLoop'}                                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '.', 'options': '--filter ArrayForEach'}                                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '.', 'options': '--filter maglev-graph-builder'}                                                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter forEach '.' | head -n 1000Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter TryBuildArrayForEach '.' | head -n 1000
Observations: unknown option: .

Observations: unknown option: .
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter ArrayForEach '.' | head -n 1000
Observations: unknown option: .
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter BuildArrayForEachLoop '.' | head -n 1000
Observations: unknown option: .
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-graph-builder '.' | head -n 1000
Observations: unknown option: .
[Step 2: Duration 16.51 seconds| Input tokens: 6,242 | Output tokens: 1,083]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=maglev-graph-builder'}                                                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'TryBuildArrayForEach|ArrayForEach', 'options': '-n maglev'}                                                                                             │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'BuildArrayIteratingBuiltin|BuildArrayForEachLoop|IteratingBuiltin', 'options': '-n maglev'}                                                             │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=maglev-graph-builder '' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable|IsCallable|CallCallable', 'options': '-n maglev'}                                                                                    │
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'TryBuildArrayForEach|ArrayForEach' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'BuildArrayIteratingBuiltin|BuildArrayForEachLoop|IteratingBuiltin' | head -n 1000

│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CreateCatchContext|catch block|exception|exception handler|SetPendingMessage', 'options': '-n maglev'}                                                  │
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'EagerDeoptContinuation|LazyDeoptContinuation|DeoptContinuation', 'options': '-n maglev'}                                                                │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'ThrowIfNotCallable|IsCallable|CallCallable' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'UnsafeSmiUntag|SmiUntag|Int32IncrementWithOverflow|UnsignedLessThan|UnsignedLessThanEqual|Int32LessThanOrEqual|Uint32', 'options': '-n maglev'}         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'CreateCatchContext|catch block|exception|exception handler|SetPendingMessage' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'EagerDeoptContinuation|LazyDeoptContinuation|DeoptContinuation' | head -n 1000
Observations: rg: TryBuildArrayForEach|ArrayForEach: IO error for operation on TryBuildArrayForEach|ArrayForEach: No such file or directory (os error 2)

error 2)
Observations: rg: BuildArrayIteratingBuiltin|BuildArrayForEachLoop|IteratingBuiltin: IO error for operation on BuildArrayIteratingBuiltin|BuildArrayForEachLoop|IteratingBuiltin: No such file or directory (os 
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ArrayLengthChanged|length change|length guard', 'options': '-n maglev'}                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: unknown option:
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'UnsafeSmiUntag|SmiUntag|Int32IncrementWithOverflow|UnsignedLessThan|UnsignedLessThanEqual|Int32LessThanOrEqual|Uint32' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ElementsKind|CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|ElementsArray|ElementsKind', 'options': '-n maglev'}                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Observations: rg: ThrowIfNotCallable|IsCallable|CallCallable: IO error for operation on ThrowIfNotCallable|IsCallable|CallCallable: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'ArrayLengthChanged|length change|length guard' | head -n 1000
2)
Observations: rg: EagerDeoptContinuation|LazyDeoptContinuation|DeoptContinuation: IO error for operation on EagerDeoptContinuation|LazyDeoptContinuation|DeoptContinuation: No such file or directory (os error 
Observations: rg: CreateCatchContext|catch block|exception|exception handler|SetPendingMessage: IO error for operation on CreateCatchContext|catch block|exception|exception handler|SetPendingMessage: No such 
file or directory (os error 2)
Observations: rg: UnsafeSmiUntag|SmiUntag|Int32IncrementWithOverflow|UnsignedLessThan|UnsignedLessThanEqual|Int32LessThanOrEqual|Uint32: IO error for operation on 
UnsafeSmiUntag|SmiUntag|Int32IncrementWithOverflow|UnsignedLessThan|UnsignedLessThanEqual|Int32LessThanOrEqual|Uint32: No such file or directory (os error 2)
Observations: rg: ArrayLengthChanged|length change|length guard: IO error for operation on ArrayLengthChanged|length change|length guard: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'ElementsKind|CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|ElementsArray|ElementsKind' | head -n 1000
Observations: rg: ElementsKind|CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|ElementsArray|ElementsKind: IO error for operation on 
ElementsKind|CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|ElementsArray|ElementsKind: No such file or directory (os error 2)
[Step 3: Duration 31.59 seconds| Input tokens: 10,195 | Output tokens: 2,776]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'forEach', 'options': '-n'}                                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n 'forEach' | head -n 1000
Observations: inspector/v8-inspector-session-impl.cc:232:  m_inspector->forEachContext(m_contextGroupId,
inspector/v8-inspector-session-impl.cc:267:  m_inspector->forEachContext(
inspector/v8-inspector-session-impl.cc:346:  m_inspector->forEachContext(
inspector/v8-inspector-session-impl.cc:355:  m_inspector->forEachContext(m_contextGroupId,
maglev/maglev-graph-builder.cc:8072:      std::cout << "  ! Failed to reduce Array.prototype.forEach - not enough "
maglev/maglev-graph-builder.cc:8082:      std::cout << "  ! Failed to reduce Array.prototype.forEach - receiver "
maglev/maglev-graph-builder.cc:8093:      std::cout << "  ! Failed to reduce Array.prototype.forEach - doesn't "
maglev/maglev-graph-builder.cc:8103:      std::cout << "  ! Failed to reduce Array.prototype.forEach - invalidated "
maglev/maglev-graph-builder.cc:8113:      std::cout << "  ! Failed to reduce Array.prototype.forEach - callback is "
inspector/v8-runtime-agent-impl.cc:919:  m_inspector->forEachContext(
inspector/v8-runtime-agent-impl.cc:946:  inspector->forEachSession(
inspector/v8-runtime-agent-impl.cc:1066:  m_inspector->forEachContext(
inspector/v8-runtime-agent-impl.cc:1114:    m_inspector->forEachContext(m_session->contextGroupId(),
inspector/v8-inspector-impl.cc:233:  forEachSession(
inspector/v8-inspector-impl.cc:256:  forEachSession(groupId, |&inspectedContext](V8InspectorSessionImpl* session) {
inspector/v8-inspector-impl.cc:272:  forEachSession(contextGroupId,
inspector/v8-inspector-impl.cc:398:void V8InspectorImpl::forEachContext(
inspector/v8-inspector-impl.cc:416:void V8InspectorImpl::forEachSession(
inspector/v8-console-message.cc:569:  inspector->forEachSession(
inspector/v8-console-message.cc:595:  m_inspector->forEachSession(m_contextGroupId,
inspector/v8-inspector-impl.h:135:  void forEachContext(int contextGroupId,
inspector/v8-inspector-impl.h:137:  void forEachSession(
inspector/v8-console.cc:190:  void forEachSession(std::function<void(V8InspectorSessionImpl*)> callback) {
inspector/v8-console.cc:191:    m_inspector->forEachSession(groupId(), std::move(callback));
inspector/v8-console.cc:378:  helper.forEachSession(|&title](V8InspectorSessionImpl* session) {
inspector/v8-console.cc:393:  helper.forEachSession(|&title](V8InspectorSessionImpl* session) {
inspector/v8-debugger.cc:131:      m_inspector->forEachSession(
inspector/v8-debugger.cc:269:  m_inspector->forEachSession(
inspector/v8-debugger.cc:513:  m_inspector->forEachSession(
inspector/v8-debugger.cc:531:  m_inspector->forEachSession(
inspector/v8-debugger.cc:544:    m_inspector->forEachSession(
inspector/v8-debugger.cc:554:  m_inspector->forEachSession(contextGroupId,
inspector/v8-debugger.cc:608:  m_inspector->forEachSession(
inspector/v8-debugger.cc:629:  m_inspector->forEachSession(
inspector/v8-debugger.cc:638:  m_inspector->forEachSession(
inspector/v8-debugger.cc:657:  m_inspector->forEachSession(
inspector/v8-debugger.cc:700:  m_inspector->forEachSession(
inspector/v8-debugger.cc:720:  m_inspector->forEachSession(
inspector/v8-debugger.cc:1350:    m_inspector->forEachSession(
init/bootstrapper.cc:2615:        isolate_, proto, "forEach", Builtin::kArrayForEach, 1, kDontAdapt);
init/bootstrapper.cc:4395:    SimpleInstallFunction(isolate_, prototype, "forEach",
init/bootstrapper.cc:4572:    SimpleInstallFunction(isolate_, prototype, "forEach",
init/bootstrapper.cc:4660:    SimpleInstallFunction(isolate_, prototype, "forEach",
init/bootstrapper.cc:5575:  SimpleInstallFunction(isolate(), iterator_prototype, "forEach",
builtins/iterator-helpers.tq:849:// --- forEach helper
builtins/iterator-helpers.tq:855:  const methodName: constexpr string = 'Iterator.prototype.forEach';
builtins/array-foreach.tq:9:  // All continuation points in the optimized forEach implementation are
builtins/array-foreach.tq:25:  // All continuation points in the optimized forEach implementation are
builtins/array-foreach.tq:94:    RequireObjectCoercible(receiver, 'Array.prototype.forEach');
builtins/builtins-definitions.h:828:  /* ES #sec-map.prototype.forEach */                                          \
builtins/builtins-collections-gen.cc:2105:  const char* const kMethodName = "Map.prototype.forEach";
builtins/builtins-collections-gen.cc:2304:  const char* const kMethodName = "Set.prototype.forEach";
builtins/typed-array-foreach.tq:8:const kBuiltinNameForEach: constexpr string = '%TypedArray%.prototype.forEach';
[Step 4: Duration 27.50 seconds| Input tokens: 14,939 | Output tokens: 3,766]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ArrayForEach|forEach', 'options': '-n maglev/maglev-graph-builder.cc'}                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc'}                                                                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n maglev/maglev-graph-builder.cc 'ArrayForEach|forEach' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'Deopt|EagerDeopt|LazyDeopt|Continuation|DeoptContinuation', 'options': '-n maglev'}                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable|Callable|IsCallable', 'options': '-n maglev'}                                                                                        │
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'Deopt|EagerDeopt|LazyDeopt|Continuation|DeoptContinuation' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Observations: rg: ArrayForEach|forEach: IO error for operation on ArrayForEach|forEach: No such file or directory (os error 2)
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|LoadFixedArrayElement|LoadElements|ElementsKind|CheckMaps', 'options': '-n maglev'}  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: rg: Deopt|EagerDeopt|LazyDeopt|Continuation|DeoptContinuation: IO error for operation on Deopt|EagerDeopt|LazyDeopt|Continuation|DeoptContinuation: No such file or directory (os error 2)
Observations: File 'maglev/maglev-graph-builder.cc' has 14107 lines and is divided into 5 sections (each section is 3000 lines).
To read this file, please specify a section number between 1 and 5 using the 'section' argument.
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'ThrowIfNotCallable|Callable|IsCallable' | head -n 1000
Observations: rg: ThrowIfNotCallable|Callable|IsCallable: IO error for operation on ThrowIfNotCallable|Callable|IsCallable: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|LoadFixedArrayElement|LoadElements|ElementsKind|CheckMaps' | head -n 1000
Observations: rg: CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|LoadFixedArrayElement|LoadElements|ElementsKind|CheckMaps: IO error for operation on 
CheckElementsKind|CheckArrayMaps|CheckFixedArrayBounds|LoadElement|LoadFixedArrayElement|LoadElements|ElementsKind|CheckMaps: No such file or directory (os error 2)
[Step 5: Duration 16.04 seconds| Input tokens: 20,953 | Output tokens: 4,542]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 6 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc', 'section': 3}                                                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Command: cd /Users/tarun/v8/v8/src && sed -n '6001,9000p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Observations: Showing section 3/5 (lines 6001-9000) of 'maglev/maglev-graph-builder.cc':
      }
      return EmitUnconditionalDeopt(reason);
    }
  }
  AddNewNode<CheckInt32Condition>({lhs, rhs}, condition, reason);
  return ReduceResult::Done();
}

ValueNode* MaglevGraphBuilder::BuildLoadElements(ValueNode* object) {
  ReduceResult known_elements =
      TryFindLoadedProperty(known_node_aspects().loaded_properties, object,
                            KnownNodeAspects::LoadedPropertyMapKey::Elements());
  if (known_elements.IsDone()) {
    DCHECK(known_elements.IsDoneWithValue());
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  * Reusing non-constant |Elements] "
                << PrintNodeLabel(graph_labeller(), known_elements.value())
                << ": " << PrintNode(graph_labeller(), known_elements.value())
                << std::endl;
    }
    return known_elements.value();
  }

  DCHECK_EQ(JSObject::kElementsOffset, JSArray::kElementsOffset);
  ValueNode* elements = BuildLoadTaggedField(object, JSObject::kElementsOffset);
  RecordKnownProperty(object,
                      KnownNodeAspects::LoadedPropertyMapKey::Elements(),
                      elements, false, compiler::AccessMode::kLoad);
  return elements;
}

ReduceResult MaglevGraphBuilder::BuildLoadTypedArrayLength(
    ValueNode* object, ElementsKind elements_kind) {
  DCHECK(IsTypedArrayOrRabGsabTypedArrayElementsKind(elements_kind));
  bool is_variable_length = IsRabGsabTypedArrayElementsKind(elements_kind);

  if (!is_variable_length) {
    // Note: We can't use broker()->length_string() here, because it could
    // conflict with redefinitions of the TypedArray length property.
    RETURN_IF_DONE(TryFindLoadedProperty(
        known_node_aspects().loaded_constant_properties, object,
        KnownNodeAspects::LoadedPropertyMapKey::TypedArrayLength()));
  }

  ValueNode* result = AddNewNode<LoadTypedArrayLength>({object}, elements_kind);
  if (!is_variable_length) {
    RecordKnownProperty(
        object, KnownNodeAspects::LoadedPropertyMapKey::TypedArrayLength(),
        result, true, compiler::AccessMode::kLoad);
  }
  return result;
}

ValueNode* MaglevGraphBuilder::BuildLoadTypedArrayElement(
    ValueNode* object, ValueNode* index, ElementsKind elements_kind) {
#define BUILD_AND_RETURN_LOAD_TYPED_ARRAY(Type)                     \
  return AddNewNode<Load##Type##TypedArrayElement>({object, index}, \
                                                   elements_kind);

  switch (elements_kind) {
    case INT8_ELEMENTS:
    case INT16_ELEMENTS:
    case INT32_ELEMENTS:
      BUILD_AND_RETURN_LOAD_TYPED_ARRAY(SignedInt);
    case UINT8_CLAMPED_ELEMENTS:
    case UINT8_ELEMENTS:
    case UINT16_ELEMENTS:
    case UINT32_ELEMENTS:
      BUILD_AND_RETURN_LOAD_TYPED_ARRAY(UnsignedInt);
    case FLOAT32_ELEMENTS:
    case FLOAT64_ELEMENTS:
      BUILD_AND_RETURN_LOAD_TYPED_ARRAY(Double);
    default:
      UNREACHABLE();
  }
#undef BUILD_AND_RETURN_LOAD_TYPED_ARRAY
}

void MaglevGraphBuilder::BuildStoreTypedArrayElement(
    ValueNode* object, ValueNode* index, ElementsKind elements_kind) {
#define BUILD_STORE_TYPED_ARRAY(Type, value)                           \
  AddNewNode<Store##Type##TypedArrayElement>({object, index, (value)}, \
                                             elements_kind);

  // TODO(leszeks): These operations have a deopt loop when the ToNumber
  // conversion sees a type other than number or oddball. Turbofan has the same
  // deopt loop, but ideally we'd avoid it.
  switch (elements_kind) {
    case UINT8_CLAMPED_ELEMENTS: {
      BUILD_STORE_TYPED_ARRAY(Int, GetAccumulatorUint8ClampedForToNumber())
      break;
    }
    case INT8_ELEMENTS:
    case INT16_ELEMENTS:
    case INT32_ELEMENTS:
    case UINT8_ELEMENTS:
    case UINT16_ELEMENTS:
    case UINT32_ELEMENTS:
      BUILD_STORE_TYPED_ARRAY(
          Int, GetAccumulatorTruncatedInt32ForToNumber(
                   NodeType::kNumberOrOddball,
                   TaggedToFloat64ConversionType::kNumberOrOddball))
      break;
    case FLOAT32_ELEMENTS:
    case FLOAT64_ELEMENTS:
      BUILD_STORE_TYPED_ARRAY(
          Double, GetAccumulatorHoleyFloat64ForToNumber(
                      NodeType::kNumberOrOddball,
                      TaggedToFloat64ConversionType::kNumberOrOddball))
      break;
    default:
      UNREACHABLE();
  }
#undef BUILD_STORE_TYPED_ARRAY
}

ReduceResult MaglevGraphBuilder::TryBuildElementAccessOnTypedArray(
    ValueNode* object, ValueNode* index_object,
    const compiler::ElementAccessInfo& access_info,
    compiler::KeyedAccessMode const& keyed_mode) {
  DCHECK(HasOnlyJSTypedArrayMaps(
      base::VectorOf(access_info.lookup_start_object_maps())));
  ElementsKind elements_kind = access_info.elements_kind();
  if (elements_kind == FLOAT16_ELEMENTS ||
      elements_kind == BIGUINT64_ELEMENTS ||
      elements_kind == BIGINT64_ELEMENTS) {
    return ReduceResult::Fail();
  }
  if (keyed_mode.access_mode() == compiler::AccessMode::kLoad &&
      LoadModeHandlesOOB(keyed_mode.load_mode())) {
    // TODO(victorgomes): Handle OOB mode.
    return ReduceResult::Fail();
  }
  if (keyed_mode.access_mode() == compiler::AccessMode::kStore &&
      StoreModeIgnoresTypeArrayOOB(keyed_mode.store_mode())) {
    // TODO(victorgomes): Handle OOB mode.
    return ReduceResult::Fail();
  }
  if (keyed_mode.access_mode() == compiler::AccessMode::kStore &&
      elements_kind == UINT8_CLAMPED_ELEMENTS &&
      !IsSupported(CpuOperation::kFloat64Round)) {
    // TODO(victorgomes): Technically we still support if value (in the
    // accumulator) is of type int32. It would be nice to have a roll back
    // mechanism instead, so that we do not need to check this early.
    return ReduceResult::Fail();
  }
  if (!broker()->dependencies()->DependOnArrayBufferDetachingProtector()) {
    // TODO(leszeks): Eliminate this check.
    AddNewNode<CheckTypedArrayNotDetached>({object});
  }
  ValueNode* index;
  ValueNode* length;
  GET_VALUE_OR_ABORT(index, GetUint32ElementIndex(index_object));
  GET_VALUE_OR_ABORT(length, BuildLoadTypedArrayLength(object, elements_kind));
  AddNewNode<CheckTypedArrayBounds>({index, length});
  switch (keyed_mode.access_mode()) {
    case compiler::AccessMode::kLoad:
      DCHECK(!LoadModeHandlesOOB(keyed_mode.load_mode()));
      return BuildLoadTypedArrayElement(object, index, elements_kind);
    case compiler::AccessMode::kStore:
      DCHECK(StoreModeIsInBounds(keyed_mode.store_mode()));
      BuildStoreTypedArrayElement(object, index, elements_kind);
      return ReduceResult::Done();
    case compiler::AccessMode::kHas:
      // TODO(victorgomes): Implement has element access.
      return ReduceResult::Fail();
    case compiler::AccessMode::kStoreInLiteral:
    case compiler::AccessMode::kDefine:
      UNREACHABLE();
  }
}

ReduceResult MaglevGraphBuilder::TryBuildElementLoadOnJSArrayOrJSObject(
    ValueNode* object, ValueNode* index_object,
    base::Vector<const compiler::MapRef> maps, ElementsKind elements_kind,
    KeyedAccessLoadMode load_mode) {
  DCHECK(IsFastElementsKind(elements_kind));
  bool is_jsarray = HasOnlyJSArrayMaps(maps);
  DCHECK(is_jsarray || HasOnlyJSObjectMaps(maps));

  ValueNode* elements_array = BuildLoadElements(object);
  ValueNode* index = GetInt32ElementIndex(index_object);
  ValueNode* length = is_jsarray ? GetInt32(BuildLoadJSArrayLength(object))
                                 : BuildLoadFixedArrayLength(elements_array);

  auto emit_load = |&]() -> ReduceResult {
    ValueNode* result;
    if (elements_kind == HOLEY_DOUBLE_ELEMENTS) {
      result = BuildLoadHoleyFixedDoubleArrayElement(
          elements_array, index,
          CanTreatHoleAsUndefined(maps) && LoadModeHandlesHoles(load_mode));
    } else if (elements_kind == PACKED_DOUBLE_ELEMENTS) {
      result = BuildLoadFixedDoubleArrayElement(elements_array, index);
    } else {
      DCHECK(!IsDoubleElementsKind(elements_kind));
      result = BuildLoadFixedArrayElement(elements_array, index);
      if (IsHoleyElementsKind(elements_kind)) {
        if (CanTreatHoleAsUndefined(maps) && LoadModeHandlesHoles(load_mode)) {
          result = BuildConvertHoleToUndefined(result);
        } else {
          RETURN_IF_ABORT(BuildCheckNotHole(result));
          if (IsSmiElementsKind(elements_kind)) {
            EnsureType(result, NodeType::kSmi);
          }
        }
      } else if (IsSmiElementsKind(elements_kind)) {
        EnsureType(result, NodeType::kSmi);
      }
    }
    return result;
  };

  if (CanTreatHoleAsUndefined(maps) && LoadModeHandlesOOB(load_mode)) {
    ValueNode* positive_index;
    GET_VALUE_OR_ABORT(positive_index, GetUint32ElementIndex(index));
    ValueNode* uint32_length = AddNewNode<UnsafeInt32ToUint32>({length});
    return SelectReduction(
        |&](auto& builder) {
          return BuildBranchIfUint32Compare(builder, Operation::kLessThan,
                                            positive_index, uint32_length);
        },
        emit_load, |&] { return GetRootConstant(RootIndex::kUndefinedValue); });
  } else {
    RETURN_IF_ABORT(TryBuildCheckInt32Condition(
        index, length, AssertCondition::kUnsignedLessThan,
        DeoptimizeReason::kOutOfBounds));
    return emit_load();
  }
}

ReduceResult MaglevGraphBuilder::ConvertForStoring(ValueNode* value,
                                                   ElementsKind kind) {
  if (IsDoubleElementsKind(kind)) {
    // Make sure we do not store signalling NaNs into double arrays.
    // TODO(leszeks): Consider making this a bit on StoreFixedDoubleArrayElement
    // rather than a separate node.
    return GetSilencedNaN(GetFloat64(value));
  }
  if (IsSmiElementsKind(kind)) return GetSmiValue(value);
  return value;
}

ReduceResult MaglevGraphBuilder::TryBuildElementStoreOnJSArrayOrJSObject(
    ValueNode* object, ValueNode* index_object, ValueNode* value,
    base::Vector<const compiler::MapRef> maps, ElementsKind elements_kind,
    const compiler::KeyedAccessMode& keyed_mode) {
  DCHECK(IsFastElementsKind(elements_kind));

  const bool is_jsarray = HasOnlyJSArrayMaps(maps);
  DCHECK(is_jsarray || HasOnlyJSObjectMaps(maps));

  // Get the elements array.
  ValueNode* elements_array = BuildLoadElements(object);
  GET_VALUE_OR_ABORT(value, ConvertForStoring(value, elements_kind));
  ValueNode* index;

  // TODO(verwaest): Loop peeling will turn the first iteration index of spread
  // literals into smi constants as well, breaking the assumption that we'll
  // have preallocated the space if we see known indices. Turn off this
  // optimization if loop peeling is on.
  if (keyed_mode.access_mode() == compiler::AccessMode::kStoreInLiteral &&
      index_object->Is<SmiConstant>() && is_jsarray && !any_peeled_loop_) {
    index = GetInt32ElementIndex(index_object);
  } else {
    // Check boundaries.
    ValueNode* elements_array_length = nullptr;
    ValueNode* length;
    if (is_jsarray) {
      length = GetInt32(BuildLoadJSArrayLength(object));
    } else {
      length = elements_array_length =
          BuildLoadFixedArrayLength(elements_array);
    }
    index = GetInt32ElementIndex(index_object);
    if (keyed_mode.store_mode() == KeyedAccessStoreMode::kGrowAndHandleCOW) {
      if (elements_array_length == nullptr) {
        elements_array_length = BuildLoadFixedArrayLength(elements_array);
      }

      // Validate the {index} depending on holeyness:
      //
      // For HOLEY_*_ELEMENTS the {index} must not exceed the {elements}
      // backing store capacity plus the maximum allowed gap, as otherwise
      // the (potential) backing store growth would normalize and thus
      // the elements kind of the {receiver} would change to slow mode.
      //
      // For JSArray PACKED_*_ELEMENTS the {index} must be within the range
      // |0,length+1| to be valid. In case {index} equals {length},
      // the {receiver} will be extended, but kept packed.
      //
      // Non-JSArray PACKED_*_ELEMENTS always grow by adding holes because they
      // lack the magical length property, which requires a map transition.
      // So we can assume that this did not happen if we did not see this map.
      ValueNode* limit =
          IsHoleyElementsKind(elements_kind)
              ? AddNewNode<Int32AddWithOverflow>(
                    {elements_array_length,
                     GetInt32Constant(JSObject::kMaxGap)})
          : is_jsarray
              ? AddNewNode<Int32AddWithOverflow>({length, GetInt32Constant(1)})
              : elements_array_length;
      RETURN_IF_ABORT(TryBuildCheckInt32Condition(
          index, limit, AssertCondition::kUnsignedLessThan,
          DeoptimizeReason::kOutOfBounds));

      // Grow backing store if necessary and handle COW.
      elements_array = AddNewNode<MaybeGrowFastElements>(
          {elements_array, object, index, elements_array_length},
          elements_kind);

      // If we didn't grow {elements}, it might still be COW, in which case we
      // copy it now.
      if (IsSmiOrObjectElementsKind(elements_kind)) {
        DCHECK_EQ(keyed_mode.store_mode(),
                  KeyedAccessStoreMode::kGrowAndHandleCOW);
        elements_array =
            AddNewNode<EnsureWritableFastElements>({elements_array, object});
      }

      // Update length if necessary.
      if (is_jsarray) {
        ValueNode* new_length =
            AddNewNode<UpdateJSArrayLength>({length, object, index});
        RecordKnownProperty(object, broker()->length_string(), new_length,
                            false, compiler::AccessMode::kStore);
      }
    } else {
      RETURN_IF_ABORT(TryBuildCheckInt32Condition(
          index, length, AssertCondition::kUnsignedLessThan,
          DeoptimizeReason::kOutOfBounds));

      // Handle COW if needed.
      if (IsSmiOrObjectElementsKind(elements_kind)) {
        if (keyed_mode.store_mode() == KeyedAccessStoreMode::kHandleCOW) {
          elements_array =
              AddNewNode<EnsureWritableFastElements>({elements_array, object});
        } else {
          // Ensure that this is not a COW FixedArray.
          RETURN_IF_ABORT(BuildCheckMaps(
              elements_array, base::VectorOf({broker()->fixed_array_map()})));
        }
      }
    }
  }

  // Do the store.
  if (IsDoubleElementsKind(elements_kind)) {
    BuildStoreFixedDoubleArrayElement(elements_array, index, value);
  } else {
    BuildStoreFixedArrayElement(elements_array, index, value);
  }

  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::TryBuildElementAccessOnJSArrayOrJSObject(
    ValueNode* object, ValueNode* index_object,
    const compiler::ElementAccessInfo& access_info,
    compiler::KeyedAccessMode const& keyed_mode) {
  if (!IsFastElementsKind(access_info.elements_kind())) {
    return ReduceResult::Fail();
  }
  switch (keyed_mode.access_mode()) {
    case compiler::AccessMode::kLoad:
      return TryBuildElementLoadOnJSArrayOrJSObject(
          object, index_object,
          base::VectorOf(access_info.lookup_start_object_maps()),
          access_info.elements_kind(), keyed_mode.load_mode());
    case compiler::AccessMode::kStoreInLiteral:
    case compiler::AccessMode::kStore: {
      base::Vector<const compiler::MapRef> maps =
          base::VectorOf(access_info.lookup_start_object_maps());
      ElementsKind elements_kind = access_info.elements_kind();
      return TryBuildElementStoreOnJSArrayOrJSObject(object, index_object,
                                                     GetAccumulator(), maps,
                                                     elements_kind, keyed_mode);
    }
    default:
      // TODO(victorgomes): Implement more access types.
      return ReduceResult::Fail();
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildElementAccess(
    ValueNode* object, ValueNode* index_object,
    compiler::ElementAccessFeedback const& feedback,
    compiler::FeedbackSource const& feedback_source,
    GenericAccessFunc&& build_generic_access) {
  const compiler::KeyedAccessMode& keyed_mode = feedback.keyed_mode();
  // Check for the megamorphic case.
  if (feedback.transition_groups().empty()) {
    if (keyed_mode.access_mode() == compiler::AccessMode::kLoad) {
      return BuildCallBuiltin<Builtin::kKeyedLoadIC_Megamorphic>(
          {GetTaggedValue(object), GetTaggedValue(index_object)},
          feedback_source);
    } else if (keyed_mode.access_mode() == compiler::AccessMode::kStore) {
      return BuildCallBuiltin<Builtin::kKeyedStoreIC_Megamorphic>(
          {GetTaggedValue(object), GetTaggedValue(index_object),
           GetTaggedValue(GetAccumulator())},
          feedback_source);
    }
    return ReduceResult::Fail();
  }

  NodeInfo* object_info = known_node_aspects().TryGetInfoFor(object);
  compiler::ElementAccessFeedback refined_feedback =
      object_info && object_info->possible_maps_are_known()
          ? feedback.Refine(broker(), object_info->possible_maps())
          : feedback;

  if (refined_feedback.HasOnlyStringMaps(broker())) {
    return TryBuildElementAccessOnString(object, index_object, keyed_mode);
  }

  compiler::AccessInfoFactory access_info_factory(broker(), zone());
  ZoneVector<compiler::ElementAccessInfo> access_infos(zone());
  if (!access_info_factory.ComputeElementAccessInfos(refined_feedback,
                                                     &access_infos) ||
      access_infos.empty()) {
    return ReduceResult::Fail();
  }

  // TODO(leszeks): This is copied without changes from TurboFan's native
  // context specialization. We should figure out a way to share this code.
  //
  // For holey stores or growing stores, we need to check that the prototype
  // chain contains no setters for elements, and we need to guard those checks
  // via code dependencies on the relevant prototype maps.
  if (keyed_mode.access_mode() == compiler::AccessMode::kStore) {
    // TODO(v8:7700): We could have a fast path here, that checks for the
    // common case of Array or Object prototype only and therefore avoids
    // the zone allocation of this vector.
    ZoneVector<compiler::MapRef> prototype_maps(zone());
    for (compiler::ElementAccessInfo const& access_info : access_infos) {
      for (compiler::MapRef receiver_map :
           access_info.lookup_start_object_maps()) {
        // If the {receiver_map} has a prototype and its elements backing
        // store is either holey, or we have a potentially growing store,
        // then we need to check that all prototypes have stable maps with
        // with no element accessors and no throwing behavior for elements (and
        // we need to guard against changes to that below).
        if ((IsHoleyOrDictionaryElementsKind(receiver_map.elements_kind()) ||
             StoreModeCanGrow(refined_feedback.keyed_mode().store_mode())) &&
            !receiver_map.PrototypesElementsDoNotHaveAccessorsOrThrow(
                broker(), &prototype_maps)) {
          return ReduceResult::Fail();
        }

        // TODO(v8:12547): Support writing to objects in shared space, which
        // need a write barrier that calls Object::Share to ensure the RHS is
        // shared.
        if (InstanceTypeChecker::IsAlwaysSharedSpaceJSObject(
                receiver_map.instance_type())) {
          return ReduceResult::Fail();
        }
      }
    }
    for (compiler::MapRef prototype_map : prototype_maps) {
      broker()->dependencies()->DependOnStableMap(prototype_map);
    }
  }

  // Check for monomorphic case.
  if (access_infos.size() == 1) {
    compiler::ElementAccessInfo const& access_info = access_infos.front();
    // TODO(victorgomes): Support RAB/GSAB backed typed arrays.
    if (IsRabGsabTypedArrayElementsKind(access_info.elements_kind())) {
      return ReduceResult::Fail();
    }

    if (!access_info.transition_sources().empty()) {
      compiler::MapRef transition_target =
          access_info.lookup_start_object_maps().front();
      const ZoneVector<compiler::MapRef>& transition_sources =
          access_info.transition_sources();

      // There are no transitions in heap number maps. If `object` is a SMI, we
      // would anyway fail the transition and deopt later.
      DCHECK_NE(transition_target.instance_type(),
                InstanceType::HEAP_NUMBER_TYPE);
#ifdef DEBUG
      for (auto& transition_source : transition_sources) {
        DCHECK_NE(transition_source.instance_type(),
                  InstanceType::HEAP_NUMBER_TYPE);
      }
#endif  // DEBUG

      BuildCheckHeapObject(object);
      ValueNode* object_map =
          BuildLoadTaggedField(object, HeapObject::kMapOffset);

      RETURN_IF_ABORT(BuildTransitionElementsKindOrCheckMap(
          object, object_map, transition_sources, transition_target));
    } else {
      RETURN_IF_ABORT(BuildCheckMaps(
          object, base::VectorOf(access_info.lookup_start_object_maps())));
    }
    if (IsTypedArrayElementsKind(access_info.elements_kind())) {
      return TryBuildElementAccessOnTypedArray(object, index_object,
                                               access_info, keyed_mode);
    }
    return TryBuildElementAccessOnJSArrayOrJSObject(object, index_object,
                                                    access_info, keyed_mode);
  } else {
    return TryBuildPolymorphicElementAccess(object, index_object, keyed_mode,
                                            access_infos, build_generic_access);
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildPolymorphicElementAccess(
    ValueNode* object, ValueNode* index_object,
    const compiler::KeyedAccessMode& keyed_mode,
    const ZoneVector<compiler::ElementAccessInfo>& access_infos,
    GenericAccessFunc&& build_generic_access) {
  if (keyed_mode.access_mode() == compiler::AccessMode::kLoad &&
      LoadModeHandlesOOB(keyed_mode.load_mode())) {
    // TODO(victorgomes): Handle OOB mode.
    return ReduceResult::Fail();
  }

  const bool is_any_store = compiler::IsAnyStore(keyed_mode.access_mode());
  const int access_info_count = static_cast<int>(access_infos.size());
  // Stores don't return a value, so we don't need a variable for the result.
  MaglevSubGraphBuilder sub_graph(this, is_any_store ? 0 : 1);
  std::optional<MaglevSubGraphBuilder::Variable> ret_val;
  std::optional<MaglevSubGraphBuilder::Label> done;
  std::optional<MaglevSubGraphBuilder::Label> generic_access;

  BuildCheckHeapObject(object);
  ValueNode* object_map = BuildLoadTaggedField(object, HeapObject::kMapOffset);

  // TODO(pthier): We could do better here than just emitting code for each map,
  // as many different maps can produce the exact samce code (e.g. TypedArray
  // access for Uint16/Uint32/Int16/Int32/...).
  for (int i = 0; i < access_info_count; i++) {
    compiler::ElementAccessInfo const& access_info = access_infos|i];
    std::optional<MaglevSubGraphBuilder::Label> check_next_map;
    const bool handle_transitions = !access_info.transition_sources().empty();
    ReduceResult map_check_result;
    if (i == access_info_count - 1) {
      if (handle_transitions) {
        compiler::MapRef transition_target =
            access_info.lookup_start_object_maps().front();
        map_check_result = BuildTransitionElementsKindOrCheckMap(
            object, object_map, access_info.transition_sources(),
            transition_target);
      } else {
        map_check_result = BuildCheckMaps(
            object, base::VectorOf(access_info.lookup_start_object_maps()),
            object_map);
      }
    } else {
      if (handle_transitions) {
        compiler::MapRef transition_target =
            access_info.lookup_start_object_maps().front();
        map_check_result = BuildTransitionElementsKindAndCompareMaps(
            object, object_map, access_info.transition_sources(),
            transition_target, &sub_graph, check_next_map);
      } else {
        map_check_result = BuildCompareMaps(
            object, object_map,
            base::VectorOf(access_info.lookup_start_object_maps()), &sub_graph,
            check_next_map);
      }
    }
    if (map_check_result.IsDoneWithAbort()) {
      // We know from known possible maps that this branch is not reachable,
      // so don't emit any code for it.
      continue;
    }
    ReduceResult result;
    // TODO(victorgomes): Support RAB/GSAB backed typed arrays.
    if (IsRabGsabTypedArrayElementsKind(access_info.elements_kind())) {
      result = ReduceResult::Fail();
    } else if (IsTypedArrayElementsKind(access_info.elements_kind())) {
      result = TryBuildElementAccessOnTypedArray(object, index_object,
                                                 access_info, keyed_mode);
    } else {
      result = TryBuildElementAccessOnJSArrayOrJSObject(
          object, index_object, access_info, keyed_mode);
    }

    switch (result.kind()) {
      case ReduceResult::kDoneWithValue:
      case ReduceResult::kDoneWithoutValue:
        DCHECK_EQ(result.HasValue(), !is_any_store);
        if (!done.has_value()) {
          // We initialize the label {done} lazily on the first possible path.
          // If no possible path exists, it is guaranteed that BuildCheckMaps
          // emitted an unconditional deopt and we return DoneWithAbort at the
          // end. We need one extra predecessor to jump from the generic case.
          const int possible_predecessors = access_info_count - i + 1;
          if (is_any_store) {
            done.emplace(&sub_graph, possible_predecessors);
          } else {
            ret_val.emplace(0);
            done.emplace(
                &sub_graph, possible_predecessors,
                std::initializer_list<MaglevSubGraphBuilder::Variable*>{
                    &*ret_val});
          }
        }
        if (!is_any_store) {
          sub_graph.set(*ret_val, result.value());
        }
        sub_graph.Goto(&*done);
        break;
      case ReduceResult::kFail:
        if (!generic_access.has_value()) {
          // Conservatively assume that all remaining branches can go into the
          // generic path, as we have to initialize the predecessors upfront.
          // TODO(pthier): Find a better way to do that.
          generic_access.emplace(&sub_graph, access_info_count - i);
        }
        sub_graph.Goto(&*generic_access);
        break;
      case ReduceResult::kDoneWithAbort:
        break;
      case ReduceResult::kNone:
        UNREACHABLE();
    }
    if (check_next_map.has_value()) {
      sub_graph.Bind(&*check_next_map);
    }
  }
  if (generic_access.has_value() &&
      !sub_graph.TrimPredecessorsAndBind(&*generic_access).IsDoneWithAbort()) {
    ReduceResult generic_result = build_generic_access();
    DCHECK(generic_result.IsDone());
    DCHECK_EQ(generic_result.IsDoneWithValue(), !is_any_store);
    if (!done.has_value()) {
      return is_any_store ? ReduceResult::Done() : generic_result.value();
    }
    if (!is_any_store) {
      sub_graph.set(*ret_val, generic_result.value());
    }
    sub_graph.Goto(&*done);
  }
  if (done.has_value()) {
    RETURN_IF_ABORT(sub_graph.TrimPredecessorsAndBind(&*done));
    return is_any_store ? ReduceResult::Done() : sub_graph.get(*ret_val);
  } else {
    return ReduceResult::DoneWithAbort();
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildPolymorphicPropertyAccess(
    ValueNode* receiver, ValueNode* lookup_start_object,
    compiler::NamedAccessFeedback const& feedback,
    compiler::AccessMode access_mode,
    const ZoneVector<compiler::PropertyAccessInfo>& access_infos,
    GenericAccessFunc&& build_generic_access) {
  const bool is_any_store = compiler::IsAnyStore(access_mode);
  const int access_info_count = static_cast<int>(access_infos.size());
  int number_map_index = -1;

  bool needs_migration = false;
  bool has_deprecated_map_without_migration_target =
      feedback.has_deprecated_map_without_migration_target();
  for (int i = 0; i < access_info_count; i++) {
    compiler::PropertyAccessInfo const& access_info = access_infos|i];
    DCHECK(!access_info.IsInvalid());
    for (compiler::MapRef map : access_info.lookup_start_object_maps()) {
      if (map.is_migration_target()) {
        needs_migration = true;
      }
      if (map.IsHeapNumberMap()) {
        GetOrCreateInfoFor(lookup_start_object);
        base::SmallVector<compiler::MapRef, 1> known_maps = {map};
        KnownMapsMerger merger(broker(), zone(), base::VectorOf(known_maps));
        merger.IntersectWithKnownNodeAspects(lookup_start_object,
                                             known_node_aspects());
        if (!merger.intersect_set().is_empty()) {
          DCHECK_EQ(number_map_index, -1);
          number_map_index = i;
        }
      }
    }
  }

  // Stores don't return a value, so we don't need a variable for the result.
  MaglevSubGraphBuilder sub_graph(this, is_any_store ? 0 : 1);
  std::optional<MaglevSubGraphBuilder::Variable> ret_val;
  std::optional<MaglevSubGraphBuilder::Label> done;
  std::optional<MaglevSubGraphBuilder::Label> is_number;
  std::optional<MaglevSubGraphBuilder::Label> generic_access;

  if (number_map_index >= 0) {
    is_number.emplace(&sub_graph, 2);
    sub_graph.GotoIfTrue<BranchIfSmi>(&*is_number, {lookup_start_object});
  } else {
    BuildCheckHeapObject(lookup_start_object);
  }
  ValueNode* lookup_start_object_map =
      BuildLoadTaggedField(lookup_start_object, HeapObject::kMapOffset);

  if (needs_migration &&
      !v8_flags.maglev_skip_migration_check_for_polymorphic_access) {
    // TODO(marja, v8:7700): Try migrating only if all comparisons failed.
    // TODO(marja, v8:7700): Investigate making polymoprhic map comparison (with
    // migration) a control node (like switch).
    lookup_start_object_map = AddNewNode<MigrateMapIfNeeded>(
        {lookup_start_object_map, lookup_start_object});
  }

  for (int i = 0; i < access_info_count; i++) {
    compiler::PropertyAccessInfo const& access_info = access_infos|i];
    std::optional<MaglevSubGraphBuilder::Label> check_next_map;
    ReduceResult map_check_result;
    const auto& maps = access_info.lookup_start_object_maps();
    if (i == access_info_count - 1) {
      map_check_result =
          BuildCheckMaps(lookup_start_object, base::VectorOf(maps), {},
                         has_deprecated_map_without_migration_target);
    } else {
      map_check_result =
          BuildCompareMaps(lookup_start_object, lookup_start_object_map,
                           base::VectorOf(maps), &sub_graph, check_next_map);
    }
    if (map_check_result.IsDoneWithAbort()) {
      // We know from known possible maps that this branch is not reachable,
      // so don't emit any code for it.
      continue;
    }
    if (i == number_map_index) {
      DCHECK(is_number.has_value());
      sub_graph.Goto(&*is_number);
      sub_graph.Bind(&*is_number);
    }

    ReduceResult result;
    if (is_any_store) {
      result = TryBuildPropertyStore(receiver, lookup_start_object,
                                     feedback.name(), access_info, access_mode);
    } else {
      result = TryBuildPropertyLoad(receiver, lookup_start_object,
                                    feedback.name(), access_info);
    }

    switch (result.kind()) {
      case ReduceResult::kDoneWithValue:
      case ReduceResult::kDoneWithoutValue:
        DCHECK_EQ(result.HasValue(), !is_any_store);
        if (!done.has_value()) {
          // We initialize the label {done} lazily on the first possible path.
          // If no possible path exists, it is guaranteed that BuildCheckMaps
          // emitted an unconditional deopt and we return DoneWithAbort at the
          // end. We need one extra predecessor to jump from the generic case.
          const int possible_predecessors = access_info_count - i + 1;
          if (is_any_store) {
            done.emplace(&sub_graph, possible_predecessors);
          } else {
            ret_val.emplace(0);
            done.emplace(
                &sub_graph, possible_predecessors,
                std::initializer_list<MaglevSubGraphBuilder::Variable*>{
                    &*ret_val});
          }
        }

        if (!is_any_store) {
          sub_graph.set(*ret_val, result.value());
        }
        sub_graph.Goto(&*done);
        break;
      case ReduceResult::kDoneWithAbort:
        break;
      case ReduceResult::kFail:
        if (!generic_access.has_value()) {
          // Conservatively assume that all remaining branches can go into the
          // generic path, as we have to initialize the predecessors upfront.
          // TODO(pthier): Find a better way to do that.
          generic_access.emplace(&sub_graph, access_info_count - i);
        }
        sub_graph.Goto(&*generic_access);
        break;
      default:
        UNREACHABLE();
    }

    if (check_next_map.has_value()) {
      sub_graph.Bind(&*check_next_map);
    }
  }

  if (generic_access.has_value() &&
      !sub_graph.TrimPredecessorsAndBind(&*generic_access).IsDoneWithAbort()) {
    ReduceResult generic_result = build_generic_access();
    DCHECK(generic_result.IsDone());
    DCHECK_EQ(generic_result.IsDoneWithValue(), !is_any_store);
    if (!done.has_value()) {
      return is_any_store ? ReduceResult::Done() : generic_result.value();
    }
    if (!is_any_store) {
      sub_graph.set(*ret_val, generic_result.value());
    }
    sub_graph.Goto(&*done);
  }

  if (done.has_value()) {
    RETURN_IF_ABORT(sub_graph.TrimPredecessorsAndBind(&*done));
    return is_any_store ? ReduceResult::Done() : sub_graph.get(*ret_val);
  } else {
    return ReduceResult::DoneWithAbort();
  }
}

void MaglevGraphBuilder::RecordKnownProperty(
    ValueNode* lookup_start_object, KnownNodeAspects::LoadedPropertyMapKey key,
    ValueNode* value, bool is_const, compiler::AccessMode access_mode) {
  DCHECK(!value->properties().is_conversion());
  KnownNodeAspects::LoadedPropertyMap& loaded_properties =
      is_const ? known_node_aspects().loaded_constant_properties
               : known_node_aspects().loaded_properties;
  // Try to get loaded_properties|key] if it already exists, otherwise
  // construct loaded_properties|key] = ZoneMap{zone()}.
  auto& props_for_key =
      loaded_properties.try_emplace(key, zone()).first->second;

  if (!is_const && IsAnyStore(access_mode)) {
    if (is_loop_effect_tracking()) {
      loop_effects_->keys_cleared.insert(key);
    }
    // We don't do any aliasing analysis, so stores clobber all other cached
    // loads of a property with that key. We only need to do this for
    // non-constant properties, since constant properties are known not to
    // change and therefore can't be clobbered.
    // TODO(leszeks): Do some light aliasing analysis here, e.g. checking
    // whether there's an intersection of known maps.
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  * Removing all non-constant cached ";
      switch (key.type()) {
        case KnownNodeAspects::LoadedPropertyMapKey::kName:
          std::cout << "properties with name " << *key.name().object();
          break;
        case KnownNodeAspects::LoadedPropertyMapKey::kElements:
          std::cout << "Elements";
          break;
        case KnownNodeAspects::LoadedPropertyMapKey::kTypedArrayLength:
          std::cout << "TypedArray length";
          break;
        case KnownNodeAspects::LoadedPropertyMapKey::kStringLength:
          std::cout << "String length";
          break;
      }
      std::cout << std::endl;
    }
    props_for_key.clear();
  }

  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "  * Recording " << (is_const ? "constant" : "non-constant")
              << " known property "
              << PrintNodeLabel(graph_labeller(), lookup_start_object) << ": "
              << PrintNode(graph_labeller(), lookup_start_object) << " |";
    switch (key.type()) {
      case KnownNodeAspects::LoadedPropertyMapKey::kName:
        std::cout << *key.name().object();
        break;
      case KnownNodeAspects::LoadedPropertyMapKey::kElements:
        std::cout << "Elements";
        break;
      case KnownNodeAspects::LoadedPropertyMapKey::kTypedArrayLength:
        std::cout << "TypedArray length";
        break;
      case KnownNodeAspects::LoadedPropertyMapKey::kStringLength:
        std::cout << "String length";
        break;
    }
    std::cout << "] = " << PrintNodeLabel(graph_labeller(), value) << ": "
              << PrintNode(graph_labeller(), value) << std::endl;
  }

  if (IsAnyStore(access_mode) && !is_const && is_loop_effect_tracking()) {
    auto updated = props_for_key.emplace(lookup_start_object, value);
    if (updated.second) {
      loop_effects_->objects_written.insert(lookup_start_object);
    } else if (updated.first->second != value) {
      updated.first->second = value;
      loop_effects_->objects_written.insert(lookup_start_object);
    }
  } else {
    props_for_key|lookup_start_object] = value;
  }
}

ReduceResult MaglevGraphBuilder::TryReuseKnownPropertyLoad(
    ValueNode* lookup_start_object, compiler::NameRef name) {
  if (ReduceResult result = TryFindLoadedProperty(
          known_node_aspects().loaded_properties, lookup_start_object, name);
      result.IsDone()) {
    if (v8_flags.trace_maglev_graph_building && result.IsDoneWithValue()) {
      std::cout << "  * Reusing non-constant loaded property "
                << PrintNodeLabel(graph_labeller(), result.value()) << ": "
                << PrintNode(graph_labeller(), result.value()) << std::endl;
    }
    return result;
  }
  if (ReduceResult result =
          TryFindLoadedProperty(known_node_aspects().loaded_constant_properties,
                                lookup_start_object, name);
      result.IsDone()) {
    if (v8_flags.trace_maglev_graph_building && result.IsDoneWithValue()) {
      std::cout << "  * Reusing constant loaded property "
                << PrintNodeLabel(graph_labeller(), result.value()) << ": "
                << PrintNode(graph_labeller(), result.value()) << std::endl;
    }
    return result;
  }
  return ReduceResult::Fail();
}

ValueNode* MaglevGraphBuilder::BuildLoadStringLength(ValueNode* string) {
  if (ReduceResult result = TryFindLoadedProperty(
          known_node_aspects().loaded_constant_properties, string,
          KnownNodeAspects::LoadedPropertyMapKey::StringLength());
      result.IsDone()) {
    if (v8_flags.trace_maglev_graph_building && result.IsDoneWithValue()) {
      std::cout << "  * Reusing constant |String length]"
                << PrintNodeLabel(graph_labeller(), result.value()) << ": "
                << PrintNode(graph_labeller(), result.value()) << std::endl;
    }
    return result.value();
  }
  ValueNode* result = AddNewNode<StringLength>({string});
  RecordKnownProperty(string,
                      KnownNodeAspects::LoadedPropertyMapKey::StringLength(),
                      result, true, compiler::AccessMode::kLoad);
  return result;
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildLoadNamedProperty(
    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
    compiler::FeedbackSource& feedback_source,
    GenericAccessFunc&& build_generic_access) {
  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(feedback_source,
                                             compiler::AccessMode::kLoad, name);
  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      return EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess);
    case compiler::ProcessedFeedback::kNamedAccess: {
      RETURN_IF_DONE(TryReuseKnownPropertyLoad(lookup_start_object, name));
      return TryBuildNamedAccess(
          receiver, lookup_start_object, processed_feedback.AsNamedAccess(),
          feedback_source, compiler::AccessMode::kLoad, build_generic_access);
    }
    default:
      return ReduceResult::Fail();
  }
}

ReduceResult MaglevGraphBuilder::TryBuildLoadNamedProperty(
    ValueNode* receiver, compiler::NameRef name,
    compiler::FeedbackSource& feedback_source) {
  auto build_generic_access = |this, &receiver, &name, &feedback_source]() {
    ValueNode* context = GetContext();
    return AddNewNode<LoadNamedGeneric>({context, receiver}, name,
                                        feedback_source);
  };
  return TryBuildLoadNamedProperty(receiver, receiver, name, feedback_source,
                                   build_generic_access);
}

void MaglevGraphBuilder::VisitGetNamedProperty() {
  // GetNamedProperty <object> <name_index> <slot>
  ValueNode* object = LoadRegister(0);
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  PROCESS_AND_RETURN_IF_DONE(
      TryBuildLoadNamedProperty(object, name, feedback_source), SetAccumulator);
  // Create a generic load in the fallthrough.
  ValueNode* context = GetContext();
  SetAccumulator(
      AddNewNode<LoadNamedGeneric>({context, object}, name, feedback_source));
}

ValueNode* MaglevGraphBuilder::GetConstant(compiler::ObjectRef ref) {
  if (ref.IsSmi()) return GetSmiConstant(ref.AsSmi());
  compiler::HeapObjectRef constant = ref.AsHeapObject();

  if (IsThinString(*constant.object())) {
    constant = MakeRefAssumeMemoryFence(
        broker(), Cast<ThinString>(*constant.object())->actual());
  }

  auto root_index = broker()->FindRootIndex(constant);
  if (root_index.has_value()) {
    return GetRootConstant(*root_index);
  }

  auto it = graph_->constants().find(constant);
  if (it == graph_->constants().end()) {
    Constant* node = CreateNewConstantNode<Constant>(0, constant);
    graph_->constants().emplace(constant, node);
    return node;
  }
  return it->second;
}

ValueNode* MaglevGraphBuilder::GetTrustedConstant(compiler::HeapObjectRef ref,
                                                  IndirectPointerTag tag) {
#ifdef V8_ENABLE_SANDBOX
  auto it = graph_->trusted_constants().find(ref);
  if (it == graph_->trusted_constants().end()) {
    TrustedConstant* node = CreateNewConstantNode<TrustedConstant>(0, ref, tag);
    graph_->trusted_constants().emplace(ref, node);
    return node;
  }
  SBXCHECK_EQ(it->second->tag(), tag);
  return it->second;
#else
  return GetConstant(ref);
#endif
}

void MaglevGraphBuilder::VisitGetNamedPropertyFromSuper() {
  // GetNamedPropertyFromSuper <receiver> <name_index> <slot>
  ValueNode* receiver = LoadRegister(0);
  ValueNode* home_object = GetAccumulator();
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  // {home_object} is guaranteed to be a HeapObject.
  ValueNode* home_object_map =
      BuildLoadTaggedField(home_object, HeapObject::kMapOffset);
  ValueNode* lookup_start_object =
      BuildLoadTaggedField(home_object_map, Map::kPrototypeOffset);

  auto build_generic_access = |this, &receiver, &lookup_start_object, &name,
                               &feedback_source]() {
    ValueNode* context = GetContext();
    return AddNewNode<LoadNamedFromSuperGeneric>(
        {context, receiver, lookup_start_object}, name, feedback_source);
  };

  PROCESS_AND_RETURN_IF_DONE(
      TryBuildLoadNamedProperty(receiver, lookup_start_object, name,
                                feedback_source, build_generic_access),
      SetAccumulator);
  // Create a generic load.
  SetAccumulator(build_generic_access());
}

bool MaglevGraphBuilder::TryBuildGetKeyedPropertyWithEnumeratedKey(
    ValueNode* object, const compiler::FeedbackSource& feedback_source,
    const compiler::ProcessedFeedback& processed_feedback) {
  if (current_for_in_state.index != nullptr &&
      current_for_in_state.enum_cache_indices != nullptr &&
      current_for_in_state.key == current_interpreter_frame_.accumulator()) {
    bool speculating_receiver_map_matches = false;
    if (current_for_in_state.receiver != object) {
      // When the feedback is uninitialized, it is either a keyed load which
      // always hits the enum cache, or a keyed load that had never been
      // reached. In either case, we can check the map of the receiver and use
      // the enum cache if the map match the {cache_type}.
      if (processed_feedback.kind() !=
          compiler::ProcessedFeedback::kInsufficient) {
        return false;
      }
      BuildCheckHeapObject(object);
      speculating_receiver_map_matches = true;
    }

    if (current_for_in_state.receiver_needs_map_check ||
        speculating_receiver_map_matches) {
      auto* receiver_map = BuildLoadTaggedField(object, HeapObject::kMapOffset);
      AddNewNode<CheckDynamicValue>(
          {receiver_map, current_for_in_state.cache_type});
      if (current_for_in_state.receiver == object) {
        current_for_in_state.receiver_needs_map_check = false;
      }
    }
    // TODO(leszeks): Cache the field index per iteration.
    auto* field_index = BuildLoadFixedArrayElement(
        current_for_in_state.enum_cache_indices, current_for_in_state.index);
    SetAccumulator(
        AddNewNode<LoadTaggedFieldByFieldIndex>({object, field_index}));
    return true;
  }
  return false;
}

void MaglevGraphBuilder::BuildGetKeyedProperty(
    ValueNode* object, const compiler::FeedbackSource& feedback_source,
    const compiler::ProcessedFeedback& processed_feedback) {
  if (TryBuildGetKeyedPropertyWithEnumeratedKey(object, feedback_source,
                                                processed_feedback)) {
    return;
  }

  auto build_generic_access = |this, object, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* key = GetAccumulator();
    return AddNewNode<GetKeyedGeneric>({context, object, key}, feedback_source);
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericKeyedAccess));

    case compiler::ProcessedFeedback::kElementAccess: {
      // Get the accumulator without conversion. TryBuildElementAccess
      // will try to pick the best representation.
      ValueNode* index = current_interpreter_frame_.accumulator();
      ReduceResult result = TryBuildElementAccess(
          object, index, processed_feedback.AsElementAccess(), feedback_source,
          build_generic_access);
      PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);
      break;
    }

    case compiler::ProcessedFeedback::kNamedAccess: {
      ValueNode* key = GetAccumulator();
      compiler::NameRef name = processed_feedback.AsNamedAccess().name();
      RETURN_VOID_IF_ABORT(BuildCheckValue(key, name));

      ReduceResult result = TryReuseKnownPropertyLoad(object, name);
      PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);

      result = TryBuildNamedAccess(
          object, object, processed_feedback.AsNamedAccess(), feedback_source,
          compiler::AccessMode::kLoad, build_generic_access);
      PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);
      break;
    }

    default:
      break;
  }

  // Create a generic load in the fallthrough.
  SetAccumulator(build_generic_access());
}

void MaglevGraphBuilder::VisitGetKeyedProperty() {
  // GetKeyedProperty <object> <slot>
  ValueNode* object = LoadRegister(0);
  // TODO(leszeks): We don't need to tag the key if it's an Int32 and a simple
  // monomorphic element load.
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback* processed_feedback =
      &broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kLoad, std::nullopt);
  if (processed_feedback->kind() ==
          compiler::ProcessedFeedback::kElementAccess &&
      processed_feedback->AsElementAccess().transition_groups().empty()) {
    if (auto constant = TryGetConstant(GetAccumulator());
        constant.has_value() && constant->IsName()) {
      compiler::NameRef name = constant->AsName();
      if (name.IsUniqueName() && !name.object()->IsArrayIndex()) {
        processed_feedback =
            &processed_feedback->AsElementAccess().Refine(broker(), name);
      }
    }
  }

  BuildGetKeyedProperty(object, feedback_source, *processed_feedback);
}

void MaglevGraphBuilder::VisitGetEnumeratedKeyedProperty() {
  // GetEnumeratedKeyedProperty <object> <enum_index> <cache_type> <slot>
  ValueNode* object = LoadRegister(0);
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kLoad, std::nullopt);

  BuildGetKeyedProperty(object, feedback_source, processed_feedback);
}

void MaglevGraphBuilder::VisitLdaModuleVariable() {
  // LdaModuleVariable <cell_index> <depth>
  int cell_index = iterator_.GetImmediateOperand(0);
  size_t depth = iterator_.GetUnsignedImmediateOperand(1);
  ValueNode* context = GetContextAtDepth(GetContext(), depth);

  ValueNode* module = LoadAndCacheContextSlot(
      context, Context::EXTENSION_INDEX, kImmutable, ContextKind::kDefault);
  ValueNode* exports_or_imports;
  if (cell_index > 0) {
    exports_or_imports =
        BuildLoadTaggedField(module, SourceTextModule::kRegularExportsOffset);
    // The actual array index is (cell_index - 1).
    cell_index -= 1;
  } else {
    exports_or_imports =
        BuildLoadTaggedField(module, SourceTextModule::kRegularImportsOffset);
    // The actual array index is (-cell_index - 1).
    cell_index = -cell_index - 1;
  }
  ValueNode* cell = BuildLoadFixedArrayElement(exports_or_imports, cell_index);
  SetAccumulator(BuildLoadTaggedField(cell, Cell::kValueOffset));
}

ValueNode* MaglevGraphBuilder::GetContextAtDepth(ValueNode* context,
                                                 size_t depth) {
  MinimizeContextChainDepth(&context, &depth);

  if (compilation_unit_->info()->specialize_to_function_context()) {
    compiler::OptionalContextRef maybe_ref =
        FunctionContextSpecialization::TryToRef(compilation_unit_, context,
                                                &depth);
    if (maybe_ref.has_value()) {
      context = GetConstant(maybe_ref.value());
    }
  }

  for (size_t i = 0; i < depth; i++) {
    context = LoadAndCacheContextSlot(context, Context::PREVIOUS_INDEX,
                                      kImmutable, ContextKind::kDefault);
  }
  return context;
}

void MaglevGraphBuilder::VisitStaModuleVariable() {
  // StaModuleVariable <cell_index> <depth>
  int cell_index = iterator_.GetImmediateOperand(0);
  if (V8_UNLIKELY(cell_index < 0)) {
    // TODO(verwaest): Make this fail as well.
    CHECK(BuildCallRuntime(Runtime::kAbort,
                           {GetSmiConstant(static_cast<int>(
                               AbortReason::kUnsupportedModuleOperation))})
              .IsDone());
    return;
  }

  size_t depth = iterator_.GetUnsignedImmediateOperand(1);
  ValueNode* context = GetContextAtDepth(GetContext(), depth);

  ValueNode* module = LoadAndCacheContextSlot(
      context, Context::EXTENSION_INDEX, kImmutable, ContextKind::kDefault);
  ValueNode* exports =
      BuildLoadTaggedField(module, SourceTextModule::kRegularExportsOffset);
  // The actual array index is (cell_index - 1).
  cell_index -= 1;
  ValueNode* cell = BuildLoadFixedArrayElement(exports, cell_index);
  BuildStoreTaggedField(cell, GetAccumulator(), Cell::kValueOffset,
                        StoreTaggedMode::kDefault);
}

void MaglevGraphBuilder::BuildLoadGlobal(
    compiler::NameRef name, compiler::FeedbackSource& feedback_source,
    TypeofMode typeof_mode) {
  const compiler::ProcessedFeedback& access_feedback =
      broker()->GetFeedbackForGlobalAccess(feedback_source);

  if (access_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForGenericGlobalAccess));
  }

  const compiler::GlobalAccessFeedback& global_access_feedback =
      access_feedback.AsGlobalAccess();
  PROCESS_AND_RETURN_IF_DONE(TryBuildGlobalLoad(global_access_feedback),
                             SetAccumulator);

  ValueNode* context = GetContext();
  SetAccumulator(
      AddNewNode<LoadGlobal>({context}, name, feedback_source, typeof_mode));
}

void MaglevGraphBuilder::VisitSetNamedProperty() {
  // SetNamedProperty <object> <name_index> <slot>
  ValueNode* object = LoadRegister(0);
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStore, name);

  auto build_generic_access = |this, object, &name, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<SetNamedGeneric>({context, object, value}, name,
                                feedback_source);
    return ReduceResult::Done();
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess));

    case compiler::ProcessedFeedback::kNamedAccess:
      RETURN_VOID_IF_DONE(TryBuildNamedAccess(
          object, object, processed_feedback.AsNamedAccess(), feedback_source,
          compiler::AccessMode::kStore, build_generic_access));
      break;
    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitDefineNamedOwnProperty() {
  // DefineNamedOwnProperty <object> <name_index> <slot>
  ValueNode* object = LoadRegister(0);
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStore, name);

  auto build_generic_access = |this, object, &name, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<DefineNamedOwnGeneric>({context, object, value}, name,
                                      feedback_source);
    return ReduceResult::Done();
  };
  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess));

    case compiler::ProcessedFeedback::kNamedAccess:
      RETURN_VOID_IF_DONE(TryBuildNamedAccess(
          object, object, processed_feedback.AsNamedAccess(), feedback_source,
          compiler::AccessMode::kDefine, build_generic_access));
      break;

    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitSetKeyedProperty() {
  // SetKeyedProperty <object> <key> <slot>
  ValueNode* object = LoadRegister(0);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStore, std::nullopt);

  auto build_generic_access = |this, object, &feedback_source]() {
    ValueNode* key = LoadRegister(1);
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<SetKeyedGeneric>({context, object, key, value}, feedback_source);
    return ReduceResult::Done();
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericKeyedAccess));

    case compiler::ProcessedFeedback::kElementAccess: {
      // Get the key without conversion. TryBuildElementAccess will try to pick
      // the best representation.
      ValueNode* index =
          current_interpreter_frame_.get(iterator_.GetRegisterOperand(1));
      RETURN_VOID_IF_DONE(TryBuildElementAccess(
          object, index, processed_feedback.AsElementAccess(), feedback_source,
          build_generic_access));
    } break;

    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitDefineKeyedOwnProperty() {
  // DefineKeyedOwnProperty <object> <key> <flags> <slot>
  ValueNode* object = LoadRegister(0);
  ValueNode* key = LoadRegister(1);
  ValueNode* flags = GetSmiConstant(GetFlag8Operand(2));
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  // TODO(victorgomes): Add monomorphic fast path.

  // Create a generic store in the fallthrough.
  ValueNode* context = GetContext();
  ValueNode* value = GetAccumulator();
  AddNewNode<DefineKeyedOwnGeneric>({context, object, key, value, flags},
                                    feedback_source);
}

void MaglevGraphBuilder::VisitStaInArrayLiteral() {
  // StaInArrayLiteral <object> <index> <slot>
  ValueNode* object = LoadRegister(0);
  ValueNode* index = LoadRegister(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStoreInLiteral, std::nullopt);

  auto build_generic_access = |this, object, index, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<StoreInArrayLiteralGeneric>({context, object, index, value},
                                           feedback_source);
    return ReduceResult::Done();
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericKeyedAccess));

    case compiler::ProcessedFeedback::kElementAccess: {
      RETURN_VOID_IF_DONE(TryBuildElementAccess(
          object, index, processed_feedback.AsElementAccess(), feedback_source,
          build_generic_access));
      break;
    }

    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitDefineKeyedOwnPropertyInLiteral() {
  ValueNode* object = LoadRegister(0);
  ValueNode* name = LoadRegister(1);
  ValueNode* value = GetAccumulator();
  ValueNode* flags = GetSmiConstant(GetFlag8Operand(2));
  ValueNode* slot = GetTaggedIndexConstant(GetSlotOperand(3).ToInt());
  ValueNode* feedback_vector = GetConstant(feedback());
  CHECK(BuildCallRuntime(Runtime::kDefineKeyedOwnPropertyInLiteral,
                         {object, name, value, flags, feedback_vector, slot})
            .IsDone());
}

void MaglevGraphBuilder::VisitAdd() { VisitBinaryOperation<Operation::kAdd>(); }
void MaglevGraphBuilder::VisitSub() {
  VisitBinaryOperation<Operation::kSubtract>();
}
void MaglevGraphBuilder::VisitMul() {
  VisitBinaryOperation<Operation::kMultiply>();
}
void MaglevGraphBuilder::VisitDiv() {
  VisitBinaryOperation<Operation::kDivide>();
}
void MaglevGraphBuilder::VisitMod() {
  VisitBinaryOperation<Operation::kModulus>();
}
void MaglevGraphBuilder::VisitExp() {
  VisitBinaryOperation<Operation::kExponentiate>();
}
void MaglevGraphBuilder::VisitBitwiseOr() {
  VisitBinaryOperation<Operation::kBitwiseOr>();
}
void MaglevGraphBuilder::VisitBitwiseXor() {
  VisitBinaryOperation<Operation::kBitwiseXor>();
}
void MaglevGraphBuilder::VisitBitwiseAnd() {
  VisitBinaryOperation<Operation::kBitwiseAnd>();
}
void MaglevGraphBuilder::VisitShiftLeft() {
  VisitBinaryOperation<Operation::kShiftLeft>();
}
void MaglevGraphBuilder::VisitShiftRight() {
  VisitBinaryOperation<Operation::kShiftRight>();
}
void MaglevGraphBuilder::VisitShiftRightLogical() {
  VisitBinaryOperation<Operation::kShiftRightLogical>();
}

void MaglevGraphBuilder::VisitAddSmi() {
  VisitBinarySmiOperation<Operation::kAdd>();
}
void MaglevGraphBuilder::VisitSubSmi() {
  VisitBinarySmiOperation<Operation::kSubtract>();
}
void MaglevGraphBuilder::VisitMulSmi() {
  VisitBinarySmiOperation<Operation::kMultiply>();
}
void MaglevGraphBuilder::VisitDivSmi() {
  VisitBinarySmiOperation<Operation::kDivide>();
}
void MaglevGraphBuilder::VisitModSmi() {
  VisitBinarySmiOperation<Operation::kModulus>();
}
void MaglevGraphBuilder::VisitExpSmi() {
  VisitBinarySmiOperation<Operation::kExponentiate>();
}
void MaglevGraphBuilder::VisitBitwiseOrSmi() {
  VisitBinarySmiOperation<Operation::kBitwiseOr>();
}
void MaglevGraphBuilder::VisitBitwiseXorSmi() {
  VisitBinarySmiOperation<Operation::kBitwiseXor>();
}
void MaglevGraphBuilder::VisitBitwiseAndSmi() {
  VisitBinarySmiOperation<Operation::kBitwiseAnd>();
}
void MaglevGraphBuilder::VisitShiftLeftSmi() {
  VisitBinarySmiOperation<Operation::kShiftLeft>();
}
void MaglevGraphBuilder::VisitShiftRightSmi() {
  VisitBinarySmiOperation<Operation::kShiftRight>();
}
void MaglevGraphBuilder::VisitShiftRightLogicalSmi() {
  VisitBinarySmiOperation<Operation::kShiftRightLogical>();
}

void MaglevGraphBuilder::VisitInc() {
  VisitUnaryOperation<Operation::kIncrement>();
}
void MaglevGraphBuilder::VisitDec() {
  VisitUnaryOperation<Operation::kDecrement>();
}
void MaglevGraphBuilder::VisitNegate() {
  VisitUnaryOperation<Operation::kNegate>();
}
void MaglevGraphBuilder::VisitBitwiseNot() {
  VisitUnaryOperation<Operation::kBitwiseNot>();
}

void MaglevGraphBuilder::VisitToBooleanLogicalNot() {
  SetAccumulator(BuildToBoolean</* flip */ true>(GetAccumulator()));
}

ValueNode* MaglevGraphBuilder::BuildLogicalNot(ValueNode* value) {
  // TODO(victorgomes): Use NodeInfo to add more type optimizations here.
  switch (value->opcode()) {
#define CASE(Name)                                         \
  case Opcode::k##Name: {                                  \
    return GetBooleanConstant(                             \
        !value->Cast<Name>()->ToBoolean(local_isolate())); \
  }
    CONSTANT_VALUE_NODE_LIST(CASE)
#undef CASE
    default:
      return AddNewNode<LogicalNot>({value});
  }
}

void MaglevGraphBuilder::VisitLogicalNot() {
  // Invariant: accumulator must already be a boolean value.
  SetAccumulator(BuildLogicalNot(GetAccumulator()));
}

void MaglevGraphBuilder::VisitTypeOf() {
  ValueNode* value = GetAccumulator();
  PROCESS_AND_RETURN_IF_DONE(TryReduceTypeOf(value), SetAccumulator);

  FeedbackNexus nexus = FeedbackNexusForOperand(0);
  TypeOfFeedback::Result feedback = nexus.GetTypeOfFeedback();
  switch (feedback) {
    case TypeOfFeedback::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForTypeOf));
    case TypeOfFeedback::kNumber:
      BuildCheckNumber(value);
      SetAccumulator(GetRootConstant(RootIndex::knumber_string));
      return;
    case TypeOfFeedback::kString:
      BuildCheckString(value);
      SetAccumulator(GetRootConstant(RootIndex::kstring_string));
      return;
    case TypeOfFeedback::kFunction:
      AddNewNode<CheckDetectableCallable>({value},
                                          GetCheckType(GetType(value)));
      EnsureType(value, NodeType::kCallable);
      SetAccumulator(GetRootConstant(RootIndex::kfunction_string));
      return;
    default:
      break;
  }

  SetAccumulator(BuildCallBuiltin<Builtin::kTypeof>({GetTaggedValue(value)}));
}

void MaglevGraphBuilder::VisitDeletePropertyStrict() {
  ValueNode* object = LoadRegister(0);
  ValueNode* key = GetAccumulator();
  ValueNode* context = GetContext();
  SetAccumulator(AddNewNode<DeleteProperty>({context, object, key},
                                            LanguageMode::kStrict));
}

void MaglevGraphBuilder::VisitDeletePropertySloppy() {
  ValueNode* object = LoadRegister(0);
  ValueNode* key = GetAccumulator();
  ValueNode* context = GetContext();
  SetAccumulator(AddNewNode<DeleteProperty>({context, object, key},
                                            LanguageMode::kSloppy));
}

void MaglevGraphBuilder::VisitGetSuperConstructor() {
  ValueNode* active_function = GetAccumulator();
  // TODO(victorgomes): Maybe BuildLoadTaggedField should support constants
  // instead.
  if (compiler::OptionalHeapObjectRef constant =
          TryGetConstant(active_function)) {
    compiler::MapRef map = constant->map(broker());
    if (map.is_stable()) {
      broker()->dependencies()->DependOnStableMap(map);
      ValueNode* map_proto = GetConstant(map.prototype(broker()));
      StoreRegister(iterator_.GetRegisterOperand(0), map_proto);
      return;
    }
  }
  ValueNode* map =
      BuildLoadTaggedField(active_function, HeapObject::kMapOffset);
  ValueNode* map_proto = BuildLoadTaggedField(map, Map::kPrototypeOffset);
  StoreRegister(iterator_.GetRegisterOperand(0), map_proto);
}

bool MaglevGraphBuilder::HasValidInitialMap(
    compiler::JSFunctionRef new_target, compiler::JSFunctionRef constructor) {
  if (!new_target.map(broker()).has_prototype_slot()) return false;
  if (!new_target.has_initial_map(broker())) return false;
  compiler::MapRef initial_map = new_target.initial_map(broker());
  return initial_map.GetConstructor(broker()).equals(constructor);
}

bool MaglevGraphBuilder::TryBuildFindNonDefaultConstructorOrConstruct(
    ValueNode* this_function, ValueNode* new_target,
    std::pair<interpreter::Register, interpreter::Register> result) {
  // See also:
  // JSNativeContextSpecialization::ReduceJSFindNonDefaultConstructorOrConstruct

  compiler::OptionalHeapObjectRef maybe_constant =
      TryGetConstant(this_function);
  if (!maybe_constant) return false;

  compiler::MapRef function_map = maybe_constant->map(broker());
  compiler::HeapObjectRef current = function_map.prototype(broker());

  // TODO(v8:13091): Don't produce incomplete stack traces when debug is active.
  // We already deopt when a breakpoint is set. But it would be even nicer to
  // avoid producting incomplete stack traces when when debug is active, even if
  // there are no breakpoints - then a user inspecting stack traces via Dev
  // Tools would always see the full stack trace.

  while (true) {
    if (!current.IsJSFunction()) return false;
    compiler::JSFunctionRef current_function = current.AsJSFunction();

    // If there are class fields, bail out. TODO(v8:13091): Handle them here.
    if (current_function.shared(broker())
            .requires_instance_members_initializer()) {
      return false;
    }

    // If there are private methods, bail out. TODO(v8:13091): Handle them here.
    if (current_function.context(broker())
            .scope_info(broker())
            .ClassScopeHasPrivateBrand()) {
      return false;
    }

    FunctionKind kind = current_function.shared(broker()).kind();
    if (kind != FunctionKind::kDefaultDerivedConstructor) {
      // The hierarchy walk will end here; this is the last change to bail out
      // before creating new nodes.
      if (!broker()->dependencies()->DependOnArrayIteratorProtector()) {
        return false;
      }

      compiler::OptionalHeapObjectRef new_target_function =
          TryGetConstant(new_target);
      if (kind == FunctionKind::kDefaultBaseConstructor) {
        // Store the result register first, so that a lazy deopt in
        // `FastNewObject` writes `true` to this register.
        StoreRegister(result.first, GetBooleanConstant(true));

        ValueNode* object;
        if (new_target_function && new_target_function->IsJSFunction() &&
            HasValidInitialMap(new_target_function->AsJSFunction(),
                               current_function)) {
          object = BuildInlinedAllocation(
              CreateJSConstructor(new_target_function->AsJSFunction()),
              AllocationType::kYoung);
          ClearCurrentAllocationBlock();
        } else {
          object = BuildCallBuiltin<Builtin::kFastNewObject>(
              {GetConstant(current_function), GetTaggedValue(new_target)});
          // We've already stored "true" into result.first, so a deopt here just
          // has to store result.second. Also mark result.first as being used,
          // since the lazy deopt frame won't have marked it since it used to be
          // a result register.
          AddDeoptUse(current_interpreter_frame_.get(result.first));
          object->lazy_deopt_info()->UpdateResultLocation(result.second, 1);
        }
        StoreRegister(result.second, object);
      } else {
        StoreRegister(result.first, GetBooleanConstant(false));
        StoreRegister(result.second, GetConstant(current));
      }

      broker()->dependencies()->DependOnStablePrototypeChain(
          function_map, WhereToStart::kStartAtReceiver, current_function);
      return true;
    }

    // Keep walking up the class tree.
    current = current_function.map(broker()).prototype(broker());
  }
}

void MaglevGraphBuilder::VisitFindNonDefaultConstructorOrConstruct() {
  ValueNode* this_function = LoadRegister(0);
  ValueNode* new_target = LoadRegister(1);

  auto register_pair = iterator_.GetRegisterPairOperand(2);

  if (TryBuildFindNonDefaultConstructorOrConstruct(this_function, new_target,
                                                   register_pair)) {
    return;
  }

  CallBuiltin* result =
      BuildCallBuiltin<Builtin::kFindNonDefaultConstructorOrConstruct>(
          {GetTaggedValue(this_function), GetTaggedValue(new_target)});
  StoreRegisterPair(register_pair, result);
}

namespace {
void ForceEscapeIfAllocation(ValueNode* value) {
  if (InlinedAllocation* alloc = value->TryCast<InlinedAllocation>()) {
    alloc->ForceEscaping();
  }
}
}  // namespace

ReduceResult MaglevGraphBuilder::BuildInlined(ValueNode* context,
                                              ValueNode* function,
                                              ValueNode* new_target,
                                              const CallArguments& args) {
  DCHECK(is_inline());

  // Manually create the prologue of the inner function graph, so that we
  // can manually set up the arguments.
  DCHECK_NOT_NULL(current_block_);

  // Set receiver.
  ValueNode* receiver =
      GetConvertReceiver(compilation_unit_->shared_function_info(), args);
  SetArgument(0, receiver);

  // The inlined function could call a builtin that iterates the frame, the
  // receiver needs to have been materialized.
  // TODO(victorgomes): Can we relax this requirement? Maybe we can allocate the
  // object lazily? This is also only required if the inlined function is not a
  // leaf (ie. it calls other functions).
  ForceEscapeIfAllocation(receiver);

  // Set remaining arguments.
  RootConstant* undefined_constant =
      GetRootConstant(RootIndex::kUndefinedValue);
  int arg_count = static_cast<int>(args.count());
  int formal_parameter_count = compilation_unit_->parameter_count() - 1;
  for (int i = 0; i < formal_parameter_count; i++) {
    ValueNode* arg_value = args|i];
    if (arg_value == nullptr) arg_value = undefined_constant;
    SetArgument(i + 1, arg_value);
  }

  // Save all arguments if we have a mismatch between arguments count and
  // parameter count.
  inlined_arguments_ = zone()->AllocateVector<ValueNode*>(arg_count + 1);
  inlined_arguments_|0] = receiver;
  for (int i = 0; i < arg_count; i++) {
    inlined_arguments_|i + 1] = args|i];
  }

  inlined_new_target_ = new_target;

  BuildRegisterFrameInitialization(context, function, new_target);
  BuildMergeStates();
  EndPrologue();
  in_prologue_ = false;

  // Build the inlined function body.
  BuildBody();

  // All returns in the inlined body jump to a merge point one past the bytecode
  // length (i.e. at offset bytecode.length()). If there isn't one already,
  // create a block at this fake offset and have it jump out of the inlined
  // function, into a new block that we create which resumes execution of the
  // outer function.
  if (!current_block_) {
    // If we don't have a merge state at the inline_exit_offset, then there is
    // no control flow that reaches the end of the inlined function, either
    // because of infinite loops or deopts
    if (merge_states_|inline_exit_offset()] == nullptr) {
      return ReduceResult::DoneWithAbort();
    }

    ProcessMergePoint(inline_exit_offset(), /*preserve_kna*/ false);
    StartNewBlock(inline_exit_offset(), /*predecessor*/ nullptr);
  }

  // Pull the returned accumulator value out of the inlined function's final
  // merged return state.
  return current_interpreter_frame_.accumulator();
}

#define TRACE_INLINING(...)                       \
  do {                                            \
    if (v8_flags.trace_maglev_inlining)           \
      StdoutStream{} << __VA_ARGS__ << std::endl; \
  } while (false)

#define TRACE_CANNOT_INLINE(...) \
  TRACE_INLINING("  cannot inline " << shared << ": " << __VA_ARGS__)

bool MaglevGraphBuilder::ShouldInlineCall(
    compiler::SharedFunctionInfoRef shared,
    compiler::OptionalFeedbackVectorRef feedback_vector, float call_frequency) {
  if (graph()->total_inlined_bytecode_size() >
      v8_flags.max_maglev_inlined_bytecode_size_cumulative) {
    compilation_unit_->info()->set_could_not_inline_all_candidates();
    TRACE_CANNOT_INLINE("maximum inlined bytecode size");
    return false;
  }
  if (!feedback_vector) {
    // TODO(verwaest): Soft deopt instead?
    TRACE_CANNOT_INLINE("it has not been compiled/run with feedback yet");
    return false;
  }
  // TODO(olivf): This is a temporary stopgap to prevent infinite recursion when
  // inlining, because we currently excempt small functions from some of the
  // negative heuristics. We should refactor these heuristics and make sure they
  // make sense in the presence of (mutually) recursive inlining. Please do
  // *not* return true before this check.
  if (inlining_depth() > v8_flags.max_maglev_hard_inline_depth) {
    TRACE_CANNOT_INLINE("inlining depth ("
                        << inlining_depth() << ") >= hard-max-depth ("
                        << v8_flags.max_maglev_hard_inline_depth << ")");
    return false;
  }
  if (compilation_unit_->shared_function_info().equals(shared)) {
    TRACE_CANNOT_INLINE("direct recursion");
    return false;
  }
  SharedFunctionInfo::Inlineability inlineability =
      shared.GetInlineability(broker());
  if (inlineability != SharedFunctionInfo::Inlineability::kIsInlineable) {
    TRACE_CANNOT_INLINE(inlineability);
    return false;
  }
  // TODO(victorgomes): Support NewTarget/RegisterInput in inlined functions.
  compiler::BytecodeArrayRef bytecode = shared.GetBytecodeArray(broker());
  if (bytecode.incoming_new_target_or_generator_register().is_valid()) {
    TRACE_CANNOT_INLINE("use unsupported NewTargetOrGenerator register");
    return false;
  }
  if (call_frequency < v8_flags.min_maglev_inlining_frequency) {
    TRACE_CANNOT_INLINE("call frequency ("
                        << call_frequency << ") < minimum threshold ("
                        << v8_flags.min_maglev_inlining_frequency << ")");
    return false;
  }
  if (bytecode.length() < v8_flags.max_maglev_inlined_bytecode_size_small) {
    TRACE_INLINING("  inlining "
                   << shared
                   << ": small function, skipping max-size and max-depth");
    return true;
  }
  if (bytecode.length() > v8_flags.max_maglev_inlined_bytecode_size) {
    TRACE_CANNOT_INLINE("big function, size ("
                        << bytecode.length() << ") >= max-size ("
                        << v8_flags.max_maglev_inlined_bytecode_size << ")");
    return false;
  }
  if (inlining_depth() > v8_flags.max_maglev_inline_depth) {
    TRACE_CANNOT_INLINE("inlining depth ("
                        << inlining_depth() << ") >= max-depth ("
                        << v8_flags.max_maglev_inline_depth << ")");
    return false;
  }
  TRACE_INLINING("  inlining " << shared);
  if (v8_flags.trace_maglev_inlining_verbose) {
    BytecodeArray::Disassemble(bytecode.object(), std::cout);
    i::Print(*feedback_vector->object(), std::cout);
  }
  graph()->add_inlined_bytecode_size(bytecode.length());
  return true;
}

ReduceResult MaglevGraphBuilder::TryBuildInlinedCall(
    ValueNode* context, ValueNode* function, ValueNode* new_target,
    compiler::SharedFunctionInfoRef shared,
    compiler::OptionalFeedbackVectorRef feedback_vector, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  DCHECK_EQ(args.mode(), CallArguments::kDefault);
  float feedback_frequency = 0.0f;
  if (feedback_source.IsValid()) {
    compiler::ProcessedFeedback const& feedback =
        broker()->GetFeedbackForCall(feedback_source);
    feedback_frequency =
        feedback.IsInsufficient() ? 0.0f : feedback.AsCall().frequency();
  }
  float call_frequency = feedback_frequency * call_frequency_;
  if (!ShouldInlineCall(shared, feedback_vector, call_frequency)) {
    return ReduceResult::Fail();
  }

  compiler::BytecodeArrayRef bytecode = shared.GetBytecodeArray(broker());

  if (v8_flags.maglev_print_inlined &&
      TopLevelFunctionPassMaglevPrintFilter() &&
      (v8_flags.print_maglev_code || v8_flags.print_maglev_graph ||
       v8_flags.print_maglev_graphs)) {
    std::cout << "== Inlining " << Brief(*shared.object()) << std::endl;
    BytecodeArray::Disassemble(bytecode.object(), std::cout);
    if (v8_flags.maglev_print_feedback) {
      i::Print(*feedback_vector->object(), std::cout);
    }
  } else if (v8_flags.trace_maglev_graph_building) {
    std::cout << "== Inlining " << shared.object() << std::endl;
  }

  graph()->inlined_functions().push_back(
      OptimizedCompilationInfo::InlinedFunctionHolder(
          shared.object(), bytecode.object(), current_source_position_));
  if (feedback_vector->object()->invocation_count_before_stable(kRelaxedLoad) >
      v8_flags.invocation_count_for_early_optimization) {
    compilation_unit_->info()->set_could_not_inline_all_candidates();
  }
  int inlining_id = static_cast<int>(graph()->inlined_functions().size() - 1);

  // Create a new compilation unit and graph builder for the inlined
  // function.
  MaglevCompilationUnit* inner_unit = MaglevCompilationUnit::NewInner(
      zone(), compilation_unit_, shared, feedback_vector.value());
  MaglevGraphBuilder inner_graph_builder(
      local_isolate_, inner_unit, graph_, call_frequency,
      BytecodeOffset(iterator_.current_offset()), IsInsideLoop(), inlining_id,
      this);

  // Merge catch block state if needed.
  CatchBlockDetails catch_block = GetCurrentTryCatchBlock();
  if (catch_block.ref && catch_block.state->exception_handler_was_used()) {
    // Merge the current state into the handler state.
    catch_block.state->MergeThrow(
        GetCurrentCatchBlockGraphBuilder(), catch_block.unit,
        *current_interpreter_frame_.known_node_aspects(),
        current_interpreter_frame_.virtual_objects());
  }

  // Propagate catch block.
  inner_graph_builder.parent_catch_ = catch_block;
  inner_graph_builder.parent_catch_deopt_frame_distance_ =
      1 + (IsInsideTryBlock() ? 0 : parent_catch_deopt_frame_distance_);

  // Set the inner graph builder to build in the current block.
  inner_graph_builder.current_block_ = current_block_;

  ReduceResult result =
      inner_graph_builder.BuildInlined(context, function, new_target, args);
  if (result.IsDoneWithAbort()) {
    DCHECK_NULL(inner_graph_builder.current_block_);
    current_block_ = nullptr;
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "== Finished inlining (abort) " << shared.object()
                << std::endl;
    }
    return ReduceResult::DoneWithAbort();
  }

  // Propagate KnownNodeAspects back to the caller.
  current_interpreter_frame_.set_known_node_aspects(
      inner_graph_builder.current_interpreter_frame_.known_node_aspects());
  unobserved_context_slot_stores_ =
      inner_graph_builder.unobserved_context_slot_stores_;

  // Propagate virtual object lists back to the caller.
  current_interpreter_frame_.set_virtual_objects(
      inner_graph_builder.current_interpreter_frame_.virtual_objects());

  DCHECK(result.IsDoneWithValue());
  // Resume execution using the final block of the inner builder.
  current_block_ = inner_graph_builder.current_block_;

  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "== Finished inlining " << shared.object() << std::endl;
  }
  return result;
}

namespace {

bool CanInlineArrayIteratingBuiltin(compiler::JSHeapBroker* broker,
                                    const PossibleMaps& maps,
                                    ElementsKind* kind_return) {
  DCHECK_NE(0, maps.size());
  *kind_return = maps.at(0).elements_kind();
  for (compiler::MapRef map : maps) {
    if (!map.supports_fast_array_iteration(broker) ||
        !UnionElementsKindUptoSize(kind_return, map.elements_kind())) {
      return false;
    }
  }
  return true;
}

}  // namespace

ReduceResult MaglevGraphBuilder::TryReduceArrayIsArray(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) return GetBooleanConstant(false);

  ValueNode* node = args|0];

  if (CheckType(node, NodeType::kJSArray)) {
    return GetBooleanConstant(true);
  }

  auto node_info = known_node_aspects().TryGetInfoFor(node);
  if (node_info && node_info->possible_maps_are_known()) {
    bool has_array_map = false;
    bool has_proxy_map = false;
    bool has_other_map = false;
    for (compiler::MapRef map : node_info->possible_maps()) {
      InstanceType type = map.instance_type();
      if (InstanceTypeChecker::IsJSArray(type)) {
        has_array_map = true;
      } else if (InstanceTypeChecker::IsJSProxy(type)) {
        has_proxy_map = true;
      } else {
        has_other_map = true;
      }
    }
    if ((has_array_map ^ has_other_map) && !has_proxy_map) {
      if (has_array_map) node_info->CombineType(NodeType::kJSArray);
      return GetBooleanConstant(has_array_map);
    }
  }

  // TODO(verwaest): Add a node that checks the instance type.
  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::TryReduceArrayForEach(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }

  ValueNode* receiver = args.receiver();
  if (!receiver) return ReduceResult::Fail();

  if (args.count() < 1) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - not enough "
                   "arguments"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  auto node_info = known_node_aspects().TryGetInfoFor(receiver);
  if (!node_info || !node_info->possible_maps_are_known()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - receiver "
                   "map is unknown"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ElementsKind elements_kind;
  if (!CanInlineArrayIteratingBuiltin(broker(), node_info->possible_maps(),
                                      &elements_kind)) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - doesn't "
                   "support fast array iteration or incompatible maps"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  // TODO(leszeks): May only be needed for holey elements kinds.
  if (!broker()->dependencies()->DependOnNoElementsProtector()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - invalidated "
                   "no elements protector"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ValueNode* callback = args|0];
  if (!callback->is_tagged()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - callback is "
                   "untagged value"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ValueNode* this_arg =
      args.count() > 1 ? args|1] : GetRootConstant(RootIndex::kUndefinedValue);

  ValueNode* original_length = BuildLoadJSArrayLength(receiver);

  // Elide the callable check if the node is known callable.
  EnsureType(callback, NodeType::kCallable, |&](NodeType old_type) {
    // ThrowIfNotCallable is wrapped in a lazy_deopt_scope to make sure the
    // exception has the right call stack.
    DeoptFrameScope lazy_deopt_scope(
        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
        base::VectorOf<ValueNode*>({receiver, callback, this_arg,
                                    GetSmiConstant(0), original_length}));
    AddNewNode<ThrowIfNotCallable>({callback});
  });

  ValueNode* original_length_int32 = GetInt32(original_length);

  // Remember the receiver map set before entering the loop the call.
  bool receiver_maps_were_unstable = node_info->possible_maps_are_unstable();
  PossibleMaps receiver_maps_before_loop(node_info->possible_maps());

  // Create a sub graph builder with two variable (index and length)
  MaglevSubGraphBuilder sub_builder(this, 2);
  MaglevSubGraphBuilder::Variable var_index(0);
  MaglevSubGraphBuilder::Variable var_length(1);

  MaglevSubGraphBuilder::Label loop_end(&sub_builder, 1);

  // ```
  // index = 0
  // bind loop_header
  // ```
  sub_builder.set(var_index, GetSmiConstant(0));
  sub_builder.set(var_length, original_length);
  MaglevSubGraphBuilder::LoopLabel loop_header =
      sub_builder.BeginLoop({&var_index, &var_length});

  // Reset known state that is cleared by BeginLoop, but is known to be true on
  // the first iteration, and will be re-checked at the end of the loop.

  // Reset the known receiver maps if necessary.
  if (receiver_maps_were_unstable) {
    node_info->SetPossibleMaps(receiver_maps_before_loop,
                               receiver_maps_were_unstable,
                               // Node type is monotonic, no need to reset it.
                               NodeType::kUnknown, broker());
    known_node_aspects().any_map_for_any_node_is_unstable = true;
  } else {
    DCHECK_EQ(node_info->possible_maps().size(),
              receiver_maps_before_loop.size());
  }
  // Reset the cached loaded array length to the length var.
  RecordKnownProperty(receiver, broker()->length_string(),
                      sub_builder.get(var_length), false,
                      compiler::AccessMode::kLoad);

  // ```
  // if (index_int32 < length_int32)
  //   fallthrough
  // else
  //   goto end
  // ```
  Phi* index_tagged = sub_builder.get(var_index)->Cast<Phi>();
  EnsureType(index_tagged, NodeType::kSmi);
  ValueNode* index_int32 = GetInt32(index_tagged);

  sub_builder.GotoIfFalse<BranchIfInt32Compare>(
      &loop_end, {index_int32, original_length_int32}, Operation::kLessThan);

  // ```
  // next_index = index + 1
  // ```
  ValueNode* next_index_int32 = nullptr;
  {
    // Eager deopt scope for index increment overflow.
    // TODO(pthier): In practice this increment can never overflow, as the max
    // possible array length is less than int32 max value. Add a new
    // Int32Increment that asserts no overflow instead of deopting.
    DeoptFrameScope eager_deopt_scope(
        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
        base::VectorOf<ValueNode*>(
            {receiver, callback, this_arg, index_int32, original_length}));
    next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32});
    EnsureType(next_index_int32, NodeType::kSmi);
  }
  // TODO(leszeks): Assert Smi.

  // ```
  // element = array.elements|index]
  // ```
  ValueNode* elements = BuildLoadElements(receiver);
  ValueNode* element;
  if (IsDoubleElementsKind(elements_kind)) {
    element = BuildLoadFixedDoubleArrayElement(elements, index_int32);
  } else {
    element = BuildLoadFixedArrayElement(elements, index_int32);
  }

  std::optional<MaglevSubGraphBuilder::Label> skip_call;
  if (IsHoleyElementsKind(elements_kind)) {
    // ```
    // if (element is hole) goto skip_call
    // ```
    skip_call.emplace(
        &sub_builder, 2,
        std::initializer_list<MaglevSubGraphBuilder::Variable*>{&var_length});
    if (elements_kind == HOLEY_DOUBLE_ELEMENTS) {
      sub_builder.GotoIfTrue<BranchIfFloat64IsHole>(&*skip_call, {element});
    } else {
      sub_builder.GotoIfTrue<BranchIfRootConstant>(&*skip_call, {element},
                                                   RootIndex::kTheHoleValue);
    }
  }

  // ```
  // callback(this_arg, element, array)
  // ```
  ReduceResult result;
  {
    DeoptFrameScope lazy_deopt_scope(
        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
        base::VectorOf<ValueNode*>(
            {receiver, callback, this_arg, next_index_int32, original_length}));

    CallArguments call_args =
        args.count() < 2
            ? CallArguments(ConvertReceiverMode::kNullOrUndefined,
                            {element, index_tagged, receiver})
            : CallArguments(ConvertReceiverMode::kAny,
                            {this_arg, element, index_tagged, receiver});

    SaveCallSpeculationScope saved(this);
    result = ReduceCall(callback, call_args, saved.value());
  }

  // ```
  // index = next_index
  // jump loop_header
  // ```
  DCHECK_IMPLIES(result.IsDoneWithAbort(), current_block_ == nullptr);

  // No need to finish the loop if this code is unreachable.
  if (!result.IsDoneWithAbort()) {
    // If any of the receiver's maps were unstable maps, we have to re-check the
    // maps on each iteration, in case the callback changed them. That said, we
    // know that the maps are valid on the first iteration, so we can rotate the
    // check to _after_ the callback, and then elide it if the receiver maps are
    // still known to be valid (i.e. the known maps after the call are contained
    // inside the known maps before the call).
    bool recheck_maps_after_call = receiver_maps_were_unstable;
    if (recheck_maps_after_call) {
      // No need to recheck maps if there are known maps...
      if (auto receiver_info_after_call =
              known_node_aspects().TryGetInfoFor(receiver)) {
        // ... and those known maps are equal to, or a subset of, the maps
        // before the call.
        if (receiver_info_after_call &&
            receiver_info_after_call->possible_maps_are_known()) {
          recheck_maps_after_call = !receiver_maps_before_loop.contains(
              receiver_info_after_call->possible_maps());
        }
      }
    }

    // Make sure to finish the loop if we eager deopt in the map check or index
    // check.
    DeoptFrameScope eager_deopt_scope(
        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
        base::VectorOf<ValueNode*>(
            {receiver, callback, this_arg, next_index_int32, original_length}));

    if (recheck_maps_after_call) {
      // Build the CheckMap manually, since we're doing it with already known
      // maps rather than feedback, and we don't need to update known node
      // aspects or types since we're at the end of the loop anyway.
      bool emit_check_with_migration = std::any_of(
          receiver_maps_before_loop.begin(), receiver_maps_before_loop.end(),
          |](compiler::MapRef map) { return map.is_migration_target(); });
      if (emit_check_with_migration) {
        AddNewNode<CheckMapsWithMigration>({receiver},
                                           receiver_maps_before_loop,
                                           CheckType::kOmitHeapObjectCheck);
      } else {
        AddNewNode<CheckMaps>({receiver}, receiver_maps_before_loop,
                              CheckType::kOmitHeapObjectCheck);
      }
    }

    // Check if the index is still in bounds, in case the callback changed the
    // length.
    ValueNode* current_length = BuildLoadJSArrayLength(receiver);
    sub_builder.set(var_length, current_length);

    // Reference compare the loaded length against the original length. If this
    // is the same value node, then we didn't have any side effects and didn't
    // clear the cached length.
    if (current_length != original_length) {
      RETURN_IF_ABORT(
          TryBuildCheckInt32Condition(original_length_int32, current_length,
                                      AssertCondition::kUnsignedLessThanEqual,
                                      DeoptimizeReason::kArrayLengthChanged));
    }
  }

  if (skip_call.has_value()) {
    sub_builder.GotoOrTrim(&*skip_call);
    sub_builder.Bind(&*skip_call);
  }

  sub_builder.set(var_index, next_index_int32);
  sub_builder.EndLoop(&loop_header);

  // ```
  // bind end
  // ```
  sub_builder.Bind(&loop_end);

  return GetRootConstant(RootIndex::kUndefinedValue);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayIteratorPrototypeNext(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }

  ValueNode* receiver = args.receiver();
  if (!receiver) return ReduceResult::Fail();

  if (!receiver->Is<InlinedAllocation>()) return ReduceResult::Fail();
  VirtualObject* iterator = receiver->Cast<InlinedAllocation>()->object();
  if (!iterator->map().IsJSArrayIteratorMap()) {
    FAIL("iterator is not a JS array iterator object");
  }

  ValueNode* iterated_object =
      iterator->get(JSArrayIterator::kIteratedObjectOffset);
  ElementsKind elements_kind;
  base::SmallVector<compiler::MapRef, 4> maps;
  if (iterated_object->Is<InlinedAllocation>()) {
    VirtualObject* array = iterated_object->Cast<InlinedAllocation>()->object();
    // TODO(victorgomes): Remove this once we track changes in the inlined
    // allocated object.
    if (iterated_object->Cast<InlinedAllocation>()->IsEscaping()) {
      FAIL("allocation is escaping, map could have been changed");
    }
    // TODO(victorgomes): This effectively disable the optimization for `for-of`
    // loops. We need to figure it out a way to re-enable this.
    if (IsInsideLoop()) {
      FAIL("we're inside a loop, iterated object map could change");
    }
    auto map = array->map();
    if (!map.supports_fast_array_iteration(broker())) {
      FAIL("no fast array iteration support");
    }
    elements_kind = map.elements_kind();
    maps.push_back(map);
  } else {
    auto node_info = known_node_aspects().TryGetInfoFor(iterated_object);
    if (!node_info || !node_info->possible_maps_are_known()) {
      FAIL("iterated object is unknown");
    }
    if (!CanInlineArrayIteratingBuiltin(broker(), node_info->possible_maps(),
                                        &elements_kind)) {
      FAIL("no fast array iteration support or incompatible maps");
    }
    for (auto map : node_info->possible_maps()) {
      maps.push_back(map);
    }
  }

  // TODO(victorgomes): Support typed arrays.
  if (IsTypedArrayElementsKind(elements_kind)) {
    FAIL("no typed arrays support");
  }

  if (IsHoleyElementsKind(elements_kind) &&
      !broker()->dependencies()->DependOnNoElementsProtector()) {
    FAIL("no elements protector");
  }

  // Load the ||NextIndex]] from the {iterator}.
  // We can assume index and length fit in Uint32.
  ValueNode* index =
      BuildLoadTaggedField(receiver, JSArrayIterator::kNextIndexOffset);
  ValueNode* uint32_index;
  GET_VALUE_OR_ABORT(uint32_index, GetUint32ElementIndex(index));
  ValueNode* uint32_length;
  GET_VALUE_OR_ABORT(uint32_length,
                     GetUint32ElementIndex(BuildLoadJSArrayLength(
                         iterated_object, IsFastElementsKind(elements_kind)
                                              ? NodeType::kSmi
                                              : NodeType::kNumber)));

  // Check next index is below length
  MaglevSubGraphBuilder subgraph(this, 2);
  MaglevSubGraphBuilder::Variable is_done(0);
  MaglevSubGraphBuilder::Variable ret_value(1);
  RETURN_IF_ABORT(subgraph.Branch(
      {&is_done, &ret_value},
      |&](auto& builder) {
        return BuildBranchIfUint32Compare(builder, Operation::kLessThan,
                                          uint32_index, uint32_length);
      },
      |&] {
        ValueNode* int32_index = GetInt32(uint32_index);
        subgraph.set(is_done, GetBooleanConstant(false));
        DCHECK(
            iterator->get(JSArrayIterator::kKindOffset)->Is<Int32Constant>());
        IterationKind iteration_kind = static_cast<IterationKind>(
            iterator->get(JSArrayIterator::kKindOffset)
                ->Cast<Int32Constant>()
                ->value());
        if (iteration_kind == IterationKind::kKeys) {
          subgraph.set(ret_value, index);
        } else {
          ValueNode* value;
          GET_VALUE_OR_ABORT(
              value,
              TryBuildElementLoadOnJSArrayOrJSObject(
                  iterated_object, int32_index, base::VectorOf(maps),
                  elements_kind, KeyedAccessLoadMode::kHandleOOBAndHoles));
          if (iteration_kind == IterationKind::kEntries) {
            subgraph.set(ret_value,
                         BuildAndAllocateKeyValueArray(index, value));
          } else {
            subgraph.set(ret_value, value);
          }
        }
        // Add 1 to index
        ValueNode* next_index = AddNewNode<Int32AddWithOverflow>(
            {int32_index, GetInt32Constant(1)});
        EnsureType(next_index, NodeType::kSmi);
        // Update ||NextIndex]]
        BuildStoreTaggedFieldNoWriteBarrier(receiver, next_index,
                                            JSArrayIterator::kNextIndexOffset,
                                            StoreTaggedMode::kDefault);
        return ReduceResult::Done();
      },
      |&] {
        // Index is greater or equal than length.
        subgraph.set(is_done, GetBooleanConstant(true));
        subgraph.set(ret_value, GetRootConstant(RootIndex::kUndefinedValue));
        if (!IsTypedArrayElementsKind(elements_kind)) {
          // Mark the {iterator} as exhausted by setting the ||NextIndex]] to a
          // value that will never pass the length check again (aka the maximum
          // value possible for the specific iterated object). Note that this is
          // different from what the specification says, which is changing the
          // ||IteratedObject]] field to undefined, but that makes it difficult
          // to eliminate the map checks and "length" accesses in for..of loops.
          //
          // This is not necessary for JSTypedArray's, since the length of those
          // cannot change later and so if we were ever out of bounds for them
          // we will stay out-of-bounds forever.
          BuildStoreTaggedField(receiver, GetFloat64Constant(kMaxUInt32),
                                JSArrayIterator::kNextIndexOffset,
                                StoreTaggedMode::kDefault);
        }
        return ReduceResult::Done();
      }));

  // Allocate result object and return.
  compiler::MapRef map =
      broker()->target_native_context().iterator_result_map(broker());
  VirtualObject* iter_result = CreateJSIteratorResult(
      map, subgraph.get(ret_value), subgraph.get(is_done));
  ValueNode* allocation =
      BuildInlinedAllocation(iter_result, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypeEntries(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  if (!CheckType(receiver, NodeType::kJSReceiver)) {
    return ReduceResult::Fail();
  }
  return BuildAndAllocateJSArrayIterator(receiver, IterationKind::kEntries);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypeKeys(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  if (!CheckType(receiver, NodeType::kJSReceiver)) {
    return ReduceResult::Fail();
  }
  return BuildAndAllocateJSArrayIterator(receiver, IterationKind::kKeys);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypeValues(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  if (!CheckType(receiver, NodeType::kJSReceiver)) {
    return ReduceResult::Fail();
  }
  return BuildAndAllocateJSArrayIterator(receiver, IterationKind::kValues);
}

ReduceResult MaglevGraphBuilder::TryReduceStringFromCharCode(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (args.count() != 1) return ReduceResult::Fail();
  return AddNewNode<BuiltinStringFromCharCode>({GetTruncatedInt32ForToNumber(
      args|0], NodeType::kNumberOrOddball,
      TaggedToFloat64ConversionType::kNumberOrOddball)});
}

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeCharCodeAt(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  ValueNode* index;
  if (args.count() == 0) {
    // Index is the undefined object. ToIntegerOrInfinity(undefined) = 0.
    index = GetInt32Constant(0);
  } else {
    index = GetInt32ElementIndex(args|0]);
  }
  // Any other argument is ignored.

  // Try to constant-fold if receiver and index are constant
  if (auto cst = TryGetConstant(receiver)) {
    if (cst->IsString() && index->Is<Int32Constant>()) {
      compiler::StringRef str = cst->AsString();
      int idx = index->Cast<Int32Constant>()->value();
      if (idx >= 0 && static_cast<uint32_t>(idx) < str.length()) {
        if (std::optional<uint16_t> value = str.GetChar(broker(), idx)) {
          return GetSmiConstant(*value);
        }
      }
    }
  }

  // Ensure that {receiver} is actually a String.
  BuildCheckString(receiver);
  // And index is below length.
  ValueNode* length = BuildLoadStringLength(receiver);
  RETURN_IF_ABORT(TryBuildCheckInt32Condition(
      index, length, AssertCondition::kUnsignedLessThan,
      DeoptimizeReason::kOutOfBounds));
  return AddNewNode<BuiltinStringPrototypeCharCodeOrCodePointAt>(
      {receiver, index},
      BuiltinStringPrototypeCharCodeOrCodePointAt::kCharCodeAt);
}

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeCodePointAt(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  ValueNode* index;
  if (args.count() == 0) {
    // Index is the undefined object. ToIntegerOrInfinity(undefined) = 0.
    index = GetInt32Constant(0);
  } else {
    index = GetInt32ElementIndex(args|0]);
  }
  // Any other argument is ignored.
  // Ensure that {receiver} is actually a String.
  BuildCheckString(receiver);
  // And index is below length.
  ValueNode* length = BuildLoadStringLength(receiver);
  RETURN_IF_ABORT(TryBuildCheckInt32Condition(
      index, length, AssertCondition::kUnsignedLessThan,
      DeoptimizeReason::kOutOfBounds));
  return AddNewNode<BuiltinStringPrototypeCharCodeOrCodePointAt>(
      {receiver, index},
      BuiltinStringPrototypeCharCodeOrCodePointAt::kCodePointAt);
}

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeIterator(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  // Ensure that {receiver} is actually a String.
  BuildCheckString(receiver);
  compiler::MapRef map =
      broker()->target_native_context().initial_string_iterator_map(broker());
  VirtualObject* string_iterator = CreateJSStringIterator(map, receiver);
  ValueNode* allocation =
      BuildInlinedAllocation(string_iterator, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

#ifdef V8_INTL_SUPPORT

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeLocaleCompareIntl(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() < 1 || args.count() > 3) return ReduceResult::Fail();

  LocalFactory* factory = local_isolate()->factory();
  compiler::ObjectRef undefined_ref = broker()->undefined_value();

  DirectHandle<Object> locales_handle;
  ValueNode* locales_node = nullptr;
  if (args.count() > 1) {
    compiler::OptionalHeapObjectRef maybe_locales = TryGetConstant(args|1]);
    if (!maybe_locales) return ReduceResult::Fail();
    compiler::HeapObjectRef locales = maybe_locales.value();
    if (locales.equals(undefined_ref)) {
      locales_handle = factory->undefined_value();
      locales_node = GetRootConstant(RootIndex::kUndefinedValue);
    } else {
      if (!locales.IsString()) return ReduceResult::Fail();
      compiler::StringRef sref = locales.AsString();
      std::optional<Handle<String>> maybe_locales_handle =
          sref.ObjectIfContentAccessible(broker());
      if (!maybe_locales_handle) return ReduceResult::Fail();
      locales_handle = *maybe_locales_handle;
      locales_node = args|1];
    }
  } else {
    locales_handle = factory->undefined_value();
    locales_node = GetRootConstant(RootIndex::kUndefinedValue);
  }

  if (args.count() > 2) {
    compiler::OptionalHeapObjectRef maybe_options = TryGetConstant(args|2]);
    if (!maybe_options) return ReduceResult::Fail();
    if (!maybe_options.value().equals(undefined_ref))
      return ReduceResult::Fail();
  }

  DCHECK(!locales_handle.is_null());
  DCHECK_NOT_NULL(locales_node);

  if (Intl::CompareStringsOptionsFor(local_isolate(), locales_handle,
                                     factory->undefined_value()) !=
      Intl::CompareStringsOptions::kTryFastPath) {
    return ReduceResult::Fail();
  }
  return BuildCallBuiltin<Builtin::kStringFastLocaleCompare>(
      {GetConstant(target),
       GetTaggedValue(GetValueOrUndefined(args.receiver())),
       GetTaggedValue(args|0]), GetTaggedValue(locales_node)});
}

#endif  // V8_INTL_SUPPORT

#ifdef V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA
ReduceResult MaglevGraphBuilder::TryReduceGetContinuationPreservedEmbedderData(
    compiler::JSFunctionRef target, CallArguments& args) {
  return AddNewNode<GetContinuationPreservedEmbedderData>({});
}

ReduceResult MaglevGraphBuilder::TryReduceSetContinuationPreservedEmbedderData(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) return ReduceResult::Fail();

  AddNewNode<SetContinuationPreservedEmbedderData>({args|0]});
  return GetRootConstant(RootIndex::kUndefinedValue);
}
#endif  // V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA

template <typename LoadNode>
ReduceResult MaglevGraphBuilder::TryBuildLoadDataView(const CallArguments& args,
                                                      ExternalArrayType type) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (!broker()->dependencies()->DependOnArrayBufferDetachingProtector()) {
    // TODO(victorgomes): Add checks whether the array has been detached.
    return ReduceResult::Fail();
  }
  // TODO(victorgomes): Add data view to known types.
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  AddNewNode<CheckInstanceType>({receiver}, CheckType::kCheckHeapObject,
                                JS_DATA_VIEW_TYPE, JS_DATA_VIEW_TYPE);
  // TODO(v8:11111): Optimize for JS_RAB_GSAB_DATA_VIEW_TYPE too.
  ValueNode* offset =
      args|0] ? GetInt32ElementIndex(args|0]) : GetInt32Constant(0);
  AddNewNode<CheckJSDataViewBounds>({receiver, offset}, type);
  ValueNode* is_little_endian = args|1] ? args|1] : GetBooleanConstant(false);
  return AddNewNode<LoadNode>({receiver, offset, is_little_endian}, type);
}

template <typename StoreNode, typename Function>
ReduceResult MaglevGraphBuilder::TryBuildStoreDataView(
    const CallArguments& args, ExternalArrayType type, Function&& getValue) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (!broker()->dependencies()->DependOnArrayBufferDetachingProtector()) {
    // TODO(victorgomes): Add checks whether the array has been detached.
    return ReduceResult::Fail();
  }
  // TODO(victorgomes): Add data view to known types.
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  AddNewNode<CheckInstanceType>({receiver}, CheckType::kCheckHeapObject,
                                JS_DATA_VIEW_TYPE, JS_DATA_VIEW_TYPE);
  // TODO(v8:11111): Optimize for JS_RAB_GSAB_DATA_VIEW_TYPE too.
  ValueNode* offset =
      args|0] ? GetInt32ElementIndex(args|0]) : GetInt32Constant(0);
  AddNewNode<CheckJSDataViewBounds>({receiver, offset},
                                    ExternalArrayType::kExternalFloat64Array);
  ValueNode* value = getValue(args|1]);
  ValueNode* is_little_endian = args|2] ? args|2] : GetBooleanConstant(false);
  AddNewNode<StoreNode>({receiver, offset, value, is_little_endian}, type);
  return GetRootConstant(RootIndex::kUndefinedValue);
}

ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetInt8(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt8Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetInt8(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt8Array,
      |&](ValueNode* value) { return value ? value : GetInt32Constant(0); });
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetInt16(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt16Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetInt16(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt16Array,
      |&](ValueNode* value) { return value ? value : GetInt32Constant(0); });
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetInt32(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt32Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetInt32(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt32Array,
      |&](ValueNode* value) { return value ? value : GetInt32Constant(0); });
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetFloat64(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadDoubleDataViewElement>(
      args, ExternalArrayType::kExternalFloat64Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetFloat64(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreDoubleDataViewElement>(
      args, ExternalArrayType::kExternalFloat64Array, |&](ValueNode* value) {
        return value ? GetHoleyFloat64ForToNumber(
                           value, NodeType::kNumberOrOddball,
                           TaggedToFloat64ConversionType::kNumberOrOddball)
                     : GetFloat64Constant(
                           std::numeric_limits<double>::quiet_NaN());
      });
}

ReduceResult MaglevGraphBuilder::TryReduceFunctionPrototypeCall(
    compiler::JSFunctionRef target, CallArguments& args) {
  // We can't reduce Function#call when there is no receiver function.
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  args.PopReceiver(ConvertReceiverMode::kAny);

  SaveCallSpeculationScope saved(this);
  return ReduceCall(receiver, args, saved.value());
}

ReduceResult MaglevGraphBuilder::TryReduceFunctionPrototypeApply(
    compiler::JSFunctionRef target, CallArguments& args) {
  compiler::OptionalHeapObjectRef maybe_receiver;
  if (current_speculation_feedback_.IsValid()) {
    const compiler::ProcessedFeedback& processed_feedback =
        broker()->GetFeedbackForCall(current_speculation_feedback_);
    DCHECK_EQ(processed_feedback.kind(), compiler::ProcessedFeedback::kCall);
    const compiler::CallFeedback& call_feedback = processed_feedback.AsCall();
    if (call_feedback.call_feedback_content() ==
        CallFeedbackContent::kReceiver) {
      maybe_receiver = call_feedback.target();
    }
  }
  return ReduceFunctionPrototypeApplyCallWithReceiver(
      maybe_receiver, args, current_speculation_feedback_);
}

namespace {

template <size_t MaxKindCount, typename KindsToIndexFunc>
bool CanInlineArrayResizingBuiltin(
    compiler::JSHeapBroker* broker, const PossibleMaps& possible_maps,
    std::array<SmallZoneVector<compiler::MapRef, 2>, MaxKindCount>& map_kinds,
    KindsToIndexFunc&& elements_kind_to_index, int* unique_kind_count,
    bool is_loading) {
  uint8_t kind_bitmap = 0;
  for (compiler::MapRef map : possible_maps) {
    if (!map.supports_fast_array_resize(broker)) {
      return false;
    }
    ElementsKind kind = map.elements_kind();
    if (is_loading && kind == HOLEY_DOUBLE_ELEMENTS) {
      return false;
    }
    // Group maps by elements kind, using the provided function to translate
    // elements kinds to indices.
    // kind_bitmap is used to get the unique kinds (predecessor count for the
    // next block).
    uint8_t kind_index = elements_kind_to_index(kind);
    kind_bitmap |= 1 << kind_index;
    map_kinds|kind_index].push_back(map);
  }

  *unique_kind_count = base::bits::CountPopulation(kind_bitmap);
  DCHECK_GE(*unique_kind_count, 1);
  return true;
}

}  // namespace

template <typename MapKindsT, typename IndexToElementsKindFunc,
          typename BuildKindSpecificFunc>
ReduceResult MaglevGraphBuilder::BuildJSArrayBuiltinMapSwitchOnElementsKind(
    ValueNode* receiver, const MapKindsT& map_kinds,
    MaglevSubGraphBuilder& sub_graph,
    std::optional<MaglevSubGraphBuilder::Label>& do_return,
    int unique_kind_count, IndexToElementsKindFunc&& index_to_elements_kind,
    BuildKindSpecificFunc&& build_kind_specific) {
  // TODO(pthier): Support map packing.
  DCHECK(!V8_MAP_PACKING_BOOL);
  ValueNode* receiver_map =
      BuildLoadTaggedField(receiver, HeapObject::kMapOffset);
  int emitted_kind_checks = 0;
  bool any_successful = false;
  for (size_t kind_index = 0; kind_index < map_kinds.size(); kind_index++) {
    const auto& maps = map_kinds|kind_index];
    // Skip kinds we haven't observed.
    if (maps.empty()) continue;
    ElementsKind kind = index_to_elements_kind(kind_index);
    // Create branches for all but the last elements kind. We don't need
    // to check the maps of the last kind, as all possible maps have already
    // been checked when the property (builtin name) was loaded.
    if (++emitted_kind_checks < unique_kind_count) {
      MaglevSubGraphBuilder::Label check_next_map(&sub_graph, 1);
      std::optional<MaglevSubGraphBuilder::Label> do_push;
      if (maps.size() > 1) {
        do_push.emplace(&sub_graph, static_cast<int>(maps.size()));
        for (size_t map_index = 1; map_index < maps.size(); map_index++) {
          sub_graph.GotoIfTrue<BranchIfReferenceEqual>(
              &*do_push, {receiver_map, GetConstant(maps|map_index])});
        }
      }
      sub_graph.GotoIfFalse<BranchIfReferenceEqual>(
          &check_next_map, {receiver_map, GetConstant(maps|0])});
      if (do_push.has_value()) {
        sub_graph.Goto(&*do_push);
        sub_graph.Bind(&*do_push);
      }
      if (!build_kind_specific(kind).IsDoneWithAbort()) {
        any_successful = true;
      }
      DCHECK(do_return.has_value());
      sub_graph.GotoOrTrim(&*do_return);
      sub_graph.Bind(&check_next_map);
    } else {
      if (!build_kind_specific(kind).IsDoneWithAbort()) {
        any_successful = true;
      }
      if (do_return.has_value()) {
        sub_graph.GotoOrTrim(&*do_return);
      }
    }
  }
  DCHECK_IMPLIES(!any_successful, !current_block_);
  return any_successful ? ReduceResult::Done() : ReduceResult::DoneWithAbort();
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypePush(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  // We can't reduce Function#call when there is no receiver function.
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - no receiver"
                << std::endl;
    }
    return ReduceResult::Fail();
  }
  // TODO(pthier): Support multiple arguments.
  if (args.count() != 1) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - invalid "
                   "argument count"
                << std::endl;
    }
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());

  auto node_info = known_node_aspects().TryGetInfoFor(receiver);
  // If the map set is not found, then we don't know anything about the map of
  // the receiver, so bail.
  if (!node_info || !node_info->possible_maps_are_known()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout
          << "  ! Failed to reduce Array.prototype.push - unknown receiver map"
          << std::endl;
    }
    return ReduceResult::Fail();
  }

  const PossibleMaps& possible_maps = node_info->possible_maps();
  // If the set of possible maps is empty, then there's no possible map for this
  // receiver, therefore this path is unreachable at runtime. We're unlikely to
  // ever hit this case, BuildCheckMaps should already unconditionally deopt,
  // but check it in case another checking operation fails to statically
  // unconditionally deopt.
  if (possible_maps.is_empty()) {
    // TODO(leszeks): Add an unreachable assert here.
    return ReduceResult::DoneWithAbort();
  }

  if (!broker()->dependencies()->DependOnNoElementsProtector()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - "
                   "NoElementsProtector invalidated"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  // Check that inlining resizing array builtins is supported and group maps
  // by elements kind.
  std::array<SmallZoneVector<compiler::MapRef, 2>, 3> map_kinds = {
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone())};
  // Function to group maps by elements kind, ignoring packedness. Packedness
  // doesn't matter for push().
  // Kinds we care about are all paired in the first 6 values of ElementsKind,
  // so we can use integer division to truncate holeyness.
  auto elements_kind_to_index = |&](ElementsKind kind) {
    static_assert(kFastElementsKindCount <= 6);
    static_assert(kFastElementsKindPackedToHoley == 1);
    return static_cast<uint8_t>(kind) / 2;
  };
  auto index_to_elements_kind = |&](uint8_t kind_index) {
    return static_cast<ElementsKind>(kind_index * 2);
  };
  int unique_kind_count;
  if (!CanInlineArrayResizingBuiltin(broker(), possible_maps, map_kinds,
                                     elements_kind_to_index, &unique_kind_count,
                                     false)) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - Map doesn't "
                   "support fast resizing"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  MaglevSubGraphBuilder sub_graph(this, 0);
[Step 6: Duration 16.52 seconds| Input tokens: 27,439 | Output tokens: 5,282]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 7 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable|CreateCatchContext|SetPendingMessage|exception handler|CatchContext', 'options': '-n maglev'}                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'ThrowIfNotCallable|CreateCatchContext|SetPendingMessage|exception handler|CatchContext' | head -n 1000
Observations: rg: ThrowIfNotCallable|CreateCatchContext|SetPendingMessage|exception handler|CatchContext: IO error for operation on ThrowIfNotCallable|CreateCatchContext|SetPendingMessage|exception 
handler|CatchContext: No such file or directory (os error 2)
[Step 7: Duration 10.21 seconds| Input tokens: 61,791 | Output tokens: 5,906]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 8 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': '-n'}                                                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CanInlineArrayIteratingBuiltin|supports_fast_array_iteration|UnionElementsKindUptoSize', 'options': '-n maglev/maglev-graph-builder.cc'}                │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'MaglevSubGraphBuilder::BeginLoop|BeginLoop\\(|EndLoop\\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant', 'options': '-n maglev'}      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ArrayForEachLoopLazyDeoptContinuation|ArrayForEachLoopEagerDeoptContinuation', 'options': '-n'}                                                         │
Command: cd /Users/tarun/v8/v8/src && rg -n maglev/maglev-graph-builder.cc 'CanInlineArrayIteratingBuiltin|supports_fast_array_iteration|UnionElementsKindUptoSize' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'builtins/array-foreach.tq'}                                                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'MaglevSubGraphBuilder::BeginLoop|BeginLoop\(|EndLoop\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant' | head -n 1000
Observations: rg: CanInlineArrayIteratingBuiltin|supports_fast_array_iteration|UnionElementsKindUptoSize: IO error for operation on 
CanInlineArrayIteratingBuiltin|supports_fast_array_iteration|UnionElementsKindUptoSize: No such file or directory (os error 2)
Observations: rg: MaglevSubGraphBuilder::BeginLoop|BeginLoop\(|EndLoop\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant: IO error for operation on 
MaglevSubGraphBuilder::BeginLoop|BeginLoop\(|EndLoop\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/builtins/array-foreach.tq'
Command: cd /Users/tarun/v8/v8/src && rg -n 'ThrowIfNotCallable' | head -n 1000
Observations: maglev/maglev-graph-builder.cc:8127:    // ThrowIfNotCallable is wrapped in a lazy_deopt_scope to make sure the
maglev/maglev-graph-builder.cc:8133:    AddNewNode<ThrowIfNotCallable>({callback});
maglev/maglev-ir.h:359:  V(ThrowIfNotCallable)                       \
maglev/maglev-ir.h:9739:class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> {
maglev/maglev-ir.h:9740:  using Base = FixedInputNodeT<1, ThrowIfNotCallable>;
maglev/maglev-ir.h:9743:  explicit ThrowIfNotCallable(uint64_t bitfield) : Base(bitfield) {}
maglev/maglev-ir.cc:5424:int ThrowIfNotCallable::MaxCallStackArgs() const { return 1; }
maglev/maglev-ir.cc:5425:void ThrowIfNotCallable::SetValueLocationConstraints() {
maglev/maglev-ir.cc:5429:void ThrowIfNotCallable::GenerateCode(MaglevAssembler* masm,
maglev/maglev-ir.cc:5432:      |](MaglevAssembler* masm, ThrowIfNotCallable* node) {
compiler/js-call-reducer.cc:369:  void ThrowIfNotCallable(TNode<Object> maybe_callable,
compiler/js-call-reducer.cc:1546:  ThrowIfNotCallable(fncallback, ForEachLoopLazyFrameState(frame_state_params,
compiler/js-call-reducer.cc:1675:  ThrowIfNotCallable(
compiler/js-call-reducer.cc:1834:  ThrowIfNotCallable(fncallback,
compiler/js-call-reducer.cc:1968:  ThrowIfNotCallable(fncallback, FilterLoopLazyFrameState(frame_state_params,
compiler/js-call-reducer.cc:2094:  ThrowIfNotCallable(
compiler/js-call-reducer.cc:2196:  ThrowIfNotCallable(
compiler/js-call-reducer.cc:2426:  ThrowIfNotCallable(executor,
codegen/code-stub-assembler.h:2660:  void ThrowIfNotCallable(TNode<Context> context, TNode<Object> value,
compiler/turboshaft/maglev-graph-building-phase.cc:1630:  maglev::ProcessResult Process(maglev::ThrowIfNotCallable* node,
codegen/code-stub-assembler.cc:7221:void CodeStubAssembler::ThrowIfNotCallable(TNode<Context> context,
Command: cd /Users/tarun/v8/v8/src && rg -n 'ArrayForEachLoopLazyDeoptContinuation|ArrayForEachLoopEagerDeoptContinuation' | head -n 1000
Observations: compiler/js-call-reducer.cc:1507:  Builtin builtin = Builtin::kArrayForEachLoopLazyDeoptContinuation;
compiler/js-call-reducer.cc:1518:  Builtin builtin = Builtin::kArrayForEachLoopEagerDeoptContinuation;
maglev/maglev-graph-builder.cc:413:      case Builtin::kArrayForEachLoopLazyDeoptContinuation:
maglev/maglev-graph-builder.cc:8130:        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
maglev/maglev-graph-builder.cc:8200:        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
maglev/maglev-graph-builder.cc:8241:        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
maglev/maglev-graph-builder.cc:8288:        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
builtins/array-foreach.tq:6:transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(
builtins/array-foreach.tq:22:transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(
Command: cd /Users/tarun/v8/v8/src && cat '/Users/tarun/v8/v8/src/builtins/array-foreach.tq'
Observations: // Copyright 2018 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

namespace array {
transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(
    js-implicit context: NativeContext, receiver: JSAny)(callback: JSAny,
    thisArg: JSAny, initialK: JSAny, length: JSAny): JSAny {
  // All continuation points in the optimized forEach implementation are
  // after the ToObject(O) call that ensures we are dealing with a
  // JSReceiver.
  const jsreceiver = Cast<JSReceiver>(receiver) otherwise unreachable;
  const callbackfn = Cast<Callable>(callback) otherwise unreachable;
  const numberK = Cast<Number>(initialK) otherwise unreachable;
  const numberLength = Cast<Number>(length) otherwise unreachable;

  return ArrayForEachLoopContinuation(
      jsreceiver, callbackfn, thisArg, Undefined, jsreceiver, numberK,
      numberLength, Undefined);
}

transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(
    js-implicit context: NativeContext, receiver: JSAny)(callback: JSAny,
    thisArg: JSAny, initialK: JSAny, length: JSAny, _result: JSAny): JSAny {
  // All continuation points in the optimized forEach implementation are
  // after the ToObject(O) call that ensures we are dealing with a
  // JSReceiver.
  const jsreceiver = Cast<JSReceiver>(receiver) otherwise unreachable;
  const callbackfn = Cast<Callable>(callback) otherwise unreachable;
  const numberK = Cast<Number>(initialK) otherwise unreachable;
  const numberLength = Cast<Number>(length) otherwise unreachable;

  return ArrayForEachLoopContinuation(
      jsreceiver, callbackfn, thisArg, Undefined, jsreceiver, numberK,
      numberLength, Undefined);
}

transitioning builtin ArrayForEachLoopContinuation(
    implicit context: Context)(_receiver: JSReceiver, callbackfn: Callable,
    thisArg: JSAny, _array: JSAny, o: JSReceiver, initialK: Number, len: Number,
    _to: JSAny): JSAny {
  // variables {array} and {to} are ignored.

  // 5. Let k be 0.
  // 6. Repeat, while k < len
  for (let k: Number = initialK; k < len; k = k + 1) {
    // 6a. Let Pk be ! ToString(k).
    // k is guaranteed to be a positive integer, hence ToString is
    // side-effect free and HasProperty/GetProperty do the conversion inline.

    // 6b. Let kPresent be ? HasProperty(O, Pk).
    const kPresent: Boolean = HasProperty_Inline(o, k);

    // 6c. If kPresent is true, then
    if (kPresent == True) {
      // 6c. i. Let kValue be ? Get(O, Pk).
      const kValue: JSAny = GetProperty(o, k);

      // 6c. ii. Perform ? Call(callbackfn, T, <kValue, k, O>).
      Call(context, callbackfn, thisArg, kValue, k, o);
    }

    // 6d. Increase k by 1. (done by the loop).
  }
  return Undefined;
}

transitioning macro FastArrayForEach(
    implicit context: Context)(o: JSReceiver, len: Number,
    callbackfn: Callable, thisArg: JSAny): JSAny
    labels Bailout(Smi) {
  let k: Smi = 0;
  const smiLen = Cast<Smi>(len) otherwise goto Bailout(k);
  const fastO = Cast<FastJSArray>(o) otherwise goto Bailout(k);
  let fastOW = NewFastJSArrayWitness(fastO);

  // Build a fast loop over the smi array.
  for (; k < smiLen; k++) {
    fastOW.Recheck() otherwise goto Bailout(k);

    // Ensure that we haven't walked beyond a possibly updated length.
    if (k >= fastOW.Get().length) goto Bailout(k);
    const value: JSAny = fastOW.LoadElementNoHole(k)
        otherwise continue;
    Call(context, callbackfn, thisArg, value, k, fastOW.Get());
  }
  return Undefined;
}

// https://tc39.github.io/ecma262/#sec-array.prototype.foreach
transitioning javascript builtin ArrayForEach(
    js-implicit context: NativeContext, receiver: JSAny)(...arguments): JSAny {
  try {
    RequireObjectCoercible(receiver, 'Array.prototype.forEach');

    // 1. Let O be ? ToObject(this value).
    const o: JSReceiver = ToObject_Inline(context, receiver);

    // 2. Let len be ? ToLength(? Get(O, "length")).
    const len: Number = GetLengthProperty(o);

    // 3. If IsCallable(callbackfn) is false, throw a TypeError exception.
    if (arguments.length == 0) {
      goto TypeError;
    }
    const callbackfn = Cast<Callable>(arguments|0]) otherwise TypeError;

    // 4. If thisArg is present, let T be thisArg; else let T be undefined.
    const thisArg: JSAny = arguments|1];

    // Special cases.
    let k: Number = 0;
    try {
      return FastArrayForEach(o, len, callbackfn, thisArg)
          otherwise Bailout;
    } label Bailout(kValue: Smi) deferred {
      k = kValue;
    }

    return ArrayForEachLoopContinuation(
        o, callbackfn, thisArg, Undefined, o, k, len, Undefined);
  } label TypeError deferred {
    ThrowCalledNonCallable(arguments|0]);
  }
}
}
[Step 8: Duration 23.89 seconds| Input tokens: 96,252 | Output tokens: 6,946]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 9 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc', 'section': 1}                                                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Command: cd /Users/tarun/v8/v8/src && sed -n '1,3000p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Observations: Showing section 1/5 (lines 1-3000) of 'maglev/maglev-graph-builder.cc':
// Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/maglev/maglev-graph-builder.h"

#include <algorithm>
#include <limits>
#include <optional>
#include <utility>

#include "src/base/bounds.h"
#include "src/base/ieee754.h"
#include "src/base/logging.h"
#include "src/base/vector.h"
#include "src/builtins/builtins-constructor.h"
#include "src/builtins/builtins.h"
#include "src/codegen/cpu-features.h"
#include "src/codegen/interface-descriptors-inl.h"
#include "src/common/assert-scope.h"
#include "src/common/globals.h"
#include "src/common/message-template.h"
#include "src/compiler/access-info.h"
#include "src/compiler/bytecode-liveness-map.h"
#include "src/compiler/compilation-dependencies.h"
#include "src/compiler/feedback-source.h"
#include "src/compiler/heap-refs.h"
#include "src/compiler/js-heap-broker-inl.h"
#include "src/compiler/js-heap-broker.h"
#include "src/compiler/processed-feedback.h"
#include "src/deoptimizer/deoptimize-reason.h"
#include "src/execution/protectors.h"
#include "src/flags/flags.h"
#include "src/handles/maybe-handles-inl.h"
#include "src/ic/handler-configuration-inl.h"
#include "src/interpreter/bytecode-array-iterator.h"
#include "src/interpreter/bytecode-flags-and-tokens.h"
#include "src/interpreter/bytecode-register.h"
#include "src/interpreter/bytecodes.h"
#include "src/maglev/maglev-compilation-info.h"
#include "src/maglev/maglev-compilation-unit.h"
#include "src/maglev/maglev-graph-printer.h"
#include "src/maglev/maglev-interpreter-frame-state.h"
#include "src/maglev/maglev-ir-inl.h"
#include "src/maglev/maglev-ir.h"
#include "src/numbers/conversions.h"
#include "src/numbers/ieee754.h"
#include "src/objects/arguments.h"
#include "src/objects/elements-kind.h"
#include "src/objects/feedback-vector.h"
#include "src/objects/fixed-array.h"
#include "src/objects/heap-number-inl.h"
#include "src/objects/js-array.h"
#include "src/objects/js-function.h"
#include "src/objects/js-objects.h"
#include "src/objects/literal-objects-inl.h"
#include "src/objects/name-inl.h"
#include "src/objects/object-list-macros.h"
#include "src/objects/property-cell.h"
#include "src/objects/property-details.h"
#include "src/objects/shared-function-info.h"
#include "src/objects/slots-inl.h"
#include "src/objects/type-hints.h"
#include "src/roots/roots.h"
#include "src/utils/utils.h"
#include "src/zone/zone.h"

#ifdef V8_INTL_SUPPORT
#include "src/objects/intl-objects.h"
#endif

#define TRACE(...)                            \
  if (v8_flags.trace_maglev_graph_building) { \
    std::cout << __VA_ARGS__ << std::endl;    \
  }

#define FAIL(...)                                                         \
  TRACE("Failed " << __func__ << ":" << __LINE__ << ": " << __VA_ARGS__); \
  return ReduceResult::Fail();

namespace v8::internal::maglev {

namespace {

enum class CpuOperation {
  kFloat64Round,
};

// TODO(leszeks): Add a generic mechanism for marking nodes as optionally
// supported.
bool IsSupported(CpuOperation op) {
  switch (op) {
    case CpuOperation::kFloat64Round:
#if defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_IA32)
      return CpuFeatures::IsSupported(SSE4_1) || CpuFeatures::IsSupported(AVX);
#elif defined(V8_TARGET_ARCH_ARM)
      return CpuFeatures::IsSupported(ARMv8);
#elif defined(V8_TARGET_ARCH_ARM64) || defined(V8_TARGET_ARCH_PPC64) ||   \
    defined(V8_TARGET_ARCH_S390X) || defined(V8_TARGET_ARCH_RISCV64) ||   \
    defined(V8_TARGET_ARCH_RISCV32) || defined(V8_TARGET_ARCH_LOONG64) || \
    defined(V8_TARGET_ARCH_MIPS64)
      return true;
#else
#error "V8 does not support this architecture."
#endif
  }
}

class FunctionContextSpecialization final : public AllStatic {
 public:
  static compiler::OptionalContextRef TryToRef(
      const MaglevCompilationUnit* unit, ValueNode* context, size_t* depth) {
    DCHECK(unit->info()->specialize_to_function_context());
    if (Constant* n = context->TryCast<Constant>()) {
      return n->ref().AsContext().previous(unit->broker(), depth);
    }
    return {};
  }
};

}  // namespace

ValueNode* MaglevGraphBuilder::TryGetParentContext(ValueNode* node) {
  if (CreateFunctionContext* n = node->TryCast<CreateFunctionContext>()) {
    return n->context().node();
  }

  if (InlinedAllocation* alloc = node->TryCast<InlinedAllocation>()) {
    return alloc->object()->get(
        Context::OffsetOfElementAt(Context::PREVIOUS_INDEX));
  }

  if (CallRuntime* n = node->TryCast<CallRuntime>()) {
    switch (n->function_id()) {
      case Runtime::kPushBlockContext:
      case Runtime::kPushCatchContext:
      case Runtime::kNewFunctionContext:
        return n->context().node();
      default:
        break;
    }
  }

  return nullptr;
}

// Attempts to walk up the context chain through the graph in order to reduce
// depth and thus the number of runtime loads.
void MaglevGraphBuilder::MinimizeContextChainDepth(ValueNode** context,
                                                   size_t* depth) {
  while (*depth > 0) {
    ValueNode* parent_context = TryGetParentContext(*context);
    if (parent_context == nullptr) return;
    *context = parent_context;
    (*depth)--;
  }
}

void MaglevGraphBuilder::EscapeContext() {
  ValueNode* context = GetContext();
  if (InlinedAllocation* alloc = context->TryCast<InlinedAllocation>()) {
    alloc->ForceEscaping();
  }
}

class CallArguments {
 public:
  enum Mode {
    kDefault,
    kWithSpread,
    kWithArrayLike,
  };

  CallArguments(ConvertReceiverMode receiver_mode,
                interpreter::RegisterList reglist,
                const InterpreterFrameState& frame, Mode mode = kDefault)
      : receiver_mode_(receiver_mode),
        args_(reglist.register_count()),
        mode_(mode) {
    for (int i = 0; i < reglist.register_count(); i++) {
      args_|i] = frame.get(reglist|i]);
    }
    DCHECK_IMPLIES(args_.size() == 0,
                   receiver_mode == ConvertReceiverMode::kNullOrUndefined);
    DCHECK_IMPLIES(mode != kDefault,
                   receiver_mode == ConvertReceiverMode::kAny);
    DCHECK_IMPLIES(mode == kWithArrayLike, args_.size() == 2);
  }

  explicit CallArguments(ConvertReceiverMode receiver_mode)
      : receiver_mode_(receiver_mode), args_(), mode_(kDefault) {
    DCHECK_EQ(receiver_mode, ConvertReceiverMode::kNullOrUndefined);
  }

  CallArguments(ConvertReceiverMode receiver_mode,
                std::initializer_list<ValueNode*> args, Mode mode = kDefault)
      : receiver_mode_(receiver_mode), args_(args), mode_(mode) {
    DCHECK_IMPLIES(mode != kDefault,
                   receiver_mode == ConvertReceiverMode::kAny);
    DCHECK_IMPLIES(mode == kWithArrayLike, args_.size() == 2);
    CheckArgumentsAreNotConversionNodes();
  }

  CallArguments(ConvertReceiverMode receiver_mode,
                base::SmallVector<ValueNode*, 8>&& args, Mode mode = kDefault)
      : receiver_mode_(receiver_mode), args_(std::move(args)), mode_(mode) {
    DCHECK_IMPLIES(mode != kDefault,
                   receiver_mode == ConvertReceiverMode::kAny);
    DCHECK_IMPLIES(mode == kWithArrayLike, args_.size() == 2);
    CheckArgumentsAreNotConversionNodes();
  }

  ValueNode* receiver() const {
    if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
      return nullptr;
    }
    return args_|0];
  }

  void set_receiver(ValueNode* receiver) {
    if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
      args_.insert(args_.data(), receiver);
      receiver_mode_ = ConvertReceiverMode::kAny;
    } else {
      DCHECK(!receiver->properties().is_conversion());
      args_|0] = receiver;
    }
  }

  ValueNode* array_like_argument() {
    DCHECK_EQ(mode_, kWithArrayLike);
    DCHECK_GT(count(), 0);
    return args_|args_.size() - 1];
  }

  size_t count() const {
    if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
      return args_.size();
    }
    return args_.size() - 1;
  }

  size_t count_with_receiver() const { return count() + 1; }

  ValueNode* operator|](size_t i) const {
    if (receiver_mode_ != ConvertReceiverMode::kNullOrUndefined) {
      i++;
    }
    if (i >= args_.size()) return nullptr;
    return args_|i];
  }

  void set_arg(size_t i, ValueNode* node) {
    if (receiver_mode_ != ConvertReceiverMode::kNullOrUndefined) {
      i++;
    }
    DCHECK_LT(i, args_.size());
    DCHECK(!node->properties().is_conversion());
    args_|i] = node;
  }

  Mode mode() const { return mode_; }

  ConvertReceiverMode receiver_mode() const { return receiver_mode_; }

  void PopArrayLikeArgument() {
    DCHECK_EQ(mode_, kWithArrayLike);
    DCHECK_GT(count(), 0);
    args_.pop_back();
  }

  void PopReceiver(ConvertReceiverMode new_receiver_mode) {
    DCHECK_NE(receiver_mode_, ConvertReceiverMode::kNullOrUndefined);
    DCHECK_NE(new_receiver_mode, ConvertReceiverMode::kNullOrUndefined);
    DCHECK_GT(args_.size(), 0);  // We have at least a receiver to pop!
    // TODO(victorgomes): Do this better!
    for (size_t i = 0; i < args_.size() - 1; i++) {
      args_|i] = args_|i + 1];
    }
    args_.pop_back();

    // If there is no non-receiver argument to become the new receiver,
    // consider the new receiver to be known undefined.
    receiver_mode_ = args_.empty() ? ConvertReceiverMode::kNullOrUndefined
                                   : new_receiver_mode;
  }

 private:
  ConvertReceiverMode receiver_mode_;
  base::SmallVector<ValueNode*, 8> args_;
  Mode mode_;

  void CheckArgumentsAreNotConversionNodes() {
#ifdef DEBUG
    // Arguments can leak to the interpreter frame if the call is inlined,
    // conversions should be stored in known_node_aspects/NodeInfo.
    for (ValueNode* arg : args_) {
      DCHECK(!arg->properties().is_conversion());
    }
#endif  // DEBUG
  }
};

class V8_NODISCARD MaglevGraphBuilder::SaveCallSpeculationScope {
 public:
  explicit SaveCallSpeculationScope(
      MaglevGraphBuilder* builder,
      compiler::FeedbackSource feedback_source = compiler::FeedbackSource())
      : builder_(builder) {
    saved_ = builder_->current_speculation_feedback_;
    // Only set the current speculation feedback if speculation is allowed.
    if (IsSpeculationAllowed(builder_->broker(), feedback_source)) {
      builder->current_speculation_feedback_ = feedback_source;
    } else {
      builder->current_speculation_feedback_ = compiler::FeedbackSource();
    }
  }
  ~SaveCallSpeculationScope() {
    builder_->current_speculation_feedback_ = saved_;
  }

  const compiler::FeedbackSource& value() { return saved_; }

 private:
  MaglevGraphBuilder* builder_;
  compiler::FeedbackSource saved_;

  static bool IsSpeculationAllowed(compiler::JSHeapBroker* broker,
                                   compiler::FeedbackSource feedback_source) {
    if (!feedback_source.IsValid()) return false;
    compiler::ProcessedFeedback const& processed_feedback =
        broker->GetFeedbackForCall(feedback_source);
    if (processed_feedback.IsInsufficient()) return false;
    return processed_feedback.AsCall().speculation_mode() ==
           SpeculationMode::kAllowSpeculation;
  }
};

class V8_NODISCARD MaglevGraphBuilder::DeoptFrameScope {
 public:
  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin continuation,
                  compiler::OptionalJSFunctionRef maybe_js_target = {})
      : builder_(builder),
        parent_(builder->current_deopt_scope_),
        data_(DeoptFrame::BuiltinContinuationFrameData{
            continuation, {}, builder->GetContext(), maybe_js_target}) {
    builder_->current_interpreter_frame().virtual_objects().Snapshot();
    builder_->current_deopt_scope_ = this;
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::BuiltinContinuationFrameData>().context);
    DCHECK(data_.get<DeoptFrame::BuiltinContinuationFrameData>()
               .parameters.empty());
  }

  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin continuation,
                  compiler::OptionalJSFunctionRef maybe_js_target,
                  base::Vector<ValueNode* const> parameters)
      : builder_(builder),
        parent_(builder->current_deopt_scope_),
        data_(DeoptFrame::BuiltinContinuationFrameData{
            continuation, builder->zone()->CloneVector(parameters),
            builder->GetContext(), maybe_js_target}) {
    builder_->current_interpreter_frame().virtual_objects().Snapshot();
    builder_->current_deopt_scope_ = this;
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::BuiltinContinuationFrameData>().context);
    if (parameters.size() > 0) {
      if (InlinedAllocation* receiver =
              parameters|0]->TryCast<InlinedAllocation>()) {
        // We escape the first argument, since the builtin continuation call can
        // trigger a stack iteration, which expects the receiver to be a
        // meterialized object.
        receiver->ForceEscaping();
      }
    }
    for (ValueNode* node :
         data_.get<DeoptFrame::BuiltinContinuationFrameData>().parameters) {
      builder_->AddDeoptUse(node);
    }
  }

  DeoptFrameScope(MaglevGraphBuilder* builder, ValueNode* receiver)
      : builder_(builder),
        parent_(builder->current_deopt_scope_),
        data_(DeoptFrame::ConstructInvokeStubFrameData{
            *builder->compilation_unit(), builder->current_source_position_,
            receiver, builder->GetContext()}) {
    builder_->current_interpreter_frame().virtual_objects().Snapshot();
    builder_->current_deopt_scope_ = this;
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::ConstructInvokeStubFrameData>().receiver);
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::ConstructInvokeStubFrameData>().context);
  }

  ~DeoptFrameScope() {
    builder_->current_deopt_scope_ = parent_;
    // We might have cached a checkpointed frame which includes this scope;
    // reset it just in case.
    builder_->latest_checkpointed_frame_.reset();
  }

  DeoptFrameScope* parent() const { return parent_; }

  bool IsLazyDeoptContinuationFrame() const {
    if (data_.tag() != DeoptFrame::FrameType::kBuiltinContinuationFrame) {
      return false;
    }
    switch (data_.get<DeoptFrame::FrameType::kBuiltinContinuationFrame>()
                .builtin_id) {
      case Builtin::kGetIteratorWithFeedbackLazyDeoptContinuation:
      case Builtin::kCallIteratorWithFeedbackLazyDeoptContinuation:
      case Builtin::kArrayForEachLoopLazyDeoptContinuation:
      case Builtin::kGenericLazyDeoptContinuation:
      case Builtin::kToBooleanLazyDeoptContinuation:
        return true;
      default:
        return false;
    }
  }

  DeoptFrame::FrameData& data() { return data_; }
  const DeoptFrame::FrameData& data() const { return data_; }

 private:
  MaglevGraphBuilder* builder_;
  DeoptFrameScope* parent_;
  DeoptFrame::FrameData data_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::Variable {
 public:
  explicit Variable(int index) : pseudo_register_(index) {}

 private:
  friend class MaglevSubGraphBuilder;

  // Variables pretend to be interpreter registers as far as the dummy
  // compilation unit and merge states are concerned.
  interpreter::Register pseudo_register_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::Label {
 public:
  Label(MaglevSubGraphBuilder* sub_builder, int predecessor_count)
      : predecessor_count_(predecessor_count),
        liveness_(
            sub_builder->builder_->zone()->New<compiler::BytecodeLivenessState>(
                sub_builder->compilation_unit_->register_count(),
                sub_builder->builder_->zone())) {}
  Label(MaglevSubGraphBuilder* sub_builder, int predecessor_count,
        std::initializer_list<Variable*> vars)
      : Label(sub_builder, predecessor_count) {
    for (Variable* var : vars) {
      liveness_->MarkRegisterLive(var->pseudo_register_.index());
    }
  }

 private:
  explicit Label(MergePointInterpreterFrameState* merge_state,
                 BasicBlock* basic_block)
      : merge_state_(merge_state), ref_(basic_block) {}

  friend class MaglevSubGraphBuilder;
  friend class BranchBuilder;
  MergePointInterpreterFrameState* merge_state_ = nullptr;
  int predecessor_count_ = -1;
  compiler::BytecodeLivenessState* liveness_ = nullptr;
  BasicBlockRef ref_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::LoopLabel {
 public:
 private:
  explicit LoopLabel(MergePointInterpreterFrameState* merge_state,
                     BasicBlock* loop_header)
      : merge_state_(merge_state), loop_header_(loop_header) {}

  friend class MaglevSubGraphBuilder;
  MergePointInterpreterFrameState* merge_state_ = nullptr;
  BasicBlock* loop_header_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::
    BorrowParentKnownNodeAspectsAndVOs {
 public:
  explicit BorrowParentKnownNodeAspectsAndVOs(
      MaglevSubGraphBuilder* sub_builder)
      : sub_builder_(sub_builder) {
    sub_builder_->TakeKnownNodeAspectsAndVOsFromParent();
  }
  ~BorrowParentKnownNodeAspectsAndVOs() {
    sub_builder_->MoveKnownNodeAspectsAndVOsToParent();
  }

 private:
  MaglevSubGraphBuilder* sub_builder_;
};

void MaglevGraphBuilder::BranchBuilder::StartFallthroughBlock(
    BasicBlock* predecessor) {
  switch (mode()) {
    case kBytecodeJumpTarget: {
      auto& data = data_.bytecode_target;
      if (data.patch_accumulator_scope &&
          (data.patch_accumulator_scope->node_ == builder_->GetAccumulator())) {
        SetAccumulatorInBranch(BranchType::kBranchIfTrue);
        builder_->MergeIntoFrameState(predecessor, data.jump_target_offset);
        SetAccumulatorInBranch(BranchType::kBranchIfFalse);
        builder_->StartFallthroughBlock(data.fallthrough_offset, predecessor);
      } else {
        builder_->MergeIntoFrameState(predecessor, data.jump_target_offset);
        builder_->StartFallthroughBlock(data.fallthrough_offset, predecessor);
      }
      break;
    }
    case kLabelJumpTarget:
      auto& data = data_.label_target;
      sub_builder_->MergeIntoLabel(data.jump_label, predecessor);
      builder_->StartNewBlock(predecessor, nullptr, data.fallthrough);
      break;
  }
}

void MaglevGraphBuilder::BranchBuilder::SetAccumulatorInBranch(
    BranchType jump_type) const {
  DCHECK_EQ(mode(), kBytecodeJumpTarget);
  auto& data = data_.bytecode_target;
  if (branch_specialization_mode_ == BranchSpecializationMode::kAlwaysBoolean) {
    builder_->SetAccumulatorInBranch(builder_->GetBooleanConstant(
        data.patch_accumulator_scope->jump_type_ == jump_type));
  } else if (data.patch_accumulator_scope->jump_type_ == jump_type) {
    builder_->SetAccumulatorInBranch(
        builder_->GetRootConstant(data.patch_accumulator_scope->root_index_));
  } else {
    builder_->SetAccumulatorInBranch(data.patch_accumulator_scope->node_);
  }
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::jump_target() {
  switch (mode()) {
    case kBytecodeJumpTarget:
      return &builder_->jump_targets_|data_.bytecode_target.jump_target_offset];
    case kLabelJumpTarget:
      return &data_.label_target.jump_label->ref_;
  }
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::fallthrough() {
  switch (mode()) {
    case kBytecodeJumpTarget:
      return &builder_->jump_targets_|data_.bytecode_target.fallthrough_offset];
    case kLabelJumpTarget:
      return &data_.label_target.fallthrough;
  }
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::true_target() {
  return jump_type_ == BranchType::kBranchIfTrue ? jump_target()
                                                 : fallthrough();
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::false_target() {
  return jump_type_ == BranchType::kBranchIfFalse ? jump_target()
                                                  : fallthrough();
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BranchBuilder::FromBool(
    bool value) const {
  switch (mode()) {
    case kBytecodeJumpTarget: {
      BranchType type_if_need_to_jump =
          (value ? BranchType::kBranchIfTrue : BranchType::kBranchIfFalse);
      builder_->MarkBranchDeadAndJumpIfNeeded(jump_type_ ==
                                              type_if_need_to_jump);
      return BranchResult::kDefault;
    }
    case kLabelJumpTarget:
      return value ? BranchResult::kAlwaysTrue : BranchResult::kAlwaysFalse;
  }
}

template <typename ControlNodeT, typename... Args>
MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BranchBuilder::Build(
    std::initializer_list<ValueNode*> control_inputs, Args&&... args) {
  static_assert(IsConditionalControlNode(Node::opcode_of<ControlNodeT>));
  BasicBlock* block = builder_->FinishBlock<ControlNodeT>(
      control_inputs, std::forward<Args>(args)..., true_target(),
      false_target());
  StartFallthroughBlock(block);
  return BranchResult::kDefault;
}

MaglevGraphBuilder::MaglevSubGraphBuilder::MaglevSubGraphBuilder(
    MaglevGraphBuilder* builder, int variable_count)
    : builder_(builder),
      compilation_unit_(MaglevCompilationUnit::NewDummy(
          builder->zone(), builder->compilation_unit(), variable_count, 0, 0)),
      pseudo_frame_(*compilation_unit_, nullptr, VirtualObject::List()) {
  // We need to set a context, since this is unconditional in the frame state,
  // so set it to the real context.
  pseudo_frame_.set(interpreter::Register::current_context(),
                    builder_->current_interpreter_frame().get(
                        interpreter::Register::current_context()));
  DCHECK_NULL(pseudo_frame_.known_node_aspects());
}

MaglevGraphBuilder::MaglevSubGraphBuilder::LoopLabel
MaglevGraphBuilder::MaglevSubGraphBuilder::BeginLoop(
    std::initializer_list<Variable*> loop_vars) {
  // Create fake liveness and loop info for the loop, with all given loop vars
  // set to be live and assigned inside the loop.
  compiler::BytecodeLivenessState* loop_header_liveness =
      builder_->zone()->New<compiler::BytecodeLivenessState>(
          compilation_unit_->register_count(), builder_->zone());
  compiler::LoopInfo* loop_info = builder_->zone()->New<compiler::LoopInfo>(
      -1, 0, kMaxInt, compilation_unit_->parameter_count(),
      compilation_unit_->register_count(), builder_->zone());
  for (Variable* var : loop_vars) {
    loop_header_liveness->MarkRegisterLive(var->pseudo_register_.index());
    loop_info->assignments().Add(var->pseudo_register_);
  }

  // Finish the current block, jumping (as a fallthrough) to the loop header.
  BasicBlockRef loop_header_ref;
  BasicBlock* loop_predecessor =
      builder_->FinishBlock<Jump>({}, &loop_header_ref);

  // Create a state for the loop header, with two predecessors (the above jump
  // and the back edge), and initialise with the current state.
  MergePointInterpreterFrameState* loop_state =
      MergePointInterpreterFrameState::NewForLoop(
          pseudo_frame_, *compilation_unit_, 0, 2, loop_header_liveness,
          loop_info);

  {
    BorrowParentKnownNodeAspectsAndVOs borrow(this);
    loop_state->Merge(builder_, *compilation_unit_, pseudo_frame_,
                      loop_predecessor);
  }

  // Start a new basic block for the loop.
  DCHECK_NULL(pseudo_frame_.known_node_aspects());
  pseudo_frame_.CopyFrom(*compilation_unit_, *loop_state);
  MoveKnownNodeAspectsAndVOsToParent();

  builder_->ProcessMergePointPredecessors(*loop_state, loop_header_ref);
  builder_->StartNewBlock(nullptr, loop_state, loop_header_ref);

  return LoopLabel{loop_state, loop_header_ref.block_ptr()};
}

template <typename ControlNodeT, typename... Args>
void MaglevGraphBuilder::MaglevSubGraphBuilder::GotoIfTrue(
    Label* true_target, std::initializer_list<ValueNode*> control_inputs,
    Args&&... args) {
  static_assert(IsConditionalControlNode(Node::opcode_of<ControlNodeT>));

  BasicBlockRef fallthrough_ref;

  // Pass through to FinishBlock, converting Labels to BasicBlockRefs and the
  // fallthrough label to the fallthrough ref.
  BasicBlock* block = builder_->FinishBlock<ControlNodeT>(
      control_inputs, std::forward<Args>(args)..., &true_target->ref_,
      &fallthrough_ref);

  MergeIntoLabel(true_target, block);

  builder_->StartNewBlock(block, nullptr, fallthrough_ref);
}

template <typename ControlNodeT, typename... Args>
void MaglevGraphBuilder::MaglevSubGraphBuilder::GotoIfFalse(
    Label* false_target, std::initializer_list<ValueNode*> control_inputs,
    Args&&... args) {
  static_assert(IsConditionalControlNode(Node::opcode_of<ControlNodeT>));

  BasicBlockRef fallthrough_ref;

  // Pass through to FinishBlock, converting Labels to BasicBlockRefs and the
  // fallthrough label to the fallthrough ref.
  BasicBlock* block = builder_->FinishBlock<ControlNodeT>(
      control_inputs, std::forward<Args>(args)..., &fallthrough_ref,
      &false_target->ref_);

  MergeIntoLabel(false_target, block);

  builder_->StartNewBlock(block, nullptr, fallthrough_ref);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::GotoOrTrim(Label* label) {
  if (builder_->current_block_ == nullptr) {
    ReducePredecessorCount(label);
    return;
  }
  Goto(label);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::Goto(Label* label) {
  CHECK_NOT_NULL(builder_->current_block_);
  BasicBlock* block = builder_->FinishBlock<Jump>({}, &label->ref_);
  MergeIntoLabel(label, block);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::ReducePredecessorCount(
    Label* label, unsigned num) {
  DCHECK_GE(label->predecessor_count_, num);
  if (num == 0) {
    return;
  }
  label->predecessor_count_ -= num;
  if (label->merge_state_ != nullptr) {
    label->merge_state_->MergeDead(*compilation_unit_, num);
  }
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::EndLoop(LoopLabel* loop_label) {
  if (builder_->current_block_ == nullptr) {
    loop_label->merge_state_->MergeDeadLoop(*compilation_unit_);
    return;
  }

  BasicBlock* block =
      builder_->FinishBlock<JumpLoop>({}, loop_label->loop_header_);
  {
    BorrowParentKnownNodeAspectsAndVOs borrow(this);
    loop_label->merge_state_->MergeLoop(builder_, *compilation_unit_,
                                        pseudo_frame_, block);
  }
  block->set_predecessor_id(loop_label->merge_state_->predecessor_count() - 1);
}

ReduceResult MaglevGraphBuilder::MaglevSubGraphBuilder::TrimPredecessorsAndBind(
    Label* label) {
  int predecessors_so_far = label->merge_state_ == nullptr
                                ? 0
                                : label->merge_state_->predecessors_so_far();
  DCHECK_LE(predecessors_so_far, label->predecessor_count_);
  builder_->current_block_ = nullptr;
  ReducePredecessorCount(label,
                         label->predecessor_count_ - predecessors_so_far);
  if (predecessors_so_far == 0) return ReduceResult::DoneWithAbort();
  Bind(label);
  return ReduceResult::Done();
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::Bind(Label* label) {
  DCHECK_NULL(builder_->current_block_);

  DCHECK_NULL(pseudo_frame_.known_node_aspects());
  pseudo_frame_.CopyFrom(*compilation_unit_, *label->merge_state_);
  MoveKnownNodeAspectsAndVOsToParent();

  CHECK_EQ(label->merge_state_->predecessors_so_far(),
           label->predecessor_count_);

  builder_->ProcessMergePointPredecessors(*label->merge_state_, label->ref_);
  builder_->StartNewBlock(nullptr, label->merge_state_, label->ref_);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::set(Variable& var,
                                                    ValueNode* value) {
  pseudo_frame_.set(var.pseudo_register_, value);
}
ValueNode* MaglevGraphBuilder::MaglevSubGraphBuilder::get(
    const Variable& var) const {
  return pseudo_frame_.get(var.pseudo_register_);
}

template <typename FCond, typename FTrue, typename FFalse>
ReduceResult MaglevGraphBuilder::MaglevSubGraphBuilder::Branch(
    std::initializer_list<MaglevSubGraphBuilder::Variable*> vars, FCond cond,
    FTrue if_true, FFalse if_false) {
  MaglevSubGraphBuilder::Label else_branch(this, 1);
  BranchBuilder builder(builder_, this, BranchType::kBranchIfFalse,
                        &else_branch);
  BranchResult branch_result = cond(builder);
  if (branch_result == BranchResult::kAlwaysTrue) {
    return if_true();
  }
  if (branch_result == BranchResult::kAlwaysFalse) {
    return if_false();
  }
  DCHECK(branch_result == BranchResult::kDefault);
  MaglevSubGraphBuilder::Label done(this, 2, vars);
  ReduceResult result_if_true = if_true();
  CHECK(result_if_true.IsDone());
  GotoOrTrim(&done);
  Bind(&else_branch);
  ReduceResult result_if_false = if_false();
  CHECK(result_if_false.IsDone());
  if (result_if_true.IsDoneWithAbort() && result_if_false.IsDoneWithAbort()) {
    return ReduceResult::DoneWithAbort();
  }
  GotoOrTrim(&done);
  Bind(&done);
  return ReduceResult::Done();
}

template <typename FCond, typename FTrue, typename FFalse>
ValueNode* MaglevGraphBuilder::Select(FCond cond, FTrue if_true,
                                      FFalse if_false) {
  MaglevSubGraphBuilder subgraph(this, 1);
  MaglevSubGraphBuilder::Label else_branch(&subgraph, 1);
  BranchBuilder builder(this, &subgraph, BranchType::kBranchIfFalse,
                        &else_branch);
  BranchResult branch_result = cond(builder);
  if (branch_result == BranchResult::kAlwaysTrue) {
    return if_true();
  }
  if (branch_result == BranchResult::kAlwaysFalse) {
    return if_false();
  }
  DCHECK(branch_result == BranchResult::kDefault);
  MaglevSubGraphBuilder::Variable ret_val(0);
  MaglevSubGraphBuilder::Label done(&subgraph, 2, {&ret_val});
  subgraph.set(ret_val, if_true());
  subgraph.Goto(&done);
  subgraph.Bind(&else_branch);
  subgraph.set(ret_val, if_false());
  subgraph.Goto(&done);
  subgraph.Bind(&done);
  return subgraph.get(ret_val);
}

template <typename FCond, typename FTrue, typename FFalse>
ReduceResult MaglevGraphBuilder::SelectReduction(FCond cond, FTrue if_true,
                                                 FFalse if_false) {
  MaglevSubGraphBuilder subgraph(this, 1);
  MaglevSubGraphBuilder::Label else_branch(&subgraph, 1);
  BranchBuilder builder(this, &subgraph, BranchType::kBranchIfFalse,
                        &else_branch);
  BranchResult branch_result = cond(builder);
  if (branch_result == BranchResult::kAlwaysTrue) {
    return if_true();
  }
  if (branch_result == BranchResult::kAlwaysFalse) {
    return if_false();
  }
  DCHECK(branch_result == BranchResult::kDefault);
  MaglevSubGraphBuilder::Variable ret_val(0);
  MaglevSubGraphBuilder::Label done(&subgraph, 2, {&ret_val});
  ReduceResult result_if_true = if_true();
  CHECK(result_if_true.IsDone());
  if (result_if_true.IsDoneWithValue()) {
    subgraph.set(ret_val, result_if_true.value());
  }
  subgraph.GotoOrTrim(&done);
  subgraph.Bind(&else_branch);
  ReduceResult result_if_false = if_false();
  CHECK(result_if_false.IsDone());
  if (result_if_true.IsDoneWithAbort() && result_if_false.IsDoneWithAbort()) {
    return ReduceResult::DoneWithAbort();
  }
  if (result_if_false.IsDoneWithValue()) {
    subgraph.set(ret_val, result_if_false.value());
  }
  subgraph.GotoOrTrim(&done);
  subgraph.Bind(&done);
  return subgraph.get(ret_val);
}

// Known node aspects for the pseudo frame are null aside from when merging --
// before each merge, we should borrow the node aspects from the parent
// builder, and after each merge point, we should copy the node aspects back
// to the parent. This is so that the parent graph builder can update its own
// known node aspects without having to worry about this pseudo frame.
void MaglevGraphBuilder::MaglevSubGraphBuilder::
    TakeKnownNodeAspectsAndVOsFromParent() {
  DCHECK_NULL(pseudo_frame_.known_node_aspects());
  DCHECK(pseudo_frame_.virtual_objects().is_empty());
  pseudo_frame_.set_known_node_aspects(
      builder_->current_interpreter_frame_.known_node_aspects());
  pseudo_frame_.set_virtual_objects(
      builder_->current_interpreter_frame_.virtual_objects());
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::
    MoveKnownNodeAspectsAndVOsToParent() {
  DCHECK_NOT_NULL(pseudo_frame_.known_node_aspects());
  builder_->current_interpreter_frame_.set_known_node_aspects(
      pseudo_frame_.known_node_aspects());
  pseudo_frame_.clear_known_node_aspects();
  builder_->current_interpreter_frame_.set_virtual_objects(
      pseudo_frame_.virtual_objects());
  pseudo_frame_.set_virtual_objects(VirtualObject::List());
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::MergeIntoLabel(
    Label* label, BasicBlock* predecessor) {
  BorrowParentKnownNodeAspectsAndVOs borrow(this);

  if (label->merge_state_ == nullptr) {
    // If there's no merge state, allocate a new one.
    label->merge_state_ = MergePointInterpreterFrameState::New(
        *compilation_unit_, pseudo_frame_, 0, label->predecessor_count_,
        predecessor, label->liveness_);
  } else {
    // If there already is a frame state, merge.
    label->merge_state_->Merge(builder_, *compilation_unit_, pseudo_frame_,
                               predecessor);
  }
}

MaglevGraphBuilder::MaglevGraphBuilder(
    LocalIsolate* local_isolate, MaglevCompilationUnit* compilation_unit,
    Graph* graph, float call_frequency, BytecodeOffset caller_bytecode_offset,
    bool caller_is_inside_loop, int inlining_id, MaglevGraphBuilder* parent)
    : local_isolate_(local_isolate),
      compilation_unit_(compilation_unit),
      parent_(parent),
      graph_(graph),
      bytecode_analysis_(bytecode().object(), zone(),
                         compilation_unit->osr_offset(), true),
      iterator_(bytecode().object()),
      source_position_iterator_(bytecode().SourcePositionTable(broker())),
      allow_loop_peeling_(v8_flags.maglev_loop_peeling),
      loop_effects_stack_(zone()),
      decremented_predecessor_offsets_(zone()),
      loop_headers_to_peel_(bytecode().length(), zone()),
      current_source_position_(SourcePosition(
          compilation_unit_->shared_function_info().StartPosition(),
          inlining_id)),
      call_frequency_(call_frequency),
      // Add an extra jump_target slot for the inline exit if needed.
      jump_targets_(zone()->AllocateArray<BasicBlockRef>(
          bytecode().length() + (is_inline() ? 1 : 0))),
      // Overallocate merge_states_ by one to allow always looking up the
      // next offset. This overallocated slot can also be used for the inline
      // exit when needed.
      merge_states_(zone()->AllocateArray<MergePointInterpreterFrameState*>(
          bytecode().length() + 1)),
      current_interpreter_frame_(
          *compilation_unit_,
          is_inline() ? parent->current_interpreter_frame_.known_node_aspects()
                      : compilation_unit_->zone()->New<KnownNodeAspects>(
                            compilation_unit_->zone()),
          is_inline() ? parent->current_interpreter_frame_.virtual_objects()
                      : VirtualObject::List()),
      caller_bytecode_offset_(caller_bytecode_offset),
      caller_is_inside_loop_(caller_is_inside_loop),
      entrypoint_(compilation_unit->is_osr()
                      ? bytecode_analysis_.osr_entry_point()
                      : 0),
      inlining_id_(inlining_id),
      catch_block_stack_(zone()),
      unobserved_context_slot_stores_(zone()) {
  memset(merge_states_, 0,
         (bytecode().length() + 1) * sizeof(InterpreterFrameState*));
  // Default construct basic block refs.
  // TODO(leszeks): This could be a memset of nullptr to ..._jump_targets_.
  for (int i = 0; i < bytecode().length(); ++i) {
    new (&jump_targets_|i]) BasicBlockRef();
  }

  if (is_inline()) {
    DCHECK_NOT_NULL(parent_);
    DCHECK_GT(compilation_unit->inlining_depth(), 0);
    // The allocation/initialisation logic here relies on inline_exit_offset
    // being the offset one past the end of the bytecode.
    DCHECK_EQ(inline_exit_offset(), bytecode().length());
    merge_states_|inline_exit_offset()] = nullptr;
    new (&jump_targets_|inline_exit_offset()]) BasicBlockRef();
    if (parent_->loop_effects_) {
      loop_effects_ = parent->loop_effects_;
      loop_effects_stack_.push_back(loop_effects_);
    }
    unobserved_context_slot_stores_ = parent_->unobserved_context_slot_stores_;
  }

  CHECK_IMPLIES(compilation_unit_->is_osr(), graph_->is_osr());
  CHECK_EQ(compilation_unit_->info()->toplevel_osr_offset() !=
               BytecodeOffset::None(),
           graph_->is_osr());
  if (compilation_unit_->is_osr()) {
    CHECK(!is_inline());

    // Make sure that we're at a valid OSR entrypoint.
    //
    // This is also a defense-in-depth check to make sure that we're not
    // compiling invalid bytecode if the OSR offset is wrong (e.g. because it
    // belongs to different bytecode).
    //
    // OSR'ing into the middle of a loop is currently not supported. There
    // should not be any issue with OSR'ing outside of loops, just we currently
    // dont do it...
    interpreter::BytecodeArrayIterator it(bytecode().object());
    it.AdvanceTo(compilation_unit_->osr_offset().ToInt());
    CHECK(it.CurrentBytecodeIsValidOSREntry());
    CHECK_EQ(entrypoint_, it.GetJumpTargetOffset());

    iterator_.AdvanceTo(entrypoint_);

    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "- Non-standard entrypoint @" << entrypoint_
                << " by OSR from @" << compilation_unit_->osr_offset().ToInt()
                << std::endl;
    }
  }
  CHECK_IMPLIES(!compilation_unit_->is_osr(), entrypoint_ == 0);

  CalculatePredecessorCounts();
}

void MaglevGraphBuilder::StartPrologue() {
  current_block_ = zone()->New<BasicBlock>(nullptr, zone());
}

BasicBlock* MaglevGraphBuilder::EndPrologue() {
  BasicBlock* first_block;
  if (!is_inline() &&
      (v8_flags.maglev_hoist_osr_value_phi_untagging && graph_->is_osr())) {
    first_block =
        FinishBlock<CheckpointedJump>({}, &jump_targets_|entrypoint_]);
  } else {
    first_block = FinishBlock<Jump>({}, &jump_targets_|entrypoint_]);
  }
  MergeIntoFrameState(first_block, entrypoint_);
  return first_block;
}

void MaglevGraphBuilder::SetArgument(int i, ValueNode* value) {
  interpreter::Register reg = interpreter::Register::FromParameterIndex(i);
  current_interpreter_frame_.set(reg, value);
}

ValueNode* MaglevGraphBuilder::GetArgument(int i) {
  DCHECK_LT(i, parameter_count());
  interpreter::Register reg = interpreter::Register::FromParameterIndex(i);
  return current_interpreter_frame_.get(reg);
}

ValueNode* MaglevGraphBuilder::GetInlinedArgument(int i) {
  DCHECK(is_inline());
  DCHECK_LT(i, argument_count());
  return inlined_arguments_|i];
}

void MaglevGraphBuilder::InitializeRegister(interpreter::Register reg,
                                            ValueNode* value) {
  current_interpreter_frame_.set(
      reg, value ? value : AddNewNode<InitialValue>({}, reg));
}

void MaglevGraphBuilder::BuildRegisterFrameInitialization(
    ValueNode* context, ValueNode* closure, ValueNode* new_target) {
  if (closure == nullptr &&
      compilation_unit_->info()->specialize_to_function_context()) {
    compiler::JSFunctionRef function = compiler::MakeRefAssumeMemoryFence(
        broker(), broker()->CanonicalPersistentHandle(
                      compilation_unit_->info()->toplevel_function()));
    closure = GetConstant(function);
    context = GetConstant(function.context(broker()));
  }
  InitializeRegister(interpreter::Register::current_context(), context);
  InitializeRegister(interpreter::Register::function_closure(), closure);

  interpreter::Register new_target_or_generator_register =
      bytecode().incoming_new_target_or_generator_register();

  int register_index = 0;

  if (compilation_unit_->is_osr()) {
    for (; register_index < register_count(); register_index++) {
      auto val =
          AddNewNode<InitialValue>({}, interpreter::Register(register_index));
      InitializeRegister(interpreter::Register(register_index), val);
      graph_->osr_values().push_back(val);
    }
    return;
  }

  // TODO(leszeks): Don't emit if not needed.
  ValueNode* undefined_value = GetRootConstant(RootIndex::kUndefinedValue);
  if (new_target_or_generator_register.is_valid()) {
    int new_target_index = new_target_or_generator_register.index();
    for (; register_index < new_target_index; register_index++) {
      current_interpreter_frame_.set(interpreter::Register(register_index),
                                     undefined_value);
    }
    current_interpreter_frame_.set(
        new_target_or_generator_register,
        new_target ? new_target
                   : GetRegisterInput(kJavaScriptCallNewTargetRegister));
    register_index++;
  }
  for (; register_index < register_count(); register_index++) {
    InitializeRegister(interpreter::Register(register_index), undefined_value);
  }
}

void MaglevGraphBuilder::BuildMergeStates() {
  auto offset_and_info = bytecode_analysis().GetLoopInfos().begin();
  auto end = bytecode_analysis().GetLoopInfos().end();
  while (offset_and_info != end && offset_and_info->first < entrypoint_) {
    ++offset_and_info;
  }
  for (; offset_and_info != end; ++offset_and_info) {
    int offset = offset_and_info->first;
    const compiler::LoopInfo& loop_info = offset_and_info->second;
    if (loop_headers_to_peel_.Contains(offset)) {
      // Peeled loops are treated like normal merges at first. We will construct
      // the proper loop header merge state when reaching the `JumpLoop` of the
      // peeled iteration.
      continue;
    }
    const compiler::BytecodeLivenessState* liveness = GetInLivenessFor(offset);
    DCHECK_NULL(merge_states_|offset]);
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "- Creating loop merge state at @" << offset << std::endl;
    }
    merge_states_|offset] = MergePointInterpreterFrameState::NewForLoop(
        current_interpreter_frame_, *compilation_unit_, offset,
        predecessor_count(offset), liveness, &loop_info);
  }

  if (bytecode().handler_table_size() > 0) {
    HandlerTable table(*bytecode().object());
    for (int i = 0; i < table.NumberOfRangeEntries(); i++) {
      const int offset = table.GetRangeHandler(i);
      const bool was_used = table.HandlerWasUsed(i);
      const interpreter::Register context_reg(table.GetRangeData(i));
      const compiler::BytecodeLivenessState* liveness =
          GetInLivenessFor(offset);
      DCHECK_EQ(predecessor_count(offset), 0);
      DCHECK_NULL(merge_states_|offset]);
      if (v8_flags.trace_maglev_graph_building) {
        std::cout << "- Creating exception merge state at @" << offset
                  << (was_used ? "" : " (never used)") << ", context register r"
                  << context_reg.index() << std::endl;
      }
      merge_states_|offset] = MergePointInterpreterFrameState::NewForCatchBlock(
          *compilation_unit_, liveness, offset, was_used, context_reg, graph_);
    }
  }
}

namespace {

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper;

// Terminal cases
template <int index>
struct GetResultLocationAndSizeHelper<index> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // TODO(leszeks): This should probably actually be "UNREACHABLE" but we have
    // lazy deopt info for interrupt budget updates at returns, not for actual
    // lazy deopts, but just for stack iteration purposes.
    return {interpreter::Register::invalid_value(), 0};
  }
  static bool HasOutputRegisterOperand() { return false; }
};

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<index, interpreter::OperandType::kRegOut,
                                      operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // We shouldn't have any other output operands than this one.
    return {iterator.GetRegisterOperand(index), 1};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<
    index, interpreter::OperandType::kRegOutPair, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // We shouldn't have any other output operands than this one.
    return {iterator.GetRegisterOperand(index), 2};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<
    index, interpreter::OperandType::kRegOutTriple, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // We shouldn't have any other output operands than this one.
    DCHECK(!(GetResultLocationAndSizeHelper<
             index + 1, operands...>::HasOutputRegisterOperand()));
    return {iterator.GetRegisterOperand(index), 3};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

// We don't support RegOutList for lazy deopts.
template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<
    index, interpreter::OperandType::kRegOutList, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    interpreter::RegisterList list = iterator.GetRegisterListOperand(index);
    return {list.first_register(), list.register_count()};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

// Induction case.
template <int index, interpreter::OperandType operand,
          interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<index, operand, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    return GetResultLocationAndSizeHelper<
        index + 1, operands...>::GetResultLocationAndSize(iterator);
  }
  static bool HasOutputRegisterOperand() {
    return GetResultLocationAndSizeHelper<
        index + 1, operands...>::HasOutputRegisterOperand();
  }
};

template <interpreter::Bytecode bytecode,
          interpreter::ImplicitRegisterUse implicit_use,
          interpreter::OperandType... operands>
std::pair<interpreter::Register, int> GetResultLocationAndSizeForBytecode(
    const interpreter::BytecodeArrayIterator& iterator) {
  // We don't support output registers for implicit registers.
  DCHECK(!interpreter::BytecodeOperands::WritesImplicitRegister(implicit_use));
  if (interpreter::BytecodeOperands::WritesAccumulator(implicit_use)) {
    // If we write the accumulator, we shouldn't also write an output register.
    DCHECK(!(GetResultLocationAndSizeHelper<
             0, operands...>::HasOutputRegisterOperand()));
    return {interpreter::Register::virtual_accumulator(), 1};
  }

  // Use template magic to output a the appropriate GetRegisterOperand call and
  // size for this bytecode.
  return GetResultLocationAndSizeHelper<
      0, operands...>::GetResultLocationAndSize(iterator);
}

}  // namespace

std::pair<interpreter::Register, int>
MaglevGraphBuilder::GetResultLocationAndSize() const {
  using Bytecode = interpreter::Bytecode;
  using OperandType = interpreter::OperandType;
  using ImplicitRegisterUse = interpreter::ImplicitRegisterUse;
  Bytecode bytecode = iterator_.current_bytecode();
  // TODO(leszeks): Only emit these cases for bytecodes we know can lazy deopt.
  switch (bytecode) {
#define CASE(Name, ...)                                           \
  case Bytecode::k##Name:                                         \
    return GetResultLocationAndSizeForBytecode<Bytecode::k##Name, \
                                               __VA_ARGS__>(iterator_);
    BYTECODE_LIST(CASE, CASE)
#undef CASE
  }
  UNREACHABLE();
}

#ifdef DEBUG
bool MaglevGraphBuilder::HasOutputRegister(interpreter::Register reg) const {
  interpreter::Bytecode bytecode = iterator_.current_bytecode();
  if (reg == interpreter::Register::virtual_accumulator()) {
    return interpreter::Bytecodes::WritesAccumulator(bytecode);
  }
  for (int i = 0; i < interpreter::Bytecodes::NumberOfOperands(bytecode); ++i) {
    if (interpreter::Bytecodes::IsRegisterOutputOperandType(
            interpreter::Bytecodes::GetOperandType(bytecode, i))) {
      interpreter::Register operand_reg = iterator_.GetRegisterOperand(i);
      int operand_range = iterator_.GetRegisterOperandRange(i);
      if (base::IsInRange(reg.index(), operand_reg.index(),
                          operand_reg.index() + operand_range)) {
        return true;
      }
    }
  }
  return false;
}
#endif

DeoptFrame* MaglevGraphBuilder::GetParentDeoptFrame() {
  if (parent_ == nullptr) return nullptr;
  if (parent_deopt_frame_ == nullptr) {
    // The parent resumes after the call, which is roughly equivalent to a lazy
    // deopt. Use the helper function directly so that we can mark the
    // accumulator as dead (since it'll be overwritten by this function's
    // return value anyway).
    // TODO(leszeks): This is true for our current set of
    // inlinings/continuations, but there might be cases in the future where it
    // isn't. We may need to store the relevant overwritten register in
    // LazyDeoptFrameScope.
    DCHECK(interpreter::Bytecodes::WritesAccumulator(
        parent_->iterator_.current_bytecode()));

    parent_deopt_frame_ =
        zone()->New<DeoptFrame>(parent_->GetDeoptFrameForLazyDeoptHelper(
            interpreter::Register::invalid_value(), 0,
            parent_->current_deopt_scope_, true));
    // Only create InlinedArgumentsDeoptFrame if we have a mismatch between
    // formal parameter and arguments count.
    if (HasMismatchedArgumentAndParameterCount()) {
      parent_deopt_frame_ = zone()->New<InlinedArgumentsDeoptFrame>(
          *compilation_unit_, caller_bytecode_offset_, GetClosure(),
          inlined_arguments_, parent_deopt_frame_);
      AddDeoptUse(GetClosure());
      for (ValueNode* arg :
           parent_deopt_frame_->as_inlined_arguments().arguments()) {
        AddDeoptUse(arg);
      }
    }
  }
  return parent_deopt_frame_;
}

DeoptFrame MaglevGraphBuilder::GetLatestCheckpointedFrame() {
  if (in_prologue_) {
    return GetDeoptFrameForEntryStackCheck();
  }
  if (!latest_checkpointed_frame_) {
    current_interpreter_frame_.virtual_objects().Snapshot();
    latest_checkpointed_frame_.emplace(InterpretedDeoptFrame(
        *compilation_unit_,
        zone()->New<CompactInterpreterFrameState>(
            *compilation_unit_, GetInLiveness(), current_interpreter_frame_),
        GetClosure(), BytecodeOffset(iterator_.current_offset()),
        current_source_position_, GetParentDeoptFrame()));

    latest_checkpointed_frame_->as_interpreted().frame_state()->ForEachValue(
        *compilation_unit_,
        |&](ValueNode* node, interpreter::Register) { AddDeoptUse(node); });
    AddDeoptUse(latest_checkpointed_frame_->as_interpreted().closure());

    // Skip lazy deopt builtin continuations.
    const DeoptFrameScope* deopt_scope = current_deopt_scope_;
    while (deopt_scope != nullptr &&
           deopt_scope->IsLazyDeoptContinuationFrame()) {
      deopt_scope = deopt_scope->parent();
    }

    if (deopt_scope != nullptr) {
      // Support exactly one eager deopt builtin continuation. This can be
      // expanded in the future if necessary.
      DCHECK_NULL(deopt_scope->parent());
      DCHECK_EQ(deopt_scope->data().tag(),
                DeoptFrame::FrameType::kBuiltinContinuationFrame);
#ifdef DEBUG
      if (deopt_scope->data().tag() ==
          DeoptFrame::FrameType::kBuiltinContinuationFrame) {
        const DeoptFrame::BuiltinContinuationFrameData& frame =
            deopt_scope->data().get<DeoptFrame::BuiltinContinuationFrameData>();
        if (frame.maybe_js_target) {
          int stack_parameter_count =
              Builtins::GetStackParameterCount(frame.builtin_id);
          DCHECK_EQ(stack_parameter_count, frame.parameters.length());
        } else {
          CallInterfaceDescriptor descriptor =
              Builtins::CallInterfaceDescriptorFor(frame.builtin_id);
          DCHECK_EQ(descriptor.GetParameterCount(), frame.parameters.length());
        }
      }
#endif

      // Wrap the above frame in the scope frame.
      latest_checkpointed_frame_.emplace(
          deopt_scope->data(),
          zone()->New<DeoptFrame>(*latest_checkpointed_frame_));
    }
  }
  return *latest_checkpointed_frame_;
}

DeoptFrame MaglevGraphBuilder::GetDeoptFrameForLazyDeopt(
    interpreter::Register result_location, int result_size) {
  return GetDeoptFrameForLazyDeoptHelper(result_location, result_size,
                                         current_deopt_scope_, false);
}

DeoptFrame MaglevGraphBuilder::GetDeoptFrameForLazyDeoptHelper(
    interpreter::Register result_location, int result_size,
    DeoptFrameScope* scope, bool mark_accumulator_dead) {
  if (scope == nullptr) {
    compiler::BytecodeLivenessState* liveness =
        zone()->New<compiler::BytecodeLivenessState>(*GetOutLiveness(), zone());
    // Remove result locations from liveness.
    if (result_location == interpreter::Register::virtual_accumulator()) {
      DCHECK_EQ(result_size, 1);
      liveness->MarkAccumulatorDead();
      mark_accumulator_dead = false;
    } else {
      DCHECK(!result_location.is_parameter());
      for (int i = 0; i < result_size; i++) {
        liveness->MarkRegisterDead(result_location.index() + i);
      }
    }
    // Explicitly drop the accumulator if needed.
    if (mark_accumulator_dead && liveness->AccumulatorIsLive()) {
      liveness->MarkAccumulatorDead();
    }
    current_interpreter_frame_.virtual_objects().Snapshot();
    InterpretedDeoptFrame ret(
        *compilation_unit_,
        zone()->New<CompactInterpreterFrameState>(*compilation_unit_, liveness,
                                                  current_interpreter_frame_),
        GetClosure(), BytecodeOffset(iterator_.current_offset()),
        current_source_position_, GetParentDeoptFrame());
    ret.frame_state()->ForEachValue(
        *compilation_unit_, |this](ValueNode* node, interpreter::Register reg) {
          // Receiver and closure values have to be materialized, even if
          // they don't otherwise escape.
          if (reg == interpreter::Register::receiver() ||
              reg == interpreter::Register::function_closure()) {
            node->add_use();
          } else {
            AddDeoptUse(node);
          }
        });
    AddDeoptUse(ret.closure());
    return ret;
  }

  // Currently only support builtin continuations for bytecodes that write to
  // the accumulator
  DCHECK(interpreter::Bytecodes::WritesOrClobbersAccumulator(
      iterator_.current_bytecode()));

#ifdef DEBUG
  if (scope->data().tag() == DeoptFrame::FrameType::kBuiltinContinuationFrame) {
    const DeoptFrame::BuiltinContinuationFrameData& frame =
        current_deopt_scope_->data()
            .get<DeoptFrame::BuiltinContinuationFrameData>();
    if (frame.maybe_js_target) {
      int stack_parameter_count =
          Builtins::GetStackParameterCount(frame.builtin_id);
      // The deopt input value is passed by the deoptimizer, so shouldn't be a
      // parameter here.
      DCHECK_EQ(stack_parameter_count, frame.parameters.length() + 1);
    } else {
      CallInterfaceDescriptor descriptor =
          Builtins::CallInterfaceDescriptorFor(frame.builtin_id);
      // The deopt input value is passed by the deoptimizer, so shouldn't be a
      // parameter here.
      DCHECK_EQ(descriptor.GetParameterCount(), frame.parameters.length() + 1);
      // The deopt input value is passed on the stack.
      DCHECK_GT(descriptor.GetStackParameterCount(), 0);
    }
  }
#endif

  // Mark the accumulator dead in parent frames since we know that the
  // continuation will write it.
  return DeoptFrame(scope->data(),
                    zone()->New<DeoptFrame>(GetDeoptFrameForLazyDeoptHelper(
                        result_location, result_size, scope->parent(),
                        scope->data().tag() ==
                            DeoptFrame::FrameType::kBuiltinContinuationFrame)));
}

InterpretedDeoptFrame MaglevGraphBuilder::GetDeoptFrameForEntryStackCheck() {
  if (entry_stack_check_frame_) return *entry_stack_check_frame_;
  DCHECK_EQ(iterator_.current_offset(), entrypoint_);
  DCHECK_NULL(parent_);
  entry_stack_check_frame_.emplace(
      *compilation_unit_,
      zone()->New<CompactInterpreterFrameState>(
          *compilation_unit_,
          GetInLivenessFor(graph_->is_osr() ? bailout_for_entrypoint() : 0),
          current_interpreter_frame_),
      GetClosure(), BytecodeOffset(bailout_for_entrypoint()),
      current_source_position_, nullptr);

  (*entry_stack_check_frame_)
      .frame_state()
      ->ForEachValue(
          *compilation_unit_,
          |&](ValueNode* node, interpreter::Register) { AddDeoptUse(node); });
  AddDeoptUse((*entry_stack_check_frame_).closure());
  return *entry_stack_check_frame_;
}

ValueNode* MaglevGraphBuilder::GetTaggedValue(
    ValueNode* value, UseReprHintRecording record_use_repr_hint) {
  if (V8_LIKELY(record_use_repr_hint == UseReprHintRecording::kRecord)) {
    RecordUseReprHintIfPhi(value, UseRepresentation::kTagged);
  }

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kTagged) return value;

  if (Int32Constant* as_int32_constant = value->TryCast<Int32Constant>();
      as_int32_constant && Smi::IsValid(as_int32_constant->value())) {
    return GetSmiConstant(as_int32_constant->value());
  }

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.tagged()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kInt32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagInt32>({value}));
      }
      return alternative.set_tagged(AddNewNode<Int32ToNumber>({value}));
    }
    case ValueRepresentation::kUint32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagUint32>({value}));
      }
      return alternative.set_tagged(AddNewNode<Uint32ToNumber>({value}));
    }
    case ValueRepresentation::kFloat64: {
      return alternative.set_tagged(AddNewNode<Float64ToTagged>(
          {value}, Float64ToTagged::ConversionMode::kCanonicalizeSmi));
    }
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_tagged(AddNewNode<HoleyFloat64ToTagged>(
          {value}, HoleyFloat64ToTagged::ConversionMode::kForceHeapNumber));
    }

    case ValueRepresentation::kTagged:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

ReduceResult MaglevGraphBuilder::GetSmiValue(
    ValueNode* value, UseReprHintRecording record_use_repr_hint) {
  if (V8_LIKELY(record_use_repr_hint == UseReprHintRecording::kRecord)) {
    RecordUseReprHintIfPhi(value, UseRepresentation::kTagged);
  }

  NodeInfo* node_info = GetOrCreateInfoFor(value);

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kTagged) {
    return BuildCheckSmi(value, !value->Is<Phi>());
  }

  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.tagged()) {
    // HoleyFloat64ToTagged does not canonicalize Smis by default, since it can
    // be expensive. If we are reading a Smi value, we should try to
    // canonicalize now.
    if (HoleyFloat64ToTagged* conversion_node =
            alt->TryCast<HoleyFloat64ToTagged>()) {
      conversion_node->SetMode(
          HoleyFloat64ToTagged::ConversionMode::kCanonicalizeSmi);
    }
    return BuildCheckSmi(alt, !value->Is<Phi>());
  }

  switch (representation) {
    case ValueRepresentation::kInt32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagInt32>({value}));
      }
      return alternative.set_tagged(AddNewNode<CheckedSmiTagInt32>({value}));
    }
    case ValueRepresentation::kUint32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagUint32>({value}));
      }
      return alternative.set_tagged(AddNewNode<CheckedSmiTagUint32>({value}));
    }
    case ValueRepresentation::kFloat64: {
      return alternative.set_tagged(AddNewNode<CheckedSmiTagFloat64>({value}));
    }
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_tagged(AddNewNode<CheckedSmiTagFloat64>({value}));
    }

    case ValueRepresentation::kTagged:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

namespace {
CheckType GetCheckType(NodeType type) {
  return NodeTypeIs(type, NodeType::kAnyHeapObject)
             ? CheckType::kOmitHeapObjectCheck
             : CheckType::kCheckHeapObject;
}
}  // namespace

ValueNode* MaglevGraphBuilder::GetInternalizedString(
    interpreter::Register reg) {
  ValueNode* node = current_interpreter_frame_.get(reg);
  NodeType old_type;
  if (CheckType(node, NodeType::kInternalizedString, &old_type)) return node;
  NodeInfo* known_info = GetOrCreateInfoFor(node);
  if (known_info->alternative().checked_value()) {
    node = known_info->alternative().checked_value();
    if (CheckType(node, NodeType::kInternalizedString, &old_type)) return node;
  }

  if (!NodeTypeIs(old_type, NodeType::kString)) {
    known_info->CombineType(NodeType::kString);
  }

  // This node may unwrap ThinStrings.
  ValueNode* maybe_unwrapping_node =
      AddNewNode<CheckedInternalizedString>({node}, GetCheckType(old_type));
  known_info->alternative().set_checked_value(maybe_unwrapping_node);

  current_interpreter_frame_.set(reg, maybe_unwrapping_node);
  return maybe_unwrapping_node;
}

ValueNode* MaglevGraphBuilder::GetTruncatedInt32ForToNumber(
    ValueNode* value, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kTruncatedInt32);

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kInt32) return value;
  if (representation == ValueRepresentation::kUint32) {
    // This node is cheap (no code gen, just a bitcast), so don't cache it.
    return AddNewNode<TruncateUint32ToInt32>({value});
  }

  // Process constants first to avoid allocating NodeInfo for them.
  switch (value->opcode()) {
    case Opcode::kConstant: {
      compiler::ObjectRef object = value->Cast<Constant>()->object();
      if (!object.IsHeapNumber()) break;
      int32_t truncated_value = DoubleToInt32(object.AsHeapNumber().value());
      if (!Smi::IsValid(truncated_value)) break;
      return GetInt32Constant(truncated_value);
    }
    case Opcode::kSmiConstant:
      return GetInt32Constant(value->Cast<SmiConstant>()->value().value());
    case Opcode::kRootConstant: {
      Tagged<Object> root_object =
          local_isolate_->root(value->Cast<RootConstant>()->index());
      if (!IsOddball(root_object, local_isolate_)) break;
      int32_t truncated_value =
          DoubleToInt32(Cast<Oddball>(root_object)->to_number_raw());
      // All oddball ToNumber truncations are valid Smis.
      DCHECK(Smi::IsValid(truncated_value));
      return GetInt32Constant(truncated_value);
    }
    case Opcode::kFloat64Constant: {
      int32_t truncated_value =
          DoubleToInt32(value->Cast<Float64Constant>()->value().get_scalar());
      if (!Smi::IsValid(truncated_value)) break;
      return GetInt32Constant(truncated_value);
    }

    // We could emit unconditional eager deopts for other kinds of constant, but
    // it's not necessary, the appropriate checking conversion nodes will deopt.
    default:
      break;
  }

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  // If there is an int32_alternative, then that works as a truncated value
  // too.
  if (ValueNode* alt = alternative.int32()) {
    return alt;
  }
  if (ValueNode* alt = alternative.truncated_int32_to_number()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kTagged: {
      NodeType old_type;
      EnsureType(value, allowed_input_type, &old_type);
      if (NodeTypeIsSmi(old_type)) {
        // Smi untagging can be cached as an int32 alternative, not just a
        // truncated alternative.
        return alternative.set_int32(BuildSmiUntag(value));
      }
      if (allowed_input_type == NodeType::kSmi) {
        return alternative.set_int32(AddNewNode<CheckedSmiUntag>({value}));
      }
      if (NodeTypeIs(old_type, allowed_input_type)) {
        return alternative.set_truncated_int32_to_number(
            AddNewNode<TruncateNumberOrOddballToInt32>({value},
                                                       conversion_type));
      }
      return alternative.set_truncated_int32_to_number(
          AddNewNode<CheckedTruncateNumberOrOddballToInt32>({value},
                                                            conversion_type));
    }
    case ValueRepresentation::kFloat64:
    // Ignore conversion_type for HoleyFloat64, and treat them like Float64.
    // ToNumber of undefined is anyway a NaN, so we'll simply truncate away
    // the NaN-ness of the hole, and don't need to do extra oddball checks so
    // we can ignore the hint (though we'll miss updating the feedback).
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_truncated_int32_to_number(
          AddNewNode<TruncateFloat64ToInt32>({value}));
    }

    case ValueRepresentation::kInt32:
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

std::optional<int32_t> MaglevGraphBuilder::TryGetInt32Constant(
    ValueNode* value) {
  switch (value->opcode()) {
    case Opcode::kInt32Constant:
      return value->Cast<Int32Constant>()->value();
    case Opcode::kUint32Constant: {
      uint32_t uint32_value = value->Cast<Uint32Constant>()->value();
      if (uint32_value <= INT32_MAX) {
        return static_cast<int32_t>(uint32_value);
      }
      return {};
    }
    case Opcode::kSmiConstant:
      return value->Cast<SmiConstant>()->value().value();
    case Opcode::kFloat64Constant: {
      double double_value =
          value->Cast<Float64Constant>()->value().get_scalar();
      if (!IsInt32Double(double_value)) return {};
      return FastD2I(value->Cast<Float64Constant>()->value().get_scalar());
    }
    default:
      break;
  }
  if (auto c = TryGetConstantAlternative(value)) {
    return TryGetInt32Constant(*c);
  }
  return {};
}

std::optional<uint32_t> MaglevGraphBuilder::TryGetUint32Constant(
    ValueNode* value) {
  switch (value->opcode()) {
    case Opcode::kInt32Constant: {
      int32_t int32_value = value->Cast<Int32Constant>()->value();
      if (int32_value >= 0) {
        return static_cast<uint32_t>(int32_value);
      }
      return {};
    }
    case Opcode::kUint32Constant:
      return value->Cast<Uint32Constant>()->value();
    case Opcode::kSmiConstant: {
      int32_t smi_value = value->Cast<SmiConstant>()->value().value();
      if (smi_value >= 0) {
        return static_cast<uint32_t>(smi_value);
      }
      return {};
    }
    case Opcode::kFloat64Constant: {
      double double_value =
          value->Cast<Float64Constant>()->value().get_scalar();
      if (!IsUint32Double(double_value)) return {};
      return FastD2UI(value->Cast<Float64Constant>()->value().get_scalar());
    }
    default:
      break;
  }
  if (auto c = TryGetConstantAlternative(value)) {
    return TryGetUint32Constant(*c);
  }
  return {};
}

ValueNode* MaglevGraphBuilder::GetInt32(ValueNode* value,
                                        bool can_be_heap_number) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kInt32);

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kInt32) return value;

  // Process constants first to avoid allocating NodeInfo for them.
  if (auto cst = TryGetInt32Constant(value)) {
    return GetInt32Constant(cst.value());
  }
  // We could emit unconditional eager deopts for other kinds of constant, but
  // it's not necessary, the appropriate checking conversion nodes will deopt.

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.int32()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kTagged: {
      if (can_be_heap_number && !CheckType(value, NodeType::kSmi)) {
        return alternative.set_int32(AddNewNode<CheckedNumberToInt32>({value}));
      }
      return alternative.set_int32(BuildSmiUntag(value));
    }
    case ValueRepresentation::kUint32: {
      if (node_info->is_smi()) {
        return alternative.set_int32(
            AddNewNode<TruncateUint32ToInt32>({value}));
      }
      return alternative.set_int32(AddNewNode<CheckedUint32ToInt32>({value}));
    }
    case ValueRepresentation::kFloat64:
    // The check here will also work for the hole NaN, so we can treat
    // HoleyFloat64 as Float64.
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_int32(
          AddNewNode<CheckedTruncateFloat64ToInt32>({value}));
    }

    case ValueRepresentation::kInt32:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

std::optional<double> MaglevGraphBuilder::TryGetFloat64Constant(
    ValueNode* value, TaggedToFloat64ConversionType conversion_type) {
  switch (value->opcode()) {
    case Opcode::kConstant: {
      compiler::ObjectRef object = value->Cast<Constant>()->object();
      if (object.IsHeapNumber()) {
        return object.AsHeapNumber().value();
      }
      // Oddballs should be RootConstants.
      DCHECK(!IsOddball(*object.object()));
      return {};
    }
    case Opcode::kInt32Constant:
      return value->Cast<Int32Constant>()->value();
    case Opcode::kSmiConstant:
      return value->Cast<SmiConstant>()->value().value();
    case Opcode::kFloat64Constant:
      return value->Cast<Float64Constant>()->value().get_scalar();
    case Opcode::kRootConstant: {
      Tagged<Object> root_object =
          local_isolate_->root(value->Cast<RootConstant>()->index());
      if (conversion_type == TaggedToFloat64ConversionType::kNumberOrBoolean &&
          IsBoolean(root_object)) {
        return Cast<Oddball>(root_object)->to_number_raw();
      }
      if (conversion_type == TaggedToFloat64ConversionType::kNumberOrOddball &&
          IsOddball(root_object)) {
        return Cast<Oddball>(root_object)->to_number_raw();
      }
      if (IsHeapNumber(root_object)) {
        return Cast<HeapNumber>(root_object)->value();
      }
      return {};
    }
    default:
      break;
  }
  if (auto c = TryGetConstantAlternative(value)) {
    return TryGetFloat64Constant(*c, conversion_type);
  }
  return {};
}

ValueNode* MaglevGraphBuilder::GetFloat64(ValueNode* value) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kFloat64);
  return GetFloat64ForToNumber(value, NodeType::kNumber,
                               TaggedToFloat64ConversionType::kOnlyNumber);
}

ValueNode* MaglevGraphBuilder::GetFloat64ForToNumber(
    ValueNode* value, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kFloat64) return value;

  // Process constants first to avoid allocating NodeInfo for them.
  if (auto cst = TryGetFloat64Constant(value, conversion_type)) {
    return GetFloat64Constant(cst.value());
  }
  // We could emit unconditional eager deopts for other kinds of constant, but
  // it's not necessary, the appropriate checking conversion nodes will deopt.

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.float64()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kTagged: {
      auto combined_type = CombineType(allowed_input_type, node_info->type());
      if (NodeTypeIs(combined_type, NodeType::kSmi)) {
        // Get the float64 value of a Smi value its int32 representation.
        return GetFloat64(GetInt32(value));
      }
      if (NodeTypeIs(combined_type, NodeType::kNumber)) {
        // Number->Float64 conversions are exact alternatives, so they can
        // also become the canonical float64_alternative.
        return alternative.set_float64(BuildNumberOrOddballToFloat64(
            value, NodeType::kNumber,
            TaggedToFloat64ConversionType::kOnlyNumber));
      }
      if (NodeTypeIs(combined_type, NodeType::kNumberOrOddball)) {
        // NumberOrOddball->Float64 conversions are not exact alternatives,
        // since they lose the information that this is an oddball, so they
        // can only become the canonical float64_alternative if they are a
        // known number (and therefore not oddball).
        return BuildNumberOrOddballToFloat64(value, combined_type,
                                             conversion_type);
      }
      // The type is impossible. We could generate an unconditional deopt here,
      // but it's too invasive. So we just generate a check which will always
      // deopt.
      return BuildNumberOrOddballToFloat64(value, allowed_input_type,
                                           conversion_type);
    }
    case ValueRepresentation::kInt32:
      return alternative.set_float64(AddNewNode<ChangeInt32ToFloat64>({value}));
    case ValueRepresentation::kUint32:
      return alternative.set_float64(
          AddNewNode<ChangeUint32ToFloat64>({value}));
    case ValueRepresentation::kHoleyFloat64: {
      switch (allowed_input_type) {
        case NodeType::kSmi:
        case NodeType::kNumber:
        case NodeType::kNumberOrBoolean:
          // Number->Float64 conversions are exact alternatives, so they can
          // also become the canonical float64_alternative. The HoleyFloat64
          // representation can represent undefined but no other oddballs, so
          // booleans cannot occur here and kNumberOrBoolean can be grouped with
          // kNumber.
          return alternative.set_float64(
              AddNewNode<CheckedHoleyFloat64ToFloat64>({value}));
        case NodeType::kNumberOrOddball:
          // NumberOrOddball->Float64 conversions are not exact alternatives,
          // since they lose the information that this is an oddball, so they
          // cannot become the canonical float64_alternative.
          return AddNewNode<HoleyFloat64ToMaybeNanFloat64>({value});
        default:
          UNREACHABLE();
      }
    }
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

ValueNode* MaglevGraphBuilder::GetHoleyFloat64ForToNumber(
    ValueNode* value, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kHoleyFloat64);
  ValueRepresentation representation =
      value->properties().value_representation();
  // Ignore the hint for
  if (representation == ValueRepresentation::kHoleyFloat64) return value;
  return GetFloat64ForToNumber(value, allowed_input_type, conversion_type);
}

namespace {
int32_t ClampToUint8(int32_t value) {
  if (value < 0) return 0;
  if (value > 255) return 255;
  return value;
}
}  // namespace

ValueNode* MaglevGraphBuilder::GetUint8ClampedForToNumber(ValueNode* value) {
  switch (value->properties().value_representation()) {
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kTagged: {
      if (SmiConstant* constant = value->TryCast<SmiConstant>()) {
        return GetInt32Constant(ClampToUint8(constant->value().value()));
      }
      NodeInfo* info = known_node_aspects().TryGetInfoFor(value);
      if (info && info->alternative().int32()) {
        return AddNewNode<Int32ToUint8Clamped>({info->alternative().int32()});
      }
      return AddNewNode<CheckedNumberToUint8Clamped>({value});
    }
    // HoleyFloat64 is treated like Float64. ToNumber of undefined is anyway a
    // NaN, so we'll simply truncate away the NaN-ness of the hole, and don't
    // need to do extra oddball checks (though we'll miss updating the
    // feedback).
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      // TODO(leszeks): Handle Float64Constant, which requires the correct
      // rounding for clamping.
      return AddNewNode<Float64ToUint8Clamped>({value});
    case ValueRepresentation::kInt32:
      if (Int32Constant* constant = value->TryCast<Int32Constant>()) {
        return GetInt32Constant(ClampToUint8(constant->value()));
      }
      return AddNewNode<Int32ToUint8Clamped>({value});
    case ValueRepresentation::kUint32:
      return AddNewNode<Uint32ToUint8Clamped>({value});
  }
  UNREACHABLE();
}

namespace {
template <Operation kOperation>
struct NodeForOperationHelper;

#define NODE_FOR_OPERATION_HELPER(Name)               \
  template <>                                         \
  struct NodeForOperationHelper<Operation::k##Name> { \
    using generic_type = Generic##Name;               \
  };
OPERATION_LIST(NODE_FOR_OPERATION_HELPER)
#undef NODE_FOR_OPERATION_HELPER

template <Operation kOperation>
using GenericNodeForOperation =
    typename NodeForOperationHelper<kOperation>::generic_type;

// Bitwise operations reinterprets the numeric input as Int32 bits for a
// bitwise operation, which means we want to do slightly different conversions.
template <Operation kOperation>
constexpr bool BinaryOperationIsBitwiseInt32() {
  switch (kOperation) {
    case Operation::kBitwiseNot:
    case Operation::kBitwiseAnd:
    case Operation::kBitwiseOr:
    case Operation::kBitwiseXor:
    case Operation::kShiftLeft:
    case Operation::kShiftRight:
    case Operation::kShiftRightLogical:
      return true;
    default:
      return false;
  }
}
}  // namespace

// MAP_OPERATION_TO_NODES are tuples with the following format:
// - Operation name,
// - Int32 operation node,
// - Identity of int32 operation (e.g, 0 for add/sub and 1 for mul/div), if it
//   exists, or otherwise {}.
#define MAP_BINARY_OPERATION_TO_INT32_NODE(V) \
  V(Add, Int32AddWithOverflow, 0)             \
  V(Subtract, Int32SubtractWithOverflow, 0)   \
  V(Multiply, Int32MultiplyWithOverflow, 1)   \
  V(Divide, Int32DivideWithOverflow, 1)       \
  V(Modulus, Int32ModulusWithOverflow, {})    \
  V(BitwiseAnd, Int32BitwiseAnd, ~0)          \
  V(BitwiseOr, Int32BitwiseOr, 0)             \
  V(BitwiseXor, Int32BitwiseXor, 0)           \
  V(ShiftLeft, Int32ShiftLeft, 0)             \
  V(ShiftRight, Int32ShiftRight, 0)           \
  V(ShiftRightLogical, Int32ShiftRightLogical, {})

#define MAP_UNARY_OPERATION_TO_INT32_NODE(V) \
  V(BitwiseNot, Int32BitwiseNot)             \
  V(Increment, Int32IncrementWithOverflow)   \
  V(Decrement, Int32DecrementWithOverflow)   \
  V(Negate, Int32NegateWithOverflow)

// MAP_OPERATION_TO_FLOAT64_NODE are tuples with the following format:
// (Operation name, Float64 operation node).
#define MAP_OPERATION_TO_FLOAT64_NODE(V) \
  V(Add, Float64Add)                     \
  V(Subtract, Float64Subtract)           \
  V(Multiply, Float64Multiply)           \
  V(Divide, Float64Divide)               \
  V(Modulus, Float64Modulus)             \
  V(Exponentiate, Float64Exponentiate)

template <Operation kOperation>
static constexpr std::optional<int> Int32Identity() {
  switch (kOperation) {
#define CASE(op, _, identity) \
  case Operation::k##op:      \
    return identity;
    MAP_BINARY_OPERATION_TO_INT32_NODE(CASE)
#undef CASE
    default:
      UNREACHABLE();
  }
}

namespace {
template <Operation kOperation>
struct Int32NodeForHelper;
#define SPECIALIZATION(op, OpNode, ...)         \
  template <>                                   \
  struct Int32NodeForHelper<Operation::k##op> { \
    using type = OpNode;                        \
  };
MAP_UNARY_OPERATION_TO_INT32_NODE(SPECIALIZATION)
MAP_BINARY_OPERATION_TO_INT32_NODE(SPECIALIZATION)
#undef SPECIALIZATION

template <Operation kOperation>
using Int32NodeFor = typename Int32NodeForHelper<kOperation>::type;

template <Operation kOperation>
struct Float64NodeForHelper;
#define SPECIALIZATION(op, OpNode)                \
  template <>                                     \
  struct Float64NodeForHelper<Operation::k##op> { \
    using type = OpNode;                          \
  };
MAP_OPERATION_TO_FLOAT64_NODE(SPECIALIZATION)
#undef SPECIALIZATION

template <Operation kOperation>
using Float64NodeFor = typename Float64NodeForHelper<kOperation>::type;
}  // namespace

template <Operation kOperation>
void MaglevGraphBuilder::BuildGenericUnaryOperationNode() {
  FeedbackSlot slot_index = GetSlotOperand(0);
  ValueNode* value = GetAccumulator();
  SetAccumulator(AddNewNode<GenericNodeForOperation<kOperation>>(
      {value}, compiler::FeedbackSource{feedback(), slot_index}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildGenericBinaryOperationNode() {
  ValueNode* left = LoadRegister(0);
  ValueNode* right = GetAccumulator();
  FeedbackSlot slot_index = GetSlotOperand(1);
  SetAccumulator(AddNewNode<GenericNodeForOperation<kOperation>>(
      {left, right}, compiler::FeedbackSource{feedback(), slot_index}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildGenericBinarySmiOperationNode() {
  ValueNode* left = GetAccumulator();
  int constant = iterator_.GetImmediateOperand(0);
  ValueNode* right = GetSmiConstant(constant);
  FeedbackSlot slot_index = GetSlotOperand(1);
  SetAccumulator(AddNewNode<GenericNodeForOperation<kOperation>>(
      {left, right}, compiler::FeedbackSource{feedback(), slot_index}));
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldInt32UnaryOperation(ValueNode* node) {
  auto cst = TryGetInt32Constant(node);
  if (!cst.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kBitwiseNot:
      return GetInt32Constant(~cst.value());
    case Operation::kIncrement:
      if (cst.value() < INT32_MAX) {
        return GetInt32Constant(cst.value() + 1);
      }
      return ReduceResult::Fail();
    case Operation::kDecrement:
      if (cst.value() > INT32_MIN) {
        return GetInt32Constant(cst.value() - 1);
      }
      return ReduceResult::Fail();
    case Operation::kNegate:
      if (cst.value() == 0) {
        return ReduceResult::Fail();
      }
      if (cst.value() != INT32_MIN) {
        return GetInt32Constant(-cst.value());
      }
      return ReduceResult::Fail();
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildInt32UnaryOperationNode() {
  // Use BuildTruncatingInt32BitwiseNotForToNumber with Smi input hint
  // for truncating operations.
  static_assert(!BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* value = GetAccumulator();
  PROCESS_AND_RETURN_IF_DONE(TryFoldInt32UnaryOperation<kOperation>(value),
                             SetAccumulator);
  using OpNodeT = Int32NodeFor<kOperation>;
  SetAccumulator(AddNewNode<OpNodeT>({value}));
}

void MaglevGraphBuilder::BuildTruncatingInt32BitwiseNotForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  ValueNode* value =
      GetTruncatedInt32ForToNumber(current_interpreter_frame_.accumulator(),
                                   allowed_input_type, conversion_type);
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32UnaryOperation<Operation::kBitwiseNot>(value),
      SetAccumulator);
  SetAccumulator(AddNewNode<Int32BitwiseNot>({value}));
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldInt32BinaryOperation(ValueNode* left,
                                                             ValueNode* right) {
  auto cst_right = TryGetInt32Constant(right);
  if (!cst_right.has_value()) return ReduceResult::Fail();
  return TryFoldInt32BinaryOperation<kOperation>(left, cst_right.value());
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldInt32BinaryOperation(
    ValueNode* left, int32_t cst_right) {
  auto cst_left = TryGetInt32Constant(left);
  if (!cst_left.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kAdd: {
      int64_t result = static_cast<int64_t>(cst_left.value()) +
                       static_cast<int64_t>(cst_right);
      if (result >= INT32_MIN && result <= INT32_MAX) {
        return GetInt32Constant(static_cast<int32_t>(result));
      }
      return ReduceResult::Fail();
    }
    case Operation::kSubtract: {
      int64_t result = static_cast<int64_t>(cst_left.value()) -
                       static_cast<int64_t>(cst_right);
      if (result >= INT32_MIN && result <= INT32_MAX) {
        return GetInt32Constant(static_cast<int32_t>(result));
      }
      return ReduceResult::Fail();
    }
    case Operation::kMultiply: {
      int64_t result = static_cast<int64_t>(cst_left.value()) *
                       static_cast<int64_t>(cst_right);
      if (result >= INT32_MIN && result <= INT32_MAX) {
        return GetInt32Constant(static_cast<int32_t>(result));
      }
      return ReduceResult::Fail();
    }
    case Operation::kModulus:
      // TODO(v8:7700): Constant fold mod.
      return ReduceResult::Fail();
    case Operation::kDivide:
      // TODO(v8:7700): Constant fold division.
      return ReduceResult::Fail();
    case Operation::kBitwiseAnd:
      return GetInt32Constant(cst_left.value() & cst_right);
    case Operation::kBitwiseOr:
      return GetInt32Constant(cst_left.value() | cst_right);
    case Operation::kBitwiseXor:
      return GetInt32Constant(cst_left.value() ^ cst_right);
    case Operation::kShiftLeft:
      return GetInt32Constant(cst_left.value()
                              << (static_cast<uint32_t>(cst_right) % 32));
    case Operation::kShiftRight:
      return GetInt32Constant(cst_left.value() >>
                              (static_cast<uint32_t>(cst_right) % 32));
    case Operation::kShiftRightLogical:
      return GetUint32Constant(static_cast<uint32_t>(cst_left.value()) >>
                               (static_cast<uint32_t>(cst_right) % 32));
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildInt32BinaryOperationNode() {
  // Use BuildTruncatingInt32BinaryOperationNodeForToNumber with Smi input hint
  // for truncating operations.
  static_assert(!BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left = LoadRegister(0);
  ValueNode* right = GetAccumulator();
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, right), SetAccumulator);
  using OpNodeT = Int32NodeFor<kOperation>;
  SetAccumulator(AddNewNode<OpNodeT>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildTruncatingInt32BinaryOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  static_assert(BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left;
  ValueNode* right;
  if (IsRegisterEqualToAccumulator(0)) {
    left = right = GetTruncatedInt32ForToNumber(
        current_interpreter_frame_.get(iterator_.GetRegisterOperand(0)),
        allowed_input_type, conversion_type);
  } else {
    left = GetTruncatedInt32ForToNumber(
        current_interpreter_frame_.get(iterator_.GetRegisterOperand(0)),
        allowed_input_type, conversion_type);
    right =
        GetTruncatedInt32ForToNumber(current_interpreter_frame_.accumulator(),
                                     allowed_input_type, conversion_type);
  }
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, right), SetAccumulator);
  SetAccumulator(AddNewNode<Int32NodeFor<kOperation>>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildInt32BinarySmiOperationNode() {
  // Truncating Int32 nodes treat their input as a signed int32 regardless
  // of whether it's really signed or not, so we allow Uint32 by loading a
  // TruncatedInt32 value.
  static_assert(!BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left = GetAccumulator();
  int32_t constant = iterator_.GetImmediateOperand(0);
  if (std::optional<int>(constant) == Int32Identity<kOperation>()) {
    // Deopt if {left} is not an Int32.
    EnsureInt32(left);
    // If the constant is the unit of the operation, it already has the right
    // value, so just return.
    return;
  }
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, constant), SetAccumulator);
  ValueNode* right = GetInt32Constant(constant);
  using OpNodeT = Int32NodeFor<kOperation>;
  SetAccumulator(AddNewNode<OpNodeT>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildTruncatingInt32BinarySmiOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  static_assert(BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left =
      GetTruncatedInt32ForToNumber(current_interpreter_frame_.accumulator(),
                                   allowed_input_type, conversion_type);
  int32_t constant = iterator_.GetImmediateOperand(0);
  if (std::optional<int>(constant) == Int32Identity<kOperation>()) {
    // If the constant is the unit of the operation, it already has the right
    // value, so use the truncated value (if not just a conversion) and return.
    if (!left->properties().is_conversion()) {
      current_interpreter_frame_.set_accumulator(left);
    }
    return;
  }
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, constant), SetAccumulator);
  ValueNode* right = GetInt32Constant(constant);
  SetAccumulator(AddNewNode<Int32NodeFor<kOperation>>({left, right}));
}

ValueNode* MaglevGraphBuilder::GetNumberConstant(double constant) {
  if (IsSmiDouble(constant)) {
    return GetInt32Constant(FastD2I(constant));
  }
  return GetFloat64Constant(constant);
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldFloat64UnaryOperationForToNumber(
    TaggedToFloat64ConversionType conversion_type, ValueNode* value) {
  auto cst = TryGetFloat64Constant(value, conversion_type);
  if (!cst.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kNegate:
      return GetNumberConstant(-cst.value());
    case Operation::kIncrement:
      return GetNumberConstant(cst.value() + 1);
    case Operation::kDecrement:
      return GetNumberConstant(cst.value() - 1);
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldFloat64BinaryOperationForToNumber(
    TaggedToFloat64ConversionType conversion_type, ValueNode* left,
    ValueNode* right) {
  auto cst_right = TryGetFloat64Constant(right, conversion_type);
  if (!cst_right.has_value()) return ReduceResult::Fail();
  return TryFoldFloat64BinaryOperationForToNumber<kOperation>(
      conversion_type, left, cst_right.value());
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldFloat64BinaryOperationForToNumber(
    TaggedToFloat64ConversionType conversion_type, ValueNode* left,
    double cst_right) {
  auto cst_left = TryGetFloat64Constant(left, conversion_type);
  if (!cst_left.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kAdd:
      return GetNumberConstant(cst_left.value() + cst_right);
    case Operation::kSubtract:
      return GetNumberConstant(cst_left.value() - cst_right);
    case Operation::kMultiply:
      return GetNumberConstant(cst_left.value() * cst_right);
    case Operation::kDivide:
      return GetNumberConstant(cst_left.value() / cst_right);
    case Operation::kModulus:
      // TODO(v8:7700): Constant fold mod.
      return ReduceResult::Fail();
    case Operation::kExponentiate:
      return GetNumberConstant(math::pow(cst_left.value(), cst_right));
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildFloat64BinarySmiOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  // TODO(v8:7700): Do constant identity folding. Make sure to normalize
  // HoleyFloat64 nodes if folded.
  ValueNode* left = GetAccumulatorHoleyFloat64ForToNumber(allowed_input_type,
                                                          conversion_type);
  double constant = static_cast<double>(iterator_.GetImmediateOperand(0));
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldFloat64BinaryOperationForToNumber<kOperation>(conversion_type,
                                                           left, constant),
      SetAccumulator);
  ValueNode* right = GetFloat64Constant(constant);
  SetAccumulator(AddNewNode<Float64NodeFor<kOperation>>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildFloat64UnaryOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  // TODO(v8:7700): Do constant identity folding. Make sure to normalize
  // HoleyFloat64 nodes if folded.
  ValueNode* value = GetAccumulatorHoleyFloat64ForToNumber(allowed_input_type,
                                                           conversion_type);
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldFloat64UnaryOperationForToNumber<kOperation>(conversion_type,
                                                          value),
      SetAccumulator);
  switch (kOperation) {
    case Operation::kNegate:
      SetAccumulator(AddNewNode<Float64Negate>({value}));
      break;
    case Operation::kIncrement:
      SetAccumulator(AddNewNode<Float64Add>({value, GetFloat64Constant(1)}));
      break;
    case Operation::kDecrement:
      SetAccumulator(
          AddNewNode<Float64Subtract>({value, GetFloat64Constant(1)}));
      break;
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildFloat64BinaryOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  // TODO(v8:7700): Do constant identity folding. Make sure to normalize
  // HoleyFloat64 nodes if folded.
  ValueNode* left = LoadRegisterHoleyFloat64ForToNumber(0, allowed_input_type,
                                                        conversion_type);
  ValueNode* right = GetAccumulatorHoleyFloat64ForToNumber(allowed_input_type,
                                                           conversion_type);
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldFloat64BinaryOperationForToNumber<kOperation>(conversion_type,
                                                           left, right),
      SetAccumulator);
  SetAccumulator(AddNewNode<Float64NodeFor<kOperation>>({left, right}));
}

namespace {
std::tuple<NodeType, TaggedToFloat64ConversionType>
BinopHintToNodeTypeAndConversionType(BinaryOperationHint hint) {
  switch (hint) {
    case BinaryOperationHint::kSignedSmall:
      return std::make_tuple(NodeType::kSmi,
                             TaggedToFloat64ConversionType::kOnlyNumber);
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
      return std::make_tuple(NodeType::kNumber,
                             TaggedToFloat64ConversionType::kOnlyNumber);
    case BinaryOperationHint::kNumberOrOddball:
      return std::make_tuple(NodeType::kNumberOrOddball,
                             TaggedToFloat64ConversionType::kNumberOrOddball);
    case BinaryOperationHint::kNone:
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      UNREACHABLE();
  }
}
}  // namespace

template <Operation kOperation>
void MaglevGraphBuilder::VisitUnaryOperation() {
  FeedbackNexus nexus = FeedbackNexusForOperand(0);
  BinaryOperationHint feedback_hint = nexus.GetBinaryOperationFeedback();
  switch (feedback_hint) {
    case BinaryOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForBinaryOperation));
    case BinaryOperationHint::kSignedSmall:
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kNumberOrOddball: {
      auto |allowed_input_type, conversion_type] =
          BinopHintToNodeTypeAndConversionType(feedback_hint);
      if constexpr (BinaryOperationIsBitwiseInt32<kOperation>()) {
        static_assert(kOperation == Operation::kBitwiseNot);
        return BuildTruncatingInt32BitwiseNotForToNumber(allowed_input_type,
                                                         conversion_type);
      } else if (feedback_hint == BinaryOperationHint::kSignedSmall) {
        return BuildInt32UnaryOperationNode<kOperation>();
      }
      return BuildFloat64UnaryOperationNodeForToNumber<kOperation>(
          allowed_input_type, conversion_type);
      break;
    }
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      // Fallback to generic node.
      break;
  }
  BuildGenericUnaryOperationNode<kOperation>();
}

template <Operation kOperation>
void MaglevGraphBuilder::VisitBinaryOperation() {
  FeedbackNexus nexus = FeedbackNexusForOperand(1);
  BinaryOperationHint feedback_hint = nexus.GetBinaryOperationFeedback();
  switch (feedback_hint) {
    case BinaryOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForBinaryOperation));
    case BinaryOperationHint::kSignedSmall:
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kNumberOrOddball: {
      auto |allowed_input_type, conversion_type] =
          BinopHintToNodeTypeAndConversionType(feedback_hint);
      if constexpr (BinaryOperationIsBitwiseInt32<kOperation>()) {
        return BuildTruncatingInt32BinaryOperationNodeForToNumber<kOperation>(
            allowed_input_type, conversion_type);
      } else if (feedback_hint == BinaryOperationHint::kSignedSmall) {
        if constexpr (kOperation == Operation::kExponentiate) {
          // Exponentiate never updates the feedback to be a Smi.
          UNREACHABLE();
        } else {
          return BuildInt32BinaryOperationNode<kOperation>();
        }
      } else {
        return BuildFloat64BinaryOperationNodeForToNumber<kOperation>(
            allowed_input_type, conversion_type);
      }
      break;
    }
    case BinaryOperationHint::kString:
      if constexpr (kOperation == Operation::kAdd) {
        ValueNode* left = LoadRegister(0);
        ValueNode* right = GetAccumulator();
        if (RootConstant* root_constant = left->TryCast<RootConstant>()) {
          if (root_constant->index() == RootIndex::kempty_string) {
            BuildCheckString(right);
            // The right side is already in the accumulator register.
            return;
          }
        }
        if (RootConstant* root_constant = right->TryCast<RootConstant>()) {
          if (root_constant->index() == RootIndex::kempty_string) {
            BuildCheckString(left);
            MoveNodeBetweenRegisters(
                iterator_.GetRegisterOperand(0),
                interpreter::Register::virtual_accumulator());
            return;
          }
        }
        BuildCheckString(left);
        BuildCheckString(right);
        SetAccumulator(AddNewNode<StringConcat>({left, right}));
        return;
      }
      break;
    case BinaryOperationHint::kStringOrStringWrapper:
      if constexpr (kOperation == Operation::kAdd) {
        if (broker()
                ->dependencies()
                ->DependOnStringWrapperToPrimitiveProtector()) {
          ValueNode* left = LoadRegister(0);
          ValueNode* right = GetAccumulator();
          BuildCheckStringOrStringWrapper(left);
          BuildCheckStringOrStringWrapper(right);
          SetAccumulator(AddNewNode<StringWrapperConcat>({left, right}));
          return;
        }
      }
      ||fallthrough]];
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      // Fallback to generic node.
      break;
  }
  BuildGenericBinaryOperationNode<kOperation>();
}

template <Operation kOperation>
void MaglevGraphBuilder::VisitBinarySmiOperation() {
  FeedbackNexus nexus = FeedbackNexusForOperand(1);
  BinaryOperationHint feedback_hint = nexus.GetBinaryOperationFeedback();
  switch (feedback_hint) {
    case BinaryOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForBinaryOperation));
    case BinaryOperationHint::kSignedSmall:
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kNumberOrOddball: {
      const auto |allowed_input_type, conversion_type] =
          BinopHintToNodeTypeAndConversionType(feedback_hint);
      if constexpr (BinaryOperationIsBitwiseInt32<kOperation>()) {
        return BuildTruncatingInt32BinarySmiOperationNodeForToNumber<
            kOperation>(allowed_input_type, conversion_type);
      } else if (feedback_hint == BinaryOperationHint::kSignedSmall) {
        if constexpr (kOperation == Operation::kExponentiate) {
          // Exponentiate never updates the feedback to be a Smi.
          UNREACHABLE();
        } else {
          return BuildInt32BinarySmiOperationNode<kOperation>();
        }
      } else {
        return BuildFloat64BinarySmiOperationNodeForToNumber<kOperation>(
            allowed_input_type, conversion_type);
      }
      break;
    }
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      // Fallback to generic node.
      break;
  }
  BuildGenericBinarySmiOperationNode<kOperation>();
}

template <Operation kOperation, typename type>
bool OperationValue(type left, type right) {
  switch (kOperation) {
    case Operation::kEqual:
    case Operation::kStrictEqual:
      return left == right;
    case Operation::kLessThan:
      return left < right;
    case Operation::kLessThanOrEqual:
      return left <= right;
    case Operation::kGreaterThan:
      return left > right;
    case Operation::kGreaterThanOrEqual:
      return left >= right;
  }
}

// static
compiler::OptionalHeapObjectRef MaglevGraphBuilder::TryGetConstant(
    compiler::JSHeapBroker* broker, LocalIsolate* isolate, ValueNode* node) {
  if (Constant* c = node->TryCast<Constant>()) {
    return c->object();
  }
  if (RootConstant* c = node->TryCast<RootConstant>()) {
    return MakeRef(broker, isolate->root_handle(c->index())).AsHeapObject();
  }
  return {};
}

compiler::OptionalHeapObjectRef MaglevGraphBuilder::TryGetConstant(
    ValueNode* node, ValueNode** constant_node) {
  if (auto result = TryGetConstant(broker(), local_isolate(), node)) {
    if (constant_node) *constant_node = node;
    return result;
  }
  if (auto c = TryGetConstantAlternative(node)) {
    return TryGetConstant(*c, constant_node);
  }
  return {};
}

std::optional<ValueNode*> MaglevGraphBuilder::TryGetConstantAlternative(
    ValueNode* node) {
  const NodeInfo* info = known_node_aspects().TryGetInfoFor(node);
  if (info) {
    if (auto c = info->alternative().checked_value()) {
      if (IsConstantNode(c->opcode())) {
        return c;
      }
    }
  }
  return {};
}

template <Operation kOperation>
bool MaglevGraphBuilder::TryReduceCompareEqualAgainstConstant() {
  if (kOperation != Operation::kStrictEqual && kOperation != Operation::kEqual)
    return false;

  ValueNode* left = LoadRegister(0);
  ValueNode* right = GetAccumulator();

  ValueNode* other = right;
  compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(left);
  if (!maybe_constant) {
    maybe_constant = TryGetConstant(right);
    other = left;
  }
  if (!maybe_constant) return false;

  if (CheckType(other, NodeType::kBoolean)) {
    auto CompareOtherWith = |&](bool constant) {
      compiler::OptionalHeapObjectRef const_other = TryGetConstant(other);
      if (const_other) {
        auto bool_other = const_other->TryGetBooleanValue(broker());
        if (bool_other.has_value()) {
          SetAccumulator(GetBooleanConstant(constant == *bool_other));
          return;
        }
      }
      if (constant) {
        SetAccumulator(other);
      } else {
        SetAccumulator(AddNewNode<LogicalNot>({other}));
      }
    };

    if (maybe_constant.equals(broker_->true_value())) {
      CompareOtherWith(true);
      return true;
    } else if (maybe_constant.equals(broker_->false_value())) {
      CompareOtherWith(false);
      return true;
    } else if (kOperation == Operation::kEqual) {
      // For `bool == num` we can convert the actual comparison `ToNumber(bool)
      // == num` into `(num == 1) ? bool : ((num == 0) ? !bool : false)`,
      std::optional<double> val = {};
      if (maybe_constant.value().IsSmi()) {
        val = maybe_constant.value().AsSmi();
      } else if (maybe_constant.value().IsHeapNumber()) {
        val = maybe_constant.value().AsHeapNumber().value();
      }
      if (val) {
        if (*val == 0) {
          CompareOtherWith(false);
        } else if (*val == 1) {
          CompareOtherWith(true);
        } else {
          // The constant number is neither equal to `ToNumber(true)` nor
          // `ToNumber(false)`.
          SetAccumulator(GetBooleanConstant(false));
        }
        return true;
      }
    }
  }

  if (kOperation != Operation::kStrictEqual) return false;

  InstanceType type = maybe_constant.value().map(broker()).instance_type();
  if (!InstanceTypeChecker::IsReferenceComparable(type)) return false;

  // If the constant is the undefined value, we can compare it
  // against holey floats.
  if (maybe_constant->IsUndefined()) {
    ValueNode* holey_float = nullptr;
    if (left->properties().value_representation() ==
        ValueRepresentation::kHoleyFloat64) {
      holey_float = left;
    } else if (right->properties().value_representation() ==
               ValueRepresentation::kHoleyFloat64) {
      holey_float = right;
    }
    if (holey_float) {
      SetAccumulator(AddNewNode<HoleyFloat64IsHole>({holey_float}));
      return true;
    }
  }

  if (left->properties().value_representation() !=
          ValueRepresentation::kTagged ||
      right->properties().value_representation() !=
          ValueRepresentation::kTagged) {
    SetAccumulator(GetBooleanConstant(false));
  } else {
    SetAccumulator(BuildTaggedEqual(left, right));
  }
  return true;
}

template <Operation kOperation>
void MaglevGraphBuilder::VisitCompareOperation() {
  if (TryReduceCompareEqualAgainstConstant<kOperation>()) return;

  // Compare opcodes are not always commutative. We sort the ones which are for
  // better CSE coverage.
  auto SortCommute = |](ValueNode*& left, ValueNode*& right) {
    if (!v8_flags.maglev_cse) return;
    if (kOperation != Operation::kEqual &&
        kOperation != Operation::kStrictEqual) {
      return;
    }
    if (left > right) {
      std::swap(left, right);
    }
  };

  auto TryConstantFoldInt32 = |&](ValueNode* left, ValueNode* right) {
    if (left->Is<Int32Constant>() && right->Is<Int32Constant>()) {
      int left_value = left->Cast<Int32Constant>()->value();
      int right_value = right->Cast<Int32Constant>()->value();
      SetAccumulator(GetBooleanConstant(
          OperationValue<kOperation>(left_value, right_value)));
      return true;
    }
    return false;
  };

  auto TryConstantFoldEqual = |&](ValueNode* left, ValueNode* right) {
    if (left == right) {
      SetAccumulator(
          GetBooleanConstant(kOperation == Operation::kEqual ||
                             kOperation == Operation::kStrictEqual ||
                             kOperation == Operation::kLessThanOrEqual ||
                             kOperation == Operation::kGreaterThanOrEqual));
      return true;
    }
    return false;
  };

  auto MaybeOddballs = |&]() {
    auto MaybeOddball = |&](ValueNode* value) {
      ValueRepresentation rep = value->value_representation();
      switch (rep) {
        case ValueRepresentation::kInt32:
        case ValueRepresentation::kUint32:
        case ValueRepresentation::kFloat64:
          return false;
        default:
          break;
      }
      return !CheckType(value, NodeType::kNumber);
    };
    return MaybeOddball(LoadRegister(0)) || MaybeOddball(GetAccumulator());
  };

  FeedbackNexus nexus = FeedbackNexusForOperand(1);
  switch (nexus.GetCompareOperationFeedback()) {
    case CompareOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForCompareOperation));

    case CompareOperationHint::kSignedSmall: {
      // TODO(victorgomes): Add a smart equality operator, that compares for
      // constants in different representations.
      ValueNode* left = GetInt32(LoadRegister(0));
      ValueNode* right = GetInt32(GetAccumulator());
      if (TryConstantFoldEqual(left, right)) return;
      if (TryConstantFoldInt32(left, right)) return;
      SortCommute(left, right);
      SetAccumulator(AddNewNode<Int32Compare>({left, right}, kOperation));
      return;
    }
    case CompareOperationHint::kNumberOrOddball:
      // TODO(leszeks): we could support all kNumberOrOddball with
      // BranchIfFloat64Compare, but we'd need to special case comparing
      // oddballs with NaN value (e.g. undefined) against themselves.
      if (MaybeOddballs()) {
        break;
      }
      ||fallthrough]];
    case CompareOperationHint::kNumberOrBoolean:
      if (kOperation == Operation::kStrictEqual && MaybeOddballs()) {
        break;
      }
      ||fallthrough]];
    case CompareOperationHint::kNumber: {
      ValueNode* left = LoadRegister(0);
      ValueNode* right = GetAccumulator();
      if (left->value_representation() == ValueRepresentation::kInt32 &&
          right->value_representation() == ValueRepresentation::kInt32) {
        if (TryConstantFoldEqual(left, right)) return;
        if (TryConstantFoldInt32(left, right)) return;
        SortCommute(left, right);
        SetAccumulator(AddNewNode<Int32Compare>({left, right}, kOperation));
        return;
      }
      // In compare operations, booleans should be converted to Float64 but
      // non-boolean oddballs shouldn't. Even if the feedback type was
      // kNumberOrOddball, we'd still pass
      // TaggedToFloat64ConversionType::kNumberOrBoolean.
      NodeType allowed_input_type;
      TaggedToFloat64ConversionType conversion_type;
      if (nexus.GetCompareOperationFeedback() ==
          CompareOperationHint::kNumberOrBoolean) {
        allowed_input_type = NodeType::kNumberOrBoolean;
        conversion_type = TaggedToFloat64ConversionType::kNumberOrBoolean;
      } else {
        allowed_input_type = NodeType::kNumber;
        conversion_type = TaggedToFloat64ConversionType::kOnlyNumber;
      }
      left = GetFloat64ForToNumber(left, allowed_input_type, conversion_type);
      right = GetFloat64ForToNumber(right, allowed_input_type, conversion_type);
      if (left->Is<Float64Constant>() && right->Is<Float64Constant>()) {
        double left_value = left->Cast<Float64Constant>()->value().get_scalar();
        double right_value =
            right->Cast<Float64Constant>()->value().get_scalar();
        SetAccumulator(GetBooleanConstant(
            OperationValue<kOperation>(left_value, right_value)));
        return;
      }
      SortCommute(left, right);
      SetAccumulator(AddNewNode<Float64Compare>({left, right}, kOperation));
      return;
    }
    case CompareOperationHint::kInternalizedString: {
      DCHECK(kOperation == Operation::kEqual ||
             kOperation == Operation::kStrictEqual);
      ValueNode *left, *right;
      if (IsRegisterEqualToAccumulator(0)) {
        left = right = GetInternalizedString(iterator_.GetRegisterOperand(0));
        SetAccumulator(GetRootConstant(RootIndex::kTrueValue));
        return;
      }
      left = GetInternalizedString(iterator_.GetRegisterOperand(0));
      right =
          GetInternalizedString(interpreter::Register::virtual_accumulator());
      if (TryConstantFoldEqual(left, right)) return;
      SetAccumulator(BuildTaggedEqual(left, right));
      return;
    }
    case CompareOperationHint::kSymbol: {
      DCHECK(kOperation == Operation::kEqual ||
             kOperation == Operation::kStrictEqual);

      ValueNode* left = LoadRegister(0);
      ValueNode* right = GetAccumulator();
      BuildCheckSymbol(left);
      BuildCheckSymbol(right);
      if (TryConstantFoldEqual(left, right)) return;
      SetAccumulator(BuildTaggedEqual(left, right));
      return;
    }
    case CompareOperationHint::kString: {
      ValueNode* left = LoadRegister(0);
      ValueNode* right = GetAccumulator();
      BuildCheckString(left);
      BuildCheckString(right);

      ValueNode* result;
      if (TryConstantFoldEqual(left, right)) return;
      ValueNode* tagged_left = GetTaggedValue(left);
      ValueNode* tagged_right = GetTaggedValue(right);
      switch (kOperation) {
        case Operation::kEqual:
        case Operation::kStrictEqual:
          result = AddNewNode<StringEqual>({tagged_left, tagged_right});
          break;
        case Operation::kLessThan:
          result = BuildCallBuiltin<Builtin::kStringLessThan>(
              {tagged_left, tagged_right});
          break;
        case Operation::kLessThanOrEqual:
          result = BuildCallBuiltin<Builtin::kStringLessThanOrEqual>(
              {tagged_left, tagged_right});
          break;
        case Operation::kGreaterThan:
          result = BuildCallBuiltin<Builtin::kStringGreaterThan>(
              {tagged_left, tagged_right});
          break;
        case Operation::kGreaterThanOrEqual:
          result = BuildCallBuiltin<Builtin::kStringGreaterThanOrEqual>(
              {tagged_left, tagged_right});
          break;
      }

      SetAccumulator(result);
      return;
    }
    case CompareOperationHint::kAny:
    case CompareOperationHint::kBigInt64:
    case CompareOperationHint::kBigInt:
    case CompareOperationHint::kReceiverOrNullOrUndefined:
      break;
    case CompareOperationHint::kReceiver: {
      DCHECK(kOperation == Operation::kEqual ||
             kOperation == Operation::kStrictEqual);

      ValueNode* left = LoadRegister(0);
[Step 9: Duration 9.56 seconds| Input tokens: 133,272 | Output tokens: 7,302]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 10 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'TryReduceArrayForEach|ArrayForEach', 'options': '-n maglev/maglev-graph-builder.cc'}                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                                                                                                                                          │
│ 'BeginLoop\\(|EndLoop\\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant|CheckMapsWithMigration|CheckMaps\\{|BuildLoadJSArrayLength|BuildLoadElements\\(', 'options': '-n                    │
│ maglev/maglev-graph-builder.cc'}                                                                                                                                                                             │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-ir.h', 'section': 4}                                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-ir.cc', 'section': 2}                                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n maglev/maglev-graph-builder.cc 'TryReduceArrayForEach|ArrayForEach' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': '-n maglev'}                                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CreateCatchContext|PushCatchContext|catch|Exception|exception|handler|SetPendingMessage', 'options': '-n maglev'}                                       │
Command: cd /Users/tarun/v8/v8/src && rg -n maglev/maglev-graph-builder.cc 'BeginLoop\(|EndLoop\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant|CheckMapsWithMigration|CheckMaps\{|BuildLoadJSArrayLength|BuildLoadElements\(' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-ir.h'
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ 'CheckArrayMaps|CheckFixedArrayBounds|CheckElementsKind|LoadFixedDoubleArrayElement|LoadFixedArrayElement|MaybeGrowFastElements|EnsureWritableFastElements', 'options': '-n maglev/maglev-graph-builder.cc'} │
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-ir.cc'Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'ThrowIfNotCallable' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                                                                                                                                          │

╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: rg: TryReduceArrayForEach|ArrayForEach: IO error for operation on TryReduceArrayForEach|ArrayForEach: No such file or directory (os error 2)

Command: cd /Users/tarun/v8/v8/src && rg -n maglev 'CreateCatchContext|PushCatchContext|catch|Exception|exception|handler|SetPendingMessage' | head -n 1000
Observations: rg: BeginLoop\(|EndLoop\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant|CheckMapsWithMigration|CheckMaps\{|BuildLoadJSArrayLength|BuildLoadElements\(: IO error for operation on
Command: cd /Users/tarun/v8/v8/src && sed -n '9001,10563p' '/Users/tarun/v8/v8/src/maglev/maglev-ir.h'
Command: cd /Users/tarun/v8/v8/src && rg -n maglev/maglev-graph-builder.cc 'CheckArrayMaps|CheckFixedArrayBounds|CheckElementsKind|LoadFixedDoubleArrayElement|LoadFixedArrayElement|MaybeGrowFastElements|EnsureWritableFastElements' | head -n 1000
Observations: rg: ThrowIfNotCallable: IO error for operation on ThrowIfNotCallable: No such file or directory (os error 2)
BeginLoop\(|EndLoop\(|BranchIfInt32Compare|BranchIfFloat64IsHole|BranchIfRootConstant|CheckMapsWithMigration|CheckMaps\{|BuildLoadJSArrayLength|BuildLoadElements\(: No such file or directory (os error 2)
CreateCatchContext|PushCatchContext|catch|Exception|exception|handler|SetPendingMessage: No such file or directory (os error 2)

Observations: rg: CreateCatchContext|PushCatchContext|catch|Exception|exception|handler|SetPendingMessage: IO error for operation on 
Observations: rg: CheckArrayMaps|CheckFixedArrayBounds|CheckElementsKind|LoadFixedDoubleArrayElement|LoadFixedArrayElement|MaybeGrowFastElements|EnsureWritableFastElements: IO error for operation on 
CheckArrayMaps|CheckFixedArrayBounds|CheckElementsKind|LoadFixedDoubleArrayElement|LoadFixedArrayElement|MaybeGrowFastElements|EnsureWritableFastElements: No such file or directory (os error 2)
Observations: Showing section 4/4 (lines 9001-10563) of 'maglev/maglev-ir.h':
  const Input& new_target() const { return input(kNewTargetIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }

  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }

  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  Builtin builtin_;
};

class CallForwardVarargs : public ValueNodeT<CallForwardVarargs> {
  using Base = ValueNodeT<CallForwardVarargs>;

 public:
  static constexpr int kFunctionIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kFixedInputCount = 2;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallForwardVarargs(uint64_t bitfield, ValueNode* function, ValueNode* context,
                     int start_index, Call::TargetType target_type)
      : Base(bitfield), start_index_(start_index), target_type_(target_type) {
    set_input(kFunctionIndex, function);
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& function() { return input(kFunctionIndex); }
  const Input& function() const { return input(kFunctionIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  int start_index() const { return start_index_; }
  Call::TargetType target_type() const { return target_type_; }

 private:
  int start_index_;
  Call::TargetType target_type_;
};

class CallRuntime : public ValueNodeT<CallRuntime> {
  using Base = ValueNodeT<CallRuntime>;

 public:
  // We assume the context as fixed input.
  static constexpr int kContextIndex = 0;
  static constexpr int kFixedInputCount = 1;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallRuntime(uint64_t bitfield, Runtime::FunctionId function_id,
              ValueNode* context)
      : Base(bitfield), function_id_(function_id) {
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Runtime::FunctionId function_id() const { return function_id_; }

  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  int ReturnCount() const {
    return Runtime::FunctionForId(function_id())->result_size;
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Runtime::FunctionId function_id_;
};

class CallWithSpread : public ValueNodeT<CallWithSpread> {
  using Base = ValueNodeT<CallWithSpread>;

 public:
  // We assume function and context as fixed inputs.
  static constexpr int kFunctionIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kFixedInputCount = 2;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallWithSpread(uint64_t bitfield, ValueNode* function, ValueNode* context)
      : Base(bitfield) {
    set_input(kFunctionIndex, function);
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& function() { return input(kFunctionIndex); }
  const Input& function() const { return input(kFunctionIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  int num_args_no_spread() const {
    DCHECK_GT(num_args(), 0);
    return num_args() - 1;
  }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args_no_spread() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args_no_spread() - 1)));
  }
  Input& spread() {
    // Spread is the last argument/input.
    return input(input_count() - 1);
  }
  Input& receiver() { return arg(0); }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CallWithArrayLike : public FixedInputValueNodeT<4, CallWithArrayLike> {
  using Base = FixedInputValueNodeT<4, CallWithArrayLike>;

 public:
  // We assume function and context as fixed inputs.
  static constexpr int kFunctionIndex = 0;
  static constexpr int kReceiverIndex = 1;
  static constexpr int kArgumentsListIndex = 2;
  static constexpr int kContextIndex = 3;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  explicit CallWithArrayLike(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties = OpProperties::JSCall();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged,
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& function() { return input(kFunctionIndex); }
  Input& receiver() { return input(kReceiverIndex); }
 private:
  Input& context() { return input(kContextIndex); }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CallSelf : public ValueNodeT<CallSelf> {
  using Base = ValueNodeT<CallSelf>;

 public:
  static constexpr int kClosureIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kReceiverIndex = 2;
  static constexpr int kNewTargetIndex = 3;
  static constexpr int kFixedInputCount = 4;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallSelf(uint64_t bitfield, int expected_parameter_count, ValueNode* closure,
           ValueNode* context, ValueNode* receiver, ValueNode* new_target)
      : Base(bitfield), expected_parameter_count_(expected_parameter_count) {
    set_input(kClosureIndex, closure);
    set_input(kContextIndex, context);
    set_input(kReceiverIndex, receiver);
    set_input(kNewTargetIndex, new_target);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& closure() { return input(kClosureIndex); }
  const Input& closure() const { return input(kClosureIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  const Input& receiver() const { return input(kReceiverIndex); }
  Input& new_target() { return input(kNewTargetIndex); }
  const Input& new_target() const { return input(kNewTargetIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  int expected_parameter_count_;
};

class CallKnownJSFunction : public ValueNodeT<CallKnownJSFunction> {
  using Base = ValueNodeT<CallKnownJSFunction>;

 public:
  static constexpr int kClosureIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kReceiverIndex = 2;
  static constexpr int kNewTargetIndex = 3;
  static constexpr int kFixedInputCount = 4;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  inline CallKnownJSFunction(
      uint64_t bitfield,
#ifdef V8_ENABLE_LEAPTIERING
      JSDispatchHandle dispatch_handle,
  Input& arguments_list() { return input(kArgumentsListIndex); }
      compiler::SharedFunctionInfoRef shared_function_info, ValueNode* closure,
      ValueNode* context, ValueNode* receiver, ValueNode* new_target);

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& closure() { return input(kClosureIndex); }
  const Input& closure() const { return input(kClosureIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  const Input& receiver() const { return input(kReceiverIndex); }
  Input& new_target() { return input(kNewTargetIndex); }
  const Input& new_target() const { return input(kNewTargetIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  compiler::SharedFunctionInfoRef shared_function_info() const {
    return shared_function_info_;
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  int expected_parameter_count() const { return expected_parameter_count_; }

 private:
#ifdef V8_ENABLE_LEAPTIERING
  JSDispatchHandle dispatch_handle_;
#endif
  const compiler::SharedFunctionInfoRef shared_function_info_;
  // Cache the expected parameter count so that we can access it in
  // MaxCallStackArgs without needing to unpark the local isolate.
  int expected_parameter_count_;
};

class CallKnownApiFunction : public ValueNodeT<CallKnownApiFunction> {
  using Base = ValueNodeT<CallKnownApiFunction>;

 public:
  enum Mode {
    // Use Builtin::kCallApiCallbackOptimizedNoProfiling.
    kNoProfiling,
    // Inline API call sequence into the generated code.
    kNoProfilingInlined,
    // Use Builtin::kCallApiCallbackOptimized.
    kGeneric,
  };

  static constexpr int kContextIndex = 0;
  static constexpr int kReceiverIndex = 1;
  static constexpr int kFixedInputCount = 2;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallKnownApiFunction(uint64_t bitfield, Mode mode,
                       compiler::FunctionTemplateInfoRef function_template_info,
                       compiler::OptionalJSObjectRef api_holder,
                       ValueNode* context, ValueNode* receiver)
      : Base(bitfield | ModeField::encode(mode)),
        function_template_info_(function_template_info),
        api_holder_(api_holder) {
    set_input(kContextIndex, context);
#endif
  }

  // TODO(ishell): introduce JSApiCall() which will take C++ ABI into account
  // when deciding which registers to splill.
  static constexpr OpProperties kProperties = OpProperties::JSCall();

  // Input& closure() { return input(kClosureIndex); }
  // const Input& closure() const { return input(kClosureIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  const Input& receiver() const { return input(kReceiverIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  Mode mode() const { return ModeField::decode(bitfield()); }

  compiler::FunctionTemplateInfoRef function_template_info() const {
    return function_template_info_;
  }
  compiler::OptionalJSObjectRef api_holder() const { return api_holder_; }

  bool inline_builtin() const { return mode() == kNoProfilingInlined; }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  using ModeField = NextBitField<Mode, 2>;

  void GenerateCallApiCallbackOptimizedInline(MaglevAssembler* masm,
                                              const ProcessingState& state);

  const compiler::FunctionTemplateInfoRef function_template_info_;
  const compiler::OptionalJSObjectRef api_holder_;
};

class ConstructWithSpread : public ValueNodeT<ConstructWithSpread> {
  using Base = ValueNodeT<ConstructWithSpread>;

 public:
  // We assume function and context as fixed inputs.
  static constexpr int kFunctionIndex = 0;
  static constexpr int kNewTargetIndex = 1;
  static constexpr int kContextIndex = 2;
  static constexpr int kFixedInputCount = 3;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  ConstructWithSpread(uint64_t bitfield, compiler::FeedbackSource feedback,
                      ValueNode* function, ValueNode* new_target,
                      ValueNode* context)
      : Base(bitfield), feedback_(feedback) {
    set_input(kFunctionIndex, function);
    set_input(kNewTargetIndex, new_target);
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& function() { return input(kFunctionIndex); }
  const Input& function() const { return input(kFunctionIndex); }
  Input& new_target() { return input(kNewTargetIndex); }
  const Input& new_target() const { return input(kNewTargetIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  int num_args_no_spread() const {
    DCHECK_GT(num_args(), 0);
    return num_args() - 1;
  }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  Input& spread() {
    // Spread is the last argument/input.
    return input(input_count() - 1);
  }
  auto args_no_spread() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args_no_spread() - 1)));
  }
  compiler::FeedbackSource feedback() const { return feedback_; }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  const compiler::FeedbackSource feedback_;
};

class ConvertReceiver : public FixedInputValueNodeT<1, ConvertReceiver> {
  using Base = FixedInputValueNodeT<1, ConvertReceiver>;

 public:
  explicit ConvertReceiver(uint64_t bitfield,
                           compiler::NativeContextRef native_context,
                           ConvertReceiverMode mode)
      : Base(bitfield), native_context_(native_context), mode_(mode) {}

  Input& receiver_input() { return input(0); }

  // The implementation currently calls runtime.
  static constexpr OpProperties kProperties =
      OpProperties::Call() | OpProperties::NotIdempotent();
  static constexpr
    set_input(kReceiverIndex, receiver);

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  auto options() const { return std::tuple{native_context_, mode_}; }

  compiler::NativeContextRef native_context() const { return native_context_; }
  ConvertReceiverMode mode() const { return mode_; }

 private:
  const compiler::NativeContextRef native_context_;
  ConvertReceiverMode mode_;
};

class CheckConstructResult
    : public FixedInputValueNodeT<2, CheckConstructResult> {
  using Base = FixedInputValueNodeT<2, CheckConstructResult>;

 public:
  explicit CheckConstructResult(uint64_t bitfield) : Base(bitfield) {}

  Input& construct_result_input() { return input(0); }
  Input& implicit_receiver_input() { return input(1); }

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CheckDerivedConstructResult
    : public FixedInputValueNodeT<1, CheckDerivedConstructResult> {
  using Base = FixedInputValueNodeT<1, CheckDerivedConstructResult>;

 public:
  explicit CheckDerivedConstructResult(uint64_t bitfield) : Base(bitfield) {}

  Input& construct_result_input() { return input(0); }

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  bool for_derived_constructor();

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CheckNotHole : public FixedInputNodeT<1, CheckNotHole> {
  using Base = FixedInputNodeT<1, CheckNotHole>;

 public:
  explicit CheckNotHole(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties = OpProperties::EagerDeopt();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& object_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ConvertHoleToUndefined
    : public FixedInputValueNodeT<1, ConvertHoleToUndefined> {
  using Base = FixedInputValueNodeT<1, ConvertHoleToUndefined>;

 public:
  explicit ConvertHoleToUndefined(uint64_t bitfield) : Base(bitfield) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};
  Input& object_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class HandleNoHeapWritesInterrupt
    : public FixedInputNodeT<0, HandleNoHeapWritesInterrupt> {
  using Base = FixedInputNodeT<0, HandleNoHeapWritesInterrupt>;

 public:
  explicit HandleNoHeapWritesInterrupt(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties = OpProperties::DeferredCall() |
                                              OpProperties::LazyDeopt() |
                                              OpProperties::NotIdempotent();

  void SetValueLocationConstraints() {}
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
  int MaxCallStackArgs() const { return 0; }
};

class ReduceInterruptBudgetForLoop
    : public FixedInputNodeT<0, ReduceInterruptBudgetForLoop> {
  using Base = FixedInputNodeT<0, ReduceInterruptBudgetForLoop>;

 public:
  explicit ReduceInterruptBudgetForLoop(uint64_t bitfield, int amount)
      : Base(bitfield), amount_(amount) {
    DCHECK_GT(amount, 0);
  }

  static constexpr OpProperties kProperties = OpProperties::DeferredCall() |
                                              OpProperties::LazyDeopt() |
                                              OpProperties::NotIdempotent();

  int amount() const { return amount_; }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  const int amount_;
};

class ReduceInterruptBudgetForReturn
    : public FixedInputNodeT<0, ReduceInterruptBudgetForReturn> {
  using Base = FixedInputNodeT<0, ReduceInterruptBudgetForReturn>;

 public:
  explicit ReduceInterruptBudgetForReturn(uint64_t bitfield, int amount)
      : Base(bitfield), amount_(amount) {
    DCHECK_GT(amount, 0);
  }

  static constexpr OpProperties kProperties =
      OpProperties::DeferredCall() | OpProperties::NotIdempotent();

  int amount() const { return amount_; }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  const int amount_;
};

class ThrowReferenceErrorIfHole
    : public FixedInputNodeT<1, ThrowReferenceErrorIfHole> {
  using Base = FixedInputNodeT<1, ThrowReferenceErrorIfHole>;

 public:
  explicit ThrowReferenceErrorIfHole(uint64_t bitfield,
                                     const compiler::NameRef name)
      : Base(bitfield), name_(name) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  compiler::NameRef name() const { return name_; }

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);


  auto options() const { return std::tuple{name_}; }

 private:
  const compiler::NameRef name_;
};

class ThrowSuperNotCalledIfHole
    : public FixedInputNodeT<1, ThrowSuperNotCalledIfHole> {
  using Base = FixedInputNodeT<1, ThrowSuperNotCalledIfHole>;

 public:
  explicit ThrowSuperNotCalledIfHole(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ThrowSuperAlreadyCalledIfNotHole
    : public FixedInputNodeT<1, ThrowSuperAlreadyCalledIfNotHole> {
  using Base = FixedInputNodeT<1, ThrowSuperAlreadyCalledIfNotHole>;

 public:
  explicit ThrowSuperAlreadyCalledIfNotHole(uint64_t bitfield)
      : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> {
  using Base = FixedInputNodeT<1, ThrowIfNotCallable>;

 public:
  explicit ThrowIfNotCallable(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ThrowIfNotSuperConstructor
    : public FixedInputNodeT<2, ThrowIfNotSuperConstructor> {
  using Base = FixedInputNodeT<2, ThrowIfNotSuperConstructor>;

 public:
  explicit ThrowIfNotSuperConstructor(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& constructor() { return Node::input(0); }
  Input& function() { return Node::input(1); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
class TransitionElementsKind
    : public FixedInputValueNodeT<2, TransitionElementsKind> {
  using Base = FixedInputValueNodeT<2, TransitionElementsKind>;

 public:
  explicit TransitionElementsKind(
      uint64_t bitfield, const ZoneVector<compiler::MapRef>& transition_sources,
      compiler::MapRef transition_target)
      : Base(bitfield),
        transition_sources_(transition_sources),
        transition_target_(transition_target) {}

  // TODO(leszeks): Special case the case where all transitions are fast.
  static constexpr OpProperties kProperties =
      OpProperties::AnySideEffects() | OpProperties::DeferredCall();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& object_input() { return input(0); }
  Input& map_input() { return input(1); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  const ZoneVector<compiler::MapRef>& transition_sources() const {
    return transition_sources_;
  }
  const compiler::MapRef transition_target() const {
    return transition_target_;
  }

 private:
  ZoneVector<compiler::MapRef> transition_sources_;
  const compiler::MapRef transition_target_;
};

class TransitionElementsKindOrCheckMap
    : public FixedInputNodeT<2, TransitionElementsKindOrCheckMap> {
  using Base = FixedInputNodeT<2, TransitionElementsKindOrCheckMap>;

 public:
  explicit TransitionElementsKindOrCheckMap(
      uint64_t bitfield, const ZoneVector<compiler::MapRef>& transition_sources,
      compiler::MapRef transition_target)
      : Base(bitfield),
        transition_sources_(transition_sources),
        transition_target_(transition_target) {}

  // TODO(leszeks): Special case the case where all transitions are fast.
  static constexpr OpProperties kProperties = OpProperties::AnySideEffects() |
                                              OpProperties::DeferredCall() |
                                              OpProperties::EagerDeopt();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& object_input() { return Node::input(0); }
  Input& map_input() { return Node::input(1); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  const ZoneVector<compiler::MapRef>& transition_sources() const {
    return transition_sources_;
  }
  const compiler::MapRef transition_target() const {
    return transition_target_;
  }

 private:
  ZoneVector<compiler::MapRef> transition_sources_;
  const compiler::MapRef transition_target_;
};

class GetContinuationPreservedEmbedderData
    : public FixedInputValueNodeT<0, GetContinuationPreservedEmbedderData> {
  using Base = FixedInputValueNodeT<0, GetContinuationPreservedEmbedderData>;

 public:
  explicit GetContinuationPreservedEmbedderData(uint64_t bitfield)
      : Base(bitfield) {}

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  static constexpr OpProperties kProperties =
      OpProperties::CanRead() | OpProperties::TaggedValue();
};

class SetContinuationPreservedEmbedderData
    : public FixedInputNodeT<1, SetContinuationPreservedEmbedderData> {
  using Base = FixedInputNodeT<1, SetContinuationPreservedEmbedderData>;

 public:
  explicit SetContinuationPreservedEmbedderData(uint64_t bitfield)
      : Base(bitfield) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& data_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  static constexpr OpProperties kProperties = OpProperties::CanWrite();
};

class ControlNode : public NodeBase {
 public:
  // A "hole" in control flow is a control node that unconditionally interrupts
  // linear control flow (either by jumping or by exiting).
  //
  // A "post-dominating" hole is a hole that is guaranteed to be be reached in
  // control flow after this node (i.e. it is a hole that is a post-dominator
  // of this node).
  ControlNode* next_post_dominating_hole() const {
    return next_post_dominating_hole_;
  }

    DCHECK_IMPLIES(node != nullptr, node->Is<UnconditionalControlNode>() ||
                                        node->Is<TerminalControlNode>() ||
                                        node->Is<Switch>());
    next_post_dominating_hole_ = node;
  }

 protected:
  using NodeBase::NodeBase;

 private:
  ControlNode* next_post_dominating_hole_ = nullptr;
};

class UnconditionalControlNode : public ControlNode {
 public:
  BasicBlock* target() const { return target_.block_ptr(); }
  int predecessor_id() const { return predecessor_id_; }
  void set_predecessor_id(int id) { predecessor_id_ = id; }

  void set_target(BasicBlock* block) { target_.set_block_ptr(block); }

 protected:
  explicit UnconditionalControlNode(uint64_t bitfield,
                                    BasicBlockRef* target_refs)
      : ControlNode(bitfield), target_(target_refs) {}
  explicit UnconditionalControlNode(uint64_t bitfield, BasicBlock* target)
      : ControlNode(bitfield), target_(target) {}

 private:
  BasicBlockRef target_;
  int predecessor_id_ = 0;
};

template <class Derived>
class UnconditionalControlNodeT
    : public FixedInputNodeTMixin<0, UnconditionalControlNode, Derived> {
  static_assert(IsUnconditionalControlNode(NodeBase::opcode_of<Derived>));

 protected:
  explicit UnconditionalControlNodeT(uint64_t bitfield,
                                     BasicBlockRef* target_refs)
      : FixedInputNodeTMixin<0, UnconditionalControlNode, Derived>(
            bitfield, target_refs) {}
  explicit UnconditionalControlNodeT(uint64_t bitfield, BasicBlock* target)
      : FixedInputNodeTMixin<0, UnconditionalControlNode, Derived>(bitfield,
                                                                   target) {}
};

class ConditionalControlNode : public ControlNode {
 public:
  explicit ConditionalControlNode(uint64_t bitfield) : ControlNode(bitfield) {}
};

class BranchControlNode : public ConditionalControlNode {
 public:
  BranchControlNode(uint64_t bitfield, BasicBlockRef* if_true_refs,
                    BasicBlockRef* if_false_refs)
      : ConditionalControlNode(bitfield),
        if_true_(if_true_refs),
        if_false_(if_false_refs) {}

  BasicBlock* if_true() const { return if_true_.block_ptr(); }
  BasicBlock* if_false() const { return if_false_.block_ptr(); }

  void set_if_true(BasicBlock* block) { if_true_.set_block_ptr(block); }
  void set_if_false(BasicBlock* block) { if_false_.set_block_ptr(block); }

 private:
  BasicBlockRef if_true_;
  BasicBlockRef if_false_;
};

class TerminalControlNode : public ControlNode {
 protected:
  explicit TerminalControlNode(uint64_t bitfield) : ControlNode(bitfield) {}
};

template <size_t InputCount, class Derived>
class TerminalControlNodeT
    : public FixedInputNodeTMixin<InputCount, TerminalControlNode, Derived> {
  static_assert(IsTerminalControlNode(NodeBase::opcode_of<Derived>));

 protected:
  explicit TerminalControlNodeT(uint64_t bitfield)
      : FixedInputNodeTMixin<InputCount, TerminalControlNode, Derived>(
            bitfield) {}
};

template <size_t InputCount, class Derived>
class BranchControlNodeT
    : public FixedInputNodeTMixin<InputCount, BranchControlNode, Derived> {
  static_assert(IsBranchControlNode(NodeBase::opcode_of<Derived>));

 protected:
  explicit BranchControlNodeT(uint64_t bitfield, BasicBlockRef* if_true_refs,
                              BasicBlockRef* if_false_refs)
      : FixedInputNodeTMixin<InputCount, BranchControlNode, Derived>(
            bitfield, if_true_refs, if_false_refs) {}
};

class Jump : public UnconditionalControlNodeT<Jump> {
  using Base = UnconditionalControlNodeT<Jump>;

 public:
  Jump(uint64_t bitfield, BasicBlockRef* target_refs)
      : Base(bitfield, target_refs) {}

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

// TODO(olivf): Unify implementation with Jump.
class CheckpointedJump : public UnconditionalControlNodeT<CheckpointedJump> {
  using Base = UnconditionalControlNodeT<CheckpointedJump>;

 public:
  CheckpointedJump(uint64_t bitfield, BasicBlockRef* target_refs)
      : Base(bitfield, target_refs) {}

  static constexpr OpProperties kProperties =
      OpProperties::DeoptCheckpoint() | Base::kProperties;
  void set_next_post_dominating_hole(ControlNode* node) {
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class JumpLoop : public UnconditionalControlNodeT<JumpLoop> {
  using Base = UnconditionalControlNodeT<JumpLoop>;

 public:
  explicit JumpLoop(uint64_t bitfield, BasicBlock* target)
      : Base(bitfield, target) {}

  explicit JumpLoop(uint64_t bitfield, BasicBlockRef* ref)
      : Base(bitfield, ref) {}

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  base::Vector<Input> used_nodes() { return used_node_locations_; }
  void set_used_nodes(base::Vector<Input> locations) {
    used_node_locations_ = locations;
  }

 private:
  base::Vector<Input> used_node_locations_;
};

class Abort : public TerminalControlNodeT<0, Abort> {
  using Base = TerminalControlNodeT<0, Abort>;

 public:
  explicit Abort(uint64_t bitfield, AbortReason reason)
      : Base(bitfield), reason_(reason) {
    DCHECK_EQ(NodeBase::opcode(), opcode_of<Abort>);
  }

  static constexpr OpProperties kProperties = OpProperties::Call();

  AbortReason reason() const { return reason_; }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  const AbortReason reason_;
};

class Return : public TerminalControlNodeT<1, Return> {
  using Base = TerminalControlNodeT<1, Return>;

 public:
  explicit Return(uint64_t bitfield) : Base(bitfield) {
    DCHECK_EQ(NodeBase::opcode(), opcode_of<Return>);
  }

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class Deopt : public TerminalControlNodeT<0, Deopt> {
  using Base = TerminalControlNodeT<0, Deopt>;

 public:
  explicit Deopt(uint64_t bitfield, DeoptimizeReason reason)
      : Base(bitfield), reason_(reason) {
    DCHECK_EQ(NodeBase::opcode(), opcode_of<Deopt>);
  }

  static constexpr OpProperties kProperties = OpProperties::EagerDeopt();

  DeoptimizeReason reason() const { return reason_; }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  DeoptimizeReason reason_;
};

class Switch : public FixedInputNodeTMixin<1, ConditionalControlNode, Switch> {
  using Base = FixedInputNodeTMixin<1, ConditionalControlNode, Switch>;

 public:
  explicit Switch(uint64_t bitfield, int value_base, BasicBlockRef* targets,
                  int size)

        value_base_(value_base),
        targets_(targets),
        size_(size),
        fallthrough_() {}

  explicit Switch(uint64_t bitfield, int value_base, BasicBlockRef* targets,
                  int size, BasicBlockRef* fallthrough)
      : Base(bitfield),
        value_base_(value_base),
        targets_(targets),
        size_(size),
        fallthrough_(fallthrough) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kInt32};

  int value_base() const { return value_base_; }
  BasicBlockRef* targets() const { return targets_; }
  int size() const { return size_; }

  bool has_fallthrough() const { return fallthrough_.has_value(); }
  BasicBlock* fallthrough() const {
    DCHECK(has_fallthrough());
    return fallthrough_.value().block_ptr();
  }

  void set_fallthrough(BasicBlock* fallthrough) {
    DCHECK(has_fallthrough());
    fallthrough_.value().set_block_ptr(fallthrough);
  }

  Input& value() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  const int value_base_;
  BasicBlockRef* targets_;
  const int size_;
  std::optional<BasicBlockRef> fallthrough_;
};

class BranchIfSmi : public BranchControlNodeT<1, BranchIfSmi> {
  using Base = BranchControlNodeT<1, BranchIfSmi>;

 public:
  explicit BranchIfSmi(uint64_t bitfield, BasicBlockRef* if_true_refs,
                       BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfRootConstant
    : public BranchControlNodeT<1, BranchIfRootConstant> {
  using Base = BranchControlNodeT<1, BranchIfRootConstant>;

 public:
  explicit BranchIfRootConstant(uint64_t bitfield, RootIndex root_index,
                                BasicBlockRef* if_true_refs,
                                BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), root_index_(root_index) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  RootIndex root_index() { return root_index_; }
  Input& condition_input() { return input(0); }

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  RootIndex root_index_;
};

class BranchIfUndefinedOrNull
    : public BranchControlNodeT<1, BranchIfUndefinedOrNull> {
  using Base = BranchControlNodeT<1, BranchIfUndefinedOrNull>;

 public:
  explicit BranchIfUndefinedOrNull(uint64_t bitfield,
                                   BasicBlockRef* if_true_refs,
                                   BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfUndetectable
    : public BranchControlNodeT<1, BranchIfUndetectable> {
  using Base = BranchControlNodeT<1, BranchIfUndetectable>;

 public:
  explicit BranchIfUndetectable(uint64_t bitfield, CheckType check_type,
                                BasicBlockRef* if_true_refs,
                                BasicBlockRef* if_false_refs)
      : Base(CheckTypeBitField::update(bitfield, check_type), if_true_refs,
             if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};
      : Base(bitfield),
  Input& condition_input() { return input(0); }
  CheckType check_type() const { return CheckTypeBitField::decode(bitfield()); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  using CheckTypeBitField = NextBitField<CheckType, 1>;
};

class BranchIfJSReceiver : public BranchControlNodeT<1, BranchIfJSReceiver> {
  using Base = BranchControlNodeT<1, BranchIfJSReceiver>;

 public:
  explicit BranchIfJSReceiver(uint64_t bitfield, BasicBlockRef* if_true_refs,
                              BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfToBooleanTrue
    : public BranchControlNodeT<1, BranchIfToBooleanTrue> {
  using Base = BranchControlNodeT<1, BranchIfToBooleanTrue>;

 public:
  explicit BranchIfToBooleanTrue(uint64_t bitfield, CheckType check_type,
                                 BasicBlockRef* if_true_refs,
                                 BasicBlockRef* if_false_refs)
      : Base(CheckTypeBitField::update(bitfield, check_type), if_true_refs,
             if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }
  CheckType check_type() const { return CheckTypeBitField::decode(bitfield()); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  using CheckTypeBitField = NextBitField<CheckType, 1>;
};

class BranchIfInt32ToBooleanTrue
    : public BranchControlNodeT<1, BranchIfInt32ToBooleanTrue> {
  using Base = BranchControlNodeT<1, BranchIfInt32ToBooleanTrue>;

 public:
  explicit BranchIfInt32ToBooleanTrue(uint64_t bitfield,
                                      BasicBlockRef* if_true_refs,
                                      BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kInt32};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfFloat64ToBooleanTrue
    : public BranchControlNodeT<1, BranchIfFloat64ToBooleanTrue> {
  using Base = BranchControlNodeT<1, BranchIfFloat64ToBooleanTrue>;

 public:
  explicit BranchIfFloat64ToBooleanTrue(uint64_t bitfield,
                                        BasicBlockRef* if_true_refs,
                                        BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kHoleyFloat64};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfFloat64IsHole
    : public BranchControlNodeT<1, BranchIfFloat64IsHole> {
  using Base = BranchControlNodeT<1, BranchIfFloat64IsHole>;

 public:
  explicit BranchIfFloat64IsHole(uint64_t bitfield, BasicBlockRef* if_true_refs,
                                 BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kHoleyFloat64};



  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfInt32Compare
    : public BranchControlNodeT<2, BranchIfInt32Compare> {
  using Base = BranchControlNodeT<2, BranchIfInt32Compare>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfInt32Compare(uint64_t bitfield, Operation operation,
                                BasicBlockRef* if_true_refs,
                                BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kInt32, ValueRepresentation::kInt32};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Operation operation() const { return operation_; }

 private:
  Operation operation_;
};

class BranchIfUint32Compare
    : public BranchControlNodeT<2, BranchIfUint32Compare> {
  using Base = BranchControlNodeT<2, BranchIfUint32Compare>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfUint32Compare(uint64_t bitfield, Operation operation,
                                 BasicBlockRef* if_true_refs,
                                 BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kUint32, ValueRepresentation::kUint32};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Operation operation() const { return operation_; }

 private:
  Operation operation_;
};

class BranchIfFloat64Compare
    : public BranchControlNodeT<2, BranchIfFloat64Compare> {
  using Base = BranchControlNodeT<2, BranchIfFloat64Compare>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfFloat64Compare(uint64_t bitfield, Operation operation,
                                  BasicBlockRef* if_true_refs,
                                  BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kFloat64, ValueRepresentation::kFloat64};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Operation operation() const { return operation_; }

 private:
  Operation operation_;
};

class BranchIfReferenceEqual
  Input& condition_input() { return input(0); }
  using Base = BranchControlNodeT<2, BranchIfReferenceEqual>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfReferenceEqual(uint64_t bitfield,
                                  BasicBlockRef* if_true_refs,
                                  BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfTypeOf : public BranchControlNodeT<1, BranchIfTypeOf> {
  using Base = BranchControlNodeT<1, BranchIfTypeOf>;

 public:
  static constexpr int kValueIndex = 0;
  Input& value_input() { return NodeBase::input(kValueIndex); }

  explicit BranchIfTypeOf(uint64_t bitfield,
                          interpreter::TestTypeOfFlags::LiteralFlag literal,
                          BasicBlockRef* if_true_refs,
                          BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), literal_(literal) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  interpreter::TestTypeOfFlags::LiteralFlag literal_;
};

constexpr inline OpProperties StaticPropertiesForOpcode(Opcode opcode) {
  switch (opcode) {
#define CASE(op)      \
  case Opcode::k##op: \
    return op::kProperties;
    NODE_BASE_LIST(CASE)
#undef CASE
  }
}

template <typename Function>
inline void NodeBase::ForAllInputsInRegallocAssignmentOrder(Function&& f) {
  auto iterate_inputs = |&](InputAllocationPolicy category) {
    for (Input& input : *this) {
      switch (compiler::UnallocatedOperand::cast(input.operand())
                  .extended_policy()) {
        case compiler::UnallocatedOperand::MUST_HAVE_REGISTER:
          if (category == InputAllocationPolicy::kArbitraryRegister)
            f(category, &input);
          break;

        case compiler::UnallocatedOperand::REGISTER_OR_SLOT_OR_CONSTANT:
          if (category == InputAllocationPolicy::kAny) f(category, &input);
          break;

        case compiler::UnallocatedOperand::FIXED_REGISTER:
        case compiler::UnallocatedOperand::FIXED_FP_REGISTER:
          if (category == InputAllocationPolicy::kFixedRegister)
            f(category, &input);
          break;

        case compiler::UnallocatedOperand::REGISTER_OR_SLOT:
        case compiler::UnallocatedOperand::SAME_AS_INPUT:
        case compiler::UnallocatedOperand::NONE:
        case compiler::UnallocatedOperand::MUST_HAVE_SLOT:
          UNREACHABLE();
      }
    }
  };

  iterate_inputs(InputAllocationPolicy::kFixedRegister);
  iterate_inputs(InputAllocationPolicy::kArbitraryRegister);
  iterate_inputs(InputAllocationPolicy::kAny);
}

NodeType StaticTypeForNode(compiler::JSHeapBroker* broker,
                           LocalIsolate* isolate, ValueNode* node);

}  // namespace maglev
}  // namespace internal
}  // namespace v8

#endif  // V8_MAGLEV_MAGLEV_IR_H_
    : public BranchControlNodeT<2, BranchIfReferenceEqual> {
Command: cd /Users/tarun/v8/v8/src && sed -n '3001,6000p' '/Users/tarun/v8/v8/src/maglev/maglev-ir.cc'
Observations: Showing section 2/3 (lines 3001-6000) of 'maglev/maglev-ir.cc':

  int element_size = compiler::ExternalArrayElementSize(type_);

  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register data_pointer = temps.Acquire();

  // We need to make sure we don't clobber is_little_endian_input by writing to
  // the result register.
  Register reg_with_result = result_reg;
  if (type_ != ExternalArrayType::kExternalInt8Array &&
      !is_little_endian_constant() &&
      result_reg == ToRegister(is_little_endian_input())) {
    reg_with_result = data_pointer;
  }

  // Load data pointer.
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));
  MemOperand element_address = __ DataViewElementOperand(data_pointer, index);
  __ LoadSignedField(reg_with_result, element_address, element_size);

  // We ignore little endian argument if type is a byte size.
  if (type_ != ExternalArrayType::kExternalInt8Array) {
    if (is_little_endian_constant()) {
      if (!V8_TARGET_BIG_ENDIAN_BOOL &&
          !FromConstantToBool(masm, is_little_endian_input().node())) {
        DCHECK_EQ(reg_with_result, result_reg);
        __ ReverseByteOrder(result_reg, element_size);
      }
    } else {
      ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
      DCHECK_NE(reg_with_result, ToRegister(is_little_endian_input()));
      __ ToBoolean(
          ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
          V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
          V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order,
          false);
      __ bind(*reverse_byte_order);
      __ ReverseByteOrder(reg_with_result, element_size);
      __ bind(*keep_byte_order);
      if (reg_with_result != result_reg) {
        __ Move(result_reg, reg_with_result);
      }
    }
  }
}

void StoreSignedIntDataViewElement::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(index_input());
  if (compiler::ExternalArrayElementSize(type_) > 1) {
    UseAndClobberRegister(value_input());
  } else {
    UseRegister(value_input());
  }
  if (is_little_endian_constant() ||
      type_ == ExternalArrayType::kExternalInt8Array) {
    UseAny(is_little_endian_input());
  } else {
    UseRegister(is_little_endian_input());
  }
  set_temporaries_needed(1);
}
void StoreSignedIntDataViewElement::GenerateCode(MaglevAssembler* masm,
                                                 const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  Register value = ToRegister(value_input());

  if (v8_flags.debug_code) {
    __ AssertObjectTypeInRange(object,
                               FIRST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               LAST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               AbortReason::kUnexpectedValue);
  }

  int element_size = compiler::ExternalArrayElementSize(type_);

  // We ignore little endian argument if type is a byte size.
  if (element_size > 1) {
    if (is_little_endian_constant()) {
      if (!V8_TARGET_BIG_ENDIAN_BOOL &&
          !FromConstantToBool(masm, is_little_endian_input().node())) {
        __ ReverseByteOrder(value, element_size);
      }
    } else {
      ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
      __ ToBoolean(
          ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
          V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
          V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order,
          false);
      __ bind(*reverse_byte_order);
      __ ReverseByteOrder(value, element_size);
      __ bind(*keep_byte_order);
    }
  }

  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register data_pointer = temps.Acquire();
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));
  MemOperand element_address = __ DataViewElementOperand(data_pointer, index);
  __ StoreField(element_address, value, element_size);
}

void LoadDoubleDataViewElement::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(index_input());
  if (is_little_endian_constant()) {
    UseAny(is_little_endian_input());
  } else {
    UseRegister(is_little_endian_input());
  }
  set_temporaries_needed(1);
  DefineAsRegister(this);
}
void LoadDoubleDataViewElement::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  DoubleRegister result_reg = ToDoubleRegister(result());
  Register data_pointer = temps.Acquire();

  if (v8_flags.debug_code) {
    __ AssertObjectTypeInRange(object,
                               FIRST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               LAST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               AbortReason::kUnexpectedValue);
  }

  // Load data pointer.
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));

  if (is_little_endian_constant()) {
    if (!V8_TARGET_BIG_ENDIAN_BOOL &&
        FromConstantToBool(masm, is_little_endian_input().node())) {
      __ LoadUnalignedFloat64(result_reg, data_pointer, index);
    } else {
      __ LoadUnalignedFloat64AndReverseByteOrder(result_reg, data_pointer,
                                                 index);
    }
  } else {
    Label done;
    ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
    // TODO(leszeks): We're likely to be calling this on an existing boolean --
    // maybe that's a case we should fast-path here and re-use that boolean
    // value?
    __ ToBoolean(
        ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
        V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
        V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order, true);
    __ bind(*keep_byte_order);
    __ LoadUnalignedFloat64(result_reg, data_pointer, index);
    __ Jump(&done);
    // We should swap the bytes if big endian.
    __ bind(*reverse_byte_order);
    __ LoadUnalignedFloat64AndReverseByteOrder(result_reg, data_pointer, index);
    __ bind(&done);
  }
}

void StoreDoubleDataViewElement::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(index_input());
  UseRegister(value_input());
  if (is_little_endian_constant()) {
    UseAny(is_little_endian_input());
  } else {
    UseRegister(is_little_endian_input());
  }
  set_temporaries_needed(1);
}
void StoreDoubleDataViewElement::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  DoubleRegister value = ToDoubleRegister(value_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register data_pointer = temps.Acquire();

  if (v8_flags.debug_code) {
    __ AssertObjectTypeInRange(object,
                               FIRST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               LAST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               AbortReason::kUnexpectedValue);
  }

  // Load data pointer.
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));

  if (is_little_endian_constant()) {
    if (!V8_TARGET_BIG_ENDIAN_BOOL &&
        FromConstantToBool(masm, is_little_endian_input().node())) {
      __ StoreUnalignedFloat64(data_pointer, index, value);
    } else {
      __ ReverseByteOrderAndStoreUnalignedFloat64(data_pointer, index, value);
    }
  } else {
    Label done;
    ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
    // TODO(leszeks): We're likely to be calling this on an existing boolean --
    // maybe that's a case we should fast-path here and re-use that boolean
    // value?
    __ ToBoolean(
        ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
        V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
        V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order, true);
    __ bind(*keep_byte_order);
    __ StoreUnalignedFloat64(data_pointer, index, value);
    __ Jump(&done);
    // We should swap the bytes if big endian.
    __ bind(*reverse_byte_order);
    __ ReverseByteOrderAndStoreUnalignedFloat64(data_pointer, index, value);
    __ bind(&done);
  }
}


void LoadEnumCacheLength::SetValueLocationConstraints() {
  UseRegister(map_input());
  DefineAsRegister(this);
}
void LoadEnumCacheLength::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register map = ToRegister(map_input());
  Register result_reg = ToRegister(result());
  __ AssertMap(map);
  __ LoadBitField<Map::Bits3::EnumLengthBits>(
      result_reg, FieldMemOperand(map, Map::kBitField3Offset));
}

int LoadGlobal::MaxCallStackArgs() const {
  if (typeof_mode() == TypeofMode::kNotInside) {
    using D = CallInterfaceDescriptorFor<Builtin::kLoadGlobalIC>::type;
    return D::GetStackParameterCount();
  } else {
    using D =
        CallInterfaceDescriptorFor<Builtin::kLoadGlobalICInsideTypeof>::type;
    return D::GetStackParameterCount();
  }
}
void LoadGlobal::SetValueLocationConstraints() {
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}
void LoadGlobal::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  if (typeof_mode() == TypeofMode::kNotInside) {
    __ CallBuiltin<Builtin::kLoadGlobalIC>(
        context(),                                    // context
        name().object(),                              // name
        TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
        feedback().vector                             // feedback vector
    );
  } else {
    DCHECK_EQ(typeof_mode(), TypeofMode::kInside);
    __ CallBuiltin<Builtin::kLoadGlobalICInsideTypeof>(
        context(),                                    // context
        name().object(),                              // name
        TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
        feedback().vector                             // feedback vector
    );
  }

  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int StoreGlobal::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreGlobalIC>::type;
  return D::GetStackParameterCount();
}
void StoreGlobal::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreGlobalIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void StoreGlobal::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStoreGlobalIC>(
      context(),                                    // context
      name().object(),                              // name
      value(),                                      // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void CheckValue::SetValueLocationConstraints() { UseRegister(target_input()); }
void CheckValue::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  Register target = ToRegister(target_input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  __ CompareTaggedAndJumpIf(target, value().object(), kNotEqual, fail);
}

void CheckValueEqualsInt32::SetValueLocationConstraints() {
  UseRegister(target_input());
}
void CheckValueEqualsInt32::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  Register target = ToRegister(target_input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  __ CompareInt32AndJumpIf(target, value(), kNotEqual, fail);
}

void CheckValueEqualsFloat64::SetValueLocationConstraints() {
  UseRegister(target_input());
  set_double_temporaries_needed(1);
}
void CheckValueEqualsFloat64::GenerateCode(MaglevAssembler* masm,
                                           const ProcessingState& state) {
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  DoubleRegister scratch = temps.AcquireDouble();
  DoubleRegister target = ToDoubleRegister(target_input());
  __ Move(scratch, value());
  __ CompareFloat64AndJumpIf(scratch, target, kNotEqual, fail, fail);
}

void CheckFloat64IsNan::SetValueLocationConstraints() {
  UseRegister(target_input());
}
void CheckFloat64IsNan::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  DoubleRegister target = ToDoubleRegister(target_input());
  __ JumpIfNotNan(target, fail);
}

void CheckValueEqualsString::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStringEqual>::type;
  UseFixed(target_input(), D::GetRegisterParameter(D::kLeft));
  RequireSpecificTemporary(D::GetRegisterParameter(D::kLength));
}
void CheckValueEqualsString::GenerateCode(MaglevAssembler* masm,
                                          const ProcessingState& state) {
  using D = CallInterfaceDescriptorFor<Builtin::kStringEqual>::type;

  ZoneLabelRef end(masm);
  DCHECK_EQ(D::GetRegisterParameter(D::kLeft), ToRegister(target_input()));
  Register target = D::GetRegisterParameter(D::kLeft);
  // Maybe the string is internalized already, do a fast reference check first.
  __ CompareTaggedAndJumpIf(target, value().object(), kEqual, *end,
                            Label::kNear);

  __ EmitEagerDeoptIfSmi(this, target, DeoptimizeReason::kWrongValue);
  __ JumpIfString(
      target,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, CheckValueEqualsString* node,
             ZoneLabelRef end) {
            Register target = D::GetRegisterParameter(D::kLeft);
            Register string_length = D::GetRegisterParameter(D::kLength);
            __ StringLength(string_length, target);
            Label* fail = __ GetDeoptLabel(node, DeoptimizeReason::kWrongValue);
            __ CompareInt32AndJumpIf(string_length, node->value().length(),
                                     kNotEqual, fail);
            RegisterSnapshot snapshot = node->register_snapshot();
            {
              SaveRegisterStateForCall save_register_state(masm, snapshot);
              __ CallBuiltin<Builtin::kStringEqual>(
                  node->target_input(),    // left
                  node->value().object(),  // right
                  string_length            // length
              );
              save_register_state.DefineSafepoint();
              // Compare before restoring registers, so that the deopt below has
              // the correct register set.
              __ CompareRoot(kReturnRegister0, RootIndex::kTrueValue);
            }
            __ EmitEagerDeoptIf(kNotEqual, DeoptimizeReason::kWrongValue, node);
            __ Jump(*end);
          },
          this, end));

  __ EmitEagerDeopt(this, DeoptimizeReason::kWrongValue);

  __ bind(*end);
}

void CheckDynamicValue::SetValueLocationConstraints() {
  UseRegister(first_input());
  UseRegister(second_input());
}
void CheckDynamicValue::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  Register first = ToRegister(first_input());
  Register second = ToRegister(second_input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  __ CompareTaggedAndJumpIf(first, second, kNotEqual, fail);
}

void CheckSmi::SetValueLocationConstraints() { UseRegister(receiver_input()); }
void CheckSmi::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  __ EmitEagerDeoptIfNotSmi(this, object, DeoptimizeReason::kNotASmi);
}

void CheckHeapObject::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckHeapObject::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kSmi);
}

void CheckSymbol::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckSymbol::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kNotASymbol);
  }
  __ JumpIfNotObjectType(object, SYMBOL_TYPE,
                         __ GetDeoptLabel(this, DeoptimizeReason::kNotASymbol));
}

void CheckInstanceType::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckInstanceType::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kWrongInstanceType);
  }
  if (first_instance_type_ == last_instance_type_) {
    __ JumpIfNotObjectType(
        object, first_instance_type_,
        __ GetDeoptLabel(this, DeoptimizeReason::kWrongInstanceType));
  } else {
    __ JumpIfObjectTypeNotInRange(
        object, first_instance_type_, last_instance_type_,
        __ GetDeoptLabel(this, DeoptimizeReason::kWrongInstanceType));
  }
}

void CheckCacheIndicesNotCleared::SetValueLocationConstraints() {
  UseRegister(indices_input());
  UseRegister(length_input());
}
void CheckCacheIndicesNotCleared::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  Register indices = ToRegister(indices_input());
  Register length = ToRegister(length_input());
  __ AssertNotSmi(indices);

  if (v8_flags.debug_code) {
    __ AssertObjectType(indices, FIXED_ARRAY_TYPE,
                        AbortReason::kOperandIsNotAFixedArray);
  }
  Label done;
  // If the cache length is zero, we don't have any indices, so we know this is
  // ok even though the indices are the empty array.
  __ CompareInt32AndJumpIf(length, 0, kEqual, &done);
  // Otherwise, an empty array with non-zero required length is not valid.
  __ JumpIfRoot(indices, RootIndex::kEmptyFixedArray,
                __ GetDeoptLabel(this, DeoptimizeReason::kWrongEnumIndices));
  __ bind(&done);
}

void CheckTypedArrayBounds::SetValueLocationConstraints() {
  UseRegister(index_input());
  UseRegister(length_input());
}
void CheckTypedArrayBounds::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  Register index = ToRegister(index_input());
  Register length = ToRegister(length_input());
  // The index must be a zero-extended Uint32 for this to work.
#ifdef V8_TARGET_ARCH_RISCV64
  // All Word32 values are been signed-extended in Register in RISCV.
  __ ZeroExtendWord(index, index);
#endif
  __ AssertZeroExtended(index);
  __ CompareIntPtrAndJumpIf(
      index, length, kUnsignedGreaterThanEqual,
      __ GetDeoptLabel(this, DeoptimizeReason::kOutOfBounds));
}

void CheckInt32Condition::SetValueLocationConstraints() {
  UseRegister(left_input());
  UseRegister(right_input());
}
void CheckInt32Condition::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Label* fail = __ GetDeoptLabel(this, reason());
  __ CompareInt32AndJumpIf(ToRegister(left_input()), ToRegister(right_input()),
                           NegateCondition(ToCondition(condition())), fail);
}

int StoreScriptContextSlotWithWriteBarrier::MaxCallStackArgs() const {
  return WriteBarrierDescriptor::GetStackParameterCount();
}

void StoreScriptContextSlotWithWriteBarrier::SetValueLocationConstraints() {
  UseFixed(context_input(), WriteBarrierDescriptor::ObjectRegister());
  UseRegister(new_value_input());
  set_temporaries_needed(2);
  set_double_temporaries_needed(1);
}

void StoreScriptContextSlotWithWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  __ RecordComment("StoreScriptContextSlotWithWriteBarrier");
  ZoneLabelRef done(masm);
  ZoneLabelRef do_normal_store(masm);

  // TODO(leszeks): Consider making this an arbitrary register and push/popping
  // in the deferred path.
  Register context = WriteBarrierDescriptor::ObjectRegister();
  Register new_value = ToRegister(new_value_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  Register old_value = temps.Acquire();

  __ AssertObjectType(context, SCRIPT_CONTEXT_TYPE,
                      AbortReason::kUnexpectedInstanceType);

  __ LoadTaggedField(old_value, context, offset());
  __ CompareTaggedAndJumpIf(old_value, new_value, kEqual, *done);

  // Load property.
  // TODO(victorgomes): Should we hoist the side_table?
  __ LoadTaggedField(
      scratch, context,
      Context::OffsetOfElementAt(Context::CONTEXT_SIDE_TABLE_PROPERTY_INDEX));
  __ LoadTaggedField(scratch, scratch,
                     FixedArray::OffsetOfElementAt(
                         index() - Context::MIN_CONTEXT_EXTENDED_SLOTS));

  __ CompareTaggedAndJumpIf(
      scratch, ContextSidePropertyCell::Other(), kNotEqual,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register context, Register old_value,
             Register new_value, Register property,
             StoreScriptContextSlotWithWriteBarrier* node, ZoneLabelRef done,
             ZoneLabelRef do_normal_store) {
            Label check_smi, check_mutable_int32, mutable_heap_number;
            __ CompareRootAndEmitEagerDeoptIf(
                property, RootIndex::kUndefinedValue, kEqual,
                DeoptimizeReason::kWrongValue, node);
            __ JumpIfSmi(property, &check_smi);
            __ AssertObjectType(property, CONTEXT_SIDE_PROPERTY_CELL_TYPE,
                                AbortReason::kUnexpectedInstanceType);
            __ LoadTaggedField(
                property, property,
                ContextSidePropertyCell::kPropertyDetailsRawOffset);
            __ bind(&check_smi);

            // Check for const case.
            __ CompareTaggedAndJumpIf(
                property, ContextSidePropertyCell::Const(), kEqual,
                __ GetDeoptLabel(node, DeoptimizeReason::kWrongValue));

            if (v8_flags.script_context_mutable_heap_number) {
              // Check for smi case
              __ CompareTaggedAndJumpIf(property,
                                        ContextSidePropertyCell::SmiMarker(),
                                        kNotEqual, &check_mutable_int32);
              __ EmitEagerDeoptIfNotSmi(node, new_value,
                                        DeoptimizeReason::kWrongValue);
              __ Jump(*do_normal_store);

              MaglevAssembler::TemporaryRegisterScope temps(masm);
              DoubleRegister double_scratch = temps.AcquireDouble();

              // Check mutable int32 case.
              __ bind(&check_mutable_int32);
              if (v8_flags.script_context_mutable_heap_int32) {
                __ CompareTaggedAndJumpIf(
                    property, ContextSidePropertyCell::MutableInt32(),
                    kNotEqual, &mutable_heap_number);
                {
                  Label new_value_is_not_smi;
                  Register new_value_int32 = property;
                  __ JumpIfNotSmi(new_value, &new_value_is_not_smi);
                  __ SmiUntag(new_value_int32, new_value);
                  __ StoreHeapInt32Value(new_value_int32, old_value);
                  __ Jump(*done);

                  __ bind(&new_value_is_not_smi);
                  __ CompareMapWithRoot(new_value, RootIndex::kHeapNumberMap,
                                        property);
                  __ EmitEagerDeoptIf(kNotEqual, DeoptimizeReason::kWrongValue,
                                      node);

                  __ LoadHeapNumberValue(double_scratch, new_value);
                  __ TryTruncateDoubleToInt32(
                      new_value_int32, double_scratch,
                      __ GetDeoptLabel(node, DeoptimizeReason::kWrongValue));
                  __ StoreHeapInt32Value(new_value_int32, old_value);
                  __ Jump(*done);
                }
              }

              // Check mutable heap number case.
              __ bind(&mutable_heap_number);
              {
                Label new_value_is_not_smi;
                Register new_value_int32 = property;
                __ JumpIfNotSmi(new_value, &new_value_is_not_smi);
                __ SmiUntag(new_value_int32, new_value);
                __ Int32ToDouble(double_scratch, new_value_int32);
                __ StoreHeapNumberValue(double_scratch, old_value);
                __ Jump(*done);

                __ bind(&new_value_is_not_smi);
                __ CompareMapWithRoot(new_value, RootIndex::kHeapNumberMap,
                                      property);
                __ EmitEagerDeoptIf(kNotEqual, DeoptimizeReason::kWrongValue,
                                    node);
                __ LoadHeapNumberValue(double_scratch, new_value);
                __ StoreHeapNumberValue(double_scratch, old_value);
                __ Jump(*done);
              }
            } else {
              __ Jump(*do_normal_store);
            }
          },
          context, old_value, new_value, scratch, this, done, do_normal_store));

  __ bind(*do_normal_store);
  __ StoreTaggedFieldWithWriteBarrier(
      context, offset(), new_value, register_snapshot(),
      new_value_input().node()->decompresses_tagged_result()
          ? MaglevAssembler::kValueIsDecompressed
          : MaglevAssembler::kValueIsCompressed,
      MaglevAssembler::kValueCanBeSmi);

  __ bind(*done);
}

void CheckString::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckString::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kNotAString);
  }
  __ JumpIfNotString(object,
                     __ GetDeoptLabel(this, DeoptimizeReason::kNotAString));
}

void CheckStringOrStringWrapper::SetValueLocationConstraints() {
  UseRegister(receiver_input());
  set_temporaries_needed(1);
}

void CheckStringOrStringWrapper::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  Register object = ToRegister(receiver_input());

  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object,
                           DeoptimizeReason::kNotAStringOrStringWrapper);
  }

  auto deopt =
      __ GetDeoptLabel(this, DeoptimizeReason::kNotAStringOrStringWrapper);
  Label done;

  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();

  __ JumpIfString(object, &done);
  __ JumpIfNotObjectType(object, InstanceType::JS_PRIMITIVE_WRAPPER_TYPE,
                         deopt);
  __ LoadMap(scratch, object);
  __ LoadBitField<Map::Bits2::ElementsKindBits>(
      scratch, FieldMemOperand(scratch, Map::kBitField2Offset));
  static_assert(FAST_STRING_WRAPPER_ELEMENTS + 1 ==
                SLOW_STRING_WRAPPER_ELEMENTS);
  __ CompareInt32AndJumpIf(scratch, FAST_STRING_WRAPPER_ELEMENTS, kLessThan,
                           deopt);
  __ CompareInt32AndJumpIf(scratch, SLOW_STRING_WRAPPER_ELEMENTS, kGreaterThan,
                           deopt);
  __ Jump(&done);
  __ bind(&done);
}

void CheckDetectableCallable::SetValueLocationConstraints() {
  UseRegister(receiver_input());
  set_temporaries_needed(1);
}

void CheckDetectableCallable::GenerateCode(MaglevAssembler* masm,
                                           const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  auto deopt = __ GetDeoptLabel(this, DeoptimizeReason::kNotDetectableReceiver);
  __ JumpIfNotCallable(object, scratch, check_type(), deopt);
  __ JumpIfUndetectable(object, scratch, CheckType::kOmitHeapObjectCheck,
                        deopt);
}

void CheckNotHole::SetValueLocationConstraints() {
  UseRegister(object_input());
}
void CheckNotHole::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  __ CompareRootAndEmitEagerDeoptIf(ToRegister(object_input()),
                                    RootIndex::kTheHoleValue, kEqual,
                                    DeoptimizeReason::kHole, this);
}

void ConvertHoleToUndefined::SetValueLocationConstraints() {
  UseRegister(object_input());
  DefineSameAsFirst(this);
}
void ConvertHoleToUndefined::GenerateCode(MaglevAssembler* masm,
                                          const ProcessingState& state) {
  Label done;
  DCHECK_EQ(ToRegister(object_input()), ToRegister(result()));
  __ JumpIfNotRoot(ToRegister(object_input()), RootIndex::kTheHoleValue, &done);
  __ LoadRoot(ToRegister(result()), RootIndex::kUndefinedValue);
  __ bind(&done);
}

int ConvertReceiver::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  return D::GetStackParameterCount();
}
void ConvertReceiver::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  static_assert(D::GetRegisterParameter(D::kInput) == kReturnRegister0);
  UseFixed(receiver_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void ConvertReceiver::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  Label convert_to_object, done;
  Register receiver = ToRegister(receiver_input());
  __ JumpIfSmi(
      receiver, &convert_to_object,
      v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);

  // If {receiver} is not primitive, no need to move it to {result}, since
  // they share the same register.
  DCHECK_EQ(receiver, ToRegister(result()));
  __ JumpIfJSAnyIsNotPrimitive(receiver, &done);

  compiler::JSHeapBroker* broker = masm->compilation_info()->broker();
  if (mode_ != ConvertReceiverMode::kNotNullOrUndefined) {
    Label convert_global_proxy;
    __ JumpIfRoot(receiver, RootIndex::kUndefinedValue, &convert_global_proxy,
                  Label::Distance::kNear);
    __ JumpIfNotRoot(
        receiver, RootIndex::kNullValue, &convert_to_object,
        v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);
    __ bind(&convert_global_proxy);
    // Patch receiver to global proxy.
    __ Move(ToRegister(result()),
            native_context_.global_proxy_object(broker).object());
    __ Jump(&done);
  }

  __ bind(&convert_to_object);
  __ CallBuiltin<Builtin::kToObject>(native_context_.object(),
                                     receiver_input());
  __ bind(&done);
}

int CheckDerivedConstructResult::MaxCallStackArgs() const { return 0; }
void CheckDerivedConstructResult::SetValueLocationConstraints() {
  UseRegister(construct_result_input());
  DefineSameAsFirst(this);
}
void CheckDerivedConstructResult::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  Register construct_result = ToRegister(construct_result_input());

  DCHECK_EQ(construct_result, ToRegister(result()));

  // If the result is an object (in the ECMA sense), we should get rid
  // of the receiver and use the result; see ECMA-262 section 13.2.2-7
  // on page 74.
  Label done, do_throw;

  __ CompareRoot(construct_result, RootIndex::kUndefinedValue);
  __ Assert(kNotEqual, AbortReason::kUnexpectedValue);

  // If the result is a smi, it is *not* an object in the ECMA sense.
  __ JumpIfSmi(construct_result, &do_throw, Label::Distance::kNear);

  // Check if the type of the result is not an object in the ECMA sense.
  __ JumpIfJSAnyIsNotPrimitive(construct_result, &done, Label::Distance::kNear);

  // Throw away the result of the constructor invocation and use the
  // implicit receiver as the result.
  __ bind(&do_throw);
  __ Jump(__ MakeDeferredCode(
      |](MaglevAssembler* masm, CheckDerivedConstructResult* node) {
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this));

  __ bind(&done);
}

int CheckConstructResult::MaxCallStackArgs() const { return 0; }
void CheckConstructResult::SetValueLocationConstraints() {
  UseRegister(construct_result_input());
  UseRegister(implicit_receiver_input());
  DefineSameAsFirst(this);
}
void CheckConstructResult::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  Register construct_result = ToRegister(construct_result_input());
  Register result_reg = ToRegister(result());

  DCHECK_EQ(construct_result, result_reg);

  // If the result is an object (in the ECMA sense), we should get rid
  // of the receiver and use the result; see ECMA-262 section 13.2.2-7
  // on page 74.
  Label done, use_receiver;

  // If the result is undefined, we'll use the implicit receiver.
  __ JumpIfRoot(construct_result, RootIndex::kUndefinedValue, &use_receiver,
                Label::Distance::kNear);

  // If the result is a smi, it is *not* an object in the ECMA sense.
  __ JumpIfSmi(construct_result, &use_receiver, Label::Distance::kNear);

  // Check if the type of the result is not an object in the ECMA sense.
  __ JumpIfJSAnyIsNotPrimitive(construct_result, &done, Label::Distance::kNear);

  // Throw away the result of the constructor invocation and use the
  // implicit receiver as the result.
  __ bind(&use_receiver);
  Register implicit_receiver = ToRegister(implicit_receiver_input());
  __ Move(result_reg, implicit_receiver);

  __ bind(&done);
}

int CreateObjectLiteral::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kCreateObjectLiteral)->nargs, 4);
  return 4;
}
void CreateObjectLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateObjectLiteral::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateObjectFromSlowBoilerplate>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      boilerplate_descriptor().object(),            // boilerplate descriptor
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CreateShallowArrayLiteral::MaxCallStackArgs() const {
  using D =
      CallInterfaceDescriptorFor<Builtin::kCreateShallowArrayLiteral>::type;
  return D::GetStackParameterCount();
}
void CreateShallowArrayLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateShallowArrayLiteral::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateShallowArrayLiteral>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      constant_elements().object(),                 // constant elements
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CreateArrayLiteral::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kCreateArrayLiteral)->nargs, 4);
  return 4;
}
void CreateArrayLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateArrayLiteral::GenerateCode(MaglevAssembler* masm,
                                      const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateArrayFromSlowBoilerplate>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      constant_elements().object(),                 // boilerplate descriptor
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CreateShallowObjectLiteral::MaxCallStackArgs() const {
  using D =
      CallInterfaceDescriptorFor<Builtin::kCreateShallowObjectLiteral>::type;
  return D::GetStackParameterCount();
}
void CreateShallowObjectLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateShallowObjectLiteral::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateShallowObjectLiteral>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      boilerplate_descriptor().object(),            // desc
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void AllocationBlock::SetValueLocationConstraints() { DefineAsRegister(this); }

void AllocationBlock::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ Allocate(register_snapshot(), ToRegister(result()), size(),
              allocation_type());
}

int CreateClosure::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(pretenured() ? Runtime::kNewClosure_Tenured
                                                : Runtime::kNewClosure)
                ->nargs,
            2);
  return 2;
}
void CreateClosure::SetValueLocationConstraints() {
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}
void CreateClosure::GenerateCode(MaglevAssembler* masm,
                                 const ProcessingState& state) {
  Runtime::FunctionId function_id =
      pretenured() ? Runtime::kNewClosure_Tenured : Runtime::kNewClosure;
  __ Push(shared_function_info().object(), feedback_cell().object());
  __ CallRuntime(function_id);
}

int FastCreateClosure::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kFastNewClosure>::type;
  return D::GetStackParameterCount();
}
void FastCreateClosure::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kFastNewClosure>::type;
  static_assert(D::HasContextParameter());
  UseFixed(context(), D::ContextRegister());
  DefineAsFixed(this, kReturnRegister0);
}
void FastCreateClosure::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  __ CallBuiltin<Builtin::kFastNewClosure>(
      context(),                        // context
      shared_function_info().object(),  // shared function info
      feedback_cell().object()          // feedback cell
  );
  masm->DefineLazyDeoptPoint(lazy_deopt_info());
}

int CreateFunctionContext::MaxCallStackArgs() const {
  if (scope_type() == FUNCTION_SCOPE) {
    using D = CallInterfaceDescriptorFor<
        Builtin::kFastNewFunctionContextFunction>::type;
    return D::GetStackParameterCount();
  } else {
    using D =
        CallInterfaceDescriptorFor<Builtin::kFastNewFunctionContextEval>::type;
    return D::GetStackParameterCount();
  }
}
void CreateFunctionContext::SetValueLocationConstraints() {
  DCHECK_LE(slot_count(),
            static_cast<uint32_t>(
                ConstructorBuiltins::MaximumFunctionContextSlots()));
  if (scope_type() == FUNCTION_SCOPE) {
    using D = CallInterfaceDescriptorFor<
        Builtin::kFastNewFunctionContextFunction>::type;
    static_assert(D::HasContextParameter());
    UseFixed(context(), D::ContextRegister());
  } else {
    DCHECK_EQ(scope_type(), ScopeType::EVAL_SCOPE);
    using D =
        CallInterfaceDescriptorFor<Builtin::kFastNewFunctionContextEval>::type;
    static_assert(D::HasContextParameter());
    UseFixed(context(), D::ContextRegister());
  }
  DefineAsFixed(this, kReturnRegister0);
}
void CreateFunctionContext::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  if (scope_type() == FUNCTION_SCOPE) {
    __ CallBuiltin<Builtin::kFastNewFunctionContextFunction>(
        context(),              // context
        scope_info().object(),  // scope info
        slot_count()            // slots
    );
  } else {
    __ CallBuiltin<Builtin::kFastNewFunctionContextEval>(
        context(),              // context
        scope_info().object(),  // scope info
        slot_count()            // slots
    );
  }
  masm->DefineLazyDeoptPoint(lazy_deopt_info());
}

int CreateRegExpLiteral::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kCreateRegExpLiteral>::type;
  return D::GetStackParameterCount();
}
void CreateRegExpLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateRegExpLiteral::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateRegExpLiteral>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      pattern().object(),                           // pattern
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineLazyDeoptPoint(lazy_deopt_info());
}

int GetTemplateObject::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kGetTemplateObject>::type;
  return D::GetStackParameterCount();
}
void GetTemplateObject::SetValueLocationConstraints() {
  using D = GetTemplateObjectDescriptor;
  UseFixed(description(), D::GetRegisterParameter(D::kDescription));
  DefineAsFixed(this, kReturnRegister0);
}
void GetTemplateObject::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  __ CallBuiltin<Builtin::kGetTemplateObject>(
      masm->native_context().object(),  // context
      shared_function_info_.object(),   // shared function info
      description(),                    // description
      feedback().index(),               // feedback slot
      feedback().vector                 // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int HasInPrototypeChain::MaxCallStackArgs() const {
  DCHECK_EQ(2, Runtime::FunctionForId(Runtime::kHasInPrototypeChain)->nargs);
  return 2;
}
void HasInPrototypeChain::SetValueLocationConstraints() {
  UseRegister(object());
  DefineAsRegister(this);
  set_temporaries_needed(2);
}
void HasInPrototypeChain::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register object_reg = ToRegister(object());
  Register result_reg = ToRegister(result());

  Label return_false, return_true;
  ZoneLabelRef done(masm);

  __ JumpIfSmi(object_reg, &return_false,
               v8_flags.debug_code ? Label::kFar : Label::kNear);

  // Loop through the prototype chain looking for the {prototype}.
  Register map = temps.Acquire();
  __ LoadMap(map, object_reg);
  Label loop;
  {
    __ bind(&loop);
    Register scratch = temps.Acquire();
    // Check if we can determine the prototype directly from the {object_map}.
    ZoneLabelRef if_objectisdirect(masm);
    Register instance_type = scratch;
    Condition jump_cond = __ CompareInstanceTypeRange(
        map, instance_type, FIRST_TYPE, LAST_SPECIAL_RECEIVER_TYPE);
    __ JumpToDeferredIf(
        jump_cond,
        |](MaglevAssembler* masm, RegisterSnapshot snapshot,
           Register object_reg, Register map, Register instance_type,
           Register result_reg, HasInPrototypeChain* node,
           ZoneLabelRef if_objectisdirect, ZoneLabelRef done) {
          Label return_runtime;
          // The {object_map} is a special receiver map or a primitive map,
          // check if we need to use the if_objectisspecial path in the runtime.
          __ JumpIfEqual(instance_type, JS_PROXY_TYPE, &return_runtime);

          int mask = Map::Bits1::HasNamedInterceptorBit::kMask |
                     Map::Bits1::IsAccessCheckNeededBit::kMask;
          __ TestUint8AndJumpIfAllClear(
              FieldMemOperand(map, Map::kBitFieldOffset), mask,
              *if_objectisdirect);

          __ bind(&return_runtime);
          {
            snapshot.live_registers.clear(result_reg);
            SaveRegisterStateForCall save_register_state(masm, snapshot);
            __ Push(object_reg, node->prototype().object());
            __ Move(kContextRegister, masm->native_context().object());
            __ CallRuntime(Runtime::kHasInPrototypeChain, 2);
            masm->DefineExceptionHandlerPoint(node);
            save_register_state.DefineSafepointWithLazyDeopt(
                node->lazy_deopt_info());
            __ Move(result_reg, kReturnRegister0);
          }
          __ Jump(*done);
        },
        register_snapshot(), object_reg, map, instance_type, result_reg, this,
        if_objectisdirect, done);
    instance_type = Register::no_reg();

    __ bind(*if_objectisdirect);
    // Check the current {object} prototype.
    Register object_prototype = scratch;
    __ LoadTaggedField(object_prototype, map, Map::kPrototypeOffset);
    __ JumpIfRoot(object_prototype, RootIndex::kNullValue, &return_false,
                  v8_flags.debug_code ? Label::kFar : Label::kNear);
    __ CompareTaggedAndJumpIf(object_prototype, prototype().object(), kEqual,
                              &return_true, Label::kNear);

    // Continue with the prototype.
    __ AssertNotSmi(object_prototype);
    __ LoadMap(map, object_prototype);
    __ Jump(&loop);
  }

  __ bind(&return_true);
  __ LoadRoot(result_reg, RootIndex::kTrueValue);
  __ Jump(*done, Label::kNear);

  __ bind(&return_false);
  __ LoadRoot(result_reg, RootIndex::kFalseValue);
  __ bind(*done);
}

void DebugBreak::SetValueLocationConstraints() {}
void DebugBreak::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  __ DebugBreak();
}

int Abort::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kAbort)->nargs, 1);
  return 1;
}
void Abort::SetValueLocationConstraints() {}
void Abort::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
  __ Push(Smi::FromInt(static_cast<int>(reason())));
  __ CallRuntime(Runtime::kAbort, 1);
  __ Trap();
}

void LogicalNot::SetValueLocationConstraints() {
  UseAny(value());
  DefineAsRegister(this);
}
void LogicalNot::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  if (v8_flags.debug_code) {
    // LogicalNot expects either TrueValue or FalseValue.
    Label next;
    __ JumpIf(__ IsRootConstant(value(), RootIndex::kFalseValue), &next);
    __ JumpIf(__ IsRootConstant(value(), RootIndex::kTrueValue), &next);
    __ Abort(AbortReason::kUnexpectedValue);
    __ bind(&next);
  }

  Label return_false, done;
  __ JumpIf(__ IsRootConstant(value(), RootIndex::kTrueValue), &return_false);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done);

  __ bind(&return_false);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);

  __ bind(&done);
}

int LoadNamedGeneric::MaxCallStackArgs() const {
  return LoadWithVectorDescriptor::GetStackParameterCount();
}
void LoadNamedGeneric::SetValueLocationConstraints() {
  using D = LoadWithVectorDescriptor;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  DefineAsFixed(this, kReturnRegister0);
}
void LoadNamedGeneric::GenerateCode(MaglevAssembler* masm,
                                    const ProcessingState& state) {
  __ CallBuiltin<Builtin::kLoadIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name().object(),                              // name
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int LoadNamedFromSuperGeneric::MaxCallStackArgs() const {
  return LoadWithReceiverAndVectorDescriptor::GetStackParameterCount();
}
void LoadNamedFromSuperGeneric::SetValueLocationConstraints() {
  using D = LoadWithReceiverAndVectorDescriptor;
  UseFixed(context(), kContextRegister);
  UseFixed(receiver(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(lookup_start_object(),
           D::GetRegisterParameter(D::kLookupStartObject));
  DefineAsFixed(this, kReturnRegister0);
}
void LoadNamedFromSuperGeneric::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ CallBuiltin<Builtin::kLoadSuperIC>(
      context(),                                    // context
      receiver(),                                   // receiver
      lookup_start_object(),                        // lookup start object
      name().object(),                              // name
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int SetNamedGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreIC>::type;
  return D::GetStackParameterCount();
}
void SetNamedGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void SetNamedGeneric::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStoreIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name().object(),                              // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int DefineNamedOwnGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineNamedOwnIC>::type;
  return D::GetStackParameterCount();
}
void DefineNamedOwnGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineNamedOwnIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void DefineNamedOwnGeneric::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  __ CallBuiltin<Builtin::kDefineNamedOwnIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name().object(),                              // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void UpdateJSArrayLength::SetValueLocationConstraints() {
  UseRegister(length_input());
  UseRegister(object_input());
  UseAndClobberRegister(index_input());
  DefineSameAsFirst(this);
}

void UpdateJSArrayLength::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register length = ToRegister(length_input());
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  DCHECK_EQ(length, ToRegister(result()));

  Label done, tag_length;
  if (v8_flags.debug_code) {
    __ AssertObjectType(object, JS_ARRAY_TYPE, AbortReason::kUnexpectedValue);
    static_assert(Internals::IsValidSmi(FixedArray::kMaxLength),
                  "MaxLength not a Smi");
    __ CompareInt32AndAssert(index, FixedArray::kMaxLength, kUnsignedLessThan,
                             AbortReason::kUnexpectedValue);
  }
  __ CompareInt32AndJumpIf(index, length, kUnsignedLessThan, &tag_length,
                           Label::kNear);
  __ IncrementInt32(index);  // This cannot overflow.
  __ SmiTag(length, index);
  __ StoreTaggedSignedField(object, JSArray::kLengthOffset, length);
  __ Jump(&done, Label::kNear);
  __ bind(&tag_length);
  __ SmiTag(length);
  __ bind(&done);
}

void EnsureWritableFastElements::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(object_input());
  set_temporaries_needed(1);
  DefineSameAsFirst(this);
}
void EnsureWritableFastElements::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register elements = ToRegister(elements_input());
  DCHECK_EQ(elements, ToRegister(result()));
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ EnsureWritableFastElements(register_snapshot(), elements, object, scratch);
}

void MaybeGrowFastElements::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(object_input());
  UseRegister(index_input());
  UseRegister(elements_length_input());
  if (IsSmiOrObjectElementsKind(elements_kind())) {
    set_temporaries_needed(1);
  }
  DefineSameAsFirst(this);
}
void MaybeGrowFastElements::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  Register elements = ToRegister(elements_input());
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  Register elements_length = ToRegister(elements_length_input());
  DCHECK_EQ(elements, ToRegister(result()));

  ZoneLabelRef done(masm);

  __ CompareInt32AndJumpIf(
      index, elements_length, kUnsignedGreaterThanEqual,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, ZoneLabelRef done, Register object,
             Register index, Register result_reg, MaybeGrowFastElements* node) {
            {
              RegisterSnapshot snapshot = node->register_snapshot();
              snapshot.live_registers.clear(result_reg);
              snapshot.live_tagged_registers.clear(result_reg);
              SaveRegisterStateForCall save_register_state(masm, snapshot);
              using D = GrowArrayElementsDescriptor;
              if (index == D::GetRegisterParameter(D::kObject)) {
                // That implies that the first parameter move will clobber the
                // index value. So we use the result register as temporary.
                // TODO(leszeks): Use parallel moves to resolve cases like this.
                __ SmiTag(result_reg, index);
                index = result_reg;
              } else {
                __ SmiTag(index);
              }
              if (IsDoubleElementsKind(node->elements_kind())) {
                __ CallBuiltin<Builtin::kGrowFastDoubleElements>(object, index);
              } else {
                __ CallBuiltin<Builtin::kGrowFastSmiOrObjectElements>(object,
                                                                      index);
              }
              save_register_state.DefineSafepoint();
              __ Move(result_reg, kReturnRegister0);
            }
            __ EmitEagerDeoptIfSmi(node, result_reg,
                                   DeoptimizeReason::kCouldNotGrowElements);
            __ Jump(*done);
          },
          done, object, index, elements, this));

  __ bind(*done);
}

void ExtendPropertiesBackingStore::SetValueLocationConstraints() {
  UseRegister(property_array_input());
  UseRegister(object_input());
  DefineAsRegister(this);
  set_temporaries_needed(2);
}

void ExtendPropertiesBackingStore::GenerateCode(MaglevAssembler* masm,
                                                const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register old_property_array = ToRegister(property_array_input());
  Register result_reg = ToRegister(result());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register new_property_array =
      result_reg == object || result_reg == old_property_array ? temps.Acquire()
                                                               : result_reg;
  Register scratch = temps.Acquire();
  DCHECK(!AreAliased(object, old_property_array, new_property_array, scratch));

  int new_length = old_length_ + JSObject::kFieldsAdded;

  // Allocate new PropertyArray.
  {
    RegisterSnapshot snapshot = register_snapshot();
    // old_property_array needs to be live, since we'll read data from it.
    // Object needs to be live, since we write the new property array into it.
    snapshot.live_registers.set(object);
    snapshot.live_registers.set(old_property_array);
    snapshot.live_tagged_registers.set(object);
    snapshot.live_tagged_registers.set(old_property_array);

    Register size_in_bytes = scratch;
    __ Move(size_in_bytes, PropertyArray::SizeFor(new_length));
    __ Allocate(snapshot, new_property_array, size_in_bytes,
                AllocationType::kYoung);
    __ SetMapAsRoot(new_property_array, RootIndex::kPropertyArrayMap);
  }

  // Copy existing properties over.
  {
    RegisterSnapshot snapshot = register_snapshot();
    snapshot.live_registers.set(object);
    snapshot.live_registers.set(old_property_array);
    snapshot.live_registers.set(new_property_array);
    snapshot.live_tagged_registers.set(object);
    snapshot.live_tagged_registers.set(old_property_array);
    snapshot.live_tagged_registers.set(new_property_array);

    for (int i = 0; i < old_length_; ++i) {
      __ LoadTaggedFieldWithoutDecompressing(
          scratch, old_property_array, PropertyArray::OffsetOfElementAt(i));

      __ StoreTaggedFieldWithWriteBarrier(
          new_property_array, PropertyArray::OffsetOfElementAt(i), scratch,
          snapshot, MaglevAssembler::kValueIsCompressed,
          MaglevAssembler::kValueCanBeSmi);
    }
  }

  // Initialize new properties to undefined.
  __ LoadRoot(scratch, RootIndex::kUndefinedValue);
  for (int i = 0; i < JSObject::kFieldsAdded; ++i) {
    __ StoreTaggedFieldNoWriteBarrier(
        new_property_array, PropertyArray::OffsetOfElementAt(old_length_ + i),
        scratch);
  }

  // Read the hash.
  if (old_length_ == 0) {
    // The object might still have a hash, stored in properties_or_hash. If
    // properties_or_hash is a SMI, then it's the hash. It can also be an empty
    // PropertyArray.
    __ LoadTaggedField(scratch, object, JSObject::kPropertiesOrHashOffset);

    Label done;
    __ JumpIfSmi(scratch, &done);

    __ Move(scratch, PropertyArray::kNoHashSentinel);

    __ bind(&done);
    __ SmiUntag(scratch);
    __ ShiftLeft(scratch, PropertyArray::HashField::kShift);
  } else {
    __ LoadTaggedField(scratch, old_property_array,
                       PropertyArray::kLengthAndHashOffset);
    __ SmiUntag(scratch);
    __ AndInt32(scratch, PropertyArray::HashField::kMask);
  }

  // Add the new length and write the length-and-hash field.
  static_assert(PropertyArray::LengthField::kShift == 0);
  __ OrInt32(scratch, new_length);

  __ UncheckedSmiTagInt32(scratch, scratch);
  __ StoreTaggedFieldNoWriteBarrier(
      new_property_array, PropertyArray::kLengthAndHashOffset, scratch);

  {
    RegisterSnapshot snapshot = register_snapshot();
    // new_property_array needs to be live since we'll return it.
    snapshot.live_registers.set(new_property_array);
    snapshot.live_tagged_registers.set(new_property_array);

    __ StoreTaggedFieldWithWriteBarrier(
        object, JSObject::kPropertiesOrHashOffset, new_property_array, snapshot,
        MaglevAssembler::kValueIsDecompressed,
        MaglevAssembler::kValueCannotBeSmi);
  }
  if (result_reg != new_property_array) {
    __ Move(result_reg, new_property_array);
  }
}

int SetKeyedGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedStoreIC>::type;
  return D::GetStackParameterCount();
}
void SetKeyedGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedStoreIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(key_input(), D::GetRegisterParameter(D::kName));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void SetKeyedGeneric::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ CallBuiltin<Builtin::kKeyedStoreIC>(
      context(),                                    // context
      object_input(),                               // receiver
      key_input(),                                  // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int DefineKeyedOwnGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineKeyedOwnIC>::type;
  return D::GetStackParameterCount();
}
void DefineKeyedOwnGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineKeyedOwnIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(key_input(), D::GetRegisterParameter(D::kName));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  UseFixed(flags_input(), D::GetRegisterParameter(D::kFlags));
  DefineAsFixed(this, kReturnRegister0);
}
void DefineKeyedOwnGeneric::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  __ CallBuiltin<Builtin::kDefineKeyedOwnIC>(
      context(),                                    // context
      object_input(),                               // receiver
      key_input(),                                  // name
      value_input(),                                // value
      flags_input(),                                // flags
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int StoreInArrayLiteralGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreInArrayLiteralIC>::type;
  return D::GetStackParameterCount();
}
void StoreInArrayLiteralGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreInArrayLiteralIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(name_input(), D::GetRegisterParameter(D::kName));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void StoreInArrayLiteralGeneric::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStoreInArrayLiteralIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name_input(),                                 // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void GeneratorRestoreRegister::SetValueLocationConstraints() {
  UseRegister(array_input());
  UseRegister(stale_input());
  DefineAsRegister(this);
  set_temporaries_needed(1);
}
void GeneratorRestoreRegister::GenerateCode(MaglevAssembler* masm,
                                            const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register temp = temps.Acquire();
  Register array = ToRegister(array_input());
  Register stale = ToRegister(stale_input());
  Register result_reg = ToRegister(result());

  // The input and the output can alias, if that happens we use a temporary
  // register and a move at the end.
  Register value = (array == result_reg ? temp : result_reg);

  // Loads the current value in the generator register file.
  __ LoadTaggedField(value, array, FixedArray::OffsetOfElementAt(index()));

  // And trashs it with StaleRegisterConstant.
  DCHECK(stale_input().node()->Is<RootConstant>());
  __ StoreTaggedFieldNoWriteBarrier(
      array, FixedArray::OffsetOfElementAt(index()), stale);

  if (value != result_reg) {
    __ Move(result_reg, value);
  }
}

int GeneratorStore::MaxCallStackArgs() const {
  return WriteBarrierDescriptor::GetStackParameterCount();
}
void GeneratorStore::SetValueLocationConstraints() {
  UseAny(context_input());
  UseRegister(generator_input());
  for (int i = 0; i < num_parameters_and_registers(); i++) {
    UseAny(parameters_and_registers(i));
  }
  RequireSpecificTemporary(WriteBarrierDescriptor::ObjectRegister());
  RequireSpecificTemporary(WriteBarrierDescriptor::SlotAddressRegister());
}
void GeneratorStore::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  Register generator = ToRegister(generator_input());
  Register array = WriteBarrierDescriptor::ObjectRegister();
  __ LoadTaggedField(array, generator,
                     JSGeneratorObject::kParametersAndRegistersOffset);

  RegisterSnapshot register_snapshot_during_store = register_snapshot();
  // Include the array and generator registers in the register snapshot while
  // storing parameters and registers, to avoid the write barrier clobbering
  // them.
  register_snapshot_during_store.live_registers.set(array);
  register_snapshot_during_store.live_tagged_registers.set(array);
  register_snapshot_during_store.live_registers.set(generator);
  register_snapshot_during_store.live_tagged_registers.set(generator);
  for (int i = 0; i < num_parameters_and_registers(); i++) {
    // Use WriteBarrierDescriptor::SlotAddressRegister() as the temporary for
    // the value -- it'll be clobbered by StoreTaggedFieldWithWriteBarrier since
    // it's not in the register snapshot, but that's ok, and a clobberable value
    // register lets the write barrier emit slightly better code.
    Input value_input = parameters_and_registers(i);
    Register value = __ FromAnyToRegister(
        value_input, WriteBarrierDescriptor::SlotAddressRegister());
    // Include the value register in the live set, in case it is used by future
    // inputs.
    register_snapshot_during_store.live_registers.set(value);
    register_snapshot_during_store.live_tagged_registers.set(value);
    __ StoreTaggedFieldWithWriteBarrier(
        array, FixedArray::OffsetOfElementAt(i), value,
        register_snapshot_during_store,
        value_input.node()->decompresses_tagged_result()
            ? MaglevAssembler::kValueIsDecompressed
            : MaglevAssembler::kValueIsCompressed,
        MaglevAssembler::kValueCanBeSmi);
  }

  __ StoreTaggedSignedField(generator, JSGeneratorObject::kContinuationOffset,
                            Smi::FromInt(suspend_id()));
  __ StoreTaggedSignedField(generator,
                            JSGeneratorObject::kInputOrDebugPosOffset,
                            Smi::FromInt(bytecode_offset()));

  // Use WriteBarrierDescriptor::SlotAddressRegister() as the scratch
  // register, see comment above. At this point we no longer need to preserve
  // the array or generator registers, so use the original register snapshot.
  Register context = __ FromAnyToRegister(
      context_input(), WriteBarrierDescriptor::SlotAddressRegister());
  __ StoreTaggedFieldWithWriteBarrier(
      generator, JSGeneratorObject::kContextOffset, context,
      register_snapshot(),
      context_input().node()->decompresses_tagged_result()
          ? MaglevAssembler::kValueIsDecompressed
          : MaglevAssembler::kValueIsCompressed,
      MaglevAssembler::kValueCannotBeSmi);
}

int GetKeyedGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedLoadIC>::type;
  return D::GetStackParameterCount();
}
void GetKeyedGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedLoadIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(key_input(), D::GetRegisterParameter(D::kName));
  DefineAsFixed(this, kReturnRegister0);
}
void GetKeyedGeneric::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ CallBuiltin<Builtin::kKeyedLoadIC>(
      context(),                                    // context
      object_input(),                               // receiver
      key_input(),                                  // name
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void Int32ToNumber::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Int32ToNumber::GenerateCode(MaglevAssembler* masm,
                                 const ProcessingState& state) {
  Register object = ToRegister(result());
  Register value = ToRegister(input());
  ZoneLabelRef done(masm);
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  // Object is not allowed to alias value, because SmiTagInt32AndJumpIfFail will
  // clobber `object` even if the tagging fails, and we don't want it to clobber
  // `value`.
  bool input_output_alias = (object == value);
  Register res = object;
  if (input_output_alias) {
    res = temps.AcquireScratch();
  }
  __ SmiTagInt32AndJumpIfFail(
      res, value,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register object, Register value,
             Register scratch, ZoneLabelRef done, Int32ToNumber* node) {
            MaglevAssembler::TemporaryRegisterScope temps(masm);
            // AllocateHeapNumber needs a scratch register, and the res scratch
            // register isn't needed anymore, so return it to the pool.
            if (scratch.is_valid()) {
              temps.IncludeScratch(scratch);
            }
            DoubleRegister double_value = temps.AcquireScratchDouble();
            __ Int32ToDouble(double_value, value);
            __ AllocateHeapNumber(node->register_snapshot(), object,
                                  double_value);
            __ Jump(*done);
          },
          object, value, input_output_alias ? res : Register::no_reg(), done,
          this));
  if (input_output_alias) {
    __ Move(object, res);
  }
  __ bind(*done);
}

void Uint32ToNumber::SetValueLocationConstraints() {
  UseRegister(input());
#ifdef V8_TARGET_ARCH_X64
  // We emit slightly more efficient code if result is the same as input.
  DefineSameAsFirst(this);
#else
  DefineAsRegister(this);
#endif
}
void Uint32ToNumber::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  ZoneLabelRef done(masm);
  Register value = ToRegister(input());
  Register object = ToRegister(result());
  // Unlike Int32ToNumber, object is allowed to alias value here (indeed, the
  // code is better if it does). The difference is that Uint32 smi tagging first
  // does a range check, and doesn't clobber `object` on failure.
  __ SmiTagUint32AndJumpIfFail(
      object, value,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register object, Register value,
             ZoneLabelRef done, Uint32ToNumber* node) {
            MaglevAssembler::TemporaryRegisterScope temps(masm);
            DoubleRegister double_value = temps.AcquireScratchDouble();
            __ Uint32ToDouble(double_value, value);
            __ AllocateHeapNumber(node->register_snapshot(), object,
                                  double_value);
            __ Jump(*done);
          },
          object, value, done, this));
  __ bind(*done);
}

void Float64ToTagged::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Float64ToTagged::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());
  Label box, done;
  if (canonicalize_smi()) {
    __ TryTruncateDoubleToInt32(object, value, &box);
    __ SmiTagInt32AndJumpIfFail(object, &box);
    __ Jump(&done, Label::kNear);
    __ bind(&box);
  }
  __ AllocateHeapNumber(register_snapshot(), object, value);
  if (canonicalize_smi()) {
    __ bind(&done);
  }
}

void Float64ToHeapNumberForField::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Float64ToHeapNumberForField::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());
  __ AllocateHeapNumber(register_snapshot(), object, value);
}

void HoleyFloat64ToTagged::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void HoleyFloat64ToTagged::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  ZoneLabelRef done(masm);
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());
  Label box;
  if (canonicalize_smi()) {
    __ TryTruncateDoubleToInt32(object, value, &box);
    __ SmiTagInt32AndJumpIfFail(object, &box);
    __ Jump(*done, Label::kNear);
    __ bind(&box);
  }
  // Using return as scratch register.
  __ JumpIfHoleNan(
      value, ToRegister(result()),
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register object, ZoneLabelRef done) {
            // TODO(leszeks): Evaluate whether this is worth deferring.
            __ LoadRoot(object, RootIndex::kUndefinedValue);
            __ Jump(*done);
          },
          object, done));
  __ AllocateHeapNumber(register_snapshot(), object, value);
  __ bind(*done);
}

void Float64Round::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
  if (kind_ == Kind::kNearest) {
    set_double_temporaries_needed(1);
  }
}

void Int32AbsWithOverflow::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}

void Float64Abs::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}

void CheckedSmiTagFloat64::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void CheckedSmiTagFloat64::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());

  __ TryTruncateDoubleToInt32(
      object, value, __ GetDeoptLabel(this, DeoptimizeReason::kNotASmi));
  __ SmiTagInt32AndJumpIfFail(
      object, __ GetDeoptLabel(this, DeoptimizeReason::kNotASmi));
}

void StoreFloat64::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(value_input());
}
void StoreFloat64::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  Register object = ToRegister(object_input());
  DoubleRegister value = ToDoubleRegister(value_input());

  __ AssertNotSmi(object);
  __ StoreFloat64(FieldMemOperand(object, offset()), value);
}

void StoreInt32::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(value_input());
}
void StoreInt32::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register value = ToRegister(value_input());

  __ AssertNotSmi(object);
  __ StoreInt32(FieldMemOperand(object, offset()), value);
}

void StoreTaggedFieldNoWriteBarrier::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(value_input());
}
void StoreTaggedFieldNoWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register value = ToRegister(value_input());

  __ AssertNotSmi(object);
  __ StoreTaggedFieldNoWriteBarrier(object, offset(), value);
}

int StringAt::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kStringCharCodeAt)->nargs, 2);
  return std::max(2, AllocateDescriptor::GetStackParameterCount());
}
void StringAt::SetValueLocationConstraints() {
  UseAndClobberRegister(string_input());
  UseAndClobberRegister(index_input());
  DefineAsRegister(this);
  set_temporaries_needed(1);
}
void StringAt::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  Register result_string = ToRegister(result());
  Register string = ToRegister(string_input());
  Register index = ToRegister(index_input());
  Register char_code = string;

  ZoneLabelRef done(masm);
  Label cached_one_byte_string;

  RegisterSnapshot save_registers = register_snapshot();
  __ StringCharCodeOrCodePointAt(
      BuiltinStringPrototypeCharCodeOrCodePointAt::kCharCodeAt, save_registers,
      char_code, string, index, scratch, Register::no_reg(),
      &cached_one_byte_string);
  __ StringFromCharCode(save_registers, &cached_one_byte_string, result_string,
                        char_code, scratch,
                        MaglevAssembler::CharCodeMaskMode::kValueIsInRange);
}

int BuiltinStringPrototypeCharCodeOrCodePointAt::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kStringCharCodeAt)->nargs, 2);
  return 2;
}
void BuiltinStringPrototypeCharCodeOrCodePointAt::
    SetValueLocationConstraints() {
  UseAndClobberRegister(string_input());
  UseAndClobberRegister(index_input());
  DefineAsRegister(this);
  // TODO(victorgomes): Add a mode to the register allocator where we ensure
  // input cannot alias with output. We can then remove the second scratch.
  set_temporaries_needed(
      mode_ == BuiltinStringPrototypeCharCodeOrCodePointAt::kCodePointAt ? 2
                                                                         : 1);
}
void BuiltinStringPrototypeCharCodeOrCodePointAt::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch1 = temps.Acquire();
  Register scratch2 = Register::no_reg();
  if (mode_ == BuiltinStringPrototypeCharCodeOrCodePointAt::kCodePointAt) {
    scratch2 = temps.Acquire();
  }
  Register string = ToRegister(string_input());
  Register index = ToRegister(index_input());
  ZoneLabelRef done(masm);
  RegisterSnapshot save_registers = register_snapshot();
  __ StringCharCodeOrCodePointAt(mode_, save_registers, ToRegister(result()),
                                 string, index, scratch1, scratch2, *done);
  __ bind(*done);
}

void StringLength::SetValueLocationConstraints() {
  UseRegister(object_input());
  DefineAsRegister(this);
}
void StringLength::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  __ StringLength(ToRegister(result()), ToRegister(object_input()));
}

void StringConcat::SetValueLocationConstraints() {
  using D = StringAdd_CheckNoneDescriptor;
  UseFixed(lhs(), D::GetRegisterParameter(D::kLeft));
  UseFixed(rhs(), D::GetRegisterParameter(D::kRight));
  DefineAsFixed(this, kReturnRegister0);
}
void StringConcat::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStringAdd_CheckNone>(
      masm->native_context().object(),  // context
      lhs(),                            // left
      rhs()                             // right
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  DCHECK_EQ(kReturnRegister0, ToRegister(result()));
}

void StringWrapperConcat::SetValueLocationConstraints() {
  using D = StringAdd_CheckNoneDescriptor;
  UseFixed(lhs(), D::GetRegisterParameter(D::kLeft));
  UseFixed(rhs(), D::GetRegisterParameter(D::kRight));
  DefineAsFixed(this, kReturnRegister0);
}

void StringWrapperConcat::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);

  Register left = ToRegister(lhs());
  Label left_done;
  __ JumpIfString(left, &left_done);
  __ LoadTaggedField(left, left, JSPrimitiveWrapper::kValueOffset);
  __ Jump(&left_done);

  __ bind(&left_done);

  Register right = ToRegister(rhs());
  Label right_done;
  __ JumpIfString(right, &right_done);
  __ LoadTaggedField(right, right, JSPrimitiveWrapper::kValueOffset);
  __ Jump(&right_done);

  __ bind(&right_done);

  __ CallBuiltin<Builtin::kStringAdd_CheckNone>(
      masm->native_context().object(),  // context
      left,                             // left
      right                             // right
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  DCHECK_EQ(kReturnRegister0, ToRegister(result()));
}

void StringEqual::SetValueLocationConstraints() {
  using D = StringEqualDescriptor;
  UseFixed(lhs(), D::GetRegisterParameter(D::kLeft));
  UseFixed(rhs(), D::GetRegisterParameter(D::kRight));
  set_temporaries_needed(1);
  RequireSpecificTemporary(D::GetRegisterParameter(D::kLength));
  DefineAsFixed(this, kReturnRegister0);
}
void StringEqual::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  using D = StringEqualDescriptor;
  Label done, if_equal, if_not_equal;
  Register left = ToRegister(lhs());
  Register right = ToRegister(rhs());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register left_length = temps.Acquire();
  Register right_length = D::GetRegisterParameter(D::kLength);

  __ CmpTagged(left, right);
  __ JumpIf(kEqual, &if_equal,
            // Debug checks in StringLength can make this jump too long for a
            // near jump.
            v8_flags.debug_code ? Label::kFar : Label::kNear);

  __ StringLength(left_length, left);
  __ StringLength(right_length, right);
  __ CompareInt32AndJumpIf(left_length, right_length, kNotEqual, &if_not_equal,
                           Label::Distance::kNear);

  // The inputs are already in the right registers. The |left| and |right|
  // inputs were required to come in in the left/right inputs of the builtin,
  // and the |length| input of the builtin is where we loaded the length of the
  // right string (which matches the length of the left string when we get
  // here).
  DCHECK_EQ(right_length, D::GetRegisterParameter(D::kLength));
  __ CallBuiltin<Builtin::kStringEqual>(lhs(), rhs(),
                                        D::GetRegisterParameter(D::kLength));
  masm->DefineLazyDeoptPoint(this->lazy_deopt_info());
  __ Jump(&done, Label::Distance::kNear);

  __ bind(&if_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done, Label::Distance::kNear);

  __ bind(&if_not_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);

  __ bind(&done);
}

void TaggedEqual::SetValueLocationConstraints() {
  UseRegister(lhs());
  UseRegister(rhs());
  DefineAsRegister(this);
}
void TaggedEqual::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Label done, if_equal;
  __ CmpTagged(ToRegister(lhs()), ToRegister(rhs()));
  __ JumpIf(kEqual, &if_equal, Label::Distance::kNear);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);
  __ Jump(&done);
  __ bind(&if_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ bind(&done);
}

void TaggedNotEqual::SetValueLocationConstraints() {
  UseRegister(lhs());
  UseRegister(rhs());
  DefineAsRegister(this);
}
void TaggedNotEqual::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  Label done, if_equal;
  __ CmpTagged(ToRegister(lhs()), ToRegister(rhs()));
  __ JumpIf(kEqual, &if_equal, Label::Distance::kNear);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done);
  __ bind(&if_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);
  __ bind(&done);
}

int TestInstanceOf::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kInstanceOf_WithFeedback>::type;
  return D::GetStackParameterCount();
}
void TestInstanceOf::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kInstanceOf_WithFeedback>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object(), D::GetRegisterParameter(D::kLeft));
  UseFixed(callable(), D::GetRegisterParameter(D::kRight));
  DefineAsFixed(this, kReturnRegister0);
}
void TestInstanceOf::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  __ CallBuiltin<Builtin::kInstanceOf_WithFeedback>(
      context(),           // context
      object(),            // left
      callable(),          // right
      feedback().index(),  // feedback slot
      feedback().vector    // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void TestTypeOf::SetValueLocationConstraints() {
  UseRegister(value());
  DefineAsRegister(this);
#ifdef V8_TARGET_ARCH_ARM
  set_temporaries_needed(1);
#endif
}
void TestTypeOf::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
#ifdef V8_TARGET_ARCH_ARM
  // Arm32 needs one extra scratch register for TestTypeOf, so take a maglev
  // temporary and allow it to be used as a macro assembler scratch register.
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  temps.IncludeScratch(temps.Acquire());
#endif
  Register object = ToRegister(value());
  Label is_true, is_false, done;
  __ TestTypeOf(object, literal_, &is_true, Label::Distance::kNear, true,
                &is_false, Label::Distance::kNear, false);
  // Fallthrough into true.
  __ bind(&is_true);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&is_false);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);
  __ bind(&done);
}

void ToBoolean::SetValueLocationConstraints() {
  UseRegister(value());
  DefineAsRegister(this);
}
void ToBoolean::GenerateCode(MaglevAssembler* masm,
                             const ProcessingState& state) {
  Register object = ToRegister(value());
  Register return_value = ToRegister(result());
  Label done;
  ZoneLabelRef object_is_true(masm), object_is_false(masm);
  // TODO(leszeks): We're likely to be calling this on an existing boolean --
  // maybe that's a case we should fast-path here and re-use that boolean value?
  __ ToBoolean(object, check_type(), object_is_true, object_is_false, true);
  __ bind(*object_is_true);
  __ LoadRoot(return_value, RootIndex::kTrueValue);
  __ Jump(&done);
  __ bind(*object_is_false);
  __ LoadRoot(return_value, RootIndex::kFalseValue);
  __ bind(&done);
}

void ToBooleanLogicalNot::SetValueLocationConstraints() {
  UseRegister(value());
  DefineAsRegister(this);
}
void ToBooleanLogicalNot::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register object = ToRegister(value());
  Register return_value = ToRegister(result());
  Label done;
  ZoneLabelRef object_is_true(masm), object_is_false(masm);
  __ ToBoolean(object, check_type(), object_is_true, object_is_false, true);
  __ bind(*object_is_true);
  __ LoadRoot(return_value, RootIndex::kFalseValue);
  __ Jump(&done);
  __ bind(*object_is_false);
  __ LoadRoot(return_value, RootIndex::kTrueValue);
  __ bind(&done);
}

int ToName::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToName>::type;
  return D::GetStackParameterCount();
}
void ToName::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToName>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void ToName::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
  __ CallBuiltin<Builtin::kToName>(context(),     // context
                                   value_input()  // input
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int ToNumberOrNumeric::MaxCallStackArgs() const {
  return TypeConversionDescriptor::GetStackParameterCount();
}
void ToNumberOrNumeric::SetValueLocationConstraints() {
  UseRegister(value_input());
  set_temporaries_needed(1);
  DefineAsRegister(this);
}
void ToNumberOrNumeric::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  ZoneLabelRef done(masm);
  Label move_and_return;
  Register object = ToRegister(value_input());
  Register result_reg = ToRegister(result());

  __ JumpIfSmi(object, &move_and_return, Label::kNear);
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ CompareMapWithRoot(object, RootIndex::kHeapNumberMap, scratch);
  __ JumpToDeferredIf(
      kNotEqual,
      |](MaglevAssembler* masm, Object::Conversion mode, Register object,
         Register result_reg, ToNumberOrNumeric* node, ZoneLabelRef done) {
        {
          RegisterSnapshot snapshot = node->register_snapshot();
          snapshot.live_registers.clear(result_reg);
          SaveRegisterStateForCall save_register_state(masm, snapshot);
          switch (mode) {
            case Object::Conversion::kToNumber:
              __ CallBuiltin<Builtin::kToNumber>(
                  masm->native_context().object(), object);
              break;
            case Object::Conversion::kToNumeric:
              __ CallBuiltin<Builtin::kToNumeric>(
                  masm->native_context().object(), object);
              break;
          }
          masm->DefineExceptionHandlerPoint(node);
          save_register_state.DefineSafepointWithLazyDeopt(
              node->lazy_deopt_info());
          __ Move(result_reg, kReturnRegister0);
        }
        __ Jump(*done);
      },
      mode(), object, result_reg, this, done);
  __ bind(&move_and_return);
  __ Move(result_reg, object);

  __ bind(*done);
}

int ToObject::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  return D::GetStackParameterCount();
}
void ToObject::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void ToObject::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  Register value = ToRegister(value_input());
  Label call_builtin, done;
  // Avoid the builtin call if {value} is a JSReceiver.
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(value);
  } else {
    __ JumpIfSmi(value, &call_builtin, Label::Distance::kNear);
  }
  __ JumpIfJSAnyIsNotPrimitive(value, &done, Label::Distance::kNear);
  __ bind(&call_builtin);
  __ CallBuiltin<Builtin::kToObject>(context(),     // context
                                     value_input()  // input
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  __ bind(&done);
}

int ToString::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToString>::type;
  return D::GetStackParameterCount();
}
void ToString::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToString>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value_input(), D::GetRegisterParameter(D::kO));
  DefineAsFixed(this, kReturnRegister0);
}
void ToString::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  Register value = ToRegister(value_input());
  Label call_builtin, done;
  // Avoid the builtin call if {value} is a string.
  __ JumpIfSmi(value, &call_builtin, Label::Distance::kNear);
  __ JumpIfString(value, &done, Label::Distance::kNear);
  if (mode() == kConvertSymbol) {
    __ JumpIfNotObjectType(value, SYMBOL_TYPE, &call_builtin,
                           Label::Distance::kNear);
    __ Push(value);
    __ CallRuntime(Runtime::kSymbolDescriptiveString, 1);
    __ Jump(&done, Label::kNear);
  }
  __ bind(&call_builtin);
  __ CallBuiltin<Builtin::kToString>(context(),     // context
                                     value_input()  // input
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  __ bind(&done);
}

void NumberToString::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kNumberToString>::type;
  UseFixed(value_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void NumberToString::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  __ CallBuiltin<Builtin::kNumberToString>(value_input());
  masm->DefineLazyDeoptPoint(this->lazy_deopt_info());
}

int ThrowReferenceErrorIfHole::MaxCallStackArgs() const { return 1; }
void ThrowReferenceErrorIfHole::SetValueLocationConstraints() {
  UseAny(value());
}
void ThrowReferenceErrorIfHole::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ JumpToDeferredIf(
      __ IsRootConstant(value(), RootIndex::kTheHoleValue),
      |](MaglevAssembler* masm, ThrowReferenceErrorIfHole* node) {
        __ Push(node->name().object());
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowAccessedUninitializedVariable, 1);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);
}

int ThrowSuperNotCalledIfHole::MaxCallStackArgs() const { return 0; }
void ThrowSuperNotCalledIfHole::SetValueLocationConstraints() {
  UseAny(value());
}
void ThrowSuperNotCalledIfHole::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ JumpToDeferredIf(
      __ IsRootConstant(value(), RootIndex::kTheHoleValue),
      |](MaglevAssembler* masm, ThrowSuperNotCalledIfHole* node) {
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowSuperNotCalled, 0);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);
}

int ThrowSuperAlreadyCalledIfNotHole::MaxCallStackArgs() const { return 0; }
void ThrowSuperAlreadyCalledIfNotHole::SetValueLocationConstraints() {
  UseAny(value());
}
void ThrowSuperAlreadyCalledIfNotHole::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  __ JumpToDeferredIf(
      NegateCondition(__ IsRootConstant(value(), RootIndex::kTheHoleValue)),
      |](MaglevAssembler* masm, ThrowSuperAlreadyCalledIfNotHole* node) {
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowSuperAlreadyCalledError, 0);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);
}

int ThrowIfNotCallable::MaxCallStackArgs() const { return 1; }
void ThrowIfNotCallable::SetValueLocationConstraints() {
  UseRegister(value());
  set_temporaries_needed(1);
}
void ThrowIfNotCallable::GenerateCode(MaglevAssembler* masm,
                                      const ProcessingState& state) {
  Label* if_not_callable = __ MakeDeferredCode(
      |](MaglevAssembler* masm, ThrowIfNotCallable* node) {
        __ Push(node->value());
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowCalledNonCallable, 1);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);

  Register value_reg = ToRegister(value());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ JumpIfNotCallable(value_reg, scratch, CheckType::kCheckHeapObject,
                       if_not_callable);
}

int ThrowIfNotSuperConstructor::MaxCallStackArgs() const { return 2; }
void ThrowIfNotSuperConstructor::SetValueLocationConstraints() {
  UseRegister(constructor());
  UseRegister(function());
  set_temporaries_needed(1);
}
void ThrowIfNotSuperConstructor::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ LoadMap(scratch, ToRegister(constructor()));
  static_assert(Map::kBitFieldOffsetEnd + 1 - Map::kBitFieldOffset == 1);
  __ TestUint8AndJumpIfAllClear(
      FieldMemOperand(scratch, Map::kBitFieldOffset),
      Map::Bits1::IsConstructorBit::kMask,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, ThrowIfNotSuperConstructor* node) {
            __ Push(ToRegister(node->constructor()),
                    ToRegister(node->function()));
            __ Move(kContextRegister, masm->native_context().object());
            __ CallRuntime(Runtime::kThrowNotSuperConstructor, 2);
            masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
            __ Abort(AbortReason::kUnexpectedReturnFromThrow);
          },
          this));
}

void TruncateUint32ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void TruncateUint32ToInt32::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  // No code emitted -- as far as the machine is concerned, int32 is uint32.
  DCHECK_EQ(ToRegister(input()), ToRegister(result()));
}

void TruncateFloat64ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void TruncateFloat64ToInt32::GenerateCode(MaglevAssembler* masm,
                                          const ProcessingState& state) {
  __ TruncateDoubleToInt32(ToRegister(result()), ToDoubleRegister(input()));
}

void CheckedTruncateFloat64ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void CheckedTruncateFloat64ToInt32::GenerateCode(MaglevAssembler* masm,
                                                 const ProcessingState& state) {
  __ TryTruncateDoubleToInt32(
      ToRegister(result()), ToDoubleRegister(input()),
      __ GetDeoptLabel(this, DeoptimizeReason::kNotInt32));
}

void CheckedTruncateFloat64ToUint32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void CheckedTruncateFloat64ToUint32::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  __ TryTruncateDoubleToUint32(
      ToRegister(result()), ToDoubleRegister(input()),
      __ GetDeoptLabel(this, DeoptimizeReason::kNotUint32));
}

void UnsafeTruncateFloat64ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void UnsafeTruncateFloat64ToInt32::GenerateCode(MaglevAssembler* masm,
                                                const ProcessingState& state) {
#ifdef DEBUG
  Label fail, start;
  __ Jump(&start);
  __ bind(&fail);
  __ Abort(AbortReason::kFloat64IsNotAInt32);

  __ bind(&start);
  __ TryTruncateDoubleToInt32(ToRegister(result()), ToDoubleRegister(input()),
                              &fail);
#else
  // TODO(dmercadier): TruncateDoubleToInt32 does additional work when the
  // double doesn't fit in a 32-bit integer. This is not necessary for
  // UnsafeTruncateFloat64ToInt32 (since we statically know that it the double
  // fits in a 32-bit int) and could be instead just a Cvttsd2si (x64) or Fcvtzs
  // (arm64).
  __ TruncateDoubleToInt32(ToRegister(result()), ToDoubleRegister(input()));
#endif
}

void CheckedUint32ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void CheckedUint32ToInt32::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  Register input_reg = ToRegister(input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kNotInt32);
  __ CompareInt32AndJumpIf(input_reg, 0, kLessThan, fail);
}

void UnsafeTruncateUint32ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void UnsafeTruncateUint32ToInt32::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
#ifdef DEBUG
  Register input_reg = ToRegister(input());
  __ CompareInt32AndAssert(input_reg, 0, kGreaterThanEqual,
                           AbortReason::kUint32IsNotAInt32);
#endif
  // No code emitted -- as far as the machine is concerned, int32 is uint32.
  DCHECK_EQ(ToRegister(input()), ToRegister(result()));
}

void Int32ToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void Int32ToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register value = ToRegister(input());
  Register result_reg = ToRegister(result());
  DCHECK_EQ(value, result_reg);
  Label min, done;
  __ CompareInt32AndJumpIf(value, 0, kLessThanEqual, &min);
  __ CompareInt32AndJumpIf(value, 255, kLessThanEqual, &done);
  __ Move(result_reg, 255);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&min);
  __ Move(result_reg, 0);
  __ bind(&done);
}

void Uint32ToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void Uint32ToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  Register value = ToRegister(input());
  DCHECK_EQ(value, ToRegister(result()));
  Label done;
  __ CompareInt32AndJumpIf(value, 255, kUnsignedLessThanEqual, &done,
                           Label::Distance::kNear);
  __ Move(value, 255);
  __ bind(&done);
}

void Float64ToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Float64ToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register result_reg = ToRegister(result());
  Label min, max, done;
  __ ToUint8Clamped(result_reg, value, &min, &max, &done);
  __ bind(&min);
  __ Move(result_reg, 0);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&max);
  __ Move(result_reg, 255);
  __ bind(&done);
}

void CheckNumber::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckNumber::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Label done;
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.AcquireScratch();
  Register value = ToRegister(receiver_input());
  // If {value} is a Smi or a HeapNumber, we're done.
  __ JumpIfSmi(
      value, &done,
      v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);
  if (mode() == Object::Conversion::kToNumeric) {
    __ LoadMapForCompare(scratch, value);
    __ CompareTaggedRoot(scratch, RootIndex::kHeapNumberMap);
    // Jump to done if it is a HeapNumber.
    __ JumpIf(
        kEqual, &done,
        v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);
    // Check if it is a BigInt.
    __ CompareTaggedRootAndEmitEagerDeoptIf(
        scratch, RootIndex::kBigIntMap, kNotEqual,
        DeoptimizeReason::kNotANumber, this);
  } else {
    __ CompareMapWithRootAndEmitEagerDeoptIf(
        value, RootIndex::kHeapNumberMap, scratch, kNotEqual,
        DeoptimizeReason::kNotANumber, this);
  }
  __ bind(&done);
}

void CheckedInternalizedString::SetValueLocationConstraints() {
  UseRegister(object_input());
  DefineSameAsFirst(this);
}
void CheckedInternalizedString::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  Register object = ToRegister(object_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register instance_type = temps.AcquireScratch();
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kWrongMap);
  }
  __ LoadInstanceType(instance_type, object);
  __ RecordComment("Test IsInternalizedString");
  // Go to the slow path if this is a non-string, or a non-internalised string.
  static_assert((kStringTag | kInternalizedTag) == 0);
  ZoneLabelRef done(masm);
  __ TestInt32AndJumpIfAnySet(
      instance_type, kIsNotStringMask | kIsNotInternalizedMask,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, ZoneLabelRef done,
             CheckedInternalizedString* node, Register object,
             Register instance_type) {
            __ RecordComment("Deferred Test IsThinString");
            // Deopt if this isn't a string.
            __ TestInt32AndJumpIfAnySet(
                instance_type, kIsNotStringMask,
                __ GetDeoptLabel(node, DeoptimizeReason::kWrongMap));
            // Deopt if this isn't a thin string.
            static_assert(base::bits::CountPopulation(kThinStringTagBit) == 1);
            __ TestInt32AndJumpIfAllClear(
                instance_type, kThinStringTagBit,
                __ GetDeoptLabel(node, DeoptimizeReason::kWrongMap));
            // Load internalized string from thin string.
            __ LoadTaggedField(object, object, offsetof(ThinString, actual_));
            if (v8_flags.debug_code) {
              __ RecordComment("DCHECK IsInternalizedString");
              Label checked;
              __ LoadInstanceType(instance_type, object);
              __ TestInt32AndJumpIfAllClear(
                  instance_type, kIsNotStringMask | kIsNotInternalizedMask,
                  &checked);
              __ Abort(AbortReason::kUnexpectedValue);
              __ bind(&checked);
            }
            __ Jump(*done);
          },
          done, this, object, instance_type));
  __ bind(*done);
}

void CheckedNumberToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
  set_temporaries_needed(1);
  set_double_temporaries_needed(1);
}
void CheckedNumberToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  Register value = ToRegister(input());
  Register result_reg = ToRegister(result());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  DoubleRegister double_value = temps.AcquireDouble();
  Label is_not_smi, min, max, done;
  // Check if Smi.
  __ JumpIfNotSmi(value, &is_not_smi);
  // If Smi, convert to Int32.
  __ SmiToInt32(value);
  // Clamp.
  __ CompareInt32AndJumpIf(value, 0, kLessThanEqual, &min);
  __ CompareInt32AndJumpIf(value, 255, kGreaterThanEqual, &max);
  __ Jump(&done);
  __ bind(&is_not_smi);
  // Check if HeapNumber, deopt otherwise.
  __ CompareMapWithRootAndEmitEagerDeoptIf(value, RootIndex::kHeapNumberMap,
                                           scratch, kNotEqual,
                                           DeoptimizeReason::kNotANumber, this);
  // If heap number, get double value.
  __ LoadHeapNumberValue(double_value, value);
  // Clamp.
  __ ToUint8Clamped(value, double_value, &min, &max, &done);
  __ bind(&min);
  __ Move(result_reg, 0);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&max);
  __ Move(result_reg, 255);
  __ bind(&done);
}

void StoreFixedArrayElementWithWriteBarrier::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(index_input());
  UseRegister(value_input());
  RequireSpecificTemporary(WriteBarrierDescriptor::ObjectRegister());
  RequireSpecificTemporary(WriteBarrierDescriptor::SlotAddressRegister());
}
void StoreFixedArrayElementWithWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  Register elements = ToRegister(elements_input());
  Register index = ToRegister(index_input());
  Register value = ToRegister(value_input());
  __ StoreFixedArrayElementWithWriteBarrier(elements, index, value,
                                            register_snapshot());
}

void StoreFixedArrayElementNoWriteBarrier::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(index_input());
  UseRegister(value_input());
}
void StoreFixedArrayElementNoWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  Register elements = ToRegister(elements_input());
  Register index = ToRegister(index_input());
  Register value = ToRegister(value_input());
  __ StoreFixedArrayElementNoWriteBarrier(elements, index, value);
}

// ---
// Arch agnostic call nodes
// ---

int Call::MaxCallStackArgs() const { return num_args(); }
void Call::SetValueLocationConstraints() {
  using D = CallTrampolineDescriptor;
  UseFixed(function(), D::GetRegisterParameter(D::kFunction));
  UseAny(arg(0));
  for (int i = 1; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}

void Call::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
  __ PushReverse(args());

  uint32_t arg_count = num_args();
  if (target_type_ == TargetType::kAny) {
    switch (receiver_mode_) {
      case ConvertReceiverMode::kNullOrUndefined:
        __ CallBuiltin<Builtin::kCall_ReceiverIsNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kNotNullOrUndefined:
        __ CallBuiltin<Builtin::kCall_ReceiverIsNotNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kAny:
        __ CallBuiltin<Builtin::kCall_ReceiverIsAny>(context(), function(),
                                                     arg_count);
        break;
    }
  } else {
    DCHECK_EQ(TargetType::kJSFunction, target_type_);
    switch (receiver_mode_) {
      case ConvertReceiverMode::kNullOrUndefined:
        __ CallBuiltin<Builtin::kCallFunction_ReceiverIsNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kNotNullOrUndefined:
        __ CallBuiltin<Builtin::kCallFunction_ReceiverIsNotNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kAny:
        __ CallBuiltin<Builtin::kCallFunction_ReceiverIsAny>(
            context(), function(), arg_count);
        break;
    }
  }

  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallForwardVarargs::MaxCallStackArgs() const { return num_args(); }
void CallForwardVarargs::SetValueLocationConstraints() {
  using D = CallTrampolineDescriptor;
  UseFixed(function(), D::GetRegisterParameter(D::kFunction));
  UseAny(arg(0));
  for (int i = 1; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}

void CallForwardVarargs::GenerateCode(MaglevAssembler* masm,
                                      const ProcessingState& state) {
  __ PushReverse(args());
  switch (target_type_) {
    case Call::TargetType::kJSFunction:
      __ CallBuiltin<Builtin::kCallFunctionForwardVarargs>(
          context(), function(), num_args(), start_index_);
      break;
    case Call::TargetType::kAny:
      __ CallBuiltin<Builtin::kCallForwardVarargs>(context(), function(),
                                                   num_args(), start_index_);
      break;
  }
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallSelf::MaxCallStackArgs() const {
  int actual_parameter_count = num_args() + 1;
  return std::max(expected_parameter_count_, actual_parameter_count);
}
void CallSelf::SetValueLocationConstraints() {
  UseAny(receiver());
  for (int i = 0; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(closure(), kJavaScriptCallTargetRegister);
  UseFixed(new_target(), kJavaScriptCallNewTargetRegister);
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
  set_temporaries_needed(1);
}

void CallSelf::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  int actual_parameter_count = num_args() + 1;
  if (actual_parameter_count < expected_parameter_count_) {
    int number_of_undefineds =
        expected_parameter_count_ - actual_parameter_count;
    __ LoadRoot(scratch, RootIndex::kUndefinedValue);
    __ PushReverse(receiver(), args(),
                   RepeatValue(scratch, number_of_undefineds));
  } else {
    __ PushReverse(receiver(), args());
  }
  DCHECK_EQ(kContextRegister, ToRegister(context()));
  DCHECK_EQ(kJavaScriptCallTargetRegister, ToRegister(closure()));
  __ Move(kJavaScriptCallArgCountRegister, actual_parameter_count);
  __ CallSelf();
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallKnownJSFunction::MaxCallStackArgs() const {
  int actual_parameter_count = num_args() + 1;
  return std::max(expected_parameter_count_, actual_parameter_count);
}
void CallKnownJSFunction::SetValueLocationConstraints() {
  UseAny(receiver());
  for (int i = 0; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(closure(), kJavaScriptCallTargetRegister);
  UseFixed(new_target(), kJavaScriptCallNewTargetRegister);
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
  set_temporaries_needed(1);
}

void CallKnownJSFunction::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  int actual_parameter_count = num_args() + 1;
  if (actual_parameter_count < expected_parameter_count_) {
    int number_of_undefineds =
        expected_parameter_count_ - actual_parameter_count;
    __ LoadRoot(scratch, RootIndex::kUndefinedValue);
    __ PushReverse(receiver(), args(),
                   RepeatValue(scratch, number_of_undefineds));
  } else {
    __ PushReverse(receiver(), args());
  }
  // From here on, we're going to do a call, so all registers are valid temps,
  // except for the ones we're going to write. This is needed in case one of the
  // helper methods below wants to use a temp and one of these is in the temp
  // list (in particular, this can happen on arm64 where cp is a temp register
  // by default).
  temps.SetAvailable(MaglevAssembler::GetAllocatableRegisters() -
                     RegList{kContextRegister, kJavaScriptCallCodeStartRegister,
                             kJavaScriptCallTargetRegister,
                             kJavaScriptCallNewTargetRegister,
                             kJavaScriptCallArgCountRegister});
  DCHECK_EQ(kContextRegister, ToRegister(context()));
  DCHECK_EQ(kJavaScriptCallTargetRegister, ToRegister(closure()));
  __ Move(kJavaScriptCallArgCountRegister, actual_parameter_count);
  if (shared_function_info().HasBuiltinId()) {
    Builtin builtin = shared_function_info().builtin_id();

    // This SBXCHECK is a defense-in-depth measure to ensure that we always
    // generate valid calls here (with matching signatures).
    SBXCHECK_EQ(expected_parameter_count_,
                Builtins::GetFormalParameterCount(builtin));

    __ CallBuiltin(builtin);
  } else {
#if V8_ENABLE_LEAPTIERING
    __ CallJSDispatchEntry(dispatch_handle_, expected_parameter_count_);
#else
    __ CallJSFunction(kJavaScriptCallTargetRegister, expected_parameter_count_);
#endif
  }
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallKnownApiFunction::MaxCallStackArgs() const {
  int actual_parameter_count = num_args() + 1;
  return actual_parameter_count;
}

void CallKnownApiFunction::SetValueLocationConstraints() {
  if (api_holder_.has_value()) {
    UseAny(receiver());
  } else {
    // This is an "Api holder is receiver" case, ask register allocator to put
    // receiver value into the right register.
    UseFixed(receiver(), CallApiCallbackOptimizedDescriptor::HolderRegister());
  }
  for (int i = 0; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(context(), kContextRegister);

  DefineAsFixed(this, kReturnRegister0);

  if (inline_builtin()) {
    set_temporaries_needed(2);
  }
}

void CallKnownApiFunction::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  __ PushReverse(receiver(), args());

  // From here on, we're going to do a call, so all registers are valid temps,
  // except for the ones we're going to write. This is needed in case one of the
  // helper methods below wants to use a temp and one of these is in the temp
  // list (in particular, this can happen on arm64 where cp is a temp register
  // by default).
  temps.SetAvailable(
      kAllocatableGeneralRegisters -
      RegList{
          kContextRegister,
          CallApiCallbackOptimizedDescriptor::HolderRegister(),
          CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister(),
          CallApiCallbackOptimizedDescriptor::FunctionTemplateInfoRegister(),
          CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister()});
  DCHECK_EQ(kContextRegister, ToRegister(context()));

  if (inline_builtin()) {
[Step 10: Duration 33.34 seconds| Input tokens: 197,452 | Output tokens: 8,317]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 11 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'init_rag_db' with arguments: {'id': 'maglev_array_foreach'}                                                                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: initialized RAG DB maglev_array_foreach at /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_array_foreach.json
[Step 11: Duration 60.45 seconds| Input tokens: 307,562 | Output tokens: 11,162]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 12 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'foreach_builder_entrypoints', 'Body': "Key entrypoints for Array.prototype.forEach specialization in Maglev reside inside                            │
│ MaglevGraphBuilder::TryReduceArrayForEach in src/maglev/maglev-graph-builder.cc. This function checks that the call is speculatable, validates the receiver and its maps support fast array iteration,       │
│ ensures the NoElementsProtector dependency, validates the callback, and then lowers into an explicit loop via MaglevSubGraphBuilder with deopt continuation builtins for eager and lazy cases.\n\nHighlights │
│ of the implementation:\n- Name: Reduce path TryReduceArrayForEach\n- File: maglev/maglev-graph-builder.cc\n- Loop setup: uses MaglevSubGraphBuilder to build a loop with variables var_index (Smi tagged)    │
│ and var_length (the JSArray length), and a LoopLabel loop_header.\n- Deopt continuations used for forEach: Builtin::kArrayForEachLoopLazyDeoptContinuation and                                               │
│ Builtin::kArrayForEachLoopEagerDeoptContinuation.\n- Elements kind specialization and hole handling: the function first derives ElementsKind from the receiver's KnownNodeAspects maps via                   │
│ CanInlineArrayIteratingBuiltin and only proceeds if all maps support fast array iteration and can unify element kinds. For HOLEY kinds, branches skip callback when element is hole (BranchIfFloat64IsHole   │
│ or BranchIfRootConstant with kTheHoleValue).\n- Representation: index is Smi-tagged phi; the int32 index used for bounds comparing and element access is obtained via GetInt32(index_tagged). It increments  │
│ via Int32IncrementWithOverflow under an eager deopt scope.\n- Length and length-change guard: original_length is built from BuildLoadJSArrayLength(receiver) and cached. After each callback, current_length │
│ is reloaded and compared with original_length using TryBuildCheckInt32Condition(..., AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged). If different value nodes, it asserts   │
│ that original_length_int32 <= current_length.\n- Elements access: loads elements via BuildLoadElements(receiver). For doubles: BuildLoadFixedDoubleArrayElement(elements, index_int32); for tagged:          │
│ BuildLoadFixedArrayElement(elements, index_int32). For HOLEY kinds, there are hole checks, using BranchIfFloat64IsHole or BranchIfRootConstant.\n- Callback validation: Ensures callback is                  │
│ NodeType::kCallable. If not statically known callable, wraps a ThrowIfNotCallable node emission inside a DeoptFrameScope lazy continuation (Builtin::kArrayForEachLoopLazyDeoptContinuation) so that         │
│ exception stack traces are proper.\n- Map re-check after callback when receiver maps were unstable: uses CheckMaps or CheckMapsWithMigration against the saved PossibleMaps set, within an eager deopt scope │
│ tied to Builtin::kArrayForEachLoopEagerDeoptContinuation.\n- Exception path: ThrowIfNotCallable calls into runtime ThrowCalledNonCallable with DefineExceptionHandlerAndLazyDeoptPoint. The deopt scope      │
│ wiring ensures the lazy continuation builtin is used as deopt frame for exceptions thrown before loop lowering proceeds.\n\nKey code snippets (see FileLine for precise locations):\n- Elements kind         │
│ gating/helper: static bool CanInlineArrayIteratingBuiltin(...)\n- TryReduceArrayForEach main lowering: emits loop, hole-skipping, callback calls, deopt scopes, and length-change guard.\n- Builtins         │
│ identifiers for continuations: handled in MaglevGraphBuilder::DeoptFrameScope and also referenced in switch at top of file.\n", 'Context': ['maglev_array_foreach', 'foreach_deopt_continuations',           │
│ 'foreach_throw_if_not_callable', 'foreach_elements_and_maps', 'foreach_representation_and_bounds'], 'Explanation': 'This entry details the key builder entrypoint and the high-level flow of Maglev lowering │
│ of Array.prototype.forEach into an explicit loop with elements specialization, deopt continuations, and callback validation.', 'FileLine': 'maglev/maglev-graph-builder.cc:8072-8320                         │
│ (TryReduceArrayForEach); maglev/maglev-graph-builder.cc:8029-8050 (CanInlineArrayIteratingBuiltin)'}                                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote foreach_builder_entrypoints to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/foreach_builder_entrypoints.json
[Step 12: Duration 30.67 seconds| Input tokens: 417,748 | Output tokens: 12,037]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 13 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'foreach_deopt_continuations', 'Body': 'Deopt continuation types and wiring for Array.prototype.forEach in Maglev:\n\n- Builtin continuation names:   │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation and Builtin::kArrayForEachLoopEagerDeoptContinuation.\n- They are used to construct DeoptFrameScope instances around specific regions in                     │
│ TryReduceArrayForEach:\n  1) Lazy deopt around ThrowIfNotCallable and around the callback call, to ensure exceptions or lazy deopts resume in the correct builtin continuation frame with parameters         │
│ [receiver, callback, this_arg, k(index), length] or [receiver, callback, this_arg, next_index_int32, original_length]. Example:\n     DeoptFrameScope lazy_deopt_scope(this,                                 │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n         base::VectorOf<ValueNode*>({receiver, callback, this_arg, GetSmiConstant(0), original_length}));\n                                        │
│ AddNewNode<ThrowIfNotCallable>({callback});\n  2) Eager deopt scopes around the index increment (Int32IncrementWithOverflow) and after callback for map and length checks:\n     DeoptFrameScope             │
│ eager_deopt_scope(this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,\n         base::VectorOf<ValueNode*>({receiver, callback, this_arg, index_int32, original_length}));\n\n- The builtin      │
│ definitions for these continuations are in builtins/array-foreach.tq, where transitioning javascript builtins ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation forward into  │
│ ArrayForEachLoopContinuation with appropriate casted parameters.\n\n- MaglevGraphBuilder::DeoptFrameScope::IsLazyDeoptContinuationFrame explicitly treats kArrayForEachLoopLazyDeoptContinuation as a lazy   │
│ continuation.\n\n- MaglevGraphBuilder maintains latest_checkpointed_frame_ and wraps it with any DeoptFrameScope data when creating lazy/eager deopt points, ensuring that exception handler and lazy deopt  │
│ points created by nodes (e.g., ThrowIfNotCallable) have the builtin continuation frames attached to the deopt info.\n', 'Context': ['maglev_array_foreach', 'foreach_builder_entrypoints'], 'Explanation':   │
│ 'Captures exactly how Maglev wires deopt continuation frames for the forEach loop lowering, including the parameters passed and where eager vs lazy scopes are positioned. It links the IR nodes to the      │
│ builtin TQ continuations.', 'FileLine': 'maglev/maglev-graph-builder.cc:8127-8135, 8200-8208, 8241-8249, 8288-8296; builtins/array-foreach.tq:6-40'}                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote foreach_deopt_continuations to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/foreach_deopt_continuations.json
[Step 13: Duration 13.28 seconds| Input tokens: 528,858 | Output tokens: 12,630]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 14 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'foreach_throw_if_not_callable', 'Body': 'ThrowIfNotCallable emission and exception wiring in Maglev for Array.prototype.forEach:\n\n- In             │
│ TryReduceArrayForEach, after extracting callback = args[0], Maglev validates callback is callable:\n  EnsureType(callback, NodeType::kCallable, [&](NodeType old_type) {\n    DeoptFrameScope                │
│ lazy_deopt_scope(\n        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n        base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                                               │
│ GetSmiConstant(0), original_length}));\n    AddNewNode<ThrowIfNotCallable>({callback});\n  });\n  This uses EnsureType with a lambda to insert a ThrowIfNotCallable node if the type wasn’t previously known │
│ callable, inside a lazy deopt scope to maintain stack trace correctness.\n\n- Node class ThrowIfNotCallable appears in maglev-ir.h and codegen in maglev-ir.cc:\n  class ThrowIfNotCallable : public         │
│ FixedInputNodeT<1, ThrowIfNotCallable> { ... } (OpProperties::CanThrow | OpProperties::DeferredCall)\n  GenerateCode uses JumpIfNotCallable and defers to runtime ThrowCalledNonCallable, then               │
│ masm->DefineExceptionHandlerAndLazyDeoptPoint(node) to attach exception handler and lazy deopt metadata.\n\n- This ensures exceptions (TypeError) thrown by non-callable callback validation are wired to    │
│ Maglev exception handling, associating the lazy deopt continuation frame (kArrayForEachLoopLazyDeoptContinuation) with the throw site.\n\n- Regarding catch handlers: MaglevGraphBuilder handles catch       │
│ blocks via MergePointInterpreterFrameState::NewForCatchBlock in BuildMergeStates and uses masm->DefineExceptionHandlerAndLazyDeoptPoint for throwing nodes/calls. For forEach, the key is that               │
│ ThrowIfNotCallable emits exception handler/lazy deopt point and the active DeoptFrameScope sets the lazy continuation to ArrayForEachLoopLazyDeoptContinuation so the interpreter/builtin continuation       │
│ handles resumption correctly.\n', 'Context': ['maglev_array_foreach', 'foreach_deopt_continuations', 'foreach_builder_entrypoints'], 'Explanation': 'Documents how callback validation is emitted in Maglev  │
│ and how exceptions are wired to deopt/handlers for forEach loop specialization. Points to the node definition and codegen that connect to runtime and exception handler points.', 'FileLine':                │
│ 'maglev/maglev-graph-builder.cc:8127-8135; maglev/maglev-ir.h:9740-9756; maglev/maglev-ir.cc:5424-5438'}                                                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote foreach_throw_if_not_callable to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/foreach_throw_if_not_callable.json
[Step 14: Duration 11.52 seconds| Input tokens: 640,614 | Output tokens: 13,201]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 15 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'foreach_representation_and_bounds', 'Body': 'Index representation, untagging, increment, comparisons, and length-change guards in Maglev for         │
│ Array.prototype.forEach loop lowering:\n\n- Index setup: var_index is initialized to Smi 0 and stored as a Phi in the loop (sub_builder.set(var_index, GetSmiConstant(0))). The length var starts as         │
│ original_length (BuildLoadJSArrayLength(receiver)). There’s a LoopLabel and BeginLoop/EndLoop.\n- Comparison: Before the loop body, it compares index_int32 < original_length_int32 using                    │
│ BranchIfInt32Compare(Operation::kLessThan). The index_int32 is obtained via GetInt32(index_tagged) where index_tagged is the phi (EnsureType(index_tagged, NodeType::kSmi)).\n- Increment with eager deopt:  │
│ The index increment uses Int32IncrementWithOverflow under an eager deopt scope with Builtin::kArrayForEachLoopEagerDeoptContinuation. Comment notes overflow cannot really happen because max array length < │
│ int32 max; TODO to replace with asserting increment.\n- Elements access: Load elements array with BuildLoadElements(receiver). Depending on elements_kind, either BuildLoadFixedDoubleArrayElement or        │
│ BuildLoadFixedArrayElement is used.\n- Hole checks: For HOLEY kinds, skip call when element is hole using BranchIfFloat64IsHole (double) or BranchIfRootConstant(..., kTheHoleValue) (tagged), with a        │
│ skip_call label.\n- Post-callback guards and length change: After the callback and any recheck of maps, reload current_length = BuildLoadJSArrayLength(receiver), set var_length to it, and if it is not     │
│ pointer-equal to original_length (different node identity), assert original_length_int32 <= current_length using TryBuildCheckInt32Condition(..., AssertCondition::kUnsignedLessThanEqual,                   │
│ DeoptimizeReason::kArrayLengthChanged). This guards against length decreasing below original, matching spec’s snapshot semantics for fast path.\n- Uint32/Unsigned comparisons: The less-than and            │
│ less-than-equal guards use unsigned comparisons for bounds and length-change (kUnsignedLessThan/kUnsignedLessThanEqual). In the loop header check, BranchIfInt32Compare is used with signed int32 compare    │
│ but both index and length are non-negative, and later TryBuildCheckInt32Condition uses AssertCondition::kUnsignedLessThanEqual for the length change guard.\n\nAssociated nodes:\n-                          │
│ Int32IncrementWithOverflow\n- BranchIfInt32Compare\n- CheckInt32Condition (used via TryBuildCheckInt32Condition)\n- BuildLoadElements, BuildLoadJSArrayLength\n- LoadFixedDoubleArrayElement,                │
│ LoadFixedArrayElement\n- BranchIfFloat64IsHole, BranchIfRootConstant\n', 'Context': ['maglev_array_foreach', 'foreach_builder_entrypoints'], 'Explanation': 'Summarizes how representation conversion, index │
│ math, and bounds/length-change checks are built in the loop lowering for forEach, including specific Maglev nodes and assert conditions.', 'FileLine': 'maglev/maglev-graph-builder.cc:8153-8218,            │
│ 8226-8287'}                                                                                                                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote foreach_representation_and_bounds to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/foreach_representation_and_bounds.json
[Step 15: Duration 13.73 seconds| Input tokens: 752,992 | Output tokens: 13,869]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 16 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'foreach_elements_and_maps', 'Body': 'Elements kinds, map checks, and feedback-based specialization for Array.prototype.forEach in Maglev:\n\n-       │
│ Elements kind selection and map gating: TryReduceArrayForEach fetches PossibleMaps for the receiver from KnownNodeAspects and calls CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind).   │
│ This helper iterates over all maps ensuring supports_fast_array_iteration(map) is true and UnionElementsKindUptoSize combines them into a unified elements kind. If any map fails, lowering aborts.\n-       │
│ Protector: It requires broker()->dependencies()->DependOnNoElementsProtector() to ensure that elements non-fast-path conditions aren’t invalidated. For HOLEY kinds, this is especially relevant.\n- Map     │
│ stability: It remembers whether receiver maps were unstable before the loop and stores receiver_maps_before_loop. After invoking the callback, if maps were unstable, it re-checks the maps against the      │
│ saved set. It emits either CheckMapsWithMigration or CheckMaps depending on whether any map is a migration target, inside an eager deopt scope.\n- Elements operations: BuildLoadElements(receiver) loads    │
│ elements array. Then element load uses BuildLoadFixedDoubleArrayElement for double kinds and BuildLoadFixedArrayElement for tagged kinds. It uses the elements_kind variable to choose the path and hole     │
│ checks to conditionally skip callback.\n- Bounds checks: BranchIfInt32Compare on index vs original_length_int32, and after callback, CheckInt32Condition with AssertCondition::kUnsignedLessThanEqual vs     │
│ current_length for length-change guard.\n- Related Element access helpers in the builder (outside forEach code) provide concrete examples of elements-kind dependent loads/stores and checks, e.g.,          │
│ TryBuildElementLoadOnJSArrayOrJSObject and TryBuildElementStoreOnJSArrayOrJSObject contain checks such as CanTreatHoleAsUndefined, CheckTypedArrayBounds, MaybeGrowFastElements, EnsureWritableFastElements, │
│ and emit LoadFixed[Double]ArrayElement/StoreFixed[Double]ArrayElement nodes. Although not directly in forEach, they show the same building blocks used for specialized array element access within           │
│ Maglev.\n', 'Context': ['maglev_array_foreach', 'foreach_builder_entrypoints', 'foreach_representation_and_bounds'], 'Explanation': 'Details the elements kind and map specialization logic for forEach      │
│ lowering in Maglev and how it reuses builder helpers for elements loads, with explicit map rechecks after callbacks for unstable maps.', 'FileLine': 'maglev/maglev-graph-builder.cc:8037-8061               │
│ (CanInlineArrayIteratingBuiltin); 8179-8198, 8258-8274 (map recheck); 8077-8098, 8121-8152 (protector, callback type), 8219-8257 (element load, hole checks)'}                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote foreach_elements_and_maps to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/foreach_elements_and_maps.json
[Step 16: Duration 11.33 seconds| Input tokens: 866,089 | Output tokens: 14,463]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 17 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_rag_db_id' with arguments: {'id': 'foreach_builder_entrypoints'}                                                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: {"body": "Key entrypoints for Array.prototype.forEach specialization in Maglev reside inside MaglevGraphBuilder::TryReduceArrayForEach in src/maglev/maglev-graph-builder.cc. This function checks
that the call is speculatable, validates the receiver and its maps support fast array iteration, ensures the NoElementsProtector dependency, validates the callback, and then lowers into an explicit loop via 
MaglevSubGraphBuilder with deopt continuation builtins for eager and lazy cases.\n\nHighlights of the implementation:\n- Name: Reduce path TryReduceArrayForEach\n- File: maglev/maglev-graph-builder.cc\n- Loop
setup: uses MaglevSubGraphBuilder to build a loop with variables var_index (Smi tagged) and var_length (the JSArray length), and a LoopLabel loop_header.\n- Deopt continuations used for forEach: 
Builtin::kArrayForEachLoopLazyDeoptContinuation and Builtin::kArrayForEachLoopEagerDeoptContinuation.\n- Elements kind specialization and hole handling: the function first derives ElementsKind from the 
receiver's KnownNodeAspects maps via CanInlineArrayIteratingBuiltin and only proceeds if all maps support fast array iteration and can unify element kinds. For HOLEY kinds, branches skip callback when element
is hole (BranchIfFloat64IsHole or BranchIfRootConstant with kTheHoleValue).\n- Representation: index is Smi-tagged phi; the int32 index used for bounds comparing and element access is obtained via 
GetInt32(index_tagged). It increments via Int32IncrementWithOverflow under an eager deopt scope.\n- Length and length-change guard: original_length is built from BuildLoadJSArrayLength(receiver) and cached. 
After each callback, current_length is reloaded and compared with original_length using TryBuildCheckInt32Condition(..., AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged). If 
different value nodes, it asserts that original_length_int32 <= current_length.\n- Elements access: loads elements via BuildLoadElements(receiver). For doubles: BuildLoadFixedDoubleArrayElement(elements, 
index_int32); for tagged: BuildLoadFixedArrayElement(elements, index_int32). For HOLEY kinds, there are hole checks, using BranchIfFloat64IsHole or BranchIfRootConstant.\n- Callback validation: Ensures 
callback is NodeType::kCallable. If not statically known callable, wraps a ThrowIfNotCallable node emission inside a DeoptFrameScope lazy continuation (Builtin::kArrayForEachLoopLazyDeoptContinuation) so that
exception stack traces are proper.\n- Map re-check after callback when receiver maps were unstable: uses CheckMaps or CheckMapsWithMigration against the saved PossibleMaps set, within an eager deopt scope 
tied to Builtin::kArrayForEachLoopEagerDeoptContinuation.\n- Exception path: ThrowIfNotCallable calls into runtime ThrowCalledNonCallable with DefineExceptionHandlerAndLazyDeoptPoint. The deopt scope wiring 
ensures the lazy continuation builtin is used as deopt frame for exceptions thrown before loop lowering proceeds.\n\nKey code snippets (see FileLine for precise locations):\n- Elements kind gating/helper: 
static bool CanInlineArrayIteratingBuiltin(...)\n- TryReduceArrayForEach main lowering: emits loop, hole-skipping, callback calls, deopt scopes, and length-change guard.\n- Builtins identifiers for 
continuations: handled in MaglevGraphBuilder::DeoptFrameScope and also referenced in switch at top of file.\n", "context": |"maglev_array_foreach", "foreach_deopt_continuations", 
"foreach_throw_if_not_callable", "foreach_elements_and_maps", "foreach_representation_and_bounds"], "explanation": "This entry details the key builder entrypoint and the high-level flow of Maglev lowering of 
Array.prototype.forEach into an explicit loop with elements specialization, deopt continuations, and callback validation.", "file_line": "maglev/maglev-graph-builder.cc:8072-8320 (TryReduceArrayForEach); 
maglev/maglev-graph-builder.cc:8029-8050 (CanInlineArrayIteratingBuiltin)"}
[Step 17: Duration 3.10 seconds| Input tokens: 979,829 | Output tokens: 14,486]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 18 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'final_answer' with arguments: {'answer': '### 1. Task outcome (short version):\n- Maglev recognizes and specializes Array.prototype.forEach in MaglevGraphBuilder::TryReduceArrayForEach      │
│ (src/maglev/maglev-graph-builder.cc). It lowers forEach into an explicit index loop built with MaglevSubGraphBuilder, specialized by array elements kind (packed/double/holey), guarded by                   │
│ NoElementsProtector and receiver map checks.\n- Deopt continuations: Builtin::kArrayForEachLoopLazyDeoptContinuation and Builtin::kArrayForEachLoopEagerDeoptContinuation are used for lazy/eager deopt      │
│ scopes around callback validation/dispatch, index increment, and post-callback map/length checks.\n- Callback validation uses ThrowIfNotCallable (maglev-ir.h/.cc), emitted within a lazy deopt scope for    │
│ correct exception stack traces; exceptions wire to defined exception handler/lazy deopt points.\n- The loop uses Smi-tagged index with Int32 untagging and Int32IncrementWithOverflow; bounds and            │
│ length-change checks use unsigned Int32 comparisons. Holey element kinds skip the callback when encountering holes.\n- Elements kinds, map checks, and feedback-based specialization:                        │
│ CanInlineArrayIteratingBuiltin gates specialization; post-callback map rechecks use CheckMaps/CheckMapsWithMigration; element loads use BuildLoad[Fixed][Double]ArrayElement.\n\n### 2. Task outcome         │
│ (extremely detailed version):\n1) Where Maglev recognizes forEach and lowers to an index loop\n- Function: MaglevGraphBuilder::TryReduceArrayForEach\n  - File: src/maglev/maglev-graph-builder.cc\n  -      │
│ Approx lines (found): 8072–8320\n  - Entry checks and setup:\n    - CanSpeculateCall() and receiver present.\n    - Fetch KnownNodeAspects for receiver; require that possible maps are known                │
│ (monomorphic/specializable receiver).\n    - Elements kind gating: CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind). It returns false if any map doesn’t support fast array iteration   │
│ or unification across maps fails.\n    - Protector dependency: broker()->dependencies()->DependOnNoElementsProtector() must hold, else bail.\n    - Callback validation and lazy deopt: EnsureType(callback, │
│ NodeType::kCallable, …) wraps AddNewNode<ThrowIfNotCallable>({callback}) inside a DeoptFrameScope with Builtin::kArrayForEachLoopLazyDeoptContinuation, passing parameters [receiver, callback, this_arg,    │
│ initialK=0 (Smi), original_length]. This ensures a thrown TypeError has a correct frame and continuation.\n  - Loop lowering using MaglevSubGraphBuilder:\n    - var_index initialized as Smi 0, var_length  │
│ initialized to original_length (BuildLoadJSArrayLength(receiver)).\n    - MaglevSubGraphBuilder::LoopLabel loop_header = sub_builder.BeginLoop({&var_index, &var_length});\n    - index_tagged is the Phi;   │
│ EnsureType(index_tagged, NodeType::kSmi); obtain index_int32 = GetInt32(index_tagged).\n    - Loop condition: sub_builder.GotoIfFalse<BranchIfInt32Compare>(&loop_end, {index_int32, original_length_int32}, │
│ Operation::kLessThan).\n    - Increment: Inside an eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation), next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32});           │
│ EnsureType(next_index_int32, NodeType::kSmi). The comment notes overflow can’t happen given max array length, TODO to assert instead of deopt.\n    - Elements access and hole handling:\n      - elements = │
│ BuildLoadElements(receiver).\n      - If IsDoubleElementsKind(elements_kind): element = BuildLoadFixedDoubleArrayElement(elements, index_int32); else element = BuildLoadFixedArrayElement(elements,         │
│ index_int32).\n      - For HOLEY kinds, branch to skip callback if hole: for HOLEY_DOUBLE use BranchIfFloat64IsHole; for tagged holey use BranchIfRootConstant(element, kTheHoleValue).\n    - Callback call │
│ within lazy deopt scope (Builtin::kArrayForEachLoopLazyDeoptContinuation) with next_index_int32 captured for resume:\n      - If args.count() < 2: CallArguments(ConvertReceiverMode::kNullOrUndefined,      │
│ {element, index_tagged, receiver}).\n      - Else: CallArguments(ConvertReceiverMode::kAny, {this_arg, element, index_tagged, receiver}).\n      - SaveCallSpeculationScope and ReduceCall(callback,         │
│ call_args, …).\n    - Post-callback rechecks and length-change guard inside eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation):\n      - If receiver maps were unstable before the loop,   │
│ re-check maps against receiver_maps_before_loop; emit AddNewNode<CheckMapsWithMigration> or AddNewNode<CheckMaps>(receiver, receiver_maps_before_loop, OmitHeapObjectCheck) as appropriate.\n      - Reload  │
│ current_length = BuildLoadJSArrayLength(receiver); set var_length=current_length.\n      - If current_length != original_length node identity, assert original_length_int32 <= current_length via            │
│ TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged).\n    - Loop bookkeeping: if hole skip label bound, then  │
│ var_index = next_index_int32; sub_builder.EndLoop(&loop_header); bind loop_end; return undefined.\n  - Helper used: CanInlineArrayIteratingBuiltin(broker, maps, &kind) (approx lines 8037–8061) verifies    │
│ supports_fast_array_iteration for all maps and unions their elements kinds.\n\n- Related builder functionality used (file: src/maglev/maglev-graph-builder.cc):\n  - BuildLoadJSArrayLength(receiver),       │
│ BuildLoadElements(receiver)\n  - BuildLoadFixedDoubleArrayElement / BuildLoadFixedArrayElement\n  - BranchIfInt32Compare; CheckInt32Condition via TryBuildCheckInt32Condition\n  -                           │
│ MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}\n\n2) Deopt continuation types and where they are attached\n- Continuations:\n  - Builtin::kArrayForEachLoopLazyDeoptContinuation\n  -          │
│ Builtin::kArrayForEachLoopEagerDeoptContinuation\n- Attach points in TryReduceArrayForEach (maglev/maglev-graph-builder.cc):\n  - Lazy deopt scope around ThrowIfNotCallable and around invoking the         │
│ callback (captures [receiver, callback, this_arg, next_index, original_length]).\n  - Eager deopt scope around index increment (Int32IncrementWithOverflow), mapping recheck, and length-change guard        │
│ (captures [receiver, callback, this_arg, next_index_int32, original_length]).\n- The lazy continuation is also recognized in DeoptFrameScope::IsLazyDeoptContinuationFrame (file header region), which       │
│ includes kArrayForEachLoopLazyDeoptContinuation in its switch.\n- The TQ builtin definitions for these continuations are in builtins/array-foreach.tq (ArrayForEachLoopEagerDeoptContinuation,               │
│ ArrayForEachLoopLazyDeoptContinuation), both forwarding to ArrayForEachLoopContinuation with typed parameters. The file comments explicitly note continuation points in the optimized forEach                │
│ implementation.\n\n3) ThrowIfNotCallable emission and exception wiring\n- Emission site:\n  - In TryReduceArrayForEach, EnsureType(callback, NodeType::kCallable, …) with                                    │
│ DeoptFrameScope(Builtin::kArrayForEachLoopLazyDeoptContinuation, …) and AddNewNode<ThrowIfNotCallable>({callback}).\n- Node definition and codegen:\n  - maglev/maglev-ir.h: class ThrowIfNotCallable :      │
│ public FixedInputNodeT<1, ThrowIfNotCallable> with OpProperties::CanThrow | OpProperties::DeferredCall.\n  - maglev/maglev-ir.cc: ThrowIfNotCallable::GenerateCode: JumpIfNotCallable, then runtime call     │
│ Runtime::kThrowCalledNonCallable; masm->DefineExceptionHandlerAndLazyDeoptPoint(node) marks exception handler and lazy deopt metadata.\n- Catch handlers materialization:\n  - maglev-graph-builder.cc       │
│ (BuildMergeStates): for bytecode handler table entries, MergePointInterpreterFrameState::NewForCatchBlock constructs exception merge states (handler depth, liveness, etc.). Throwing nodes and builtins     │
│ define exception handler and lazy deopt points; the active DeoptFrameScope ensures that on throw, continuation resumes at kArrayForEachLoopLazyDeoptContinuation with captured parameters.\n\n4)             │
│ Representation selection/untagging, comparisons, and length-change guards\n- Index representation:\n  - Loop index is a Smi-tagged Phi (var_index). It is untagged to Int32 via GetInt32(index_tagged) after │
│ EnsureType(index_tagged, NodeType::kSmi). Under the hood, GetInt32 uses BuildSmiUntag when possible.\n- Increment and overflow handling:\n  - AddNewNode<Int32IncrementWithOverflow>({index_int32}) within   │
│ eager deopt scope (Eager continuation). TODO comment suggests replacing with an asserting increment since real overflow is impossible (array length < int32 max).\n- Bounds and comparisons:\n  - Loop       │
│ guard: BranchIfInt32Compare(Operation::kLessThan) between index_int32 and original_length_int32.\n  - Length-change guard: TryBuildCheckInt32Condition(original_length_int32, current_length,                │
│ AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged) — unsigned compare semantics.\n- Nodes involved:\n  - Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition, │
│ UnsafeSmiUntag (via GetInt32/GetSmiValue paths), Checked/Unsafe Smi tag/untag helpers in builder used as needed.\n\n5) Elements kinds, map checks, feedback-based specialization\n- Elements kind/path       │
│ selection:\n  - CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind): requires supports_fast_array_iteration for every map and unifies elements kinds (UnionElementsKindUptoSize). If holey │
│ kinds, emits hole checks to skip callback via BranchIfFloat64IsHole or BranchIfRootConstant(TheHoleValue).\n- Protector requirement:\n  - DependOnNoElementsProtector() must hold before specializing.\n-    │
│ Receiver map stability and recheck:\n  - Tracks whether receiver maps were unstable and the set before loop (receiver_maps_before_loop). After callback, in eager scope, re-checks maps via                  │
│ CheckMapsWithMigration or CheckMaps (OmitHeapObjectCheck), since the callback might mutate.\n- Element loads (specialized):\n  - Load elements array via BuildLoadElements(receiver), then element load      │
│ depends on elements_kind: BuildLoadFixedDoubleArrayElement or BuildLoadFixedArrayElement. No COW/grow/migration here since forEach doesn’t store, but mapping and bounds/length change are guarded.\n-       │
│ Broader Maglev element access (for reference):\n  - TryBuildElementLoadOnJSArrayOrJSObject/Store contain map checks, hole treatment (CanTreatHoleAsUndefined), bounds checks,                                │
│ MaybeGrowFastElements/EnsureWritableFastElements, and use Load/StoreFixed[Double]ArrayElement; these demonstrate the same building blocks used for forEach’s specialized loads.\n\n6) Files, canonical       │
│ symbol names, nodes, and comments\n- Files and key symbols:\n  - src/maglev/maglev-graph-builder.cc:\n    - MaglevGraphBuilder::TryReduceArrayForEach (main lowering of forEach)\n    -                      │
│ CanInlineArrayIteratingBuiltin (supports fast array iteration, unify kinds)\n    - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}\n    - BuildLoadJSArrayLength, BuildLoadElements\n    -      │
│ BranchIfInt32Compare, CheckInt32Condition, Int32IncrementWithOverflow\n    - CheckMapsWithMigration / CheckMaps\n    - BranchIfFloat64IsHole, BranchIfRootConstant\n  - src/maglev/maglev-ir.h and           │
│ src/maglev/maglev-ir.cc:\n    - class ThrowIfNotCallable; ThrowIfNotCallable::GenerateCode\n    - Nodes used by loop: Int32IncrementWithOverflow, BranchIfInt32Compare, BranchIfFloat64IsHole,               │
│ BranchIfRootConstant, CheckInt32Condition, Load/Store element nodes\n  - src/builtins/array-foreach.tq:\n    - transitioning javascript builtins ArrayForEachLoopEagerDeoptContinuation and                  │
│ ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation; comments document the continuation points.\n\n- Inline comments mentioning forEach specialization:\n  - In maglev-graph-builder.cc  │
│ around TryReduceArrayForEach: verbose trace logs like "Failed to reduce Array.prototype.forEach ..." and comments on increment overflow and re-checking maps after callback; length-change guard notes       │
│ UnsignedLessThanEqual and DeoptimizeReason::kArrayLengthChanged.\n  - In builtins/array-foreach.tq: Header comments explicitly say "All continuation points in the optimized forEach implementation…" and    │
│ show how continuations route to the generic loop.\n\nConcrete pointers (as seen by searching/reading):\n- maglev/maglev-graph-builder.cc:\n  - TryReduceArrayForEach: lines ~8072–8320 (search hit strings:  │
│ "Failed to reduce Array.prototype.forEach", ThrowIfNotCallable block at ~8127–8135; eager deopt scopes at ~8200, ~8288; map recheck and length-change guard at ~8258–8274; hole checks and element loads     │
│ ~8219–8257; CanInlineArrayIteratingBuiltin ~8037–8061)\n  - DeoptFrameScope::IsLazyDeoptContinuationFrame includes kArrayForEachLoopLazyDeoptContinuation (~413)\n- builtins/array-foreach.tq:\n  -          │
│ ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation plus ArrayForEachLoopContinuation definitions (file start region)\n- maglev/maglev-ir.h/.cc:\n  - ThrowIfNotCallable class  │
│ (~9740–9756) and GenerateCode (~5424–5438), with DefineExceptionHandlerAndLazyDeoptPoint\n\n### 3. Additional context (if relevant):\n- The forEach lowering leverages MaglevSubGraphBuilder to construct    │
│ local control/data flow with variables and loop headers, preserving KnownNodeAspects/Known maps across merges.\n- Exception flow: while Try/Catch general handling is in BuildMergeStates (creating catch    │
│ merge states using HandlerTable), individual nodes that can throw call DefineExceptionHandlerAndLazyDeoptPoint to install metadata; the active DeoptFrameScope determines whether exceptions are treated as  │
│ lazy or eager continuations. For forEach’s callback validation/calls, the lazy continuation is used to ensure proper re-entry semantics.\n- Although not directly used in forEach, broader element access    │
│ code demonstrates how Maglev emits checks like CheckTypedArrayBounds, MaybeGrowFastElements, and EnsureWritableFastElements when needed; forEach only reads and does not grow or write.\n\nWritten RAG DB    │
│ IDs (what they contain):\n- foreach_builder_entrypoints: Summarizes TryReduceArrayForEach, loop lowering, elements specialization, indices/lengths, and where continuations are used.\n-                     │
│ foreach_deopt_continuations: Details the eager/lazy continuation builtins, where DeoptFrameScope wraps code, parameters passed, and the tie-in to TQ builtins.\n- foreach_throw_if_not_callable: Shows the   │
│ ThrowIfNotCallable emission inside a lazy deopt scope and its codegen/runtime throw wiring with exception handler points.\n- foreach_representation_and_bounds: Documents index Smi/Int32 handling,          │
│ increment, BranchIfInt32Compare, CheckInt32Condition, unsigned comparisons, and the length-change guard.\n- foreach_elements_and_maps: Captures elements kind gating via CanInlineArrayIteratingBuiltin,     │
│ NoElementsProtector requirement, post-callback map rechecks (CheckMaps/CheckMapsWithMigration), and the element load and hole-skipping logic.\n'}                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: ### 1. Task outcome (short version):
- Maglev recognizes and specializes Array.prototype.forEach in MaglevGraphBuilder::TryReduceArrayForEach (src/maglev/maglev-graph-builder.cc). It lowers forEach into an explicit index loop built with 
MaglevSubGraphBuilder, specialized by array elements kind (packed/double/holey), guarded by NoElementsProtector and receiver map checks.
- Deopt continuations: Builtin::kArrayForEachLoopLazyDeoptContinuation and Builtin::kArrayForEachLoopEagerDeoptContinuation are used for lazy/eager deopt scopes around callback validation/dispatch, index 
increment, and post-callback map/length checks.
- Callback validation uses ThrowIfNotCallable (maglev-ir.h/.cc), emitted within a lazy deopt scope for correct exception stack traces; exceptions wire to defined exception handler/lazy deopt points.
- The loop uses Smi-tagged index with Int32 untagging and Int32IncrementWithOverflow; bounds and length-change checks use unsigned Int32 comparisons. Holey element kinds skip the callback when encountering 
holes.
- Elements kinds, map checks, and feedback-based specialization: CanInlineArrayIteratingBuiltin gates specialization; post-callback map rechecks use CheckMaps/CheckMapsWithMigration; element loads use 
BuildLoad|Fixed]|Double]ArrayElement.

### 2. Task outcome (extremely detailed version):
1) Where Maglev recognizes forEach and lowers to an index loop
- Function: MaglevGraphBuilder::TryReduceArrayForEach
  - File: src/maglev/maglev-graph-builder.cc
  - Approx lines (found): 8072–8320
  - Entry checks and setup:
    - CanSpeculateCall() and receiver present.
    - Fetch KnownNodeAspects for receiver; require that possible maps are known (monomorphic/specializable receiver).
    - Elements kind gating: CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind). It returns false if any map doesn’t support fast array iteration or unification across maps fails.
    - Protector dependency: broker()->dependencies()->DependOnNoElementsProtector() must hold, else bail.
    - Callback validation and lazy deopt: EnsureType(callback, NodeType::kCallable, …) wraps AddNewNode<ThrowIfNotCallable>({callback}) inside a DeoptFrameScope with 
Builtin::kArrayForEachLoopLazyDeoptContinuation, passing parameters |receiver, callback, this_arg, initialK=0 (Smi), original_length]. This ensures a thrown TypeError has a correct frame and continuation.
  - Loop lowering using MaglevSubGraphBuilder:
    - var_index initialized as Smi 0, var_length initialized to original_length (BuildLoadJSArrayLength(receiver)).
    - MaglevSubGraphBuilder::LoopLabel loop_header = sub_builder.BeginLoop({&var_index, &var_length});
    - index_tagged is the Phi; EnsureType(index_tagged, NodeType::kSmi); obtain index_int32 = GetInt32(index_tagged).
    - Loop condition: sub_builder.GotoIfFalse<BranchIfInt32Compare>(&loop_end, {index_int32, original_length_int32}, Operation::kLessThan).
    - Increment: Inside an eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation), next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32}); EnsureType(next_index_int32, 
NodeType::kSmi). The comment notes overflow can’t happen given max array length, TODO to assert instead of deopt.
    - Elements access and hole handling:
      - elements = BuildLoadElements(receiver).
      - If IsDoubleElementsKind(elements_kind): element = BuildLoadFixedDoubleArrayElement(elements, index_int32); else element = BuildLoadFixedArrayElement(elements, index_int32).
      - For HOLEY kinds, branch to skip callback if hole: for HOLEY_DOUBLE use BranchIfFloat64IsHole; for tagged holey use BranchIfRootConstant(element, kTheHoleValue).
    - Callback call within lazy deopt scope (Builtin::kArrayForEachLoopLazyDeoptContinuation) with next_index_int32 captured for resume:
      - If args.count() < 2: CallArguments(ConvertReceiverMode::kNullOrUndefined, {element, index_tagged, receiver}).
      - Else: CallArguments(ConvertReceiverMode::kAny, {this_arg, element, index_tagged, receiver}).
      - SaveCallSpeculationScope and ReduceCall(callback, call_args, …).
    - Post-callback rechecks and length-change guard inside eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation):
      - If receiver maps were unstable before the loop, re-check maps against receiver_maps_before_loop; emit AddNewNode<CheckMapsWithMigration> or AddNewNode<CheckMaps>(receiver, receiver_maps_before_loop, 
OmitHeapObjectCheck) as appropriate.
      - Reload current_length = BuildLoadJSArrayLength(receiver); set var_length=current_length.
      - If current_length != original_length node identity, assert original_length_int32 <= current_length via TryBuildCheckInt32Condition(original_length_int32, current_length, 
AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged).
    - Loop bookkeeping: if hole skip label bound, then var_index = next_index_int32; sub_builder.EndLoop(&loop_header); bind loop_end; return undefined.
  - Helper used: CanInlineArrayIteratingBuiltin(broker, maps, &kind) (approx lines 8037–8061) verifies supports_fast_array_iteration for all maps and unions their elements kinds.

- Related builder functionality used (file: src/maglev/maglev-graph-builder.cc):
  - BuildLoadJSArrayLength(receiver), BuildLoadElements(receiver)
  - BuildLoadFixedDoubleArrayElement / BuildLoadFixedArrayElement
  - BranchIfInt32Compare; CheckInt32Condition via TryBuildCheckInt32Condition
  - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}

2) Deopt continuation types and where they are attached
- Continuations:
  - Builtin::kArrayForEachLoopLazyDeoptContinuation
  - Builtin::kArrayForEachLoopEagerDeoptContinuation
- Attach points in TryReduceArrayForEach (maglev/maglev-graph-builder.cc):
  - Lazy deopt scope around ThrowIfNotCallable and around invoking the callback (captures |receiver, callback, this_arg, next_index, original_length]).
  - Eager deopt scope around index increment (Int32IncrementWithOverflow), mapping recheck, and length-change guard (captures |receiver, callback, this_arg, next_index_int32, original_length]).
- The lazy continuation is also recognized in DeoptFrameScope::IsLazyDeoptContinuationFrame (file header region), which includes kArrayForEachLoopLazyDeoptContinuation in its switch.
- The TQ builtin definitions for these continuations are in builtins/array-foreach.tq (ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation), both forwarding to 
ArrayForEachLoopContinuation with typed parameters. The file comments explicitly note continuation points in the optimized forEach implementation.

3) ThrowIfNotCallable emission and exception wiring
- Emission site:
  - In TryReduceArrayForEach, EnsureType(callback, NodeType::kCallable, …) with DeoptFrameScope(Builtin::kArrayForEachLoopLazyDeoptContinuation, …) and AddNewNode<ThrowIfNotCallable>({callback}).
- Node definition and codegen:
  - maglev/maglev-ir.h: class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> with OpProperties::CanThrow | OpProperties::DeferredCall.
  - maglev/maglev-ir.cc: ThrowIfNotCallable::GenerateCode: JumpIfNotCallable, then runtime call Runtime::kThrowCalledNonCallable; masm->DefineExceptionHandlerAndLazyDeoptPoint(node) marks exception handler 
and lazy deopt metadata.
- Catch handlers materialization:
  - maglev-graph-builder.cc (BuildMergeStates): for bytecode handler table entries, MergePointInterpreterFrameState::NewForCatchBlock constructs exception merge states (handler depth, liveness, etc.). 
Throwing nodes and builtins define exception handler and lazy deopt points; the active DeoptFrameScope ensures that on throw, continuation resumes at kArrayForEachLoopLazyDeoptContinuation with captured 
parameters.

4) Representation selection/untagging, comparisons, and length-change guards
- Index representation:
  - Loop index is a Smi-tagged Phi (var_index). It is untagged to Int32 via GetInt32(index_tagged) after EnsureType(index_tagged, NodeType::kSmi). Under the hood, GetInt32 uses BuildSmiUntag when possible.
- Increment and overflow handling:
  - AddNewNode<Int32IncrementWithOverflow>({index_int32}) within eager deopt scope (Eager continuation). TODO comment suggests replacing with an asserting increment since real overflow is impossible (array 
length < int32 max).
- Bounds and comparisons:
  - Loop guard: BranchIfInt32Compare(Operation::kLessThan) between index_int32 and original_length_int32.
  - Length-change guard: TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged) — unsigned compare semantics.
- Nodes involved:
  - Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition, UnsafeSmiUntag (via GetInt32/GetSmiValue paths), Checked/Unsafe Smi tag/untag helpers in builder used as needed.

5) Elements kinds, map checks, feedback-based specialization
- Elements kind/path selection:
  - CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind): requires supports_fast_array_iteration for every map and unifies elements kinds (UnionElementsKindUptoSize). If holey kinds, emits 
hole checks to skip callback via BranchIfFloat64IsHole or BranchIfRootConstant(TheHoleValue).
- Protector requirement:
  - DependOnNoElementsProtector() must hold before specializing.
- Receiver map stability and recheck:
  - Tracks whether receiver maps were unstable and the set before loop (receiver_maps_before_loop). After callback, in eager scope, re-checks maps via CheckMapsWithMigration or CheckMaps 
(OmitHeapObjectCheck), since the callback might mutate.
- Element loads (specialized):
  - Load elements array via BuildLoadElements(receiver), then element load depends on elements_kind: BuildLoadFixedDoubleArrayElement or BuildLoadFixedArrayElement. No COW/grow/migration here since forEach 
doesn’t store, but mapping and bounds/length change are guarded.
- Broader Maglev element access (for reference):
  - TryBuildElementLoadOnJSArrayOrJSObject/Store contain map checks, hole treatment (CanTreatHoleAsUndefined), bounds checks, MaybeGrowFastElements/EnsureWritableFastElements, and use 
Load/StoreFixed|Double]ArrayElement; these demonstrate the same building blocks used for forEach’s specialized loads.

6) Files, canonical symbol names, nodes, and comments
- Files and key symbols:
  - src/maglev/maglev-graph-builder.cc:
    - MaglevGraphBuilder::TryReduceArrayForEach (main lowering of forEach)
    - CanInlineArrayIteratingBuiltin (supports fast array iteration, unify kinds)
    - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}
    - BuildLoadJSArrayLength, BuildLoadElements
    - BranchIfInt32Compare, CheckInt32Condition, Int32IncrementWithOverflow
    - CheckMapsWithMigration / CheckMaps
    - BranchIfFloat64IsHole, BranchIfRootConstant
  - src/maglev/maglev-ir.h and src/maglev/maglev-ir.cc:
    - class ThrowIfNotCallable; ThrowIfNotCallable::GenerateCode
    - Nodes used by loop: Int32IncrementWithOverflow, BranchIfInt32Compare, BranchIfFloat64IsHole, BranchIfRootConstant, CheckInt32Condition, Load/Store element nodes
  - src/builtins/array-foreach.tq:
    - transitioning javascript builtins ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation; comments document the continuation points.

- Inline comments mentioning forEach specialization:
  - In maglev-graph-builder.cc around TryReduceArrayForEach: verbose trace logs like "Failed to reduce Array.prototype.forEach ..." and comments on increment overflow and re-checking maps after callback; 
length-change guard notes UnsignedLessThanEqual and DeoptimizeReason::kArrayLengthChanged.
  - In builtins/array-foreach.tq: Header comments explicitly say "All continuation points in the optimized forEach implementation…" and show how continuations route to the generic loop.

Concrete pointers (as seen by searching/reading):
- maglev/maglev-graph-builder.cc:
  - TryReduceArrayForEach: lines ~8072–8320 (search hit strings: "Failed to reduce Array.prototype.forEach", ThrowIfNotCallable block at ~8127–8135; eager deopt scopes at ~8200, ~8288; map recheck and 
length-change guard at ~8258–8274; hole checks and element loads ~8219–8257; CanInlineArrayIteratingBuiltin ~8037–8061)
  - DeoptFrameScope::IsLazyDeoptContinuationFrame includes kArrayForEachLoopLazyDeoptContinuation (~413)
- builtins/array-foreach.tq:
  - ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation plus ArrayForEachLoopContinuation definitions (file start region)
- maglev/maglev-ir.h/.cc:
  - ThrowIfNotCallable class (~9740–9756) and GenerateCode (~5424–5438), with DefineExceptionHandlerAndLazyDeoptPoint

### 3. Additional context (if relevant):
- The forEach lowering leverages MaglevSubGraphBuilder to construct local control/data flow with variables and loop headers, preserving KnownNodeAspects/Known maps across merges.
- Exception flow: while Try/Catch general handling is in BuildMergeStates (creating catch merge states using HandlerTable), individual nodes that can throw call DefineExceptionHandlerAndLazyDeoptPoint to 
install metadata; the active DeoptFrameScope determines whether exceptions are treated as lazy or eager continuations. For forEach’s callback validation/calls, the lazy continuation is used to ensure proper 
re-entry semantics.
- Although not directly used in forEach, broader element access code demonstrates how Maglev emits checks like CheckTypedArrayBounds, MaybeGrowFastElements, and EnsureWritableFastElements when needed; forEach
only reads and does not grow or write.

Written RAG DB IDs (what they contain):
- foreach_builder_entrypoints: Summarizes TryReduceArrayForEach, loop lowering, elements specialization, indices/lengths, and where continuations are used.
- foreach_deopt_continuations: Details the eager/lazy continuation builtins, where DeoptFrameScope wraps code, parameters passed, and the tie-in to TQ builtins.
- foreach_throw_if_not_callable: Shows the ThrowIfNotCallable emission inside a lazy deopt scope and its codegen/runtime throw wiring with exception handler points.
- foreach_representation_and_bounds: Documents index Smi/Int32 handling, increment, BranchIfInt32Compare, CheckInt32Condition, unsigned comparisons, and the length-change guard.
- foreach_elements_and_maps: Captures elements kind gating via CanInlineArrayIteratingBuiltin, NoElementsProtector requirement, post-callback map rechecks (CheckMaps/CheckMapsWithMigration), and the element 
load and hole-skipping logic.
Final answer: ### 1. Task outcome (short version):
- Maglev recognizes and specializes Array.prototype.forEach in MaglevGraphBuilder::TryReduceArrayForEach (src/maglev/maglev-graph-builder.cc). It lowers forEach into an explicit index loop built with 
MaglevSubGraphBuilder, specialized by array elements kind (packed/double/holey), guarded by NoElementsProtector and receiver map checks.
- Deopt continuations: Builtin::kArrayForEachLoopLazyDeoptContinuation and Builtin::kArrayForEachLoopEagerDeoptContinuation are used for lazy/eager deopt scopes around callback validation/dispatch, index 
increment, and post-callback map/length checks.
- Callback validation uses ThrowIfNotCallable (maglev-ir.h/.cc), emitted within a lazy deopt scope for correct exception stack traces; exceptions wire to defined exception handler/lazy deopt points.
- The loop uses Smi-tagged index with Int32 untagging and Int32IncrementWithOverflow; bounds and length-change checks use unsigned Int32 comparisons. Holey element kinds skip the callback when encountering 
holes.
- Elements kinds, map checks, and feedback-based specialization: CanInlineArrayIteratingBuiltin gates specialization; post-callback map rechecks use CheckMaps/CheckMapsWithMigration; element loads use 
BuildLoad[Fixed][Double]ArrayElement.

### 2. Task outcome (extremely detailed version):
1) Where Maglev recognizes forEach and lowers to an index loop
- Function: MaglevGraphBuilder::TryReduceArrayForEach
  - File: src/maglev/maglev-graph-builder.cc
  - Approx lines (found): 8072–8320
  - Entry checks and setup:
    - CanSpeculateCall() and receiver present.
    - Fetch KnownNodeAspects for receiver; require that possible maps are known (monomorphic/specializable receiver).
    - Elements kind gating: CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind). It returns false if any map doesn’t support fast array iteration or unification across maps fails.
    - Protector dependency: broker()->dependencies()->DependOnNoElementsProtector() must hold, else bail.
    - Callback validation and lazy deopt: EnsureType(callback, NodeType::kCallable, …) wraps AddNewNode<ThrowIfNotCallable>({callback}) inside a DeoptFrameScope with 
Builtin::kArrayForEachLoopLazyDeoptContinuation, passing parameters [receiver, callback, this_arg, initialK=0 (Smi), original_length]. This ensures a thrown TypeError has a correct frame and continuation.
  - Loop lowering using MaglevSubGraphBuilder:
    - var_index initialized as Smi 0, var_length initialized to original_length (BuildLoadJSArrayLength(receiver)).
    - MaglevSubGraphBuilder::LoopLabel loop_header = sub_builder.BeginLoop({&var_index, &var_length});
    - index_tagged is the Phi; EnsureType(index_tagged, NodeType::kSmi); obtain index_int32 = GetInt32(index_tagged).
    - Loop condition: sub_builder.GotoIfFalse<BranchIfInt32Compare>(&loop_end, {index_int32, original_length_int32}, Operation::kLessThan).
    - Increment: Inside an eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation), next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32}); EnsureType(next_index_int32, 
NodeType::kSmi). The comment notes overflow can’t happen given max array length, TODO to assert instead of deopt.
    - Elements access and hole handling:
      - elements = BuildLoadElements(receiver).
      - If IsDoubleElementsKind(elements_kind): element = BuildLoadFixedDoubleArrayElement(elements, index_int32); else element = BuildLoadFixedArrayElement(elements, index_int32).
      - For HOLEY kinds, branch to skip callback if hole: for HOLEY_DOUBLE use BranchIfFloat64IsHole; for tagged holey use BranchIfRootConstant(element, kTheHoleValue).
    - Callback call within lazy deopt scope (Builtin::kArrayForEachLoopLazyDeoptContinuation) with next_index_int32 captured for resume:
      - If args.count() < 2: CallArguments(ConvertReceiverMode::kNullOrUndefined, {element, index_tagged, receiver}).
      - Else: CallArguments(ConvertReceiverMode::kAny, {this_arg, element, index_tagged, receiver}).
      - SaveCallSpeculationScope and ReduceCall(callback, call_args, …).
    - Post-callback rechecks and length-change guard inside eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation):
      - If receiver maps were unstable before the loop, re-check maps against receiver_maps_before_loop; emit AddNewNode<CheckMapsWithMigration> or AddNewNode<CheckMaps>(receiver, receiver_maps_before_loop, 
OmitHeapObjectCheck) as appropriate.
      - Reload current_length = BuildLoadJSArrayLength(receiver); set var_length=current_length.
      - If current_length != original_length node identity, assert original_length_int32 <= current_length via TryBuildCheckInt32Condition(original_length_int32, current_length, 
AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged).
    - Loop bookkeeping: if hole skip label bound, then var_index = next_index_int32; sub_builder.EndLoop(&loop_header); bind loop_end; return undefined.
  - Helper used: CanInlineArrayIteratingBuiltin(broker, maps, &kind) (approx lines 8037–8061) verifies supports_fast_array_iteration for all maps and unions their elements kinds.

- Related builder functionality used (file: src/maglev/maglev-graph-builder.cc):
  - BuildLoadJSArrayLength(receiver), BuildLoadElements(receiver)
  - BuildLoadFixedDoubleArrayElement / BuildLoadFixedArrayElement
  - BranchIfInt32Compare; CheckInt32Condition via TryBuildCheckInt32Condition
  - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}

2) Deopt continuation types and where they are attached
- Continuations:
  - Builtin::kArrayForEachLoopLazyDeoptContinuation
  - Builtin::kArrayForEachLoopEagerDeoptContinuation
- Attach points in TryReduceArrayForEach (maglev/maglev-graph-builder.cc):
  - Lazy deopt scope around ThrowIfNotCallable and around invoking the callback (captures [receiver, callback, this_arg, next_index, original_length]).
  - Eager deopt scope around index increment (Int32IncrementWithOverflow), mapping recheck, and length-change guard (captures [receiver, callback, this_arg, next_index_int32, original_length]).
- The lazy continuation is also recognized in DeoptFrameScope::IsLazyDeoptContinuationFrame (file header region), which includes kArrayForEachLoopLazyDeoptContinuation in its switch.
- The TQ builtin definitions for these continuations are in builtins/array-foreach.tq (ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation), both forwarding to 
ArrayForEachLoopContinuation with typed parameters. The file comments explicitly note continuation points in the optimized forEach implementation.

3) ThrowIfNotCallable emission and exception wiring
- Emission site:
  - In TryReduceArrayForEach, EnsureType(callback, NodeType::kCallable, …) with DeoptFrameScope(Builtin::kArrayForEachLoopLazyDeoptContinuation, …) and AddNewNode<ThrowIfNotCallable>({callback}).
- Node definition and codegen:
  - maglev/maglev-ir.h: class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> with OpProperties::CanThrow | OpProperties::DeferredCall.
  - maglev/maglev-ir.cc: ThrowIfNotCallable::GenerateCode: JumpIfNotCallable, then runtime call Runtime::kThrowCalledNonCallable; masm->DefineExceptionHandlerAndLazyDeoptPoint(node) marks exception handler 
and lazy deopt metadata.
- Catch handlers materialization:
  - maglev-graph-builder.cc (BuildMergeStates): for bytecode handler table entries, MergePointInterpreterFrameState::NewForCatchBlock constructs exception merge states (handler depth, liveness, etc.). 
Throwing nodes and builtins define exception handler and lazy deopt points; the active DeoptFrameScope ensures that on throw, continuation resumes at kArrayForEachLoopLazyDeoptContinuation with captured 
parameters.

4) Representation selection/untagging, comparisons, and length-change guards
- Index representation:
  - Loop index is a Smi-tagged Phi (var_index). It is untagged to Int32 via GetInt32(index_tagged) after EnsureType(index_tagged, NodeType::kSmi). Under the hood, GetInt32 uses BuildSmiUntag when possible.
- Increment and overflow handling:
  - AddNewNode<Int32IncrementWithOverflow>({index_int32}) within eager deopt scope (Eager continuation). TODO comment suggests replacing with an asserting increment since real overflow is impossible (array 
length < int32 max).
- Bounds and comparisons:
  - Loop guard: BranchIfInt32Compare(Operation::kLessThan) between index_int32 and original_length_int32.
  - Length-change guard: TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged) — unsigned compare semantics.
- Nodes involved:
  - Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition, UnsafeSmiUntag (via GetInt32/GetSmiValue paths), Checked/Unsafe Smi tag/untag helpers in builder used as needed.

5) Elements kinds, map checks, feedback-based specialization
- Elements kind/path selection:
  - CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind): requires supports_fast_array_iteration for every map and unifies elements kinds (UnionElementsKindUptoSize). If holey kinds, emits 
hole checks to skip callback via BranchIfFloat64IsHole or BranchIfRootConstant(TheHoleValue).
- Protector requirement:
  - DependOnNoElementsProtector() must hold before specializing.
- Receiver map stability and recheck:
  - Tracks whether receiver maps were unstable and the set before loop (receiver_maps_before_loop). After callback, in eager scope, re-checks maps via CheckMapsWithMigration or CheckMaps 
(OmitHeapObjectCheck), since the callback might mutate.
- Element loads (specialized):
  - Load elements array via BuildLoadElements(receiver), then element load depends on elements_kind: BuildLoadFixedDoubleArrayElement or BuildLoadFixedArrayElement. No COW/grow/migration here since forEach 
doesn’t store, but mapping and bounds/length change are guarded.
- Broader Maglev element access (for reference):
  - TryBuildElementLoadOnJSArrayOrJSObject/Store contain map checks, hole treatment (CanTreatHoleAsUndefined), bounds checks, MaybeGrowFastElements/EnsureWritableFastElements, and use 
Load/StoreFixed[Double]ArrayElement; these demonstrate the same building blocks used for forEach’s specialized loads.

6) Files, canonical symbol names, nodes, and comments
- Files and key symbols:
  - src/maglev/maglev-graph-builder.cc:
    - MaglevGraphBuilder::TryReduceArrayForEach (main lowering of forEach)
    - CanInlineArrayIteratingBuiltin (supports fast array iteration, unify kinds)
    - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}
    - BuildLoadJSArrayLength, BuildLoadElements
    - BranchIfInt32Compare, CheckInt32Condition, Int32IncrementWithOverflow
    - CheckMapsWithMigration / CheckMaps
    - BranchIfFloat64IsHole, BranchIfRootConstant
  - src/maglev/maglev-ir.h and src/maglev/maglev-ir.cc:
    - class ThrowIfNotCallable; ThrowIfNotCallable::GenerateCode
    - Nodes used by loop: Int32IncrementWithOverflow, BranchIfInt32Compare, BranchIfFloat64IsHole, BranchIfRootConstant, CheckInt32Condition, Load/Store element nodes
  - src/builtins/array-foreach.tq:
    - transitioning javascript builtins ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation; comments document the continuation points.

- Inline comments mentioning forEach specialization:
  - In maglev-graph-builder.cc around TryReduceArrayForEach: verbose trace logs like "Failed to reduce Array.prototype.forEach ..." and comments on increment overflow and re-checking maps after callback; 
length-change guard notes UnsignedLessThanEqual and DeoptimizeReason::kArrayLengthChanged.
  - In builtins/array-foreach.tq: Header comments explicitly say "All continuation points in the optimized forEach implementation…" and show how continuations route to the generic loop.

Concrete pointers (as seen by searching/reading):
- maglev/maglev-graph-builder.cc:
  - TryReduceArrayForEach: lines ~8072–8320 (search hit strings: "Failed to reduce Array.prototype.forEach", ThrowIfNotCallable block at ~8127–8135; eager deopt scopes at ~8200, ~8288; map recheck and 
length-change guard at ~8258–8274; hole checks and element loads ~8219–8257; CanInlineArrayIteratingBuiltin ~8037–8061)
  - DeoptFrameScope::IsLazyDeoptContinuationFrame includes kArrayForEachLoopLazyDeoptContinuation (~413)
- builtins/array-foreach.tq:
  - ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation plus ArrayForEachLoopContinuation definitions (file start region)
- maglev/maglev-ir.h/.cc:
  - ThrowIfNotCallable class (~9740–9756) and GenerateCode (~5424–5438), with DefineExceptionHandlerAndLazyDeoptPoint

### 3. Additional context (if relevant):
- The forEach lowering leverages MaglevSubGraphBuilder to construct local control/data flow with variables and loop headers, preserving KnownNodeAspects/Known maps across merges.
- Exception flow: while Try/Catch general handling is in BuildMergeStates (creating catch merge states using HandlerTable), individual nodes that can throw call DefineExceptionHandlerAndLazyDeoptPoint to 
install metadata; the active DeoptFrameScope determines whether exceptions are treated as lazy or eager continuations. For forEach’s callback validation/calls, the lazy continuation is used to ensure proper 
re-entry semantics.
- Although not directly used in forEach, broader element access code demonstrates how Maglev emits checks like CheckTypedArrayBounds, MaybeGrowFastElements, and EnsureWritableFastElements when needed; forEach
only reads and does not grow or write.

Written RAG DB IDs (what they contain):
- foreach_builder_entrypoints: Summarizes TryReduceArrayForEach, loop lowering, elements specialization, indices/lengths, and where continuations are used.
- foreach_deopt_continuations: Details the eager/lazy continuation builtins, where DeoptFrameScope wraps code, parameters passed, and the tie-in to TQ builtins.
- foreach_throw_if_not_callable: Shows the ThrowIfNotCallable emission inside a lazy deopt scope and its codegen/runtime throw wiring with exception handler points.
- foreach_representation_and_bounds: Documents index Smi/Int32 handling, increment, BranchIfInt32Compare, CheckInt32Condition, unsigned comparisons, and the length-change guard.
- foreach_elements_and_maps: Captures elements kind gating via CanInlineArrayIteratingBuiltin, NoElementsProtector requirement, post-callback map rechecks (CheckMaps/CheckMapsWithMigration), and the element 
load and hole-skipping logic.

[Step 18: Duration 163.06 seconds| Input tokens: 1,094,470 | Output tokens: 19,443]
Observations: Here is the final answer from your managed agent 'V8Search':
### 1. Task outcome (short version):
- Maglev recognizes and specializes Array.prototype.forEach in MaglevGraphBuilder::TryReduceArrayForEach (src/maglev/maglev-graph-builder.cc). It lowers forEach into an explicit index loop built with 
MaglevSubGraphBuilder, specialized by array elements kind (packed/double/holey), guarded by NoElementsProtector and receiver map checks.
- Deopt continuations: Builtin::kArrayForEachLoopLazyDeoptContinuation and Builtin::kArrayForEachLoopEagerDeoptContinuation are used for lazy/eager deopt scopes around callback validation/dispatch, index 
increment, and post-callback map/length checks.
- Callback validation uses ThrowIfNotCallable (maglev-ir.h/.cc), emitted within a lazy deopt scope for correct exception stack traces; exceptions wire to defined exception handler/lazy deopt points.
- The loop uses Smi-tagged index with Int32 untagging and Int32IncrementWithOverflow; bounds and length-change checks use unsigned Int32 comparisons. Holey element kinds skip the callback when encountering 
holes.
- Elements kinds, map checks, and feedback-based specialization: CanInlineArrayIteratingBuiltin gates specialization; post-callback map rechecks use CheckMaps/CheckMapsWithMigration; element loads use 
BuildLoad|Fixed]|Double]ArrayElement.

### 2. Task outcome (extremely detailed version):
1) Where Maglev recognizes forEach and lowers to an index loop
- Function: MaglevGraphBuilder::TryReduceArrayForEach
  - File: src/maglev/maglev-graph-builder.cc
  - Approx lines (found): 8072–8320
  - Entry checks and setup:
    - CanSpeculateCall() and receiver present.
    - Fetch KnownNodeAspects for receiver; require that possible maps are known (monomorphic/specializable receiver).
    - Elements kind gating: CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind). It returns false if any map doesn’t support fast array iteration or unification across maps fails.
    - Protector dependency: broker()->dependencies()->DependOnNoElementsProtector() must hold, else bail.
    - Callback validation and lazy deopt: EnsureType(callback, NodeType::kCallable, …) wraps AddNewNode<ThrowIfNotCallable>({callback}) inside a DeoptFrameScope with 
Builtin::kArrayForEachLoopLazyDeoptContinuation, passing parameters |receiver, callback, this_arg, initialK=0 (Smi), original_length]. This ensures a thrown TypeError has a correct frame and continuation.
  - Loop lowering using MaglevSubGraphBuilder:
    - var_index initialized as Smi 0, var_length initialized to original_length (BuildLoadJSArrayLength(receiver)).
    - MaglevSubGraphBuilder::LoopLabel loop_header = sub_builder.BeginLoop({&var_index, &var_length});
    - index_tagged is the Phi; EnsureType(index_tagged, NodeType::kSmi); obtain index_int32 = GetInt32(index_tagged).
    - Loop condition: sub_builder.GotoIfFalse<BranchIfInt32Compare>(&loop_end, {index_int32, original_length_int32}, Operation::kLessThan).
    - Increment: Inside an eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation), next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32}); EnsureType(next_index_int32, 
NodeType::kSmi). The comment notes overflow can’t happen given max array length, TODO to assert instead of deopt.
    - Elements access and hole handling:
      - elements = BuildLoadElements(receiver).
      - If IsDoubleElementsKind(elements_kind): element = BuildLoadFixedDoubleArrayElement(elements, index_int32); else element = BuildLoadFixedArrayElement(elements, index_int32).
      - For HOLEY kinds, branch to skip callback if hole: for HOLEY_DOUBLE use BranchIfFloat64IsHole; for tagged holey use BranchIfRootConstant(element, kTheHoleValue).
    - Callback call within lazy deopt scope (Builtin::kArrayForEachLoopLazyDeoptContinuation) with next_index_int32 captured for resume:
      - If args.count() < 2: CallArguments(ConvertReceiverMode::kNullOrUndefined, {element, index_tagged, receiver}).
      - Else: CallArguments(ConvertReceiverMode::kAny, {this_arg, element, index_tagged, receiver}).
      - SaveCallSpeculationScope and ReduceCall(callback, call_args, …).
    - Post-callback rechecks and length-change guard inside eager deopt scope (Builtin::kArrayForEachLoopEagerDeoptContinuation):
      - If receiver maps were unstable before the loop, re-check maps against receiver_maps_before_loop; emit AddNewNode<CheckMapsWithMigration> or AddNewNode<CheckMaps>(receiver, receiver_maps_before_loop, 
OmitHeapObjectCheck) as appropriate.
      - Reload current_length = BuildLoadJSArrayLength(receiver); set var_length=current_length.
      - If current_length != original_length node identity, assert original_length_int32 <= current_length via TryBuildCheckInt32Condition(original_length_int32, current_length, 
AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged).
    - Loop bookkeeping: if hole skip label bound, then var_index = next_index_int32; sub_builder.EndLoop(&loop_header); bind loop_end; return undefined.
  - Helper used: CanInlineArrayIteratingBuiltin(broker, maps, &kind) (approx lines 8037–8061) verifies supports_fast_array_iteration for all maps and unions their elements kinds.

- Related builder functionality used (file: src/maglev/maglev-graph-builder.cc):
  - BuildLoadJSArrayLength(receiver), BuildLoadElements(receiver)
  - BuildLoadFixedDoubleArrayElement / BuildLoadFixedArrayElement
  - BranchIfInt32Compare; CheckInt32Condition via TryBuildCheckInt32Condition
  - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}

2) Deopt continuation types and where they are attached
- Continuations:
  - Builtin::kArrayForEachLoopLazyDeoptContinuation
  - Builtin::kArrayForEachLoopEagerDeoptContinuation
- Attach points in TryReduceArrayForEach (maglev/maglev-graph-builder.cc):
  - Lazy deopt scope around ThrowIfNotCallable and around invoking the callback (captures |receiver, callback, this_arg, next_index, original_length]).
  - Eager deopt scope around index increment (Int32IncrementWithOverflow), mapping recheck, and length-change guard (captures |receiver, callback, this_arg, next_index_int32, original_length]).
- The lazy continuation is also recognized in DeoptFrameScope::IsLazyDeoptContinuationFrame (file header region), which includes kArrayForEachLoopLazyDeoptContinuation in its switch.
- The TQ builtin definitions for these continuations are in builtins/array-foreach.tq (ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation), both forwarding to 
ArrayForEachLoopContinuation with typed parameters. The file comments explicitly note continuation points in the optimized forEach implementation.

3) ThrowIfNotCallable emission and exception wiring
- Emission site:
  - In TryReduceArrayForEach, EnsureType(callback, NodeType::kCallable, …) with DeoptFrameScope(Builtin::kArrayForEachLoopLazyDeoptContinuation, …) and AddNewNode<ThrowIfNotCallable>({callback}).
- Node definition and codegen:
  - maglev/maglev-ir.h: class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> with OpProperties::CanThrow | OpProperties::DeferredCall.
  - maglev/maglev-ir.cc: ThrowIfNotCallable::GenerateCode: JumpIfNotCallable, then runtime call Runtime::kThrowCalledNonCallable; masm->DefineExceptionHandlerAndLazyDeoptPoint(node) marks exception handler 
and lazy deopt metadata.
- Catch handlers materialization:
  - maglev-graph-builder.cc (BuildMergeStates): for bytecode handler table entries, MergePointInterpreterFrameState::NewForCatchBlock constructs exception merge states (handler depth, liveness, etc.). 
Throwing nodes and builtins define exception handler and lazy deopt points; the active DeoptFrameScope ensures that on throw, continuation resumes at kArrayForEachLoopLazyDeoptContinuation with captured 
parameters.

4) Representation selection/untagging, comparisons, and length-change guards
- Index representation:
  - Loop index is a Smi-tagged Phi (var_index). It is untagged to Int32 via GetInt32(index_tagged) after EnsureType(index_tagged, NodeType::kSmi). Under the hood, GetInt32 uses BuildSmiUntag when possible.
- Increment and overflow handling:
  - AddNewNode<Int32IncrementWithOverflow>({index_int32}) within eager deopt scope (Eager continuation). TODO comment suggests replacing with an asserting increment since real overflow is impossible (array 
length < int32 max).
- Bounds and comparisons:
  - Loop guard: BranchIfInt32Compare(Operation::kLessThan) between index_int32 and original_length_int32.
  - Length-change guard: TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged) — unsigned compare semantics.
- Nodes involved:
  - Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition, UnsafeSmiUntag (via GetInt32/GetSmiValue paths), Checked/Unsafe Smi tag/untag helpers in builder used as needed.

5) Elements kinds, map checks, feedback-based specialization
- Elements kind/path selection:
  - CanInlineArrayIteratingBuiltin(broker, possible_maps, &elements_kind): requires supports_fast_array_iteration for every map and unifies elements kinds (UnionElementsKindUptoSize). If holey kinds, emits 
hole checks to skip callback via BranchIfFloat64IsHole or BranchIfRootConstant(TheHoleValue).
- Protector requirement:
  - DependOnNoElementsProtector() must hold before specializing.
- Receiver map stability and recheck:
  - Tracks whether receiver maps were unstable and the set before loop (receiver_maps_before_loop). After callback, in eager scope, re-checks maps via CheckMapsWithMigration or CheckMaps 
(OmitHeapObjectCheck), since the callback might mutate.
- Element loads (specialized):
  - Load elements array via BuildLoadElements(receiver), then element load depends on elements_kind: BuildLoadFixedDoubleArrayElement or BuildLoadFixedArrayElement. No COW/grow/migration here since forEach 
doesn’t store, but mapping and bounds/length change are guarded.
- Broader Maglev element access (for reference):
  - TryBuildElementLoadOnJSArrayOrJSObject/Store contain map checks, hole treatment (CanTreatHoleAsUndefined), bounds checks, MaybeGrowFastElements/EnsureWritableFastElements, and use 
Load/StoreFixed|Double]ArrayElement; these demonstrate the same building blocks used for forEach’s specialized loads.

6) Files, canonical symbol names, nodes, and comments
- Files and key symbols:
  - src/maglev/maglev-graph-builder.cc:
    - MaglevGraphBuilder::TryReduceArrayForEach (main lowering of forEach)
    - CanInlineArrayIteratingBuiltin (supports fast array iteration, unify kinds)
    - MaglevSubGraphBuilder::{BeginLoop, EndLoop, Variable, Label}
    - BuildLoadJSArrayLength, BuildLoadElements
    - BranchIfInt32Compare, CheckInt32Condition, Int32IncrementWithOverflow
    - CheckMapsWithMigration / CheckMaps
    - BranchIfFloat64IsHole, BranchIfRootConstant
  - src/maglev/maglev-ir.h and src/maglev/maglev-ir.cc:
    - class ThrowIfNotCallable; ThrowIfNotCallable::GenerateCode
    - Nodes used by loop: Int32IncrementWithOverflow, BranchIfInt32Compare, BranchIfFloat64IsHole, BranchIfRootConstant, CheckInt32Condition, Load/Store element nodes
  - src/builtins/array-foreach.tq:
    - transitioning javascript builtins ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation; comments document the continuation points.

- Inline comments mentioning forEach specialization:
  - In maglev-graph-builder.cc around TryReduceArrayForEach: verbose trace logs like "Failed to reduce Array.prototype.forEach ..." and comments on increment overflow and re-checking maps after callback; 
length-change guard notes UnsignedLessThanEqual and DeoptimizeReason::kArrayLengthChanged.
  - In builtins/array-foreach.tq: Header comments explicitly say "All continuation points in the optimized forEach implementation…" and show how continuations route to the generic loop.

Concrete pointers (as seen by searching/reading):
- maglev/maglev-graph-builder.cc:
  - TryReduceArrayForEach: lines ~8072–8320 (search hit strings: "Failed to reduce Array.prototype.forEach", ThrowIfNotCallable block at ~8127–8135; eager deopt scopes at ~8200, ~8288; map recheck and 
length-change guard at ~8258–8274; hole checks and element loads ~8219–8257; CanInlineArrayIteratingBuiltin ~8037–8061)
  - DeoptFrameScope::IsLazyDeoptContinuationFrame includes kArrayForEachLoopLazyDeoptContinuation (~413)
- builtins/array-foreach.tq:
  - ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation plus ArrayForEachLoopContinuation definitions (file start region)
- maglev/maglev-ir.h/.cc:
  - ThrowIfNotCallable class (~9740–9756) and GenerateCode (~5424–5438), with DefineExceptionHandlerAndLazyDeoptPoint

### 3. Additional context (if relevant):
- The forEach lowering leverages MaglevSubGraphBuilder to construct local control/data flow with variables and loop headers, preserving KnownNodeAspects/Known maps across merges.
- Exception flow: while Try/Catch general handling is in BuildMergeStates (creating catch merge states using HandlerTable), individual nodes that can throw call DefineExceptionHandlerAndLazyDeoptPoint to 
install metadata; the active DeoptFrameScope determines whether exceptions are treated as lazy or eager continuations. For forEach’s callback validation/calls, the lazy continuation is used to ensure proper 
re-entry semantics.
- Although not directly used in forEach, broader element access code demonstrates how Maglev emits checks like CheckTypedArrayBounds, MaybeGrowFastElements, and EnsureWritableFastElements when needed; forEach
only reads and does not grow or write.

Written RAG DB IDs (what they contain):
- foreach_builder_entrypoints: Summarizes TryReduceArrayForEach, loop lowering, elements specialization, indices/lengths, and where continuations are used.
- foreach_deopt_continuations: Details the eager/lazy continuation builtins, where DeoptFrameScope wraps code, parameters passed, and the tie-in to TQ builtins.
- foreach_throw_if_not_callable: Shows the ThrowIfNotCallable emission inside a lazy deopt scope and its codegen/runtime throw wiring with exception handler points.
- foreach_representation_and_bounds: Documents index Smi/Int32 handling, increment, BranchIfInt32Compare, CheckInt32Condition, unsigned comparisons, and the length-change guard.
- foreach_elements_and_maps: Captures elements kind gating via CanInlineArrayIteratingBuiltin, NoElementsProtector requirement, post-callback map rechecks (CheckMaps/CheckMapsWithMigration), and the element 
load and hole-skipping logic.
[Step 1: Duration 535.07 seconds| Input tokens: 2,787 | Output tokens: 932]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'V8Search' with arguments: {'task': 'Stage 1: Build an initial map of Maglev’s specialization and loop lowering for Array.prototype.forEach and closely-related mechanisms. Identify the       │
│ concrete files, classes, and functions in V8 that do the following:\n- Recognize and lower Array.prototype.forEach into an index loop in the Maglev graph builder. Look for                                  │
│ MaglevGraphBuilder::TryReduceArrayForEach (or similarly named), and any helpers like CanInlineArrayIteratingBuiltin, BuildLoadJSArrayLength, BuildLoadElements, and MaglevSubGraphBuilder loop               │
│ construction.\n- Define and use the deopt continuation builtins for this loop: ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation (Torque .tq files and Maglev deopt scopes).  │
│ Show where DeoptFrameScope is created and how continuations are referenced from Maglev.\n- Emit ThrowIfNotCallable for the callback and wire exception edges/handlers; find ThrowIfNotCallable node          │
│ definition and codegen, and where Maglev marks exception handlers and lazy deopt points. Also locate CreateCatchContext and SetPendingMessage materialization/wiring in Maglev’s try/catch handling.\n-      │
│ Representation/untagging and loop arithmetic nodes: UnsafeSmiUntag, Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition with UnsignedLessThanEqual, and the place where ArrayLengthChanged │
│ deopt reason is used.\n- Elements kinds specialization, map checks, NoElementsProtector dependency, hole checks for HOLEY kinds, and the concrete load nodes (FixedArray vs FixedDoubleArray) used by        │
│ forEach.\nReturn file paths and the canonical symbol names; include RAG IDs for each hit so we can fetch code blocks later.'}                                                                                │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────── New run - V8Search ─────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                              │
│ You're a helpful agent named 'V8Search'.                                                                                                                                                                     │
│ You have been submitted this task by your manager.                                                                                                                                                           │
│ ---                                                                                                                                                                                                          │
│ Task:                                                                                                                                                                                                        │
│ Stage 1: Build an initial map of Maglev’s specialization and loop lowering for Array.prototype.forEach and closely-related mechanisms. Identify the concrete files, classes, and functions in V8 that do the │
│ following:                                                                                                                                                                                                   │
│ - Recognize and lower Array.prototype.forEach into an index loop in the Maglev graph builder. Look for MaglevGraphBuilder::TryReduceArrayForEach (or similarly named), and any helpers like                  │
│ CanInlineArrayIteratingBuiltin, BuildLoadJSArrayLength, BuildLoadElements, and MaglevSubGraphBuilder loop construction.                                                                                      │
│ - Define and use the deopt continuation builtins for this loop: ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation (Torque .tq files and Maglev deopt scopes). Show where      │
│ DeoptFrameScope is created and how continuations are referenced from Maglev.                                                                                                                                 │
│ - Emit ThrowIfNotCallable for the callback and wire exception edges/handlers; find ThrowIfNotCallable node definition and codegen, and where Maglev marks exception handlers and lazy deopt points. Also     │
│ locate CreateCatchContext and SetPendingMessage materialization/wiring in Maglev’s try/catch handling.                                                                                                       │
│ - Representation/untagging and loop arithmetic nodes: UnsafeSmiUntag, Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition with UnsignedLessThanEqual, and the place where                  │
│ ArrayLengthChanged deopt reason is used.                                                                                                                                                                     │
│ - Elements kinds specialization, map checks, NoElementsProtector dependency, hole checks for HOLEY kinds, and the concrete load nodes (FixedArray vs FixedDoubleArray) used by forEach.                      │
│ Return file paths and the canonical symbol names; include RAG IDs for each hit so we can fetch code blocks later.                                                                                            │
│ ---                                                                                                                                                                                                          │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.                    │
│                                                                                                                                                                                                              │
│ Your final_answer WILL HAVE to contain these parts:                                                                                                                                                          │
│ ### 1. Task outcome (short version):                                                                                                                                                                         │
│ ### 2. Task outcome (extremely detailed version):                                                                                                                                                            │
│ ### 3. Additional context (if relevant):                                                                                                                                                                     │
│                                                                                                                                                                                                              │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.                                                                                        │
│ And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.                                                              │
│                                                                                                                                                                                                              │
╰─ LiteLLMModel - gpt-5 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'options': '-L 2 maglev'}                                                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'forEach', 'options': '--filter'}                                                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'MaglevGraphBuilder', 'options': '--filter'}                                                                                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && tree -L 2 maglev | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ArrayForEachLoop', 'options': '--filter'}                                                                                                          │

╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'forEach' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': '--filter'}                                                                                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'MaglevGraphBuilder' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'MaglevGraphBuilder' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'ArrayForEachLoop' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'DeoptFrameScope', 'options': '--filter'}                                                                                                           │
Observations: None
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'ThrowIfNotCallable' | head -n 1000

│   ├── maglev-assembler-arm-inl.h
│   ├── maglev-assembler-arm.cc
│   └── maglev-ir-arm.cc
├── arm64
│   ├── maglev-assembler-arm64-inl.h
│   ├── maglev-assembler-arm64.cc
│   └── maglev-ir-arm64.cc
├── DEPS
├── DIR_METADATA
├── maglev-assembler-inl.h
├── maglev-assembler.cc
├── maglev-assembler.h
├── maglev-basic-block.h
├── maglev-code-gen-state.h
├── maglev-code-generator.cc
├── maglev-code-generator.h
├── maglev-compilation-info.cc
├── maglev-compilation-info.h
├── maglev-compilation-unit.cc
├── maglev-compilation-unit.h
├── maglev-compiler.cc
├── maglev-compiler.h
├── maglev-concurrent-dispatcher.cc
├── maglev-concurrent-dispatcher.h
├── maglev-graph-builder.cc
├── maglev-graph-builder.h
├── maglev-graph-labeller.h
├── maglev-graph-printer.cc
├── maglev-graph-printer.h
├── maglev-graph-processor.h
├── maglev-graph-verifier.h
├── maglev-graph.h
├── maglev-interpreter-frame-state.cc
├── maglev-interpreter-frame-state.h
├── maglev-ir-inl.h
├── maglev-ir.cc
├── maglev-ir.h
├── maglev-phi-representation-selector.cc
├── maglev-phi-representation-selector.h
├── maglev-pipeline-statistics.cc
├── maglev-pipeline-statistics.h
├── maglev-post-hoc-optimizations-processors.h
├── maglev-pre-regalloc-codegen-processors.h
├── maglev-regalloc-data.h
├── maglev-regalloc.cc
├── maglev-regalloc.h
├── maglev-register-frame-array.h
├── maglev.cc
├── maglev.h
├── OWNERS
├── riscv
│   ├── maglev-assembler-riscv-inl.h
│   ├── maglev-assembler-riscv.cc
│   └── maglev-ir-riscv.cc
├── s390
│   ├── maglev-assembler-s390-inl.h
│   ├── maglev-assembler-s390.cc
│   └── maglev-ir-s390.cc
└── x64
    ├── maglev-assembler-x64-inl.h
    ├── maglev-assembler-x64.cc
    └── maglev-ir-x64.cc

6 directories, 58 files
├── arm
Observations: maglev
Observations: None
Observations: None
Observations: None
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'DeoptFrameScope' | head -n 1000
Observations: None
[Step 1: Duration 8.82 seconds| Input tokens: 2,430 | Output tokens: 418]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'forEach', 'options': '-n --hidden --no-ignore-vcs'}                                                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'forEach' | head -n 1000
Observations: init/bootstrapper.cc:2615:        isolate_, proto, "forEach", Builtin::kArrayForEach, 1, kDontAdapt);
init/bootstrapper.cc:4395:    SimpleInstallFunction(isolate_, prototype, "forEach",
init/bootstrapper.cc:4572:    SimpleInstallFunction(isolate_, prototype, "forEach",
init/bootstrapper.cc:4660:    SimpleInstallFunction(isolate_, prototype, "forEach",
init/bootstrapper.cc:5575:  SimpleInstallFunction(isolate(), iterator_prototype, "forEach",
inspector/v8-inspector-session-impl.cc:232:  m_inspector->forEachContext(m_contextGroupId,
inspector/v8-inspector-session-impl.cc:267:  m_inspector->forEachContext(
inspector/v8-inspector-session-impl.cc:346:  m_inspector->forEachContext(
inspector/v8-inspector-session-impl.cc:355:  m_inspector->forEachContext(m_contextGroupId,
inspector/v8-runtime-agent-impl.cc:919:  m_inspector->forEachContext(
inspector/v8-runtime-agent-impl.cc:946:  inspector->forEachSession(
inspector/v8-runtime-agent-impl.cc:1066:  m_inspector->forEachContext(
inspector/v8-runtime-agent-impl.cc:1114:    m_inspector->forEachContext(m_session->contextGroupId(),
inspector/v8-inspector-impl.cc:233:  forEachSession(
inspector/v8-inspector-impl.cc:256:  forEachSession(groupId, |&inspectedContext](V8InspectorSessionImpl* session) {
inspector/v8-inspector-impl.cc:272:  forEachSession(contextGroupId,
inspector/v8-inspector-impl.cc:398:void V8InspectorImpl::forEachContext(
inspector/v8-inspector-impl.cc:416:void V8InspectorImpl::forEachSession(
maglev/maglev-graph-builder.cc:8072:      std::cout << "  ! Failed to reduce Array.prototype.forEach - not enough "
maglev/maglev-graph-builder.cc:8082:      std::cout << "  ! Failed to reduce Array.prototype.forEach - receiver "
maglev/maglev-graph-builder.cc:8093:      std::cout << "  ! Failed to reduce Array.prototype.forEach - doesn't "
maglev/maglev-graph-builder.cc:8103:      std::cout << "  ! Failed to reduce Array.prototype.forEach - invalidated "
maglev/maglev-graph-builder.cc:8113:      std::cout << "  ! Failed to reduce Array.prototype.forEach - callback is "
inspector/v8-console-message.cc:569:  inspector->forEachSession(
inspector/v8-console-message.cc:595:  m_inspector->forEachSession(m_contextGroupId,
inspector/v8-inspector-impl.h:135:  void forEachContext(int contextGroupId,
inspector/v8-inspector-impl.h:137:  void forEachSession(
inspector/v8-console.cc:190:  void forEachSession(std::function<void(V8InspectorSessionImpl*)> callback) {
inspector/v8-console.cc:191:    m_inspector->forEachSession(groupId(), std::move(callback));
inspector/v8-console.cc:378:  helper.forEachSession(|&title](V8InspectorSessionImpl* session) {
inspector/v8-console.cc:393:  helper.forEachSession(|&title](V8InspectorSessionImpl* session) {
inspector/v8-debugger.cc:131:      m_inspector->forEachSession(
inspector/v8-debugger.cc:269:  m_inspector->forEachSession(
inspector/v8-debugger.cc:513:  m_inspector->forEachSession(
inspector/v8-debugger.cc:531:  m_inspector->forEachSession(
inspector/v8-debugger.cc:544:    m_inspector->forEachSession(
inspector/v8-debugger.cc:554:  m_inspector->forEachSession(contextGroupId,
inspector/v8-debugger.cc:608:  m_inspector->forEachSession(
inspector/v8-debugger.cc:629:  m_inspector->forEachSession(
inspector/v8-debugger.cc:638:  m_inspector->forEachSession(
inspector/v8-debugger.cc:657:  m_inspector->forEachSession(
inspector/v8-debugger.cc:700:  m_inspector->forEachSession(
inspector/v8-debugger.cc:720:  m_inspector->forEachSession(
inspector/v8-debugger.cc:1350:    m_inspector->forEachSession(
builtins/iterator-helpers.tq:849:// --- forEach helper
builtins/iterator-helpers.tq:855:  const methodName: constexpr string = 'Iterator.prototype.forEach';
builtins/typed-array-foreach.tq:8:const kBuiltinNameForEach: constexpr string = '%TypedArray%.prototype.forEach';
builtins/array-foreach.tq:9:  // All continuation points in the optimized forEach implementation are
builtins/array-foreach.tq:25:  // All continuation points in the optimized forEach implementation are
builtins/array-foreach.tq:94:    RequireObjectCoercible(receiver, 'Array.prototype.forEach');
builtins/builtins-definitions.h:828:  /* ES #sec-map.prototype.forEach */                                          \
builtins/builtins-collections-gen.cc:2105:  const char* const kMethodName = "Map.prototype.forEach";
builtins/builtins-collections-gen.cc:2304:  const char* const kMethodName = "Set.prototype.forEach";
[Step 2: Duration 5.41 seconds| Input tokens: 6,208 | Output tokens: 648]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'TryReduceArrayForEach|CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder', 'options': '-n --hidden           │
│ --no-ignore-vcs maglev/ maglev'}                                                                                                                                                                             │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ maglev 'TryReduceArrayForEach|CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder' | head -n 1000
Observations: maglev/maglev-compiler.h:11:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-code-generator.cc:5:#include "src/maglev/maglev-code-generator.h"
maglev/maglev-code-generator.cc:28:#include "src/maglev/maglev-assembler-inl.h"
maglev/maglev-code-generator.cc:29:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-code-generator.cc:30:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-code-generator.cc:31:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-code-generator.cc:32:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-code-generator.cc:33:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-code-generator.cc:34:#include "src/maglev/maglev-graph.h"
maglev/maglev-code-generator.cc:35:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-code-generator.cc:36:#include "src/maglev/maglev-regalloc-data.h"
maglev/maglev-pipeline-statistics.h:13:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-graph-labeller.h:10:#include "src/maglev/maglev-graph.h"
maglev/maglev-graph-labeller.h:11:#include "src/maglev/maglev-ir.h"
maglev/arm64/maglev-assembler-arm64-inl.h:12:#include "src/maglev/maglev-assembler.h"
maglev/arm64/maglev-assembler-arm64-inl.h:13:#include "src/maglev/maglev-basic-block.h"
maglev/arm64/maglev-assembler-arm64-inl.h:14:#include "src/maglev/maglev-code-gen-state.h"
maglev/arm64/maglev-assembler-arm64-inl.h:15:#include "src/maglev/maglev-ir.h"
maglev/arm64/maglev-ir-arm64.cc:9:#include "src/maglev/arm64/maglev-assembler-arm64-inl.h"
maglev/arm64/maglev-ir-arm64.cc:10:#include "src/maglev/maglev-assembler-inl.h"
maglev/arm64/maglev-ir-arm64.cc:11:#include "src/maglev/maglev-graph-processor.h"
maglev/arm64/maglev-ir-arm64.cc:12:#include "src/maglev/maglev-graph.h"
maglev/arm64/maglev-ir-arm64.cc:13:#include "src/maglev/maglev-ir-inl.h"
maglev/arm64/maglev-ir-arm64.cc:14:#include "src/maglev/maglev-ir.h"
maglev/arm64/maglev-assembler-arm64.cc:7:#include "src/maglev/maglev-assembler-inl.h"
maglev/arm64/maglev-assembler-arm64.cc:8:#include "src/maglev/maglev-graph.h"
maglev/maglev-regalloc.cc:5:#include "src/maglev/maglev-regalloc.h"
maglev/maglev-regalloc.cc:17:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-regalloc.cc:18:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-regalloc.cc:19:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-regalloc.cc:20:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-regalloc.cc:21:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-regalloc.cc:22:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-regalloc.cc:23:#include "src/maglev/maglev-graph.h"
maglev/maglev-regalloc.cc:24:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-regalloc.cc:25:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-regalloc.cc:26:#include "src/maglev/maglev-ir.h"
maglev/maglev-regalloc.cc:27:#include "src/maglev/maglev-regalloc-data.h"
maglev/riscv/maglev-assembler-riscv-inl.h:12:#include "src/maglev/maglev-assembler.h"
maglev/riscv/maglev-assembler-riscv-inl.h:13:#include "src/maglev/maglev-basic-block.h"
maglev/riscv/maglev-assembler-riscv-inl.h:14:#include "src/maglev/maglev-code-gen-state.h"
maglev/riscv/maglev-assembler-riscv-inl.h:15:#include "src/maglev/maglev-ir.h"
maglev/riscv/maglev-ir-riscv.cc:9:#include "src/maglev/maglev-assembler-inl.h"
maglev/riscv/maglev-ir-riscv.cc:10:#include "src/maglev/maglev-graph-processor.h"
maglev/riscv/maglev-ir-riscv.cc:11:#include "src/maglev/maglev-graph.h"
maglev/riscv/maglev-ir-riscv.cc:12:#include "src/maglev/maglev-ir-inl.h"
maglev/riscv/maglev-ir-riscv.cc:13:#include "src/maglev/maglev-ir.h"
maglev/riscv/maglev-ir-riscv.cc:14:#include "src/maglev/riscv/maglev-assembler-riscv-inl.h"
maglev/riscv/maglev-assembler-riscv.cc:7:#include "src/maglev/maglev-assembler-inl.h"
maglev/riscv/maglev-assembler-riscv.cc:8:#include "src/maglev/maglev-graph.h"
maglev/arm/maglev-ir-arm.cc:8:#include "src/maglev/arm/maglev-assembler-arm-inl.h"
maglev/arm/maglev-ir-arm.cc:9:#include "src/maglev/maglev-assembler-inl.h"
maglev/arm/maglev-ir-arm.cc:10:#include "src/maglev/maglev-graph-processor.h"
maglev/arm/maglev-ir-arm.cc:11:#include "src/maglev/maglev-graph.h"
maglev/arm/maglev-ir-arm.cc:12:#include "src/maglev/maglev-ir-inl.h"
maglev/arm/maglev-ir-arm.cc:13:#include "src/maglev/maglev-ir.h"
maglev/arm/maglev-assembler-arm-inl.h:13:#include "src/maglev/maglev-assembler.h"
maglev/arm/maglev-assembler-arm-inl.h:14:#include "src/maglev/maglev-basic-block.h"
maglev/arm/maglev-assembler-arm-inl.h:15:#include "src/maglev/maglev-code-gen-state.h"
maglev/arm/maglev-assembler-arm.cc:7:#include "src/maglev/maglev-assembler-inl.h"
maglev/arm/maglev-assembler-arm.cc:8:#include "src/maglev/maglev-graph.h"
maglev/maglev-assembler.cc:5:#include "src/maglev/maglev-assembler.h"
maglev/maglev-assembler.cc:9:#include "src/maglev/maglev-assembler-inl.h"
maglev/maglev-assembler.cc:10:#include "src/maglev/maglev-code-generator.h"
maglev/maglev-phi-representation-selector.cc:5:#include "src/maglev/maglev-phi-representation-selector.h"
maglev/maglev-phi-representation-selector.cc:15:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-phi-representation-selector.cc:16:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-phi-representation-selector.cc:17:#include "src/maglev/maglev-ir.h"
maglev/maglev-compiler.cc:5:#include "src/maglev/maglev-compiler.h"
maglev/maglev-compiler.cc:31:#include "src/maglev/maglev-basic-block.h"
maglev/maglev-compiler.cc:32:#include "src/maglev/maglev-code-generator.h"
maglev/maglev-compiler.cc:33:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-compiler.cc:34:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-compiler.cc:35:#include "src/maglev/maglev-graph-builder.h"
maglev/maglev-compiler.cc:36:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-compiler.cc:37:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-compiler.cc:38:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-compiler.cc:39:#include "src/maglev/maglev-graph-verifier.h"
maglev/maglev-compiler.cc:40:#include "src/maglev/maglev-graph.h"
maglev/maglev-compiler.cc:41:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-compiler.cc:42:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-compiler.cc:43:#include "src/maglev/maglev-ir.h"
maglev/maglev-compiler.cc:44:#include "src/maglev/maglev-phi-representation-selector.h"
maglev/maglev-compiler.cc:45:#include "src/maglev/maglev-post-hoc-optimizations-processors.h"
maglev/maglev-compiler.cc:46:#include "src/maglev/maglev-pre-regalloc-codegen-processors.h"
maglev/maglev-compiler.cc:47:#include "src/maglev/maglev-regalloc-data.h"
maglev/maglev-compiler.cc:48:#include "src/maglev/maglev-regalloc.h"
maglev/maglev.cc:5:#include "src/maglev/maglev.h"
maglev/maglev.cc:11:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev.cc:12:#include "src/maglev/maglev-compiler.h"
maglev/maglev-code-generator.h:11:#include "src/maglev/maglev-assembler.h"
maglev/maglev-code-generator.h:12:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-code-gen-state.h:16:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-code-gen-state.h:17:#include "src/maglev/maglev-ir.h"
maglev/maglev-assembler-inl.h:14:#include "src/maglev/maglev-assembler.h"
maglev/maglev-assembler-inl.h:17:#include "src/maglev/arm/maglev-assembler-arm-inl.h"
maglev/maglev-assembler-inl.h:19:#include "src/maglev/arm64/maglev-assembler-arm64-inl.h"
maglev/maglev-assembler-inl.h:21:#include "src/maglev/riscv/maglev-assembler-riscv-inl.h"
maglev/maglev-assembler-inl.h:23:#include "src/maglev/x64/maglev-assembler-x64-inl.h"
maglev/maglev-assembler-inl.h:25:#include "src/maglev/s390/maglev-assembler-s390-inl.h"
maglev/maglev-ir.h:40:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-graph-processor.h:10:#include "src/maglev/maglev-basic-block.h"
maglev/maglev-graph-processor.h:11:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-graph-processor.h:12:#include "src/maglev/maglev-graph.h"
maglev/maglev-graph-processor.h:13:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-graph-processor.h:14:#include "src/maglev/maglev-ir.h"
maglev/maglev-graph.h:12:#include "src/maglev/maglev-basic-block.h"
maglev/maglev-graph.h:13:#include "src/maglev/maglev-ir.h"
maglev/maglev-register-frame-array.h:9:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-ir-inl.h:9:#include "src/maglev/maglev-ir.h"
maglev/maglev-graph-printer.h:13:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-graph-printer.h:14:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-graph-printer.h:15:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-graph-printer.h:16:#include "src/maglev/maglev-ir.h"
maglev/maglev-regalloc.h:10:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-regalloc.h:11:#include "src/maglev/maglev-graph.h"
maglev/maglev-regalloc.h:12:#include "src/maglev/maglev-ir.h"
maglev/maglev-regalloc.h:13:#include "src/maglev/maglev-regalloc-data.h"
maglev/maglev-graph-printer.cc:7:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-graph-printer.cc:19:#include "src/maglev/maglev-basic-block.h"
maglev/maglev-graph-printer.cc:20:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-graph-printer.cc:21:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-graph-printer.cc:22:#include "src/maglev/maglev-graph.h"
maglev/maglev-graph-printer.cc:23:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-basic-block.h:13:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-basic-block.h:14:#include "src/maglev/maglev-ir.h"
maglev/maglev-ir.cc:5:#include "src/maglev/maglev-ir.h"
maglev/maglev-ir.cc:35:#include "src/maglev/maglev-assembler-inl.h"
maglev/maglev-ir.cc:36:#include "src/maglev/maglev-assembler.h"
maglev/maglev-ir.cc:37:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-ir.cc:39:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-ir.cc:40:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-ir.cc:41:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-ir.cc:42:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-concurrent-dispatcher.h:13:#include "src/maglev/maglev-pipeline-statistics.h"
maglev/maglev-phi-representation-selector.h:12:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-phi-representation-selector.h:13:#include "src/maglev/maglev-graph-builder.h"
maglev/maglev-phi-representation-selector.h:14:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-interpreter-frame-state.h:14:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-interpreter-frame-state.h:15:#include "src/maglev/maglev-ir.h"
maglev/maglev-interpreter-frame-state.h:17:#include "src/maglev/maglev-regalloc-data.h"
maglev/maglev-interpreter-frame-state.h:19:#include "src/maglev/maglev-register-frame-array.h"
maglev/maglev-assembler.h:13:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-assembler.h:14:#include "src/maglev/maglev-ir.h"
maglev/x64/maglev-ir-x64.cc:10:#include "src/maglev/maglev-assembler-inl.h"
maglev/x64/maglev-ir-x64.cc:11:#include "src/maglev/maglev-graph-processor.h"
maglev/x64/maglev-ir-x64.cc:12:#include "src/maglev/maglev-graph.h"
maglev/x64/maglev-ir-x64.cc:13:#include "src/maglev/maglev-ir-inl.h"
maglev/x64/maglev-ir-x64.cc:14:#include "src/maglev/maglev-ir.h"
maglev/maglev-compilation-unit.cc:5:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-compilation-unit.cc:9:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-compilation-unit.cc:10:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-graph-verifier.h:8:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-graph-verifier.h:9:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-graph-verifier.h:10:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-graph-verifier.h:11:#include "src/maglev/maglev-ir.h"
maglev/maglev-regalloc-data.h:11:#include "src/maglev/maglev-assembler.h"
maglev/maglev-pipeline-statistics.cc:5:#include "src/maglev/maglev-pipeline-statistics.h"
maglev/maglev-graph-builder.cc:5:#include "src/maglev/maglev-graph-builder.h"
maglev/maglev-graph-builder.cc:40:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-graph-builder.cc:41:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-graph-builder.cc:42:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-graph-builder.cc:43:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-graph-builder.cc:44:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-graph-builder.cc:45:#include "src/maglev/maglev-ir.h"
maglev/maglev-interpreter-frame-state.cc:5:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-interpreter-frame-state.cc:11:#include "src/maglev/maglev-basic-block.h"
maglev/maglev-interpreter-frame-state.cc:12:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-interpreter-frame-state.cc:13:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-interpreter-frame-state.cc:14:#include "src/maglev/maglev-graph-builder.h"
maglev/maglev-interpreter-frame-state.cc:15:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-interpreter-frame-state.cc:16:#include "src/maglev/maglev-graph.h"
maglev/maglev-interpreter-frame-state.cc:17:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-interpreter-frame-state.cc:18:#include "src/maglev/maglev-ir.h"
maglev/maglev-compilation-info.cc:5:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-compilation-info.cc:16:#include "src/maglev/maglev-code-generator.h"
maglev/maglev-compilation-info.cc:17:#include "src/maglev/maglev-concurrent-dispatcher.h"
maglev/maglev-compilation-info.cc:19:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-compilation-info.cc:20:#include "src/maglev/maglev-graph-labeller.h"
maglev/x64/maglev-assembler-x64.cc:10:#include "src/maglev/maglev-assembler-inl.h"
maglev/x64/maglev-assembler-x64.cc:11:#include "src/maglev/maglev-assembler.h"
maglev/x64/maglev-assembler-x64.cc:12:#include "src/maglev/maglev-graph.h"
maglev/x64/maglev-assembler-x64.cc:13:#include "src/maglev/maglev-ir.h"
maglev/s390/maglev-assembler-s390.cc:7:#include "src/maglev/maglev-assembler-inl.h"
maglev/s390/maglev-assembler-s390.cc:8:#include "src/maglev/maglev-graph.h"
maglev/s390/maglev-ir-s390.cc:8:#include "src/maglev/maglev-assembler-inl.h"
maglev/s390/maglev-ir-s390.cc:9:#include "src/maglev/maglev-graph-processor.h"
maglev/s390/maglev-ir-s390.cc:10:#include "src/maglev/maglev-graph.h"
maglev/s390/maglev-ir-s390.cc:11:#include "src/maglev/maglev-ir-inl.h"
maglev/s390/maglev-ir-s390.cc:12:#include "src/maglev/maglev-ir.h"
maglev/s390/maglev-ir-s390.cc:13:#include "src/maglev/s390/maglev-assembler-s390-inl.h"
maglev/maglev-concurrent-dispatcher.cc:5:#include "src/maglev/maglev-concurrent-dispatcher.h"
maglev/maglev-concurrent-dispatcher.cc:17:#include "src/maglev/maglev-code-generator.h"
maglev/maglev-concurrent-dispatcher.cc:18:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-concurrent-dispatcher.cc:19:#include "src/maglev/maglev-compiler.h"
maglev/maglev-concurrent-dispatcher.cc:20:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-concurrent-dispatcher.cc:21:#include "src/maglev/maglev-pipeline-statistics.h"
maglev/maglev-post-hoc-optimizations-processors.h:9:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-post-hoc-optimizations-processors.h:10:#include "src/maglev/maglev-graph-builder.h"
maglev/maglev-post-hoc-optimizations-processors.h:11:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-post-hoc-optimizations-processors.h:12:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-post-hoc-optimizations-processors.h:13:#include "src/maglev/maglev-graph.h"
maglev/maglev-post-hoc-optimizations-processors.h:14:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-post-hoc-optimizations-processors.h:15:#include "src/maglev/maglev-ir.h"
maglev/x64/maglev-assembler-x64-inl.h:17:#include "src/maglev/maglev-assembler.h"
maglev/x64/maglev-assembler-x64-inl.h:18:#include "src/maglev/maglev-basic-block.h"
maglev/x64/maglev-assembler-x64-inl.h:19:#include "src/maglev/maglev-code-gen-state.h"
maglev/s390/maglev-assembler-s390-inl.h:14:#include "src/maglev/maglev-assembler.h"
maglev/s390/maglev-assembler-s390-inl.h:15:#include "src/maglev/maglev-basic-block.h"
maglev/s390/maglev-assembler-s390-inl.h:16:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-graph-builder.h:35:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-graph-builder.h:36:#include "src/maglev/maglev-graph-printer.h"
maglev/maglev-graph-builder.h:37:#include "src/maglev/maglev-graph.h"
maglev/maglev-graph-builder.h:38:#include "src/maglev/maglev-interpreter-frame-state.h"
maglev/maglev-graph-builder.h:39:#include "src/maglev/maglev-ir.h"
maglev/maglev-pre-regalloc-codegen-processors.h:9:#include "src/maglev/maglev-compilation-info.h"
maglev/maglev-pre-regalloc-codegen-processors.h:10:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-pre-regalloc-codegen-processors.h:11:#include "src/maglev/maglev-graph.h"
maglev/maglev-pre-regalloc-codegen-processors.h:12:#include "src/maglev/maglev-ir.h"
[Step 3: Duration 10.07 seconds| Input tokens: 11,263 | Output tokens: 972]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'TryReduceArrayForEach', 'options': '-n --hidden --no-ignore-vcs'}                                                                                       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'TryReduceArrayForEach' | head -n 1000
Observations: maglev/maglev-graph-builder.cc:8061:ReduceResult MaglevGraphBuilder::TryReduceArrayForEach(
[Step 4: Duration 5.26 seconds| Input tokens: 22,354 | Output tokens: 1,140]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder', 'options': '-n --hidden --no-ignore-vcs maglev/'}       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': '-n --hidden --no-ignore-vcs'}                                                                                          │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'UnsafeSmiUntag|Int32IncrementWithOverflow|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged', 'options': '-n --hidden   │
│ --no-ignore-vcs maglev/'}                                                                                                                                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CreateCatchContext|SetPendingMessage', 'options': '-n --hidden --no-ignore-vcs maglev/ builtins/'}                                                      │

│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ArrayForEachLoopEagerDeoptContinuation|ArrayForEachLoopLazyDeoptContinuation', 'options': '-n --hidden --no-ignore-vcs'}                                │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'ThrowIfNotCallable' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'UnsafeSmiUntag|Int32IncrementWithOverflow|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'NoElementsProtector|ElementsKind|FixedArray|FixedDoubleArray|LoadFixed.*Element|Check.*Hole', 'options': '-n --hidden --no-ignore-vcs maglev/           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ builtins/ 'CreateCatchContext|SetPendingMessage' | head -n 1000
│ src/maglev/'}                                                                                                                                                                                                │
CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder: No such file or directory (os error 2)
Observations: rg: CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder: IO error for operation on 

Observations: maglev/maglev-ir.h:359:  V(ThrowIfNotCallable)                       \
maglev/maglev-ir.h:9739:class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> {
maglev/maglev-ir.h:9740:  using Base = FixedInputNodeT<1, ThrowIfNotCallable>;
maglev/maglev-ir.h:9743:  explicit ThrowIfNotCallable(uint64_t bitfield) : Base(bitfield) {}
maglev/maglev-ir.cc:5424:int ThrowIfNotCallable::MaxCallStackArgs() const { return 1; }
maglev/maglev-ir.cc:5425:void ThrowIfNotCallable::SetValueLocationConstraints() {
maglev/maglev-ir.cc:5429:void ThrowIfNotCallable::GenerateCode(MaglevAssembler* masm,
maglev/maglev-ir.cc:5432:      |](MaglevAssembler* masm, ThrowIfNotCallable* node) {
maglev/maglev-graph-builder.cc:8127:    // ThrowIfNotCallable is wrapped in a lazy_deopt_scope to make sure the
maglev/maglev-graph-builder.cc:8133:    AddNewNode<ThrowIfNotCallable>({callback});
codegen/code-stub-assembler.h:2660:  void ThrowIfNotCallable(TNode<Context> context, TNode<Object> value,
compiler/js-call-reducer.cc:369:  void ThrowIfNotCallable(TNode<Object> maybe_callable,
compiler/js-call-reducer.cc:1546:  ThrowIfNotCallable(fncallback, ForEachLoopLazyFrameState(frame_state_params,
compiler/js-call-reducer.cc:1675:  ThrowIfNotCallable(
compiler/js-call-reducer.cc:1834:  ThrowIfNotCallable(fncallback,
compiler/js-call-reducer.cc:1968:  ThrowIfNotCallable(fncallback, FilterLoopLazyFrameState(frame_state_params,
compiler/js-call-reducer.cc:2094:  ThrowIfNotCallable(
compiler/js-call-reducer.cc:2196:  ThrowIfNotCallable(
compiler/js-call-reducer.cc:2426:  ThrowIfNotCallable(executor,
codegen/code-stub-assembler.cc:7221:void CodeStubAssembler::ThrowIfNotCallable(TNode<Context> context,
compiler/turboshaft/maglev-graph-building-phase.cc:1630:  maglev::ProcessResult Process(maglev::ThrowIfNotCallable* node,
Observations: rg: UnsafeSmiUntag|Int32IncrementWithOverflow|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged: IO error for operation on 
UnsafeSmiUntag|Int32IncrementWithOverflow|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged: No such file or directory (os error 2)
Observations: rg: CreateCatchContext|SetPendingMessage: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'ArrayForEachLoopEagerDeoptContinuation|ArrayForEachLoopLazyDeoptContinuation' | head -n 1000
Observations: compiler/js-call-reducer.cc:1507:  Builtin builtin = Builtin::kArrayForEachLoopLazyDeoptContinuation;
compiler/js-call-reducer.cc:1518:  Builtin builtin = Builtin::kArrayForEachLoopEagerDeoptContinuation;
maglev/maglev-graph-builder.cc:413:      case Builtin::kArrayForEachLoopLazyDeoptContinuation:
maglev/maglev-graph-builder.cc:8130:        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
maglev/maglev-graph-builder.cc:8200:        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
maglev/maglev-graph-builder.cc:8241:        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
maglev/maglev-graph-builder.cc:8288:        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
builtins/array-foreach.tq:6:transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(
builtins/array-foreach.tq:22:transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ src/maglev/ 'NoElementsProtector|ElementsKind|FixedArray|FixedDoubleArray|LoadFixed.*Element|Check.*Hole' | head -n 1000
Observations: rg: src/maglev/: No such file or directory (os error 2)
rg: NoElementsProtector|ElementsKind|FixedArray|FixedDoubleArray|LoadFixed.*Element|Check.*Hole: No such file or directory (os error 2)
[Step 5: Duration 16.95 seconds| Input tokens: 33,516 | Output tokens: 1,958]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 6 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'DeoptFrameScope|deopt scope|lazy_deopt_scope|EagerDeopt|LazyDeopt', 'options': '-n --hidden --no-ignore-vcs maglev/'}                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'DeoptFrameScope|deopt scope|lazy_deopt_scope|EagerDeopt|LazyDeopt' | head -n 1000
Observations: rg: DeoptFrameScope|deopt scope|lazy_deopt_scope|EagerDeopt|LazyDeopt: IO error for operation on DeoptFrameScope|deopt scope|lazy_deopt_scope|EagerDeopt|LazyDeopt: No such file or directory (os 
error 2)
[Step 6: Duration 12.73 seconds| Input tokens: 46,033 | Output tokens: 2,146]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 7 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'TryReduceArrayForEach', 'options': '-n       │
│ --hidden --no-ignore-vcs maglev/maglev-graph-builder.cc'}                                         │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc',         │
│ 'section': 3}                                                                                     │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-graph-builder.cc 'TryReduceArrayForEach' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'lazy_deopt|EagerDeopt|DeoptFrameScope|LazyDeopt|deopt scope|LazyDeoptScope|EagerDeoptScope',     │
│ 'options': '-n --hidden --no-ignore-vcs maglev/'}                                                 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder|Bu │
│ ildLoad|CanInlineArray', 'options': '-n --hidden --no-ignore-vcs maglev/'}                        │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': '-n --hidden │
│ --no-ignore-vcs maglev/ maglev/maglev-ir.*'}                                                      │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
file or directory (os error 2)
Observations: rg: TryReduceArrayForEach: IO error for operation on TryReduceArrayForEach: No such 
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ maglev/maglev-ir.* 'ThrowIfNotCallable' | head -n 1000
Observations: maglev/maglev-ir.cc:5:#include "src/maglev/maglev-ir.h"
maglev/maglev-ir.cc:35:#include "src/maglev/maglev-assembler-inl.h"
maglev/maglev-ir.cc:36:#include "src/maglev/maglev-assembler.h"
maglev/maglev-ir.cc:37:#include "src/maglev/maglev-code-gen-state.h"
maglev/maglev-ir.cc:39:#include "src/maglev/maglev-compilation-unit.h"
maglev/maglev-ir.cc:40:#include "src/maglev/maglev-graph-labeller.h"
maglev/maglev-ir.cc:41:#include "src/maglev/maglev-graph-processor.h"
maglev/maglev-ir.cc:42:#include "src/maglev/maglev-ir-inl.h"
maglev/maglev-ir.h:40:#include "src/maglev/maglev-compilation-unit.h"
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder|BuildLoad|CanInlineArray' | head -n 1000
Observations: rg: 
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'lazy_deopt|EagerDeopt|DeoptFrameScope|LazyDeopt|deopt scope|LazyDeoptScope|EagerDeoptScope' | head -n 1000
ad|CanInlineArray: IO error for operation on 
CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder|BuildLo
ad|CanInlineArray: No such file or directory (os error 2)
CanInlineArrayIteratingBuiltin|BuildLoadJSArrayLength|BuildLoadElements|MaglevSubGraphBuilder|BuildLo
Observations: rg: lazy_deopt|EagerDeopt|DeoptFrameScope|LazyDeopt|deopt 
scope|LazyDeoptScope|EagerDeoptScope: IO error for operation on 
lazy_deopt|EagerDeopt|DeoptFrameScope|LazyDeopt|deopt scope|LazyDeoptScope|EagerDeoptScope: No such 
file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && sed -n '6001,9000p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Observations: Showing section 3/5 (lines 6001-9000) of 'maglev/maglev-graph-builder.cc':
      }
      return EmitUnconditionalDeopt(reason);
    }
  }
  AddNewNode<CheckInt32Condition>({lhs, rhs}, condition, reason);
  return ReduceResult::Done();
}

ValueNode* MaglevGraphBuilder::BuildLoadElements(ValueNode* object) {
  ReduceResult known_elements =
      TryFindLoadedProperty(known_node_aspects().loaded_properties, object,
                            KnownNodeAspects::LoadedPropertyMapKey::Elements());
  if (known_elements.IsDone()) {
    DCHECK(known_elements.IsDoneWithValue());
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  * Reusing non-constant |Elements] "
                << PrintNodeLabel(graph_labeller(), known_elements.value())
                << ": " << PrintNode(graph_labeller(), known_elements.value())
                << std::endl;
    }
    return known_elements.value();
  }

  DCHECK_EQ(JSObject::kElementsOffset, JSArray::kElementsOffset);
  ValueNode* elements = BuildLoadTaggedField(object, JSObject::kElementsOffset);
  RecordKnownProperty(object,
                      KnownNodeAspects::LoadedPropertyMapKey::Elements(),
                      elements, false, compiler::AccessMode::kLoad);
  return elements;
}

ReduceResult MaglevGraphBuilder::BuildLoadTypedArrayLength(
    ValueNode* object, ElementsKind elements_kind) {
  DCHECK(IsTypedArrayOrRabGsabTypedArrayElementsKind(elements_kind));
  bool is_variable_length = IsRabGsabTypedArrayElementsKind(elements_kind);

  if (!is_variable_length) {
    // Note: We can't use broker()->length_string() here, because it could
    // conflict with redefinitions of the TypedArray length property.
    RETURN_IF_DONE(TryFindLoadedProperty(
        known_node_aspects().loaded_constant_properties, object,
        KnownNodeAspects::LoadedPropertyMapKey::TypedArrayLength()));
  }

  ValueNode* result = AddNewNode<LoadTypedArrayLength>({object}, elements_kind);
  if (!is_variable_length) {
    RecordKnownProperty(
        object, KnownNodeAspects::LoadedPropertyMapKey::TypedArrayLength(),
        result, true, compiler::AccessMode::kLoad);
  }
  return result;
}

ValueNode* MaglevGraphBuilder::BuildLoadTypedArrayElement(
    ValueNode* object, ValueNode* index, ElementsKind elements_kind) {
#define BUILD_AND_RETURN_LOAD_TYPED_ARRAY(Type)                     \
  return AddNewNode<Load##Type##TypedArrayElement>({object, index}, \
                                                   elements_kind);

  switch (elements_kind) {
    case INT8_ELEMENTS:
    case INT16_ELEMENTS:
    case INT32_ELEMENTS:
      BUILD_AND_RETURN_LOAD_TYPED_ARRAY(SignedInt);
    case UINT8_CLAMPED_ELEMENTS:
    case UINT8_ELEMENTS:
    case UINT16_ELEMENTS:
    case UINT32_ELEMENTS:
      BUILD_AND_RETURN_LOAD_TYPED_ARRAY(UnsignedInt);
    case FLOAT32_ELEMENTS:
    case FLOAT64_ELEMENTS:
      BUILD_AND_RETURN_LOAD_TYPED_ARRAY(Double);
    default:
      UNREACHABLE();
  }
#undef BUILD_AND_RETURN_LOAD_TYPED_ARRAY
}

void MaglevGraphBuilder::BuildStoreTypedArrayElement(
    ValueNode* object, ValueNode* index, ElementsKind elements_kind) {
#define BUILD_STORE_TYPED_ARRAY(Type, value)                           \
  AddNewNode<Store##Type##TypedArrayElement>({object, index, (value)}, \
                                             elements_kind);

  // TODO(leszeks): These operations have a deopt loop when the ToNumber
  // conversion sees a type other than number or oddball. Turbofan has the same
  // deopt loop, but ideally we'd avoid it.
  switch (elements_kind) {
    case UINT8_CLAMPED_ELEMENTS: {
      BUILD_STORE_TYPED_ARRAY(Int, GetAccumulatorUint8ClampedForToNumber())
      break;
    }
    case INT8_ELEMENTS:
    case INT16_ELEMENTS:
    case INT32_ELEMENTS:
    case UINT8_ELEMENTS:
    case UINT16_ELEMENTS:
    case UINT32_ELEMENTS:
      BUILD_STORE_TYPED_ARRAY(
          Int, GetAccumulatorTruncatedInt32ForToNumber(
                   NodeType::kNumberOrOddball,
                   TaggedToFloat64ConversionType::kNumberOrOddball))
      break;
    case FLOAT32_ELEMENTS:
    case FLOAT64_ELEMENTS:
      BUILD_STORE_TYPED_ARRAY(
          Double, GetAccumulatorHoleyFloat64ForToNumber(
                      NodeType::kNumberOrOddball,
                      TaggedToFloat64ConversionType::kNumberOrOddball))
      break;
    default:
      UNREACHABLE();
  }
#undef BUILD_STORE_TYPED_ARRAY
}

ReduceResult MaglevGraphBuilder::TryBuildElementAccessOnTypedArray(
    ValueNode* object, ValueNode* index_object,
    const compiler::ElementAccessInfo& access_info,
    compiler::KeyedAccessMode const& keyed_mode) {
  DCHECK(HasOnlyJSTypedArrayMaps(
      base::VectorOf(access_info.lookup_start_object_maps())));
  ElementsKind elements_kind = access_info.elements_kind();
  if (elements_kind == FLOAT16_ELEMENTS ||
      elements_kind == BIGUINT64_ELEMENTS ||
      elements_kind == BIGINT64_ELEMENTS) {
    return ReduceResult::Fail();
  }
  if (keyed_mode.access_mode() == compiler::AccessMode::kLoad &&
      LoadModeHandlesOOB(keyed_mode.load_mode())) {
    // TODO(victorgomes): Handle OOB mode.
    return ReduceResult::Fail();
  }
  if (keyed_mode.access_mode() == compiler::AccessMode::kStore &&
      StoreModeIgnoresTypeArrayOOB(keyed_mode.store_mode())) {
    // TODO(victorgomes): Handle OOB mode.
    return ReduceResult::Fail();
  }
  if (keyed_mode.access_mode() == compiler::AccessMode::kStore &&
      elements_kind == UINT8_CLAMPED_ELEMENTS &&
      !IsSupported(CpuOperation::kFloat64Round)) {
    // TODO(victorgomes): Technically we still support if value (in the
    // accumulator) is of type int32. It would be nice to have a roll back
    // mechanism instead, so that we do not need to check this early.
    return ReduceResult::Fail();
  }
  if (!broker()->dependencies()->DependOnArrayBufferDetachingProtector()) {
    // TODO(leszeks): Eliminate this check.
    AddNewNode<CheckTypedArrayNotDetached>({object});
  }
  ValueNode* index;
  ValueNode* length;
  GET_VALUE_OR_ABORT(index, GetUint32ElementIndex(index_object));
  GET_VALUE_OR_ABORT(length, BuildLoadTypedArrayLength(object, elements_kind));
  AddNewNode<CheckTypedArrayBounds>({index, length});
  switch (keyed_mode.access_mode()) {
    case compiler::AccessMode::kLoad:
      DCHECK(!LoadModeHandlesOOB(keyed_mode.load_mode()));
      return BuildLoadTypedArrayElement(object, index, elements_kind);
    case compiler::AccessMode::kStore:
      DCHECK(StoreModeIsInBounds(keyed_mode.store_mode()));
      BuildStoreTypedArrayElement(object, index, elements_kind);
      return ReduceResult::Done();
    case compiler::AccessMode::kHas:
      // TODO(victorgomes): Implement has element access.
      return ReduceResult::Fail();
    case compiler::AccessMode::kStoreInLiteral:
    case compiler::AccessMode::kDefine:
      UNREACHABLE();
  }
}

ReduceResult MaglevGraphBuilder::TryBuildElementLoadOnJSArrayOrJSObject(
    ValueNode* object, ValueNode* index_object,
    base::Vector<const compiler::MapRef> maps, ElementsKind elements_kind,
    KeyedAccessLoadMode load_mode) {
  DCHECK(IsFastElementsKind(elements_kind));
  bool is_jsarray = HasOnlyJSArrayMaps(maps);
  DCHECK(is_jsarray || HasOnlyJSObjectMaps(maps));

  ValueNode* elements_array = BuildLoadElements(object);
  ValueNode* index = GetInt32ElementIndex(index_object);
  ValueNode* length = is_jsarray ? GetInt32(BuildLoadJSArrayLength(object))
                                 : BuildLoadFixedArrayLength(elements_array);

  auto emit_load = |&]() -> ReduceResult {
    ValueNode* result;
    if (elements_kind == HOLEY_DOUBLE_ELEMENTS) {
      result = BuildLoadHoleyFixedDoubleArrayElement(
          elements_array, index,
          CanTreatHoleAsUndefined(maps) && LoadModeHandlesHoles(load_mode));
    } else if (elements_kind == PACKED_DOUBLE_ELEMENTS) {
      result = BuildLoadFixedDoubleArrayElement(elements_array, index);
    } else {
      DCHECK(!IsDoubleElementsKind(elements_kind));
      result = BuildLoadFixedArrayElement(elements_array, index);
      if (IsHoleyElementsKind(elements_kind)) {
        if (CanTreatHoleAsUndefined(maps) && LoadModeHandlesHoles(load_mode)) {
          result = BuildConvertHoleToUndefined(result);
        } else {
          RETURN_IF_ABORT(BuildCheckNotHole(result));
          if (IsSmiElementsKind(elements_kind)) {
            EnsureType(result, NodeType::kSmi);
          }
        }
      } else if (IsSmiElementsKind(elements_kind)) {
        EnsureType(result, NodeType::kSmi);
      }
    }
    return result;
  };

  if (CanTreatHoleAsUndefined(maps) && LoadModeHandlesOOB(load_mode)) {
    ValueNode* positive_index;
    GET_VALUE_OR_ABORT(positive_index, GetUint32ElementIndex(index));
    ValueNode* uint32_length = AddNewNode<UnsafeInt32ToUint32>({length});
    return SelectReduction(
        |&](auto& builder) {
          return BuildBranchIfUint32Compare(builder, Operation::kLessThan,
                                            positive_index, uint32_length);
        },
        emit_load, |&] { return GetRootConstant(RootIndex::kUndefinedValue); });
  } else {
    RETURN_IF_ABORT(TryBuildCheckInt32Condition(
        index, length, AssertCondition::kUnsignedLessThan,
        DeoptimizeReason::kOutOfBounds));
    return emit_load();
  }
}

ReduceResult MaglevGraphBuilder::ConvertForStoring(ValueNode* value,
                                                   ElementsKind kind) {
  if (IsDoubleElementsKind(kind)) {
    // Make sure we do not store signalling NaNs into double arrays.
    // TODO(leszeks): Consider making this a bit on StoreFixedDoubleArrayElement
    // rather than a separate node.
    return GetSilencedNaN(GetFloat64(value));
  }
  if (IsSmiElementsKind(kind)) return GetSmiValue(value);
  return value;
}

ReduceResult MaglevGraphBuilder::TryBuildElementStoreOnJSArrayOrJSObject(
    ValueNode* object, ValueNode* index_object, ValueNode* value,
    base::Vector<const compiler::MapRef> maps, ElementsKind elements_kind,
    const compiler::KeyedAccessMode& keyed_mode) {
  DCHECK(IsFastElementsKind(elements_kind));

  const bool is_jsarray = HasOnlyJSArrayMaps(maps);
  DCHECK(is_jsarray || HasOnlyJSObjectMaps(maps));

  // Get the elements array.
  ValueNode* elements_array = BuildLoadElements(object);
  GET_VALUE_OR_ABORT(value, ConvertForStoring(value, elements_kind));
  ValueNode* index;

  // TODO(verwaest): Loop peeling will turn the first iteration index of spread
  // literals into smi constants as well, breaking the assumption that we'll
  // have preallocated the space if we see known indices. Turn off this
  // optimization if loop peeling is on.
  if (keyed_mode.access_mode() == compiler::AccessMode::kStoreInLiteral &&
      index_object->Is<SmiConstant>() && is_jsarray && !any_peeled_loop_) {
    index = GetInt32ElementIndex(index_object);
  } else {
    // Check boundaries.
    ValueNode* elements_array_length = nullptr;
    ValueNode* length;
    if (is_jsarray) {
      length = GetInt32(BuildLoadJSArrayLength(object));
    } else {
      length = elements_array_length =
          BuildLoadFixedArrayLength(elements_array);
    }
    index = GetInt32ElementIndex(index_object);
    if (keyed_mode.store_mode() == KeyedAccessStoreMode::kGrowAndHandleCOW) {
      if (elements_array_length == nullptr) {
        elements_array_length = BuildLoadFixedArrayLength(elements_array);
      }

      // Validate the {index} depending on holeyness:
      //
      // For HOLEY_*_ELEMENTS the {index} must not exceed the {elements}
      // backing store capacity plus the maximum allowed gap, as otherwise
      // the (potential) backing store growth would normalize and thus
      // the elements kind of the {receiver} would change to slow mode.
      //
      // For JSArray PACKED_*_ELEMENTS the {index} must be within the range
      // |0,length+1| to be valid. In case {index} equals {length},
      // the {receiver} will be extended, but kept packed.
      //
      // Non-JSArray PACKED_*_ELEMENTS always grow by adding holes because they
      // lack the magical length property, which requires a map transition.
      // So we can assume that this did not happen if we did not see this map.
      ValueNode* limit =
          IsHoleyElementsKind(elements_kind)
              ? AddNewNode<Int32AddWithOverflow>(
                    {elements_array_length,
                     GetInt32Constant(JSObject::kMaxGap)})
          : is_jsarray
              ? AddNewNode<Int32AddWithOverflow>({length, GetInt32Constant(1)})
              : elements_array_length;
      RETURN_IF_ABORT(TryBuildCheckInt32Condition(
          index, limit, AssertCondition::kUnsignedLessThan,
          DeoptimizeReason::kOutOfBounds));

      // Grow backing store if necessary and handle COW.
      elements_array = AddNewNode<MaybeGrowFastElements>(
          {elements_array, object, index, elements_array_length},
          elements_kind);

      // If we didn't grow {elements}, it might still be COW, in which case we
      // copy it now.
      if (IsSmiOrObjectElementsKind(elements_kind)) {
        DCHECK_EQ(keyed_mode.store_mode(),
                  KeyedAccessStoreMode::kGrowAndHandleCOW);
        elements_array =
            AddNewNode<EnsureWritableFastElements>({elements_array, object});
      }

      // Update length if necessary.
      if (is_jsarray) {
        ValueNode* new_length =
            AddNewNode<UpdateJSArrayLength>({length, object, index});
        RecordKnownProperty(object, broker()->length_string(), new_length,
                            false, compiler::AccessMode::kStore);
      }
    } else {
      RETURN_IF_ABORT(TryBuildCheckInt32Condition(
          index, length, AssertCondition::kUnsignedLessThan,
          DeoptimizeReason::kOutOfBounds));

      // Handle COW if needed.
      if (IsSmiOrObjectElementsKind(elements_kind)) {
        if (keyed_mode.store_mode() == KeyedAccessStoreMode::kHandleCOW) {
          elements_array =
              AddNewNode<EnsureWritableFastElements>({elements_array, object});
        } else {
          // Ensure that this is not a COW FixedArray.
          RETURN_IF_ABORT(BuildCheckMaps(
              elements_array, base::VectorOf({broker()->fixed_array_map()})));
        }
      }
    }
  }

  // Do the store.
  if (IsDoubleElementsKind(elements_kind)) {
    BuildStoreFixedDoubleArrayElement(elements_array, index, value);
  } else {
    BuildStoreFixedArrayElement(elements_array, index, value);
  }

  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::TryBuildElementAccessOnJSArrayOrJSObject(
    ValueNode* object, ValueNode* index_object,
    const compiler::ElementAccessInfo& access_info,
    compiler::KeyedAccessMode const& keyed_mode) {
  if (!IsFastElementsKind(access_info.elements_kind())) {
    return ReduceResult::Fail();
  }
  switch (keyed_mode.access_mode()) {
    case compiler::AccessMode::kLoad:
      return TryBuildElementLoadOnJSArrayOrJSObject(
          object, index_object,
          base::VectorOf(access_info.lookup_start_object_maps()),
          access_info.elements_kind(), keyed_mode.load_mode());
    case compiler::AccessMode::kStoreInLiteral:
    case compiler::AccessMode::kStore: {
      base::Vector<const compiler::MapRef> maps =
          base::VectorOf(access_info.lookup_start_object_maps());
      ElementsKind elements_kind = access_info.elements_kind();
      return TryBuildElementStoreOnJSArrayOrJSObject(object, index_object,
                                                     GetAccumulator(), maps,
                                                     elements_kind, keyed_mode);
    }
    default:
      // TODO(victorgomes): Implement more access types.
      return ReduceResult::Fail();
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildElementAccess(
    ValueNode* object, ValueNode* index_object,
    compiler::ElementAccessFeedback const& feedback,
    compiler::FeedbackSource const& feedback_source,
    GenericAccessFunc&& build_generic_access) {
  const compiler::KeyedAccessMode& keyed_mode = feedback.keyed_mode();
  // Check for the megamorphic case.
  if (feedback.transition_groups().empty()) {
    if (keyed_mode.access_mode() == compiler::AccessMode::kLoad) {
      return BuildCallBuiltin<Builtin::kKeyedLoadIC_Megamorphic>(
          {GetTaggedValue(object), GetTaggedValue(index_object)},
          feedback_source);
    } else if (keyed_mode.access_mode() == compiler::AccessMode::kStore) {
      return BuildCallBuiltin<Builtin::kKeyedStoreIC_Megamorphic>(
          {GetTaggedValue(object), GetTaggedValue(index_object),
           GetTaggedValue(GetAccumulator())},
          feedback_source);
    }
    return ReduceResult::Fail();
  }

  NodeInfo* object_info = known_node_aspects().TryGetInfoFor(object);
  compiler::ElementAccessFeedback refined_feedback =
      object_info && object_info->possible_maps_are_known()
          ? feedback.Refine(broker(), object_info->possible_maps())
          : feedback;

  if (refined_feedback.HasOnlyStringMaps(broker())) {
    return TryBuildElementAccessOnString(object, index_object, keyed_mode);
  }

  compiler::AccessInfoFactory access_info_factory(broker(), zone());
  ZoneVector<compiler::ElementAccessInfo> access_infos(zone());
  if (!access_info_factory.ComputeElementAccessInfos(refined_feedback,
                                                     &access_infos) ||
      access_infos.empty()) {
    return ReduceResult::Fail();
  }

  // TODO(leszeks): This is copied without changes from TurboFan's native
  // context specialization. We should figure out a way to share this code.
  //
  // For holey stores or growing stores, we need to check that the prototype
  // chain contains no setters for elements, and we need to guard those checks
  // via code dependencies on the relevant prototype maps.
  if (keyed_mode.access_mode() == compiler::AccessMode::kStore) {
    // TODO(v8:7700): We could have a fast path here, that checks for the
    // common case of Array or Object prototype only and therefore avoids
    // the zone allocation of this vector.
    ZoneVector<compiler::MapRef> prototype_maps(zone());
    for (compiler::ElementAccessInfo const& access_info : access_infos) {
      for (compiler::MapRef receiver_map :
           access_info.lookup_start_object_maps()) {
        // If the {receiver_map} has a prototype and its elements backing
        // store is either holey, or we have a potentially growing store,
        // then we need to check that all prototypes have stable maps with
        // with no element accessors and no throwing behavior for elements (and
        // we need to guard against changes to that below).
        if ((IsHoleyOrDictionaryElementsKind(receiver_map.elements_kind()) ||
             StoreModeCanGrow(refined_feedback.keyed_mode().store_mode())) &&
            !receiver_map.PrototypesElementsDoNotHaveAccessorsOrThrow(
                broker(), &prototype_maps)) {
          return ReduceResult::Fail();
        }

        // TODO(v8:12547): Support writing to objects in shared space, which
        // need a write barrier that calls Object::Share to ensure the RHS is
        // shared.
        if (InstanceTypeChecker::IsAlwaysSharedSpaceJSObject(
                receiver_map.instance_type())) {
          return ReduceResult::Fail();
        }
      }
    }
    for (compiler::MapRef prototype_map : prototype_maps) {
      broker()->dependencies()->DependOnStableMap(prototype_map);
    }
  }

  // Check for monomorphic case.
  if (access_infos.size() == 1) {
    compiler::ElementAccessInfo const& access_info = access_infos.front();
    // TODO(victorgomes): Support RAB/GSAB backed typed arrays.
    if (IsRabGsabTypedArrayElementsKind(access_info.elements_kind())) {
      return ReduceResult::Fail();
    }

    if (!access_info.transition_sources().empty()) {
      compiler::MapRef transition_target =
          access_info.lookup_start_object_maps().front();
      const ZoneVector<compiler::MapRef>& transition_sources =
          access_info.transition_sources();

      // There are no transitions in heap number maps. If `object` is a SMI, we
      // would anyway fail the transition and deopt later.
      DCHECK_NE(transition_target.instance_type(),
                InstanceType::HEAP_NUMBER_TYPE);
#ifdef DEBUG
      for (auto& transition_source : transition_sources) {
        DCHECK_NE(transition_source.instance_type(),
                  InstanceType::HEAP_NUMBER_TYPE);
      }
#endif  // DEBUG

      BuildCheckHeapObject(object);
      ValueNode* object_map =
          BuildLoadTaggedField(object, HeapObject::kMapOffset);

      RETURN_IF_ABORT(BuildTransitionElementsKindOrCheckMap(
          object, object_map, transition_sources, transition_target));
    } else {
      RETURN_IF_ABORT(BuildCheckMaps(
          object, base::VectorOf(access_info.lookup_start_object_maps())));
    }
    if (IsTypedArrayElementsKind(access_info.elements_kind())) {
      return TryBuildElementAccessOnTypedArray(object, index_object,
                                               access_info, keyed_mode);
    }
    return TryBuildElementAccessOnJSArrayOrJSObject(object, index_object,
                                                    access_info, keyed_mode);
  } else {
    return TryBuildPolymorphicElementAccess(object, index_object, keyed_mode,
                                            access_infos, build_generic_access);
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildPolymorphicElementAccess(
    ValueNode* object, ValueNode* index_object,
    const compiler::KeyedAccessMode& keyed_mode,
    const ZoneVector<compiler::ElementAccessInfo>& access_infos,
    GenericAccessFunc&& build_generic_access) {
  if (keyed_mode.access_mode() == compiler::AccessMode::kLoad &&
      LoadModeHandlesOOB(keyed_mode.load_mode())) {
    // TODO(victorgomes): Handle OOB mode.
    return ReduceResult::Fail();
  }

  const bool is_any_store = compiler::IsAnyStore(keyed_mode.access_mode());
  const int access_info_count = static_cast<int>(access_infos.size());
  // Stores don't return a value, so we don't need a variable for the result.
  MaglevSubGraphBuilder sub_graph(this, is_any_store ? 0 : 1);
  std::optional<MaglevSubGraphBuilder::Variable> ret_val;
  std::optional<MaglevSubGraphBuilder::Label> done;
  std::optional<MaglevSubGraphBuilder::Label> generic_access;

  BuildCheckHeapObject(object);
  ValueNode* object_map = BuildLoadTaggedField(object, HeapObject::kMapOffset);

  // TODO(pthier): We could do better here than just emitting code for each map,
  // as many different maps can produce the exact samce code (e.g. TypedArray
  // access for Uint16/Uint32/Int16/Int32/...).
  for (int i = 0; i < access_info_count; i++) {
    compiler::ElementAccessInfo const& access_info = access_infos|i];
    std::optional<MaglevSubGraphBuilder::Label> check_next_map;
    const bool handle_transitions = !access_info.transition_sources().empty();
    ReduceResult map_check_result;
    if (i == access_info_count - 1) {
      if (handle_transitions) {
        compiler::MapRef transition_target =
            access_info.lookup_start_object_maps().front();
        map_check_result = BuildTransitionElementsKindOrCheckMap(
            object, object_map, access_info.transition_sources(),
            transition_target);
      } else {
        map_check_result = BuildCheckMaps(
            object, base::VectorOf(access_info.lookup_start_object_maps()),
            object_map);
      }
    } else {
      if (handle_transitions) {
        compiler::MapRef transition_target =
            access_info.lookup_start_object_maps().front();
        map_check_result = BuildTransitionElementsKindAndCompareMaps(
            object, object_map, access_info.transition_sources(),
            transition_target, &sub_graph, check_next_map);
      } else {
        map_check_result = BuildCompareMaps(
            object, object_map,
            base::VectorOf(access_info.lookup_start_object_maps()), &sub_graph,
            check_next_map);
      }
    }
    if (map_check_result.IsDoneWithAbort()) {
      // We know from known possible maps that this branch is not reachable,
      // so don't emit any code for it.
      continue;
    }
    ReduceResult result;
    // TODO(victorgomes): Support RAB/GSAB backed typed arrays.
    if (IsRabGsabTypedArrayElementsKind(access_info.elements_kind())) {
      result = ReduceResult::Fail();
    } else if (IsTypedArrayElementsKind(access_info.elements_kind())) {
      result = TryBuildElementAccessOnTypedArray(object, index_object,
                                                 access_info, keyed_mode);
    } else {
      result = TryBuildElementAccessOnJSArrayOrJSObject(
          object, index_object, access_info, keyed_mode);
    }

    switch (result.kind()) {
      case ReduceResult::kDoneWithValue:
      case ReduceResult::kDoneWithoutValue:
        DCHECK_EQ(result.HasValue(), !is_any_store);
        if (!done.has_value()) {
          // We initialize the label {done} lazily on the first possible path.
          // If no possible path exists, it is guaranteed that BuildCheckMaps
          // emitted an unconditional deopt and we return DoneWithAbort at the
          // end. We need one extra predecessor to jump from the generic case.
          const int possible_predecessors = access_info_count - i + 1;
          if (is_any_store) {
            done.emplace(&sub_graph, possible_predecessors);
          } else {
            ret_val.emplace(0);
            done.emplace(
                &sub_graph, possible_predecessors,
                std::initializer_list<MaglevSubGraphBuilder::Variable*>{
                    &*ret_val});
          }
        }
        if (!is_any_store) {
          sub_graph.set(*ret_val, result.value());
        }
        sub_graph.Goto(&*done);
        break;
      case ReduceResult::kFail:
        if (!generic_access.has_value()) {
          // Conservatively assume that all remaining branches can go into the
          // generic path, as we have to initialize the predecessors upfront.
          // TODO(pthier): Find a better way to do that.
          generic_access.emplace(&sub_graph, access_info_count - i);
        }
        sub_graph.Goto(&*generic_access);
        break;
      case ReduceResult::kDoneWithAbort:
        break;
      case ReduceResult::kNone:
        UNREACHABLE();
    }
    if (check_next_map.has_value()) {
      sub_graph.Bind(&*check_next_map);
    }
  }
  if (generic_access.has_value() &&
      !sub_graph.TrimPredecessorsAndBind(&*generic_access).IsDoneWithAbort()) {
    ReduceResult generic_result = build_generic_access();
    DCHECK(generic_result.IsDone());
    DCHECK_EQ(generic_result.IsDoneWithValue(), !is_any_store);
    if (!done.has_value()) {
      return is_any_store ? ReduceResult::Done() : generic_result.value();
    }
    if (!is_any_store) {
      sub_graph.set(*ret_val, generic_result.value());
    }
    sub_graph.Goto(&*done);
  }
  if (done.has_value()) {
    RETURN_IF_ABORT(sub_graph.TrimPredecessorsAndBind(&*done));
    return is_any_store ? ReduceResult::Done() : sub_graph.get(*ret_val);
  } else {
    return ReduceResult::DoneWithAbort();
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildPolymorphicPropertyAccess(
    ValueNode* receiver, ValueNode* lookup_start_object,
    compiler::NamedAccessFeedback const& feedback,
    compiler::AccessMode access_mode,
    const ZoneVector<compiler::PropertyAccessInfo>& access_infos,
    GenericAccessFunc&& build_generic_access) {
  const bool is_any_store = compiler::IsAnyStore(access_mode);
  const int access_info_count = static_cast<int>(access_infos.size());
  int number_map_index = -1;

  bool needs_migration = false;
  bool has_deprecated_map_without_migration_target =
      feedback.has_deprecated_map_without_migration_target();
  for (int i = 0; i < access_info_count; i++) {
    compiler::PropertyAccessInfo const& access_info = access_infos|i];
    DCHECK(!access_info.IsInvalid());
    for (compiler::MapRef map : access_info.lookup_start_object_maps()) {
      if (map.is_migration_target()) {
        needs_migration = true;
      }
      if (map.IsHeapNumberMap()) {
        GetOrCreateInfoFor(lookup_start_object);
        base::SmallVector<compiler::MapRef, 1> known_maps = {map};
        KnownMapsMerger merger(broker(), zone(), base::VectorOf(known_maps));
        merger.IntersectWithKnownNodeAspects(lookup_start_object,
                                             known_node_aspects());
        if (!merger.intersect_set().is_empty()) {
          DCHECK_EQ(number_map_index, -1);
          number_map_index = i;
        }
      }
    }
  }

  // Stores don't return a value, so we don't need a variable for the result.
  MaglevSubGraphBuilder sub_graph(this, is_any_store ? 0 : 1);
  std::optional<MaglevSubGraphBuilder::Variable> ret_val;
  std::optional<MaglevSubGraphBuilder::Label> done;
  std::optional<MaglevSubGraphBuilder::Label> is_number;
  std::optional<MaglevSubGraphBuilder::Label> generic_access;

  if (number_map_index >= 0) {
    is_number.emplace(&sub_graph, 2);
    sub_graph.GotoIfTrue<BranchIfSmi>(&*is_number, {lookup_start_object});
  } else {
    BuildCheckHeapObject(lookup_start_object);
  }
  ValueNode* lookup_start_object_map =
      BuildLoadTaggedField(lookup_start_object, HeapObject::kMapOffset);

  if (needs_migration &&
      !v8_flags.maglev_skip_migration_check_for_polymorphic_access) {
    // TODO(marja, v8:7700): Try migrating only if all comparisons failed.
    // TODO(marja, v8:7700): Investigate making polymoprhic map comparison (with
    // migration) a control node (like switch).
    lookup_start_object_map = AddNewNode<MigrateMapIfNeeded>(
        {lookup_start_object_map, lookup_start_object});
  }

  for (int i = 0; i < access_info_count; i++) {
    compiler::PropertyAccessInfo const& access_info = access_infos|i];
    std::optional<MaglevSubGraphBuilder::Label> check_next_map;
    ReduceResult map_check_result;
    const auto& maps = access_info.lookup_start_object_maps();
    if (i == access_info_count - 1) {
      map_check_result =
          BuildCheckMaps(lookup_start_object, base::VectorOf(maps), {},
                         has_deprecated_map_without_migration_target);
    } else {
      map_check_result =
          BuildCompareMaps(lookup_start_object, lookup_start_object_map,
                           base::VectorOf(maps), &sub_graph, check_next_map);
    }
    if (map_check_result.IsDoneWithAbort()) {
      // We know from known possible maps that this branch is not reachable,
      // so don't emit any code for it.
      continue;
    }
    if (i == number_map_index) {
      DCHECK(is_number.has_value());
      sub_graph.Goto(&*is_number);
      sub_graph.Bind(&*is_number);
    }

    ReduceResult result;
    if (is_any_store) {
      result = TryBuildPropertyStore(receiver, lookup_start_object,
                                     feedback.name(), access_info, access_mode);
    } else {
      result = TryBuildPropertyLoad(receiver, lookup_start_object,
                                    feedback.name(), access_info);
    }

    switch (result.kind()) {
      case ReduceResult::kDoneWithValue:
      case ReduceResult::kDoneWithoutValue:
        DCHECK_EQ(result.HasValue(), !is_any_store);
        if (!done.has_value()) {
          // We initialize the label {done} lazily on the first possible path.
          // If no possible path exists, it is guaranteed that BuildCheckMaps
          // emitted an unconditional deopt and we return DoneWithAbort at the
          // end. We need one extra predecessor to jump from the generic case.
          const int possible_predecessors = access_info_count - i + 1;
          if (is_any_store) {
            done.emplace(&sub_graph, possible_predecessors);
          } else {
            ret_val.emplace(0);
            done.emplace(
                &sub_graph, possible_predecessors,
                std::initializer_list<MaglevSubGraphBuilder::Variable*>{
                    &*ret_val});
          }
        }

        if (!is_any_store) {
          sub_graph.set(*ret_val, result.value());
        }
        sub_graph.Goto(&*done);
        break;
      case ReduceResult::kDoneWithAbort:
        break;
      case ReduceResult::kFail:
        if (!generic_access.has_value()) {
          // Conservatively assume that all remaining branches can go into the
          // generic path, as we have to initialize the predecessors upfront.
          // TODO(pthier): Find a better way to do that.
          generic_access.emplace(&sub_graph, access_info_count - i);
        }
        sub_graph.Goto(&*generic_access);
        break;
      default:
        UNREACHABLE();
    }

    if (check_next_map.has_value()) {
      sub_graph.Bind(&*check_next_map);
    }
  }

  if (generic_access.has_value() &&
      !sub_graph.TrimPredecessorsAndBind(&*generic_access).IsDoneWithAbort()) {
    ReduceResult generic_result = build_generic_access();
    DCHECK(generic_result.IsDone());
    DCHECK_EQ(generic_result.IsDoneWithValue(), !is_any_store);
    if (!done.has_value()) {
      return is_any_store ? ReduceResult::Done() : generic_result.value();
    }
    if (!is_any_store) {
      sub_graph.set(*ret_val, generic_result.value());
    }
    sub_graph.Goto(&*done);
  }

  if (done.has_value()) {
    RETURN_IF_ABORT(sub_graph.TrimPredecessorsAndBind(&*done));
    return is_any_store ? ReduceResult::Done() : sub_graph.get(*ret_val);
  } else {
    return ReduceResult::DoneWithAbort();
  }
}

void MaglevGraphBuilder::RecordKnownProperty(
    ValueNode* lookup_start_object, KnownNodeAspects::LoadedPropertyMapKey key,
    ValueNode* value, bool is_const, compiler::AccessMode access_mode) {
  DCHECK(!value->properties().is_conversion());
  KnownNodeAspects::LoadedPropertyMap& loaded_properties =
      is_const ? known_node_aspects().loaded_constant_properties
               : known_node_aspects().loaded_properties;
  // Try to get loaded_properties|key] if it already exists, otherwise
  // construct loaded_properties|key] = ZoneMap{zone()}.
  auto& props_for_key =
      loaded_properties.try_emplace(key, zone()).first->second;

  if (!is_const && IsAnyStore(access_mode)) {
    if (is_loop_effect_tracking()) {
      loop_effects_->keys_cleared.insert(key);
    }
    // We don't do any aliasing analysis, so stores clobber all other cached
    // loads of a property with that key. We only need to do this for
    // non-constant properties, since constant properties are known not to
    // change and therefore can't be clobbered.
    // TODO(leszeks): Do some light aliasing analysis here, e.g. checking
    // whether there's an intersection of known maps.
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  * Removing all non-constant cached ";
      switch (key.type()) {
        case KnownNodeAspects::LoadedPropertyMapKey::kName:
          std::cout << "properties with name " << *key.name().object();
          break;
        case KnownNodeAspects::LoadedPropertyMapKey::kElements:
          std::cout << "Elements";
          break;
        case KnownNodeAspects::LoadedPropertyMapKey::kTypedArrayLength:
          std::cout << "TypedArray length";
          break;
        case KnownNodeAspects::LoadedPropertyMapKey::kStringLength:
          std::cout << "String length";
          break;
      }
      std::cout << std::endl;
    }
    props_for_key.clear();
  }

  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "  * Recording " << (is_const ? "constant" : "non-constant")
              << " known property "
              << PrintNodeLabel(graph_labeller(), lookup_start_object) << ": "
              << PrintNode(graph_labeller(), lookup_start_object) << " |";
    switch (key.type()) {
      case KnownNodeAspects::LoadedPropertyMapKey::kName:
        std::cout << *key.name().object();
        break;
      case KnownNodeAspects::LoadedPropertyMapKey::kElements:
        std::cout << "Elements";
        break;
      case KnownNodeAspects::LoadedPropertyMapKey::kTypedArrayLength:
        std::cout << "TypedArray length";
        break;
      case KnownNodeAspects::LoadedPropertyMapKey::kStringLength:
        std::cout << "String length";
        break;
    }
    std::cout << "] = " << PrintNodeLabel(graph_labeller(), value) << ": "
              << PrintNode(graph_labeller(), value) << std::endl;
  }

  if (IsAnyStore(access_mode) && !is_const && is_loop_effect_tracking()) {
    auto updated = props_for_key.emplace(lookup_start_object, value);
    if (updated.second) {
      loop_effects_->objects_written.insert(lookup_start_object);
    } else if (updated.first->second != value) {
      updated.first->second = value;
      loop_effects_->objects_written.insert(lookup_start_object);
    }
  } else {
    props_for_key|lookup_start_object] = value;
  }
}

ReduceResult MaglevGraphBuilder::TryReuseKnownPropertyLoad(
    ValueNode* lookup_start_object, compiler::NameRef name) {
  if (ReduceResult result = TryFindLoadedProperty(
          known_node_aspects().loaded_properties, lookup_start_object, name);
      result.IsDone()) {
    if (v8_flags.trace_maglev_graph_building && result.IsDoneWithValue()) {
      std::cout << "  * Reusing non-constant loaded property "
                << PrintNodeLabel(graph_labeller(), result.value()) << ": "
                << PrintNode(graph_labeller(), result.value()) << std::endl;
    }
    return result;
  }
  if (ReduceResult result =
          TryFindLoadedProperty(known_node_aspects().loaded_constant_properties,
                                lookup_start_object, name);
      result.IsDone()) {
    if (v8_flags.trace_maglev_graph_building && result.IsDoneWithValue()) {
      std::cout << "  * Reusing constant loaded property "
                << PrintNodeLabel(graph_labeller(), result.value()) << ": "
                << PrintNode(graph_labeller(), result.value()) << std::endl;
    }
    return result;
  }
  return ReduceResult::Fail();
}

ValueNode* MaglevGraphBuilder::BuildLoadStringLength(ValueNode* string) {
  if (ReduceResult result = TryFindLoadedProperty(
          known_node_aspects().loaded_constant_properties, string,
          KnownNodeAspects::LoadedPropertyMapKey::StringLength());
      result.IsDone()) {
    if (v8_flags.trace_maglev_graph_building && result.IsDoneWithValue()) {
      std::cout << "  * Reusing constant |String length]"
                << PrintNodeLabel(graph_labeller(), result.value()) << ": "
                << PrintNode(graph_labeller(), result.value()) << std::endl;
    }
    return result.value();
  }
  ValueNode* result = AddNewNode<StringLength>({string});
  RecordKnownProperty(string,
                      KnownNodeAspects::LoadedPropertyMapKey::StringLength(),
                      result, true, compiler::AccessMode::kLoad);
  return result;
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildLoadNamedProperty(
    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
    compiler::FeedbackSource& feedback_source,
    GenericAccessFunc&& build_generic_access) {
  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(feedback_source,
                                             compiler::AccessMode::kLoad, name);
  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      return EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess);
    case compiler::ProcessedFeedback::kNamedAccess: {
      RETURN_IF_DONE(TryReuseKnownPropertyLoad(lookup_start_object, name));
      return TryBuildNamedAccess(
          receiver, lookup_start_object, processed_feedback.AsNamedAccess(),
          feedback_source, compiler::AccessMode::kLoad, build_generic_access);
    }
    default:
      return ReduceResult::Fail();
  }
}

ReduceResult MaglevGraphBuilder::TryBuildLoadNamedProperty(
    ValueNode* receiver, compiler::NameRef name,
    compiler::FeedbackSource& feedback_source) {
  auto build_generic_access = |this, &receiver, &name, &feedback_source]() {
    ValueNode* context = GetContext();
    return AddNewNode<LoadNamedGeneric>({context, receiver}, name,
                                        feedback_source);
  };
  return TryBuildLoadNamedProperty(receiver, receiver, name, feedback_source,
                                   build_generic_access);
}

void MaglevGraphBuilder::VisitGetNamedProperty() {
  // GetNamedProperty <object> <name_index> <slot>
  ValueNode* object = LoadRegister(0);
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  PROCESS_AND_RETURN_IF_DONE(
      TryBuildLoadNamedProperty(object, name, feedback_source), SetAccumulator);
  // Create a generic load in the fallthrough.
  ValueNode* context = GetContext();
  SetAccumulator(
      AddNewNode<LoadNamedGeneric>({context, object}, name, feedback_source));
}

ValueNode* MaglevGraphBuilder::GetConstant(compiler::ObjectRef ref) {
  if (ref.IsSmi()) return GetSmiConstant(ref.AsSmi());
  compiler::HeapObjectRef constant = ref.AsHeapObject();

  if (IsThinString(*constant.object())) {
    constant = MakeRefAssumeMemoryFence(
        broker(), Cast<ThinString>(*constant.object())->actual());
  }

  auto root_index = broker()->FindRootIndex(constant);
  if (root_index.has_value()) {
    return GetRootConstant(*root_index);
  }

  auto it = graph_->constants().find(constant);
  if (it == graph_->constants().end()) {
    Constant* node = CreateNewConstantNode<Constant>(0, constant);
    graph_->constants().emplace(constant, node);
    return node;
  }
  return it->second;
}

ValueNode* MaglevGraphBuilder::GetTrustedConstant(compiler::HeapObjectRef ref,
                                                  IndirectPointerTag tag) {
#ifdef V8_ENABLE_SANDBOX
  auto it = graph_->trusted_constants().find(ref);
  if (it == graph_->trusted_constants().end()) {
    TrustedConstant* node = CreateNewConstantNode<TrustedConstant>(0, ref, tag);
    graph_->trusted_constants().emplace(ref, node);
    return node;
  }
  SBXCHECK_EQ(it->second->tag(), tag);
  return it->second;
#else
  return GetConstant(ref);
#endif
}

void MaglevGraphBuilder::VisitGetNamedPropertyFromSuper() {
  // GetNamedPropertyFromSuper <receiver> <name_index> <slot>
  ValueNode* receiver = LoadRegister(0);
  ValueNode* home_object = GetAccumulator();
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  // {home_object} is guaranteed to be a HeapObject.
  ValueNode* home_object_map =
      BuildLoadTaggedField(home_object, HeapObject::kMapOffset);
  ValueNode* lookup_start_object =
      BuildLoadTaggedField(home_object_map, Map::kPrototypeOffset);

  auto build_generic_access = |this, &receiver, &lookup_start_object, &name,
                               &feedback_source]() {
    ValueNode* context = GetContext();
    return AddNewNode<LoadNamedFromSuperGeneric>(
        {context, receiver, lookup_start_object}, name, feedback_source);
  };

  PROCESS_AND_RETURN_IF_DONE(
      TryBuildLoadNamedProperty(receiver, lookup_start_object, name,
                                feedback_source, build_generic_access),
      SetAccumulator);
  // Create a generic load.
  SetAccumulator(build_generic_access());
}

bool MaglevGraphBuilder::TryBuildGetKeyedPropertyWithEnumeratedKey(
    ValueNode* object, const compiler::FeedbackSource& feedback_source,
    const compiler::ProcessedFeedback& processed_feedback) {
  if (current_for_in_state.index != nullptr &&
      current_for_in_state.enum_cache_indices != nullptr &&
      current_for_in_state.key == current_interpreter_frame_.accumulator()) {
    bool speculating_receiver_map_matches = false;
    if (current_for_in_state.receiver != object) {
      // When the feedback is uninitialized, it is either a keyed load which
      // always hits the enum cache, or a keyed load that had never been
      // reached. In either case, we can check the map of the receiver and use
      // the enum cache if the map match the {cache_type}.
      if (processed_feedback.kind() !=
          compiler::ProcessedFeedback::kInsufficient) {
        return false;
      }
      BuildCheckHeapObject(object);
      speculating_receiver_map_matches = true;
    }

    if (current_for_in_state.receiver_needs_map_check ||
        speculating_receiver_map_matches) {
      auto* receiver_map = BuildLoadTaggedField(object, HeapObject::kMapOffset);
      AddNewNode<CheckDynamicValue>(
          {receiver_map, current_for_in_state.cache_type});
      if (current_for_in_state.receiver == object) {
        current_for_in_state.receiver_needs_map_check = false;
      }
    }
    // TODO(leszeks): Cache the field index per iteration.
    auto* field_index = BuildLoadFixedArrayElement(
        current_for_in_state.enum_cache_indices, current_for_in_state.index);
    SetAccumulator(
        AddNewNode<LoadTaggedFieldByFieldIndex>({object, field_index}));
    return true;
  }
  return false;
}

void MaglevGraphBuilder::BuildGetKeyedProperty(
    ValueNode* object, const compiler::FeedbackSource& feedback_source,
    const compiler::ProcessedFeedback& processed_feedback) {
  if (TryBuildGetKeyedPropertyWithEnumeratedKey(object, feedback_source,
                                                processed_feedback)) {
    return;
  }

  auto build_generic_access = |this, object, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* key = GetAccumulator();
    return AddNewNode<GetKeyedGeneric>({context, object, key}, feedback_source);
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericKeyedAccess));

    case compiler::ProcessedFeedback::kElementAccess: {
      // Get the accumulator without conversion. TryBuildElementAccess
      // will try to pick the best representation.
      ValueNode* index = current_interpreter_frame_.accumulator();
      ReduceResult result = TryBuildElementAccess(
          object, index, processed_feedback.AsElementAccess(), feedback_source,
          build_generic_access);
      PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);
      break;
    }

    case compiler::ProcessedFeedback::kNamedAccess: {
      ValueNode* key = GetAccumulator();
      compiler::NameRef name = processed_feedback.AsNamedAccess().name();
      RETURN_VOID_IF_ABORT(BuildCheckValue(key, name));

      ReduceResult result = TryReuseKnownPropertyLoad(object, name);
      PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);

      result = TryBuildNamedAccess(
          object, object, processed_feedback.AsNamedAccess(), feedback_source,
          compiler::AccessMode::kLoad, build_generic_access);
      PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);
      break;
    }

    default:
      break;
  }

  // Create a generic load in the fallthrough.
  SetAccumulator(build_generic_access());
}

void MaglevGraphBuilder::VisitGetKeyedProperty() {
  // GetKeyedProperty <object> <slot>
  ValueNode* object = LoadRegister(0);
  // TODO(leszeks): We don't need to tag the key if it's an Int32 and a simple
  // monomorphic element load.
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback* processed_feedback =
      &broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kLoad, std::nullopt);
  if (processed_feedback->kind() ==
          compiler::ProcessedFeedback::kElementAccess &&
      processed_feedback->AsElementAccess().transition_groups().empty()) {
    if (auto constant = TryGetConstant(GetAccumulator());
        constant.has_value() && constant->IsName()) {
      compiler::NameRef name = constant->AsName();
      if (name.IsUniqueName() && !name.object()->IsArrayIndex()) {
        processed_feedback =
            &processed_feedback->AsElementAccess().Refine(broker(), name);
      }
    }
  }

  BuildGetKeyedProperty(object, feedback_source, *processed_feedback);
}

void MaglevGraphBuilder::VisitGetEnumeratedKeyedProperty() {
  // GetEnumeratedKeyedProperty <object> <enum_index> <cache_type> <slot>
  ValueNode* object = LoadRegister(0);
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kLoad, std::nullopt);

  BuildGetKeyedProperty(object, feedback_source, processed_feedback);
}

void MaglevGraphBuilder::VisitLdaModuleVariable() {
  // LdaModuleVariable <cell_index> <depth>
  int cell_index = iterator_.GetImmediateOperand(0);
  size_t depth = iterator_.GetUnsignedImmediateOperand(1);
  ValueNode* context = GetContextAtDepth(GetContext(), depth);

  ValueNode* module = LoadAndCacheContextSlot(
      context, Context::EXTENSION_INDEX, kImmutable, ContextKind::kDefault);
  ValueNode* exports_or_imports;
  if (cell_index > 0) {
    exports_or_imports =
        BuildLoadTaggedField(module, SourceTextModule::kRegularExportsOffset);
    // The actual array index is (cell_index - 1).
    cell_index -= 1;
  } else {
    exports_or_imports =
        BuildLoadTaggedField(module, SourceTextModule::kRegularImportsOffset);
    // The actual array index is (-cell_index - 1).
    cell_index = -cell_index - 1;
  }
  ValueNode* cell = BuildLoadFixedArrayElement(exports_or_imports, cell_index);
  SetAccumulator(BuildLoadTaggedField(cell, Cell::kValueOffset));
}

ValueNode* MaglevGraphBuilder::GetContextAtDepth(ValueNode* context,
                                                 size_t depth) {
  MinimizeContextChainDepth(&context, &depth);

  if (compilation_unit_->info()->specialize_to_function_context()) {
    compiler::OptionalContextRef maybe_ref =
        FunctionContextSpecialization::TryToRef(compilation_unit_, context,
                                                &depth);
    if (maybe_ref.has_value()) {
      context = GetConstant(maybe_ref.value());
    }
  }

  for (size_t i = 0; i < depth; i++) {
    context = LoadAndCacheContextSlot(context, Context::PREVIOUS_INDEX,
                                      kImmutable, ContextKind::kDefault);
  }
  return context;
}

void MaglevGraphBuilder::VisitStaModuleVariable() {
  // StaModuleVariable <cell_index> <depth>
  int cell_index = iterator_.GetImmediateOperand(0);
  if (V8_UNLIKELY(cell_index < 0)) {
    // TODO(verwaest): Make this fail as well.
    CHECK(BuildCallRuntime(Runtime::kAbort,
                           {GetSmiConstant(static_cast<int>(
                               AbortReason::kUnsupportedModuleOperation))})
              .IsDone());
    return;
  }

  size_t depth = iterator_.GetUnsignedImmediateOperand(1);
  ValueNode* context = GetContextAtDepth(GetContext(), depth);

  ValueNode* module = LoadAndCacheContextSlot(
      context, Context::EXTENSION_INDEX, kImmutable, ContextKind::kDefault);
  ValueNode* exports =
      BuildLoadTaggedField(module, SourceTextModule::kRegularExportsOffset);
  // The actual array index is (cell_index - 1).
  cell_index -= 1;
  ValueNode* cell = BuildLoadFixedArrayElement(exports, cell_index);
  BuildStoreTaggedField(cell, GetAccumulator(), Cell::kValueOffset,
                        StoreTaggedMode::kDefault);
}

void MaglevGraphBuilder::BuildLoadGlobal(
    compiler::NameRef name, compiler::FeedbackSource& feedback_source,
    TypeofMode typeof_mode) {
  const compiler::ProcessedFeedback& access_feedback =
      broker()->GetFeedbackForGlobalAccess(feedback_source);

  if (access_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForGenericGlobalAccess));
  }

  const compiler::GlobalAccessFeedback& global_access_feedback =
      access_feedback.AsGlobalAccess();
  PROCESS_AND_RETURN_IF_DONE(TryBuildGlobalLoad(global_access_feedback),
                             SetAccumulator);

  ValueNode* context = GetContext();
  SetAccumulator(
      AddNewNode<LoadGlobal>({context}, name, feedback_source, typeof_mode));
}

void MaglevGraphBuilder::VisitSetNamedProperty() {
  // SetNamedProperty <object> <name_index> <slot>
  ValueNode* object = LoadRegister(0);
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStore, name);

  auto build_generic_access = |this, object, &name, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<SetNamedGeneric>({context, object, value}, name,
                                feedback_source);
    return ReduceResult::Done();
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess));

    case compiler::ProcessedFeedback::kNamedAccess:
      RETURN_VOID_IF_DONE(TryBuildNamedAccess(
          object, object, processed_feedback.AsNamedAccess(), feedback_source,
          compiler::AccessMode::kStore, build_generic_access));
      break;
    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitDefineNamedOwnProperty() {
  // DefineNamedOwnProperty <object> <name_index> <slot>
  ValueNode* object = LoadRegister(0);
  compiler::NameRef name = GetRefOperand<Name>(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStore, name);

  auto build_generic_access = |this, object, &name, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<DefineNamedOwnGeneric>({context, object, value}, name,
                                      feedback_source);
    return ReduceResult::Done();
  };
  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess));

    case compiler::ProcessedFeedback::kNamedAccess:
      RETURN_VOID_IF_DONE(TryBuildNamedAccess(
          object, object, processed_feedback.AsNamedAccess(), feedback_source,
          compiler::AccessMode::kDefine, build_generic_access));
      break;

    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitSetKeyedProperty() {
  // SetKeyedProperty <object> <key> <slot>
  ValueNode* object = LoadRegister(0);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStore, std::nullopt);

  auto build_generic_access = |this, object, &feedback_source]() {
    ValueNode* key = LoadRegister(1);
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<SetKeyedGeneric>({context, object, key, value}, feedback_source);
    return ReduceResult::Done();
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericKeyedAccess));

    case compiler::ProcessedFeedback::kElementAccess: {
      // Get the key without conversion. TryBuildElementAccess will try to pick
      // the best representation.
      ValueNode* index =
          current_interpreter_frame_.get(iterator_.GetRegisterOperand(1));
      RETURN_VOID_IF_DONE(TryBuildElementAccess(
          object, index, processed_feedback.AsElementAccess(), feedback_source,
          build_generic_access));
    } break;

    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitDefineKeyedOwnProperty() {
  // DefineKeyedOwnProperty <object> <key> <flags> <slot>
  ValueNode* object = LoadRegister(0);
  ValueNode* key = LoadRegister(1);
  ValueNode* flags = GetSmiConstant(GetFlag8Operand(2));
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  // TODO(victorgomes): Add monomorphic fast path.

  // Create a generic store in the fallthrough.
  ValueNode* context = GetContext();
  ValueNode* value = GetAccumulator();
  AddNewNode<DefineKeyedOwnGeneric>({context, object, key, value, flags},
                                    feedback_source);
}

void MaglevGraphBuilder::VisitStaInArrayLiteral() {
  // StaInArrayLiteral <object> <index> <slot>
  ValueNode* object = LoadRegister(0);
  ValueNode* index = LoadRegister(1);
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForPropertyAccess(
          feedback_source, compiler::AccessMode::kStoreInLiteral, std::nullopt);

  auto build_generic_access = |this, object, index, &feedback_source]() {
    ValueNode* context = GetContext();
    ValueNode* value = GetAccumulator();
    AddNewNode<StoreInArrayLiteralGeneric>({context, object, index, value},
                                           feedback_source);
    return ReduceResult::Done();
  };

  switch (processed_feedback.kind()) {
    case compiler::ProcessedFeedback::kInsufficient:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForGenericKeyedAccess));

    case compiler::ProcessedFeedback::kElementAccess: {
      RETURN_VOID_IF_DONE(TryBuildElementAccess(
          object, index, processed_feedback.AsElementAccess(), feedback_source,
          build_generic_access));
      break;
    }

    default:
      break;
  }

  // Create a generic store in the fallthrough.
  RETURN_VOID_IF_ABORT(build_generic_access());
}

void MaglevGraphBuilder::VisitDefineKeyedOwnPropertyInLiteral() {
  ValueNode* object = LoadRegister(0);
  ValueNode* name = LoadRegister(1);
  ValueNode* value = GetAccumulator();
  ValueNode* flags = GetSmiConstant(GetFlag8Operand(2));
  ValueNode* slot = GetTaggedIndexConstant(GetSlotOperand(3).ToInt());
  ValueNode* feedback_vector = GetConstant(feedback());
  CHECK(BuildCallRuntime(Runtime::kDefineKeyedOwnPropertyInLiteral,
                         {object, name, value, flags, feedback_vector, slot})
            .IsDone());
}

void MaglevGraphBuilder::VisitAdd() { VisitBinaryOperation<Operation::kAdd>(); }
void MaglevGraphBuilder::VisitSub() {
  VisitBinaryOperation<Operation::kSubtract>();
}
void MaglevGraphBuilder::VisitMul() {
  VisitBinaryOperation<Operation::kMultiply>();
}
void MaglevGraphBuilder::VisitDiv() {
  VisitBinaryOperation<Operation::kDivide>();
}
void MaglevGraphBuilder::VisitMod() {
  VisitBinaryOperation<Operation::kModulus>();
}
void MaglevGraphBuilder::VisitExp() {
  VisitBinaryOperation<Operation::kExponentiate>();
}
void MaglevGraphBuilder::VisitBitwiseOr() {
  VisitBinaryOperation<Operation::kBitwiseOr>();
}
void MaglevGraphBuilder::VisitBitwiseXor() {
  VisitBinaryOperation<Operation::kBitwiseXor>();
}
void MaglevGraphBuilder::VisitBitwiseAnd() {
  VisitBinaryOperation<Operation::kBitwiseAnd>();
}
void MaglevGraphBuilder::VisitShiftLeft() {
  VisitBinaryOperation<Operation::kShiftLeft>();
}
void MaglevGraphBuilder::VisitShiftRight() {
  VisitBinaryOperation<Operation::kShiftRight>();
}
void MaglevGraphBuilder::VisitShiftRightLogical() {
  VisitBinaryOperation<Operation::kShiftRightLogical>();
}

void MaglevGraphBuilder::VisitAddSmi() {
  VisitBinarySmiOperation<Operation::kAdd>();
}
void MaglevGraphBuilder::VisitSubSmi() {
  VisitBinarySmiOperation<Operation::kSubtract>();
}
void MaglevGraphBuilder::VisitMulSmi() {
  VisitBinarySmiOperation<Operation::kMultiply>();
}
void MaglevGraphBuilder::VisitDivSmi() {
  VisitBinarySmiOperation<Operation::kDivide>();
}
void MaglevGraphBuilder::VisitModSmi() {
  VisitBinarySmiOperation<Operation::kModulus>();
}
void MaglevGraphBuilder::VisitExpSmi() {
  VisitBinarySmiOperation<Operation::kExponentiate>();
}
void MaglevGraphBuilder::VisitBitwiseOrSmi() {
  VisitBinarySmiOperation<Operation::kBitwiseOr>();
}
void MaglevGraphBuilder::VisitBitwiseXorSmi() {
  VisitBinarySmiOperation<Operation::kBitwiseXor>();
}
void MaglevGraphBuilder::VisitBitwiseAndSmi() {
  VisitBinarySmiOperation<Operation::kBitwiseAnd>();
}
void MaglevGraphBuilder::VisitShiftLeftSmi() {
  VisitBinarySmiOperation<Operation::kShiftLeft>();
}
void MaglevGraphBuilder::VisitShiftRightSmi() {
  VisitBinarySmiOperation<Operation::kShiftRight>();
}
void MaglevGraphBuilder::VisitShiftRightLogicalSmi() {
  VisitBinarySmiOperation<Operation::kShiftRightLogical>();
}

void MaglevGraphBuilder::VisitInc() {
  VisitUnaryOperation<Operation::kIncrement>();
}
void MaglevGraphBuilder::VisitDec() {
  VisitUnaryOperation<Operation::kDecrement>();
}
void MaglevGraphBuilder::VisitNegate() {
  VisitUnaryOperation<Operation::kNegate>();
}
void MaglevGraphBuilder::VisitBitwiseNot() {
  VisitUnaryOperation<Operation::kBitwiseNot>();
}

void MaglevGraphBuilder::VisitToBooleanLogicalNot() {
  SetAccumulator(BuildToBoolean</* flip */ true>(GetAccumulator()));
}

ValueNode* MaglevGraphBuilder::BuildLogicalNot(ValueNode* value) {
  // TODO(victorgomes): Use NodeInfo to add more type optimizations here.
  switch (value->opcode()) {
#define CASE(Name)                                         \
  case Opcode::k##Name: {                                  \
    return GetBooleanConstant(                             \
        !value->Cast<Name>()->ToBoolean(local_isolate())); \
  }
    CONSTANT_VALUE_NODE_LIST(CASE)
#undef CASE
    default:
      return AddNewNode<LogicalNot>({value});
  }
}

void MaglevGraphBuilder::VisitLogicalNot() {
  // Invariant: accumulator must already be a boolean value.
  SetAccumulator(BuildLogicalNot(GetAccumulator()));
}

void MaglevGraphBuilder::VisitTypeOf() {
  ValueNode* value = GetAccumulator();
  PROCESS_AND_RETURN_IF_DONE(TryReduceTypeOf(value), SetAccumulator);

  FeedbackNexus nexus = FeedbackNexusForOperand(0);
  TypeOfFeedback::Result feedback = nexus.GetTypeOfFeedback();
  switch (feedback) {
    case TypeOfFeedback::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForTypeOf));
    case TypeOfFeedback::kNumber:
      BuildCheckNumber(value);
      SetAccumulator(GetRootConstant(RootIndex::knumber_string));
      return;
    case TypeOfFeedback::kString:
      BuildCheckString(value);
      SetAccumulator(GetRootConstant(RootIndex::kstring_string));
      return;
    case TypeOfFeedback::kFunction:
      AddNewNode<CheckDetectableCallable>({value},
                                          GetCheckType(GetType(value)));
      EnsureType(value, NodeType::kCallable);
      SetAccumulator(GetRootConstant(RootIndex::kfunction_string));
      return;
    default:
      break;
  }

  SetAccumulator(BuildCallBuiltin<Builtin::kTypeof>({GetTaggedValue(value)}));
}

void MaglevGraphBuilder::VisitDeletePropertyStrict() {
  ValueNode* object = LoadRegister(0);
  ValueNode* key = GetAccumulator();
  ValueNode* context = GetContext();
  SetAccumulator(AddNewNode<DeleteProperty>({context, object, key},
                                            LanguageMode::kStrict));
}

void MaglevGraphBuilder::VisitDeletePropertySloppy() {
  ValueNode* object = LoadRegister(0);
  ValueNode* key = GetAccumulator();
  ValueNode* context = GetContext();
  SetAccumulator(AddNewNode<DeleteProperty>({context, object, key},
                                            LanguageMode::kSloppy));
}

void MaglevGraphBuilder::VisitGetSuperConstructor() {
  ValueNode* active_function = GetAccumulator();
  // TODO(victorgomes): Maybe BuildLoadTaggedField should support constants
  // instead.
  if (compiler::OptionalHeapObjectRef constant =
          TryGetConstant(active_function)) {
    compiler::MapRef map = constant->map(broker());
    if (map.is_stable()) {
      broker()->dependencies()->DependOnStableMap(map);
      ValueNode* map_proto = GetConstant(map.prototype(broker()));
      StoreRegister(iterator_.GetRegisterOperand(0), map_proto);
      return;
    }
  }
  ValueNode* map =
      BuildLoadTaggedField(active_function, HeapObject::kMapOffset);
  ValueNode* map_proto = BuildLoadTaggedField(map, Map::kPrototypeOffset);
  StoreRegister(iterator_.GetRegisterOperand(0), map_proto);
}

bool MaglevGraphBuilder::HasValidInitialMap(
    compiler::JSFunctionRef new_target, compiler::JSFunctionRef constructor) {
  if (!new_target.map(broker()).has_prototype_slot()) return false;
  if (!new_target.has_initial_map(broker())) return false;
  compiler::MapRef initial_map = new_target.initial_map(broker());
  return initial_map.GetConstructor(broker()).equals(constructor);
}

bool MaglevGraphBuilder::TryBuildFindNonDefaultConstructorOrConstruct(
    ValueNode* this_function, ValueNode* new_target,
    std::pair<interpreter::Register, interpreter::Register> result) {
  // See also:
  // JSNativeContextSpecialization::ReduceJSFindNonDefaultConstructorOrConstruct

  compiler::OptionalHeapObjectRef maybe_constant =
      TryGetConstant(this_function);
  if (!maybe_constant) return false;

  compiler::MapRef function_map = maybe_constant->map(broker());
  compiler::HeapObjectRef current = function_map.prototype(broker());

  // TODO(v8:13091): Don't produce incomplete stack traces when debug is active.
  // We already deopt when a breakpoint is set. But it would be even nicer to
  // avoid producting incomplete stack traces when when debug is active, even if
  // there are no breakpoints - then a user inspecting stack traces via Dev
  // Tools would always see the full stack trace.

  while (true) {
    if (!current.IsJSFunction()) return false;
    compiler::JSFunctionRef current_function = current.AsJSFunction();

    // If there are class fields, bail out. TODO(v8:13091): Handle them here.
    if (current_function.shared(broker())
            .requires_instance_members_initializer()) {
      return false;
    }

    // If there are private methods, bail out. TODO(v8:13091): Handle them here.
    if (current_function.context(broker())
            .scope_info(broker())
            .ClassScopeHasPrivateBrand()) {
      return false;
    }

    FunctionKind kind = current_function.shared(broker()).kind();
    if (kind != FunctionKind::kDefaultDerivedConstructor) {
      // The hierarchy walk will end here; this is the last change to bail out
      // before creating new nodes.
      if (!broker()->dependencies()->DependOnArrayIteratorProtector()) {
        return false;
      }

      compiler::OptionalHeapObjectRef new_target_function =
          TryGetConstant(new_target);
      if (kind == FunctionKind::kDefaultBaseConstructor) {
        // Store the result register first, so that a lazy deopt in
        // `FastNewObject` writes `true` to this register.
        StoreRegister(result.first, GetBooleanConstant(true));

        ValueNode* object;
        if (new_target_function && new_target_function->IsJSFunction() &&
            HasValidInitialMap(new_target_function->AsJSFunction(),
                               current_function)) {
          object = BuildInlinedAllocation(
              CreateJSConstructor(new_target_function->AsJSFunction()),
              AllocationType::kYoung);
          ClearCurrentAllocationBlock();
        } else {
          object = BuildCallBuiltin<Builtin::kFastNewObject>(
              {GetConstant(current_function), GetTaggedValue(new_target)});
          // We've already stored "true" into result.first, so a deopt here just
          // has to store result.second. Also mark result.first as being used,
          // since the lazy deopt frame won't have marked it since it used to be
          // a result register.
          AddDeoptUse(current_interpreter_frame_.get(result.first));
          object->lazy_deopt_info()->UpdateResultLocation(result.second, 1);
        }
        StoreRegister(result.second, object);
      } else {
        StoreRegister(result.first, GetBooleanConstant(false));
        StoreRegister(result.second, GetConstant(current));
      }

      broker()->dependencies()->DependOnStablePrototypeChain(
          function_map, WhereToStart::kStartAtReceiver, current_function);
      return true;
    }

    // Keep walking up the class tree.
    current = current_function.map(broker()).prototype(broker());
  }
}

void MaglevGraphBuilder::VisitFindNonDefaultConstructorOrConstruct() {
  ValueNode* this_function = LoadRegister(0);
  ValueNode* new_target = LoadRegister(1);

  auto register_pair = iterator_.GetRegisterPairOperand(2);

  if (TryBuildFindNonDefaultConstructorOrConstruct(this_function, new_target,
                                                   register_pair)) {
    return;
  }

  CallBuiltin* result =
      BuildCallBuiltin<Builtin::kFindNonDefaultConstructorOrConstruct>(
          {GetTaggedValue(this_function), GetTaggedValue(new_target)});
  StoreRegisterPair(register_pair, result);
}

namespace {
void ForceEscapeIfAllocation(ValueNode* value) {
  if (InlinedAllocation* alloc = value->TryCast<InlinedAllocation>()) {
    alloc->ForceEscaping();
  }
}
}  // namespace

ReduceResult MaglevGraphBuilder::BuildInlined(ValueNode* context,
                                              ValueNode* function,
                                              ValueNode* new_target,
                                              const CallArguments& args) {
  DCHECK(is_inline());

  // Manually create the prologue of the inner function graph, so that we
  // can manually set up the arguments.
  DCHECK_NOT_NULL(current_block_);

  // Set receiver.
  ValueNode* receiver =
      GetConvertReceiver(compilation_unit_->shared_function_info(), args);
  SetArgument(0, receiver);

  // The inlined function could call a builtin that iterates the frame, the
  // receiver needs to have been materialized.
  // TODO(victorgomes): Can we relax this requirement? Maybe we can allocate the
  // object lazily? This is also only required if the inlined function is not a
  // leaf (ie. it calls other functions).
  ForceEscapeIfAllocation(receiver);

  // Set remaining arguments.
  RootConstant* undefined_constant =
      GetRootConstant(RootIndex::kUndefinedValue);
  int arg_count = static_cast<int>(args.count());
  int formal_parameter_count = compilation_unit_->parameter_count() - 1;
  for (int i = 0; i < formal_parameter_count; i++) {
    ValueNode* arg_value = args|i];
    if (arg_value == nullptr) arg_value = undefined_constant;
    SetArgument(i + 1, arg_value);
  }

  // Save all arguments if we have a mismatch between arguments count and
  // parameter count.
  inlined_arguments_ = zone()->AllocateVector<ValueNode*>(arg_count + 1);
  inlined_arguments_|0] = receiver;
  for (int i = 0; i < arg_count; i++) {
    inlined_arguments_|i + 1] = args|i];
  }

  inlined_new_target_ = new_target;

  BuildRegisterFrameInitialization(context, function, new_target);
  BuildMergeStates();
  EndPrologue();
  in_prologue_ = false;

  // Build the inlined function body.
  BuildBody();

  // All returns in the inlined body jump to a merge point one past the bytecode
  // length (i.e. at offset bytecode.length()). If there isn't one already,
  // create a block at this fake offset and have it jump out of the inlined
  // function, into a new block that we create which resumes execution of the
  // outer function.
  if (!current_block_) {
    // If we don't have a merge state at the inline_exit_offset, then there is
    // no control flow that reaches the end of the inlined function, either
    // because of infinite loops or deopts
    if (merge_states_|inline_exit_offset()] == nullptr) {
      return ReduceResult::DoneWithAbort();
    }

    ProcessMergePoint(inline_exit_offset(), /*preserve_kna*/ false);
    StartNewBlock(inline_exit_offset(), /*predecessor*/ nullptr);
  }

  // Pull the returned accumulator value out of the inlined function's final
  // merged return state.
  return current_interpreter_frame_.accumulator();
}

#define TRACE_INLINING(...)                       \
  do {                                            \
    if (v8_flags.trace_maglev_inlining)           \
      StdoutStream{} << __VA_ARGS__ << std::endl; \
  } while (false)

#define TRACE_CANNOT_INLINE(...) \
  TRACE_INLINING("  cannot inline " << shared << ": " << __VA_ARGS__)

bool MaglevGraphBuilder::ShouldInlineCall(
    compiler::SharedFunctionInfoRef shared,
    compiler::OptionalFeedbackVectorRef feedback_vector, float call_frequency) {
  if (graph()->total_inlined_bytecode_size() >
      v8_flags.max_maglev_inlined_bytecode_size_cumulative) {
    compilation_unit_->info()->set_could_not_inline_all_candidates();
    TRACE_CANNOT_INLINE("maximum inlined bytecode size");
    return false;
  }
  if (!feedback_vector) {
    // TODO(verwaest): Soft deopt instead?
    TRACE_CANNOT_INLINE("it has not been compiled/run with feedback yet");
    return false;
  }
  // TODO(olivf): This is a temporary stopgap to prevent infinite recursion when
  // inlining, because we currently excempt small functions from some of the
  // negative heuristics. We should refactor these heuristics and make sure they
  // make sense in the presence of (mutually) recursive inlining. Please do
  // *not* return true before this check.
  if (inlining_depth() > v8_flags.max_maglev_hard_inline_depth) {
    TRACE_CANNOT_INLINE("inlining depth ("
                        << inlining_depth() << ") >= hard-max-depth ("
                        << v8_flags.max_maglev_hard_inline_depth << ")");
    return false;
  }
  if (compilation_unit_->shared_function_info().equals(shared)) {
    TRACE_CANNOT_INLINE("direct recursion");
    return false;
  }
  SharedFunctionInfo::Inlineability inlineability =
      shared.GetInlineability(broker());
  if (inlineability != SharedFunctionInfo::Inlineability::kIsInlineable) {
    TRACE_CANNOT_INLINE(inlineability);
    return false;
  }
  // TODO(victorgomes): Support NewTarget/RegisterInput in inlined functions.
  compiler::BytecodeArrayRef bytecode = shared.GetBytecodeArray(broker());
  if (bytecode.incoming_new_target_or_generator_register().is_valid()) {
    TRACE_CANNOT_INLINE("use unsupported NewTargetOrGenerator register");
    return false;
  }
  if (call_frequency < v8_flags.min_maglev_inlining_frequency) {
    TRACE_CANNOT_INLINE("call frequency ("
                        << call_frequency << ") < minimum threshold ("
                        << v8_flags.min_maglev_inlining_frequency << ")");
    return false;
  }
  if (bytecode.length() < v8_flags.max_maglev_inlined_bytecode_size_small) {
    TRACE_INLINING("  inlining "
                   << shared
                   << ": small function, skipping max-size and max-depth");
    return true;
  }
  if (bytecode.length() > v8_flags.max_maglev_inlined_bytecode_size) {
    TRACE_CANNOT_INLINE("big function, size ("
                        << bytecode.length() << ") >= max-size ("
                        << v8_flags.max_maglev_inlined_bytecode_size << ")");
    return false;
  }
  if (inlining_depth() > v8_flags.max_maglev_inline_depth) {
    TRACE_CANNOT_INLINE("inlining depth ("
                        << inlining_depth() << ") >= max-depth ("
                        << v8_flags.max_maglev_inline_depth << ")");
    return false;
  }
  TRACE_INLINING("  inlining " << shared);
  if (v8_flags.trace_maglev_inlining_verbose) {
    BytecodeArray::Disassemble(bytecode.object(), std::cout);
    i::Print(*feedback_vector->object(), std::cout);
  }
  graph()->add_inlined_bytecode_size(bytecode.length());
  return true;
}

ReduceResult MaglevGraphBuilder::TryBuildInlinedCall(
    ValueNode* context, ValueNode* function, ValueNode* new_target,
    compiler::SharedFunctionInfoRef shared,
    compiler::OptionalFeedbackVectorRef feedback_vector, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  DCHECK_EQ(args.mode(), CallArguments::kDefault);
  float feedback_frequency = 0.0f;
  if (feedback_source.IsValid()) {
    compiler::ProcessedFeedback const& feedback =
        broker()->GetFeedbackForCall(feedback_source);
    feedback_frequency =
        feedback.IsInsufficient() ? 0.0f : feedback.AsCall().frequency();
  }
  float call_frequency = feedback_frequency * call_frequency_;
  if (!ShouldInlineCall(shared, feedback_vector, call_frequency)) {
    return ReduceResult::Fail();
  }

  compiler::BytecodeArrayRef bytecode = shared.GetBytecodeArray(broker());

  if (v8_flags.maglev_print_inlined &&
      TopLevelFunctionPassMaglevPrintFilter() &&
      (v8_flags.print_maglev_code || v8_flags.print_maglev_graph ||
       v8_flags.print_maglev_graphs)) {
    std::cout << "== Inlining " << Brief(*shared.object()) << std::endl;
    BytecodeArray::Disassemble(bytecode.object(), std::cout);
    if (v8_flags.maglev_print_feedback) {
      i::Print(*feedback_vector->object(), std::cout);
    }
  } else if (v8_flags.trace_maglev_graph_building) {
    std::cout << "== Inlining " << shared.object() << std::endl;
  }

  graph()->inlined_functions().push_back(
      OptimizedCompilationInfo::InlinedFunctionHolder(
          shared.object(), bytecode.object(), current_source_position_));
  if (feedback_vector->object()->invocation_count_before_stable(kRelaxedLoad) >
      v8_flags.invocation_count_for_early_optimization) {
    compilation_unit_->info()->set_could_not_inline_all_candidates();
  }
  int inlining_id = static_cast<int>(graph()->inlined_functions().size() - 1);

  // Create a new compilation unit and graph builder for the inlined
  // function.
  MaglevCompilationUnit* inner_unit = MaglevCompilationUnit::NewInner(
      zone(), compilation_unit_, shared, feedback_vector.value());
  MaglevGraphBuilder inner_graph_builder(
      local_isolate_, inner_unit, graph_, call_frequency,
      BytecodeOffset(iterator_.current_offset()), IsInsideLoop(), inlining_id,
      this);

  // Merge catch block state if needed.
  CatchBlockDetails catch_block = GetCurrentTryCatchBlock();
  if (catch_block.ref && catch_block.state->exception_handler_was_used()) {
    // Merge the current state into the handler state.
    catch_block.state->MergeThrow(
        GetCurrentCatchBlockGraphBuilder(), catch_block.unit,
        *current_interpreter_frame_.known_node_aspects(),
        current_interpreter_frame_.virtual_objects());
  }

  // Propagate catch block.
  inner_graph_builder.parent_catch_ = catch_block;
  inner_graph_builder.parent_catch_deopt_frame_distance_ =
      1 + (IsInsideTryBlock() ? 0 : parent_catch_deopt_frame_distance_);

  // Set the inner graph builder to build in the current block.
  inner_graph_builder.current_block_ = current_block_;

  ReduceResult result =
      inner_graph_builder.BuildInlined(context, function, new_target, args);
  if (result.IsDoneWithAbort()) {
    DCHECK_NULL(inner_graph_builder.current_block_);
    current_block_ = nullptr;
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "== Finished inlining (abort) " << shared.object()
                << std::endl;
    }
    return ReduceResult::DoneWithAbort();
  }

  // Propagate KnownNodeAspects back to the caller.
  current_interpreter_frame_.set_known_node_aspects(
      inner_graph_builder.current_interpreter_frame_.known_node_aspects());
  unobserved_context_slot_stores_ =
      inner_graph_builder.unobserved_context_slot_stores_;

  // Propagate virtual object lists back to the caller.
  current_interpreter_frame_.set_virtual_objects(
      inner_graph_builder.current_interpreter_frame_.virtual_objects());

  DCHECK(result.IsDoneWithValue());
  // Resume execution using the final block of the inner builder.
  current_block_ = inner_graph_builder.current_block_;

  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "== Finished inlining " << shared.object() << std::endl;
  }
  return result;
}

namespace {

bool CanInlineArrayIteratingBuiltin(compiler::JSHeapBroker* broker,
                                    const PossibleMaps& maps,
                                    ElementsKind* kind_return) {
  DCHECK_NE(0, maps.size());
  *kind_return = maps.at(0).elements_kind();
  for (compiler::MapRef map : maps) {
    if (!map.supports_fast_array_iteration(broker) ||
        !UnionElementsKindUptoSize(kind_return, map.elements_kind())) {
      return false;
    }
  }
  return true;
}

}  // namespace

ReduceResult MaglevGraphBuilder::TryReduceArrayIsArray(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) return GetBooleanConstant(false);

  ValueNode* node = args|0];

  if (CheckType(node, NodeType::kJSArray)) {
    return GetBooleanConstant(true);
  }

  auto node_info = known_node_aspects().TryGetInfoFor(node);
  if (node_info && node_info->possible_maps_are_known()) {
    bool has_array_map = false;
    bool has_proxy_map = false;
    bool has_other_map = false;
    for (compiler::MapRef map : node_info->possible_maps()) {
      InstanceType type = map.instance_type();
      if (InstanceTypeChecker::IsJSArray(type)) {
        has_array_map = true;
      } else if (InstanceTypeChecker::IsJSProxy(type)) {
        has_proxy_map = true;
      } else {
        has_other_map = true;
      }
    }
    if ((has_array_map ^ has_other_map) && !has_proxy_map) {
      if (has_array_map) node_info->CombineType(NodeType::kJSArray);
      return GetBooleanConstant(has_array_map);
    }
  }

  // TODO(verwaest): Add a node that checks the instance type.
  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::TryReduceArrayForEach(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }

  ValueNode* receiver = args.receiver();
  if (!receiver) return ReduceResult::Fail();

  if (args.count() < 1) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - not enough "
                   "arguments"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  auto node_info = known_node_aspects().TryGetInfoFor(receiver);
  if (!node_info || !node_info->possible_maps_are_known()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - receiver "
                   "map is unknown"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ElementsKind elements_kind;
  if (!CanInlineArrayIteratingBuiltin(broker(), node_info->possible_maps(),
                                      &elements_kind)) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - doesn't "
                   "support fast array iteration or incompatible maps"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  // TODO(leszeks): May only be needed for holey elements kinds.
  if (!broker()->dependencies()->DependOnNoElementsProtector()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - invalidated "
                   "no elements protector"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ValueNode* callback = args|0];
  if (!callback->is_tagged()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.forEach - callback is "
                   "untagged value"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ValueNode* this_arg =
      args.count() > 1 ? args|1] : GetRootConstant(RootIndex::kUndefinedValue);

  ValueNode* original_length = BuildLoadJSArrayLength(receiver);

  // Elide the callable check if the node is known callable.
  EnsureType(callback, NodeType::kCallable, |&](NodeType old_type) {
    // ThrowIfNotCallable is wrapped in a lazy_deopt_scope to make sure the
    // exception has the right call stack.
    DeoptFrameScope lazy_deopt_scope(
        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
        base::VectorOf<ValueNode*>({receiver, callback, this_arg,
                                    GetSmiConstant(0), original_length}));
    AddNewNode<ThrowIfNotCallable>({callback});
  });

  ValueNode* original_length_int32 = GetInt32(original_length);

  // Remember the receiver map set before entering the loop the call.
  bool receiver_maps_were_unstable = node_info->possible_maps_are_unstable();
  PossibleMaps receiver_maps_before_loop(node_info->possible_maps());

  // Create a sub graph builder with two variable (index and length)
  MaglevSubGraphBuilder sub_builder(this, 2);
  MaglevSubGraphBuilder::Variable var_index(0);
  MaglevSubGraphBuilder::Variable var_length(1);

  MaglevSubGraphBuilder::Label loop_end(&sub_builder, 1);

  // ```
  // index = 0
  // bind loop_header
  // ```
  sub_builder.set(var_index, GetSmiConstant(0));
  sub_builder.set(var_length, original_length);
  MaglevSubGraphBuilder::LoopLabel loop_header =
      sub_builder.BeginLoop({&var_index, &var_length});

  // Reset known state that is cleared by BeginLoop, but is known to be true on
  // the first iteration, and will be re-checked at the end of the loop.

  // Reset the known receiver maps if necessary.
  if (receiver_maps_were_unstable) {
    node_info->SetPossibleMaps(receiver_maps_before_loop,
                               receiver_maps_were_unstable,
                               // Node type is monotonic, no need to reset it.
                               NodeType::kUnknown, broker());
    known_node_aspects().any_map_for_any_node_is_unstable = true;
  } else {
    DCHECK_EQ(node_info->possible_maps().size(),
              receiver_maps_before_loop.size());
  }
  // Reset the cached loaded array length to the length var.
  RecordKnownProperty(receiver, broker()->length_string(),
                      sub_builder.get(var_length), false,
                      compiler::AccessMode::kLoad);

  // ```
  // if (index_int32 < length_int32)
  //   fallthrough
  // else
  //   goto end
  // ```
  Phi* index_tagged = sub_builder.get(var_index)->Cast<Phi>();
  EnsureType(index_tagged, NodeType::kSmi);
  ValueNode* index_int32 = GetInt32(index_tagged);

  sub_builder.GotoIfFalse<BranchIfInt32Compare>(
      &loop_end, {index_int32, original_length_int32}, Operation::kLessThan);

  // ```
  // next_index = index + 1
  // ```
  ValueNode* next_index_int32 = nullptr;
  {
    // Eager deopt scope for index increment overflow.
    // TODO(pthier): In practice this increment can never overflow, as the max
    // possible array length is less than int32 max value. Add a new
    // Int32Increment that asserts no overflow instead of deopting.
    DeoptFrameScope eager_deopt_scope(
        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
        base::VectorOf<ValueNode*>(
            {receiver, callback, this_arg, index_int32, original_length}));
    next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32});
    EnsureType(next_index_int32, NodeType::kSmi);
  }
  // TODO(leszeks): Assert Smi.

  // ```
  // element = array.elements|index]
  // ```
  ValueNode* elements = BuildLoadElements(receiver);
  ValueNode* element;
  if (IsDoubleElementsKind(elements_kind)) {
    element = BuildLoadFixedDoubleArrayElement(elements, index_int32);
  } else {
    element = BuildLoadFixedArrayElement(elements, index_int32);
  }

  std::optional<MaglevSubGraphBuilder::Label> skip_call;
  if (IsHoleyElementsKind(elements_kind)) {
    // ```
    // if (element is hole) goto skip_call
    // ```
    skip_call.emplace(
        &sub_builder, 2,
        std::initializer_list<MaglevSubGraphBuilder::Variable*>{&var_length});
    if (elements_kind == HOLEY_DOUBLE_ELEMENTS) {
      sub_builder.GotoIfTrue<BranchIfFloat64IsHole>(&*skip_call, {element});
    } else {
      sub_builder.GotoIfTrue<BranchIfRootConstant>(&*skip_call, {element},
                                                   RootIndex::kTheHoleValue);
    }
  }

  // ```
  // callback(this_arg, element, array)
  // ```
  ReduceResult result;
  {
    DeoptFrameScope lazy_deopt_scope(
        this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,
        base::VectorOf<ValueNode*>(
            {receiver, callback, this_arg, next_index_int32, original_length}));

    CallArguments call_args =
        args.count() < 2
            ? CallArguments(ConvertReceiverMode::kNullOrUndefined,
                            {element, index_tagged, receiver})
            : CallArguments(ConvertReceiverMode::kAny,
                            {this_arg, element, index_tagged, receiver});

    SaveCallSpeculationScope saved(this);
    result = ReduceCall(callback, call_args, saved.value());
  }

  // ```
  // index = next_index
  // jump loop_header
  // ```
  DCHECK_IMPLIES(result.IsDoneWithAbort(), current_block_ == nullptr);

  // No need to finish the loop if this code is unreachable.
  if (!result.IsDoneWithAbort()) {
    // If any of the receiver's maps were unstable maps, we have to re-check the
    // maps on each iteration, in case the callback changed them. That said, we
    // know that the maps are valid on the first iteration, so we can rotate the
    // check to _after_ the callback, and then elide it if the receiver maps are
    // still known to be valid (i.e. the known maps after the call are contained
    // inside the known maps before the call).
    bool recheck_maps_after_call = receiver_maps_were_unstable;
    if (recheck_maps_after_call) {
      // No need to recheck maps if there are known maps...
      if (auto receiver_info_after_call =
              known_node_aspects().TryGetInfoFor(receiver)) {
        // ... and those known maps are equal to, or a subset of, the maps
        // before the call.
        if (receiver_info_after_call &&
            receiver_info_after_call->possible_maps_are_known()) {
          recheck_maps_after_call = !receiver_maps_before_loop.contains(
              receiver_info_after_call->possible_maps());
        }
      }
    }

    // Make sure to finish the loop if we eager deopt in the map check or index
    // check.
    DeoptFrameScope eager_deopt_scope(
        this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,
        base::VectorOf<ValueNode*>(
            {receiver, callback, this_arg, next_index_int32, original_length}));

    if (recheck_maps_after_call) {
      // Build the CheckMap manually, since we're doing it with already known
      // maps rather than feedback, and we don't need to update known node
      // aspects or types since we're at the end of the loop anyway.
      bool emit_check_with_migration = std::any_of(
          receiver_maps_before_loop.begin(), receiver_maps_before_loop.end(),
          |](compiler::MapRef map) { return map.is_migration_target(); });
      if (emit_check_with_migration) {
        AddNewNode<CheckMapsWithMigration>({receiver},
                                           receiver_maps_before_loop,
                                           CheckType::kOmitHeapObjectCheck);
      } else {
        AddNewNode<CheckMaps>({receiver}, receiver_maps_before_loop,
                              CheckType::kOmitHeapObjectCheck);
      }
    }

    // Check if the index is still in bounds, in case the callback changed the
    // length.
    ValueNode* current_length = BuildLoadJSArrayLength(receiver);
    sub_builder.set(var_length, current_length);

    // Reference compare the loaded length against the original length. If this
    // is the same value node, then we didn't have any side effects and didn't
    // clear the cached length.
    if (current_length != original_length) {
      RETURN_IF_ABORT(
          TryBuildCheckInt32Condition(original_length_int32, current_length,
                                      AssertCondition::kUnsignedLessThanEqual,
                                      DeoptimizeReason::kArrayLengthChanged));
    }
  }

  if (skip_call.has_value()) {
    sub_builder.GotoOrTrim(&*skip_call);
    sub_builder.Bind(&*skip_call);
  }

  sub_builder.set(var_index, next_index_int32);
  sub_builder.EndLoop(&loop_header);

  // ```
  // bind end
  // ```
  sub_builder.Bind(&loop_end);

  return GetRootConstant(RootIndex::kUndefinedValue);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayIteratorPrototypeNext(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }

  ValueNode* receiver = args.receiver();
  if (!receiver) return ReduceResult::Fail();

  if (!receiver->Is<InlinedAllocation>()) return ReduceResult::Fail();
  VirtualObject* iterator = receiver->Cast<InlinedAllocation>()->object();
  if (!iterator->map().IsJSArrayIteratorMap()) {
    FAIL("iterator is not a JS array iterator object");
  }

  ValueNode* iterated_object =
      iterator->get(JSArrayIterator::kIteratedObjectOffset);
  ElementsKind elements_kind;
  base::SmallVector<compiler::MapRef, 4> maps;
  if (iterated_object->Is<InlinedAllocation>()) {
    VirtualObject* array = iterated_object->Cast<InlinedAllocation>()->object();
    // TODO(victorgomes): Remove this once we track changes in the inlined
    // allocated object.
    if (iterated_object->Cast<InlinedAllocation>()->IsEscaping()) {
      FAIL("allocation is escaping, map could have been changed");
    }
    // TODO(victorgomes): This effectively disable the optimization for `for-of`
    // loops. We need to figure it out a way to re-enable this.
    if (IsInsideLoop()) {
      FAIL("we're inside a loop, iterated object map could change");
    }
    auto map = array->map();
    if (!map.supports_fast_array_iteration(broker())) {
      FAIL("no fast array iteration support");
    }
    elements_kind = map.elements_kind();
    maps.push_back(map);
  } else {
    auto node_info = known_node_aspects().TryGetInfoFor(iterated_object);
    if (!node_info || !node_info->possible_maps_are_known()) {
      FAIL("iterated object is unknown");
    }
    if (!CanInlineArrayIteratingBuiltin(broker(), node_info->possible_maps(),
                                        &elements_kind)) {
      FAIL("no fast array iteration support or incompatible maps");
    }
    for (auto map : node_info->possible_maps()) {
      maps.push_back(map);
    }
  }

  // TODO(victorgomes): Support typed arrays.
  if (IsTypedArrayElementsKind(elements_kind)) {
    FAIL("no typed arrays support");
  }

  if (IsHoleyElementsKind(elements_kind) &&
      !broker()->dependencies()->DependOnNoElementsProtector()) {
    FAIL("no elements protector");
  }

  // Load the ||NextIndex]] from the {iterator}.
  // We can assume index and length fit in Uint32.
  ValueNode* index =
      BuildLoadTaggedField(receiver, JSArrayIterator::kNextIndexOffset);
  ValueNode* uint32_index;
  GET_VALUE_OR_ABORT(uint32_index, GetUint32ElementIndex(index));
  ValueNode* uint32_length;
  GET_VALUE_OR_ABORT(uint32_length,
                     GetUint32ElementIndex(BuildLoadJSArrayLength(
                         iterated_object, IsFastElementsKind(elements_kind)
                                              ? NodeType::kSmi
                                              : NodeType::kNumber)));

  // Check next index is below length
  MaglevSubGraphBuilder subgraph(this, 2);
  MaglevSubGraphBuilder::Variable is_done(0);
  MaglevSubGraphBuilder::Variable ret_value(1);
  RETURN_IF_ABORT(subgraph.Branch(
      {&is_done, &ret_value},
      |&](auto& builder) {
        return BuildBranchIfUint32Compare(builder, Operation::kLessThan,
                                          uint32_index, uint32_length);
      },
      |&] {
        ValueNode* int32_index = GetInt32(uint32_index);
        subgraph.set(is_done, GetBooleanConstant(false));
        DCHECK(
            iterator->get(JSArrayIterator::kKindOffset)->Is<Int32Constant>());
        IterationKind iteration_kind = static_cast<IterationKind>(
            iterator->get(JSArrayIterator::kKindOffset)
                ->Cast<Int32Constant>()
                ->value());
        if (iteration_kind == IterationKind::kKeys) {
          subgraph.set(ret_value, index);
        } else {
          ValueNode* value;
          GET_VALUE_OR_ABORT(
              value,
              TryBuildElementLoadOnJSArrayOrJSObject(
                  iterated_object, int32_index, base::VectorOf(maps),
                  elements_kind, KeyedAccessLoadMode::kHandleOOBAndHoles));
          if (iteration_kind == IterationKind::kEntries) {
            subgraph.set(ret_value,
                         BuildAndAllocateKeyValueArray(index, value));
          } else {
            subgraph.set(ret_value, value);
          }
        }
        // Add 1 to index
        ValueNode* next_index = AddNewNode<Int32AddWithOverflow>(
            {int32_index, GetInt32Constant(1)});
        EnsureType(next_index, NodeType::kSmi);
        // Update ||NextIndex]]
        BuildStoreTaggedFieldNoWriteBarrier(receiver, next_index,
                                            JSArrayIterator::kNextIndexOffset,
                                            StoreTaggedMode::kDefault);
        return ReduceResult::Done();
      },
      |&] {
        // Index is greater or equal than length.
        subgraph.set(is_done, GetBooleanConstant(true));
        subgraph.set(ret_value, GetRootConstant(RootIndex::kUndefinedValue));
        if (!IsTypedArrayElementsKind(elements_kind)) {
          // Mark the {iterator} as exhausted by setting the ||NextIndex]] to a
          // value that will never pass the length check again (aka the maximum
          // value possible for the specific iterated object). Note that this is
          // different from what the specification says, which is changing the
          // ||IteratedObject]] field to undefined, but that makes it difficult
          // to eliminate the map checks and "length" accesses in for..of loops.
          //
          // This is not necessary for JSTypedArray's, since the length of those
          // cannot change later and so if we were ever out of bounds for them
          // we will stay out-of-bounds forever.
          BuildStoreTaggedField(receiver, GetFloat64Constant(kMaxUInt32),
                                JSArrayIterator::kNextIndexOffset,
                                StoreTaggedMode::kDefault);
        }
        return ReduceResult::Done();
      }));

  // Allocate result object and return.
  compiler::MapRef map =
      broker()->target_native_context().iterator_result_map(broker());
  VirtualObject* iter_result = CreateJSIteratorResult(
      map, subgraph.get(ret_value), subgraph.get(is_done));
  ValueNode* allocation =
      BuildInlinedAllocation(iter_result, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypeEntries(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  if (!CheckType(receiver, NodeType::kJSReceiver)) {
    return ReduceResult::Fail();
  }
  return BuildAndAllocateJSArrayIterator(receiver, IterationKind::kEntries);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypeKeys(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  if (!CheckType(receiver, NodeType::kJSReceiver)) {
    return ReduceResult::Fail();
  }
  return BuildAndAllocateJSArrayIterator(receiver, IterationKind::kKeys);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypeValues(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  if (!CheckType(receiver, NodeType::kJSReceiver)) {
    return ReduceResult::Fail();
  }
  return BuildAndAllocateJSArrayIterator(receiver, IterationKind::kValues);
}

ReduceResult MaglevGraphBuilder::TryReduceStringFromCharCode(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (args.count() != 1) return ReduceResult::Fail();
  return AddNewNode<BuiltinStringFromCharCode>({GetTruncatedInt32ForToNumber(
      args|0], NodeType::kNumberOrOddball,
      TaggedToFloat64ConversionType::kNumberOrOddball)});
}

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeCharCodeAt(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  ValueNode* index;
  if (args.count() == 0) {
    // Index is the undefined object. ToIntegerOrInfinity(undefined) = 0.
    index = GetInt32Constant(0);
  } else {
    index = GetInt32ElementIndex(args|0]);
  }
  // Any other argument is ignored.

  // Try to constant-fold if receiver and index are constant
  if (auto cst = TryGetConstant(receiver)) {
    if (cst->IsString() && index->Is<Int32Constant>()) {
      compiler::StringRef str = cst->AsString();
      int idx = index->Cast<Int32Constant>()->value();
      if (idx >= 0 && static_cast<uint32_t>(idx) < str.length()) {
        if (std::optional<uint16_t> value = str.GetChar(broker(), idx)) {
          return GetSmiConstant(*value);
        }
      }
    }
  }

  // Ensure that {receiver} is actually a String.
  BuildCheckString(receiver);
  // And index is below length.
  ValueNode* length = BuildLoadStringLength(receiver);
  RETURN_IF_ABORT(TryBuildCheckInt32Condition(
      index, length, AssertCondition::kUnsignedLessThan,
      DeoptimizeReason::kOutOfBounds));
  return AddNewNode<BuiltinStringPrototypeCharCodeOrCodePointAt>(
      {receiver, index},
      BuiltinStringPrototypeCharCodeOrCodePointAt::kCharCodeAt);
}

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeCodePointAt(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  ValueNode* index;
  if (args.count() == 0) {
    // Index is the undefined object. ToIntegerOrInfinity(undefined) = 0.
    index = GetInt32Constant(0);
  } else {
    index = GetInt32ElementIndex(args|0]);
  }
  // Any other argument is ignored.
  // Ensure that {receiver} is actually a String.
  BuildCheckString(receiver);
  // And index is below length.
  ValueNode* length = BuildLoadStringLength(receiver);
  RETURN_IF_ABORT(TryBuildCheckInt32Condition(
      index, length, AssertCondition::kUnsignedLessThan,
      DeoptimizeReason::kOutOfBounds));
  return AddNewNode<BuiltinStringPrototypeCharCodeOrCodePointAt>(
      {receiver, index},
      BuiltinStringPrototypeCharCodeOrCodePointAt::kCodePointAt);
}

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeIterator(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  // Ensure that {receiver} is actually a String.
  BuildCheckString(receiver);
  compiler::MapRef map =
      broker()->target_native_context().initial_string_iterator_map(broker());
  VirtualObject* string_iterator = CreateJSStringIterator(map, receiver);
  ValueNode* allocation =
      BuildInlinedAllocation(string_iterator, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

#ifdef V8_INTL_SUPPORT

ReduceResult MaglevGraphBuilder::TryReduceStringPrototypeLocaleCompareIntl(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() < 1 || args.count() > 3) return ReduceResult::Fail();

  LocalFactory* factory = local_isolate()->factory();
  compiler::ObjectRef undefined_ref = broker()->undefined_value();

  DirectHandle<Object> locales_handle;
  ValueNode* locales_node = nullptr;
  if (args.count() > 1) {
    compiler::OptionalHeapObjectRef maybe_locales = TryGetConstant(args|1]);
    if (!maybe_locales) return ReduceResult::Fail();
    compiler::HeapObjectRef locales = maybe_locales.value();
    if (locales.equals(undefined_ref)) {
      locales_handle = factory->undefined_value();
      locales_node = GetRootConstant(RootIndex::kUndefinedValue);
    } else {
      if (!locales.IsString()) return ReduceResult::Fail();
      compiler::StringRef sref = locales.AsString();
      std::optional<Handle<String>> maybe_locales_handle =
          sref.ObjectIfContentAccessible(broker());
      if (!maybe_locales_handle) return ReduceResult::Fail();
      locales_handle = *maybe_locales_handle;
      locales_node = args|1];
    }
  } else {
    locales_handle = factory->undefined_value();
    locales_node = GetRootConstant(RootIndex::kUndefinedValue);
  }

  if (args.count() > 2) {
    compiler::OptionalHeapObjectRef maybe_options = TryGetConstant(args|2]);
    if (!maybe_options) return ReduceResult::Fail();
    if (!maybe_options.value().equals(undefined_ref))
      return ReduceResult::Fail();
  }

  DCHECK(!locales_handle.is_null());
  DCHECK_NOT_NULL(locales_node);

  if (Intl::CompareStringsOptionsFor(local_isolate(), locales_handle,
                                     factory->undefined_value()) !=
      Intl::CompareStringsOptions::kTryFastPath) {
    return ReduceResult::Fail();
  }
  return BuildCallBuiltin<Builtin::kStringFastLocaleCompare>(
      {GetConstant(target),
       GetTaggedValue(GetValueOrUndefined(args.receiver())),
       GetTaggedValue(args|0]), GetTaggedValue(locales_node)});
}

#endif  // V8_INTL_SUPPORT

#ifdef V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA
ReduceResult MaglevGraphBuilder::TryReduceGetContinuationPreservedEmbedderData(
    compiler::JSFunctionRef target, CallArguments& args) {
  return AddNewNode<GetContinuationPreservedEmbedderData>({});
}

ReduceResult MaglevGraphBuilder::TryReduceSetContinuationPreservedEmbedderData(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) return ReduceResult::Fail();

  AddNewNode<SetContinuationPreservedEmbedderData>({args|0]});
  return GetRootConstant(RootIndex::kUndefinedValue);
}
#endif  // V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA

template <typename LoadNode>
ReduceResult MaglevGraphBuilder::TryBuildLoadDataView(const CallArguments& args,
                                                      ExternalArrayType type) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (!broker()->dependencies()->DependOnArrayBufferDetachingProtector()) {
    // TODO(victorgomes): Add checks whether the array has been detached.
    return ReduceResult::Fail();
  }
  // TODO(victorgomes): Add data view to known types.
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  AddNewNode<CheckInstanceType>({receiver}, CheckType::kCheckHeapObject,
                                JS_DATA_VIEW_TYPE, JS_DATA_VIEW_TYPE);
  // TODO(v8:11111): Optimize for JS_RAB_GSAB_DATA_VIEW_TYPE too.
  ValueNode* offset =
      args|0] ? GetInt32ElementIndex(args|0]) : GetInt32Constant(0);
  AddNewNode<CheckJSDataViewBounds>({receiver, offset}, type);
  ValueNode* is_little_endian = args|1] ? args|1] : GetBooleanConstant(false);
  return AddNewNode<LoadNode>({receiver, offset, is_little_endian}, type);
}

template <typename StoreNode, typename Function>
ReduceResult MaglevGraphBuilder::TryBuildStoreDataView(
    const CallArguments& args, ExternalArrayType type, Function&& getValue) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (!broker()->dependencies()->DependOnArrayBufferDetachingProtector()) {
    // TODO(victorgomes): Add checks whether the array has been detached.
    return ReduceResult::Fail();
  }
  // TODO(victorgomes): Add data view to known types.
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  AddNewNode<CheckInstanceType>({receiver}, CheckType::kCheckHeapObject,
                                JS_DATA_VIEW_TYPE, JS_DATA_VIEW_TYPE);
  // TODO(v8:11111): Optimize for JS_RAB_GSAB_DATA_VIEW_TYPE too.
  ValueNode* offset =
      args|0] ? GetInt32ElementIndex(args|0]) : GetInt32Constant(0);
  AddNewNode<CheckJSDataViewBounds>({receiver, offset},
                                    ExternalArrayType::kExternalFloat64Array);
  ValueNode* value = getValue(args|1]);
  ValueNode* is_little_endian = args|2] ? args|2] : GetBooleanConstant(false);
  AddNewNode<StoreNode>({receiver, offset, value, is_little_endian}, type);
  return GetRootConstant(RootIndex::kUndefinedValue);
}

ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetInt8(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt8Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetInt8(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt8Array,
      |&](ValueNode* value) { return value ? value : GetInt32Constant(0); });
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetInt16(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt16Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetInt16(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt16Array,
      |&](ValueNode* value) { return value ? value : GetInt32Constant(0); });
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetInt32(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt32Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetInt32(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreSignedIntDataViewElement>(
      args, ExternalArrayType::kExternalInt32Array,
      |&](ValueNode* value) { return value ? value : GetInt32Constant(0); });
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeGetFloat64(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildLoadDataView<LoadDoubleDataViewElement>(
      args, ExternalArrayType::kExternalFloat64Array);
}
ReduceResult MaglevGraphBuilder::TryReduceDataViewPrototypeSetFloat64(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryBuildStoreDataView<StoreDoubleDataViewElement>(
      args, ExternalArrayType::kExternalFloat64Array, |&](ValueNode* value) {
        return value ? GetHoleyFloat64ForToNumber(
                           value, NodeType::kNumberOrOddball,
                           TaggedToFloat64ConversionType::kNumberOrOddball)
                     : GetFloat64Constant(
                           std::numeric_limits<double>::quiet_NaN());
      });
}

ReduceResult MaglevGraphBuilder::TryReduceFunctionPrototypeCall(
    compiler::JSFunctionRef target, CallArguments& args) {
  // We can't reduce Function#call when there is no receiver function.
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());
  args.PopReceiver(ConvertReceiverMode::kAny);

  SaveCallSpeculationScope saved(this);
  return ReduceCall(receiver, args, saved.value());
}

ReduceResult MaglevGraphBuilder::TryReduceFunctionPrototypeApply(
    compiler::JSFunctionRef target, CallArguments& args) {
  compiler::OptionalHeapObjectRef maybe_receiver;
  if (current_speculation_feedback_.IsValid()) {
    const compiler::ProcessedFeedback& processed_feedback =
        broker()->GetFeedbackForCall(current_speculation_feedback_);
    DCHECK_EQ(processed_feedback.kind(), compiler::ProcessedFeedback::kCall);
    const compiler::CallFeedback& call_feedback = processed_feedback.AsCall();
    if (call_feedback.call_feedback_content() ==
        CallFeedbackContent::kReceiver) {
      maybe_receiver = call_feedback.target();
    }
  }
  return ReduceFunctionPrototypeApplyCallWithReceiver(
      maybe_receiver, args, current_speculation_feedback_);
}

namespace {

template <size_t MaxKindCount, typename KindsToIndexFunc>
bool CanInlineArrayResizingBuiltin(
    compiler::JSHeapBroker* broker, const PossibleMaps& possible_maps,
    std::array<SmallZoneVector<compiler::MapRef, 2>, MaxKindCount>& map_kinds,
    KindsToIndexFunc&& elements_kind_to_index, int* unique_kind_count,
    bool is_loading) {
  uint8_t kind_bitmap = 0;
  for (compiler::MapRef map : possible_maps) {
    if (!map.supports_fast_array_resize(broker)) {
      return false;
    }
    ElementsKind kind = map.elements_kind();
    if (is_loading && kind == HOLEY_DOUBLE_ELEMENTS) {
      return false;
    }
    // Group maps by elements kind, using the provided function to translate
    // elements kinds to indices.
    // kind_bitmap is used to get the unique kinds (predecessor count for the
    // next block).
    uint8_t kind_index = elements_kind_to_index(kind);
    kind_bitmap |= 1 << kind_index;
    map_kinds|kind_index].push_back(map);
  }

  *unique_kind_count = base::bits::CountPopulation(kind_bitmap);
  DCHECK_GE(*unique_kind_count, 1);
  return true;
}

}  // namespace

template <typename MapKindsT, typename IndexToElementsKindFunc,
          typename BuildKindSpecificFunc>
ReduceResult MaglevGraphBuilder::BuildJSArrayBuiltinMapSwitchOnElementsKind(
    ValueNode* receiver, const MapKindsT& map_kinds,
    MaglevSubGraphBuilder& sub_graph,
    std::optional<MaglevSubGraphBuilder::Label>& do_return,
    int unique_kind_count, IndexToElementsKindFunc&& index_to_elements_kind,
    BuildKindSpecificFunc&& build_kind_specific) {
  // TODO(pthier): Support map packing.
  DCHECK(!V8_MAP_PACKING_BOOL);
  ValueNode* receiver_map =
      BuildLoadTaggedField(receiver, HeapObject::kMapOffset);
  int emitted_kind_checks = 0;
  bool any_successful = false;
  for (size_t kind_index = 0; kind_index < map_kinds.size(); kind_index++) {
    const auto& maps = map_kinds|kind_index];
    // Skip kinds we haven't observed.
    if (maps.empty()) continue;
    ElementsKind kind = index_to_elements_kind(kind_index);
    // Create branches for all but the last elements kind. We don't need
    // to check the maps of the last kind, as all possible maps have already
    // been checked when the property (builtin name) was loaded.
    if (++emitted_kind_checks < unique_kind_count) {
      MaglevSubGraphBuilder::Label check_next_map(&sub_graph, 1);
      std::optional<MaglevSubGraphBuilder::Label> do_push;
      if (maps.size() > 1) {
        do_push.emplace(&sub_graph, static_cast<int>(maps.size()));
        for (size_t map_index = 1; map_index < maps.size(); map_index++) {
          sub_graph.GotoIfTrue<BranchIfReferenceEqual>(
              &*do_push, {receiver_map, GetConstant(maps|map_index])});
        }
      }
      sub_graph.GotoIfFalse<BranchIfReferenceEqual>(
          &check_next_map, {receiver_map, GetConstant(maps|0])});
      if (do_push.has_value()) {
        sub_graph.Goto(&*do_push);
        sub_graph.Bind(&*do_push);
      }
      if (!build_kind_specific(kind).IsDoneWithAbort()) {
        any_successful = true;
      }
      DCHECK(do_return.has_value());
      sub_graph.GotoOrTrim(&*do_return);
      sub_graph.Bind(&check_next_map);
    } else {
      if (!build_kind_specific(kind).IsDoneWithAbort()) {
        any_successful = true;
      }
      if (do_return.has_value()) {
        sub_graph.GotoOrTrim(&*do_return);
      }
    }
  }
  DCHECK_IMPLIES(!any_successful, !current_block_);
  return any_successful ? ReduceResult::Done() : ReduceResult::DoneWithAbort();
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypePush(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  // We can't reduce Function#call when there is no receiver function.
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - no receiver"
                << std::endl;
    }
    return ReduceResult::Fail();
  }
  // TODO(pthier): Support multiple arguments.
  if (args.count() != 1) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - invalid "
                   "argument count"
                << std::endl;
    }
    return ReduceResult::Fail();
  }
  ValueNode* receiver = GetValueOrUndefined(args.receiver());

  auto node_info = known_node_aspects().TryGetInfoFor(receiver);
  // If the map set is not found, then we don't know anything about the map of
  // the receiver, so bail.
  if (!node_info || !node_info->possible_maps_are_known()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout
          << "  ! Failed to reduce Array.prototype.push - unknown receiver map"
          << std::endl;
    }
    return ReduceResult::Fail();
  }

  const PossibleMaps& possible_maps = node_info->possible_maps();
  // If the set of possible maps is empty, then there's no possible map for this
  // receiver, therefore this path is unreachable at runtime. We're unlikely to
  // ever hit this case, BuildCheckMaps should already unconditionally deopt,
  // but check it in case another checking operation fails to statically
  // unconditionally deopt.
  if (possible_maps.is_empty()) {
    // TODO(leszeks): Add an unreachable assert here.
    return ReduceResult::DoneWithAbort();
  }

  if (!broker()->dependencies()->DependOnNoElementsProtector()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - "
                   "NoElementsProtector invalidated"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  // Check that inlining resizing array builtins is supported and group maps
  // by elements kind.
  std::array<SmallZoneVector<compiler::MapRef, 2>, 3> map_kinds = {
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone())};
  // Function to group maps by elements kind, ignoring packedness. Packedness
  // doesn't matter for push().
  // Kinds we care about are all paired in the first 6 values of ElementsKind,
  // so we can use integer division to truncate holeyness.
  auto elements_kind_to_index = |&](ElementsKind kind) {
    static_assert(kFastElementsKindCount <= 6);
    static_assert(kFastElementsKindPackedToHoley == 1);
    return static_cast<uint8_t>(kind) / 2;
  };
  auto index_to_elements_kind = |&](uint8_t kind_index) {
    return static_cast<ElementsKind>(kind_index * 2);
  };
  int unique_kind_count;
  if (!CanInlineArrayResizingBuiltin(broker(), possible_maps, map_kinds,
                                     elements_kind_to_index, &unique_kind_count,
                                     false)) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.push - Map doesn't "
                   "support fast resizing"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  MaglevSubGraphBuilder sub_graph(this, 0);
[Step 7: Duration 17.25 seconds| Input tokens: 58,679 | Output tokens: 2,909]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 8 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'DeoptFrameScope', 'options': '-n --hidden    │
│ --no-ignore-vcs'}                                                                                 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'DeoptFrameScope' | head -n 1000
Observations: maglev/maglev-graph-builder.cc:339:class V8_NODISCARD 
MaglevGraphBuilder::DeoptFrameScope {
maglev/maglev-graph-builder.cc:341:  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin 
continuation,
maglev/maglev-graph-builder.cc:355:  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin 
continuation,
maglev/maglev-graph-builder.cc:382:  DeoptFrameScope(MaglevGraphBuilder* builder, ValueNode* 
receiver)
maglev/maglev-graph-builder.cc:396:  ~DeoptFrameScope() {
maglev/maglev-graph-builder.cc:403:  DeoptFrameScope* parent() const { return parent_; }
maglev/maglev-graph-builder.cc:427:  DeoptFrameScope* parent_;
maglev/maglev-graph-builder.cc:1289:    // LazyDeoptFrameScope.
maglev/maglev-graph-builder.cc:1332:    const DeoptFrameScope* deopt_scope = current_deopt_scope_;
maglev/maglev-graph-builder.cc:1378:    DeoptFrameScope* scope, bool mark_accumulator_dead) {
maglev/maglev-graph-builder.cc:8129:    DeoptFrameScope lazy_deopt_scope(
maglev/maglev-graph-builder.cc:8199:    DeoptFrameScope eager_deopt_scope(
maglev/maglev-graph-builder.cc:8240:    DeoptFrameScope lazy_deopt_scope(
maglev/maglev-graph-builder.cc:8287:    DeoptFrameScope eager_deopt_scope(
maglev/maglev-graph-builder.cc:9521:  DeoptFrameScope continuation_scope(this, 
Float64Round::continuation(kind));
maglev/maglev-graph-builder.cc:10995:      DeoptFrameScope construct(this, implicit_receiver);
maglev/maglev-graph-builder.cc:11036:    DeoptFrameScope construct(this, implicit_receiver);
maglev/maglev-graph-builder.cc:11478:      DeoptFrameScope continuation_scope(
maglev/maglev-graph-builder.cc:14023:    DeoptFrameScope deopt_continuation(
maglev/maglev-graph-builder.cc:14045:    DeoptFrameScope deopt_continuation(
maglev/maglev-graph-builder.h:181:  class DeoptFrameScope;
maglev/maglev-graph-builder.h:350:  const DeoptFrameScope* current_deopt_scope() const {
maglev/maglev-graph-builder.h:1690:      DeoptFrameScope* scope, bool mark_accumulator_dead);
maglev/maglev-graph-builder.h:3041:  DeoptFrameScope* current_deopt_scope_ = nullptr;
[Step 8: Duration 32.79 seconds| Input tokens: 103,452 | Output tokens: 4,292]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 9 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'CreateCatchContext|SetPendingMessage|PendingMessage', 'options': '-n --hidden --no-ignore-vcs    │
│ maglev/ runtime/ interpreter/'}                                                                   │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'exception                                    │
│ handler|SetPendingMessage|CreateCatchContext|catch context|throw|try-catch|CreateCatch',          │
│ 'options': '-n --hidden --no-ignore-vcs maglev/'}                                                 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ runtime/ interpreter/ 'CreateCatchContext|SetPendingMessage|PendingMessage' | head -n 1000
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'ArrayForEachLoopEagerDeoptContinuation|ArrayForEachLoopLazyDeoptContinuation|forEach|Array.proto │
│ type.forEach', 'options': '-n --hidden --no-ignore-vcs builtins/'}                                │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'UnsafeSmiUntag|UnsafeSmiToInt32|UnsafeTaggedToInt32|UnsafeInt32ToUint32|GetInt32|GetUint32Elemen │
│ 'options': '-n --hidden --no-ignore-vcs maglev/'}                                                 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'exception handler|SetPendingMessage|CreateCatchContext|catch context|throw|try-catch|CreateCatch' | head -n 1000
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'NoElementsProtector|DependOnNoElementsProtector|supports_fast_array_iteration|supports_fast_arra │
│ y_resize|Holey|HOLEY|PACKED_DOUBLE|FixedDoubleArray|FixedArrayElement', 'options': '-n --hidden   │
│ tIndex|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged',        │
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs builtins/ 'ArrayForEachLoopEagerDeoptContinuation|ArrayForEachLoopLazyDeoptContinuation|forEach|Array.prototype.forEach' | head -n 1000
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
│ --no-ignore-vcs maglev/ compiler/'}                                                               │
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'MaglevSubGraphBuilder::BeginLoop|LoopLabel|GotoIfFalse<BranchIfInt32Compare>|GotoIfTrue<BranchIf │
│ maglev/maglev-graph-builder.cc'}                                                                  │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'UnsafeSmiUntag|UnsafeSmiToInt32|UnsafeTaggedToInt32|UnsafeInt32ToUint32|GetInt32|GetUint32ElementIndex|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged' | head -n 1000
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
Observations: runtime/runtime-test.cc:37:#include "src/maglev/maglev-concurrent-dispatcher.h"
│ Float64IsHole>|BuildLoadFixedDoubleArrayElement', 'options': '-n --hidden --no-ignore-vcs         │

runtime/runtime-test.cc:49:#include "src/maglev/maglev.h"
Observations: rg: exception handler|SetPendingMessage|CreateCatchContext|catch 
context|throw|try-catch|CreateCatch: IO error for operation on exception 
handler|SetPendingMessage|CreateCatchContext|catch context|throw|try-catch|CreateCatch: No such file 
or directory (os error 2)
Observations: rg: 
ArrayForEachLoopEagerDeoptContinuation|ArrayForEachLoopLazyDeoptContinuation|forEach|Array.prototype.
forEach: IO error for operation on 
ArrayForEachLoopEagerDeoptContinuation|ArrayForEachLoopLazyDeoptContinuation|forEach|Array.prototype.
forEach: No such file or directory (os error 2)
Observations: rg: 
UnsafeSmiUntag|UnsafeSmiToInt32|UnsafeTaggedToInt32|UnsafeInt32ToUint32|GetInt32|GetUint32ElementInde
x|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged: IO error for 
operation on 
UnsafeSmiUntag|UnsafeSmiToInt32|UnsafeTaggedToInt32|UnsafeInt32ToUint32|GetInt32|GetUint32ElementInde
x|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged: No such file or 
directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ compiler/ 'NoElementsProtector|DependOnNoElementsProtector|supports_fast_array_iteration|supports_fast_array_resize|Holey|HOLEY|PACKED_DOUBLE|FixedDoubleArray|FixedArrayElement' | head -n 1000
Observations: compiler/turboshaft/maglev-graph-building-phase.cc:42:#include 
"src/maglev/maglev-basic-block.h"
compiler/turboshaft/maglev-graph-building-phase.cc:43:#include "src/maglev/maglev-compilation-info.h"
compiler/turboshaft/maglev-graph-building-phase.cc:44:#include "src/maglev/maglev-compilation-unit.h"
compiler/turboshaft/maglev-graph-building-phase.cc:45:#include "src/maglev/maglev-graph-builder.h"
compiler/turboshaft/maglev-graph-building-phase.cc:46:#include "src/maglev/maglev-graph-labeller.h"
compiler/turboshaft/maglev-graph-building-phase.cc:47:#include "src/maglev/maglev-graph-processor.h"
compiler/turboshaft/maglev-graph-building-phase.cc:48:#include "src/maglev/maglev-graph-verifier.h"
compiler/turboshaft/maglev-graph-building-phase.cc:49:#include "src/maglev/maglev-ir-inl.h"
compiler/turboshaft/maglev-graph-building-phase.cc:50:#include "src/maglev/maglev-ir.h"
compiler/turboshaft/maglev-graph-building-phase.cc:51:#include 
"src/maglev/maglev-phi-representation-selector.h"
compiler/turboshaft/maglev-graph-building-phase.cc:52:#include 
"src/maglev/maglev-post-hoc-optimizations-processors.h"
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-graph-builder.cc 'MaglevSubGraphBuilder::BeginLoop|LoopLabel|GotoIfFalse<BranchIfInt32Compare>|GotoIfTrue<BranchIfFloat64IsHole>|BuildLoadFixedDoubleArrayElement' | head -n 1000
Observations: rg: 
MaglevSubGraphBuilder::BeginLoop|LoopLabel|GotoIfFalse<BranchIfInt32Compare>|GotoIfTrue<BranchIfFloat
64IsHole>|BuildLoadFixedDoubleArrayElement: IO error for operation on 
MaglevSubGraphBuilder::BeginLoop|LoopLabel|GotoIfFalse<BranchIfInt32Compare>|GotoIfTrue<BranchIfFloat
64IsHole>|BuildLoadFixedDoubleArrayElement: No such file or directory (os error 2)
[Step 9: Duration 22.86 seconds| Input tokens: 148,868 | Output tokens: 5,079]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 10 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'CreateCatchContext|SetPendingMessage|PendingMessage|CreateCatch', 'options': '-n --hidden        │
│ --no-ignore-vcs'}                                                                                 │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'VisitTry|try                                 │
│ catch|TryCatch|VisitThrow|BuildTry|catch', 'options': '-n --hidden --no-ignore-vcs                │
│ maglev/maglev-graph-builder.cc'}                                                                  │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'BranchIfInt32Compare|CheckInt32Condition|BranchIfFloat64IsHole|CheckMapsWithMigration|Int32Incre │
│ mentWithOverflow|UnsafeInt32ToUint32', 'options': '-n --hidden --no-ignore-vcs maglev/maglev-ir.h │
│ maglev/maglev-ir.cc maglev/maglev-graph-builder.cc'}                                              │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-graph-builder.cc 'VisitTry|try catch|TryCatch|VisitThrow|BuildTry|catch' | head -n 1000
Observations: rg: VisitTry|try catch|TryCatch|VisitThrow|BuildTry|catch: IO error for operation on 
VisitTry|try catch|TryCatch|VisitThrow|BuildTry|catch: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-ir.h maglev/maglev-ir.cc maglev/maglev-graph-builder.cc 'BranchIfInt32Compare|CheckInt32Condition|BranchIfFloat64IsHole|CheckMapsWithMigration|Int32IncrementWithOverflow|UnsafeInt32ToUint32' | head -n 1000
Observations: maglev/maglev-ir.cc:5:#include "src/maglev/maglev-ir.h"
maglev/maglev-graph-builder.cc:45:#include "src/maglev/maglev-ir.h"
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'CreateCatchContext|SetPendingMessage|PendingMessage|CreateCatch' | head -n 1000
Observations: debug/debug-evaluate.cc:478:    case Bytecode::kCreateCatchContext:
debug/debug-evaluate.cc:529:    case Bytecode::kSetPendingMessage:
interpreter/bytecode-generator.cc:238:      
builder()->LoadTheHole().SetPendingMessage().StoreAccumulatorInRegister(
interpreter/bytecode-generator.cc:270:          .SetPendingMessage();
interpreter/bytecode-generator.cc:1858:          builder()->LoadTheHole().SetPendingMessage();
interpreter/bytecode-generator.cc:1919:                  .SetPendingMessage()
interpreter/bytecode-generator.cc:2867:                    .SetPendingMessage()
interpreter/bytecode-generator.cc:3186:          builder()->LoadTheHole().SetPendingMessage();
interpreter/bytecode-generator.cc:8206:  builder()->CreateCatchContext(exception, scope);
interpreter/bytecodes.cc:212:    case Bytecode::kCreateCatchContext:
interpreter/bytecode-array-builder.h:255:  BytecodeArrayBuilder& CreateCatchContext(Register 
exception,
interpreter/bytecode-array-builder.h:468:  BytecodeArrayBuilder& SetPendingMessage();
interpreter/interpreter-generator.cc:2777:// CreateCatchContext <exception> <scope_info_idx>
interpreter/interpreter-generator.cc:2781:IGNITION_HANDLER(CreateCatchContext, InterpreterAssembler) 
{
interpreter/interpreter-generator.cc:2895:// SetPendingMessage
interpreter/interpreter-generator.cc:2899:IGNITION_HANDLER(SetPendingMessage, InterpreterAssembler) {
interpreter/interpreter-generator.cc:2900:  TNode<HeapObject> previous_message = GetPendingMessage();
interpreter/interpreter-generator.cc:2901:  SetPendingMessage(CAST(GetAccumulator()));
compiler/js-call-reducer.cc:2469:    ClearPendingMessage();
compiler/js-create-lowering.h:69:  Reduction ReduceJSCreateCatchContext(Node* node);
interpreter/bytecodes.h:352:  V(CreateCatchContext, ImplicitRegisterUse::kWriteAccumulator,          
\
interpreter/bytecodes.h:434:  V(SetPendingMessage, ImplicitRegisterUse::kReadWriteAccumulator)       
\
maglev/maglev-ir.h:269:  V(SetPendingMessage)                              \
maglev/maglev-ir.h:4243:class SetPendingMessage : public FixedInputValueNodeT<1, SetPendingMessage> {
maglev/maglev-ir.h:4244:  using Base = FixedInputValueNodeT<1, SetPendingMessage>;
maglev/maglev-ir.h:4247:  explicit SetPendingMessage(uint64_t bitfield) : Base(bitfield) {}
interpreter/bytecode-array-builder.cc:1034:BytecodeArrayBuilder& 
BytecodeArrayBuilder::CreateCatchContext(
interpreter/bytecode-array-builder.cc:1037:  OutputCreateCatchContext(exception, scope_index);
interpreter/bytecode-array-builder.cc:1384:BytecodeArrayBuilder& 
BytecodeArrayBuilder::SetPendingMessage() {
interpreter/bytecode-array-builder.cc:1385:  OutputSetPendingMessage();
codegen/compiler.cc:4000:      isolate->ReportPendingMessages();
codegen/compiler.cc:4163:    if (maybe_result.is_null()) isolate->ReportPendingMessages();
maglev/maglev-ir.cc:6466:void SetPendingMessage::SetValueLocationConstraints() {
maglev/maglev-ir.cc:6471:void SetPendingMessage::GenerateCode(MaglevAssembler* masm,
execution/execution.cc:291:    isolate->ReportPendingMessages(params.message_handling ==
execution/execution.cc:318:        isolate->ReportPendingMessages(params.message_handling ==
execution/execution.cc:343:        isolate->ReportPendingMessages(params.message_handling ==
execution/execution.cc:362:    isolate->ReportPendingMessages(params.message_handling ==
execution/execution.cc:469:    isolate->ReportPendingMessages(params.message_handling ==
maglev/maglev-graph-builder.cc:4278:    case Opcode::kSetPendingMessage:
maglev/maglev-graph-builder.cc:12733:void MaglevGraphBuilder::VisitCreateCatchContext() {
maglev/maglev-graph-builder.cc:12734:  // CreateCatchContext <exception> <scope_info_idx>
maglev/maglev-graph-builder.cc:13761:void MaglevGraphBuilder::VisitSetPendingMessage() {
maglev/maglev-graph-builder.cc:13763:  SetAccumulator(AddNewNode<SetPendingMessage>({message}));
compiler/js-create-lowering.cc:98:    case IrOpcode::kJSCreateCatchContext:
compiler/js-create-lowering.cc:99:      return ReduceJSCreateCatchContext(node);
compiler/js-create-lowering.cc:1313:Reduction JSCreateLowering::ReduceJSCreateCatchContext(Node* 
node) {
compiler/js-create-lowering.cc:1314:  DCHECK_EQ(IrOpcode::kJSCreateCatchContext, node->opcode());
codegen/code-stub-assembler.cc:7291:TNode<HeapObject> CodeStubAssembler::GetPendingMessage() {
codegen/code-stub-assembler.cc:7296:void CodeStubAssembler::SetPendingMessage(TNode<HeapObject> 
message) {
codegen/code-stub-assembler.cc:7306:  TNode<HeapObject> pending_message = GetPendingMessage();
api/api-natives.cc:30:      isolate_->ReportPendingMessages();
compiler/js-generic-lowering.cc:857:void JSGenericLowering::LowerJSCreateCatchContext(Node* node) {
execution/isolate.h:1073:  void ReportPendingMessages(bool report = true);
api/api.cc:2857:  if (has_exception) i_isolate->ReportPendingMessages();
api/api.cc:2877:  if (has_exception) i_isolate->ReportPendingMessages();
compiler/verifier.cc:862:    case IrOpcode::kJSCreateCatchContext:
torque/implementation-visitor.cc:3511:                                 "GetAndResetPendingMessage"),
torque/implementation-visitor.cc:3517:                                 "GetAndResetPendingMessage"),
compiler/operator-properties.cc:72:    case IrOpcode::kJSCreateCatchContext:
compiler/js-operator.h:1114:  const Operator* CreateCatchContext(ScopeInfoRef scope_info);
compiler/js-operator.cc:1497:const Operator* JSOperatorBuilder::CreateCatchContext(ScopeInfoRef 
scope_info) {
compiler/js-operator.cc:1499:      IrOpcode::kJSCreateCatchContext, Operator::kNoProperties,  // 
opcode
compiler/js-operator.cc:1500:      "JSCreateCatchContext",                                    // name
compiler/js-operator.cc:1524:         IrOpcode::kJSCreateCatchContext == op->opcode());
d8/d8.cc:1693:  // isolate->ReportPendingMessages().
compiler/bytecode-graph-builder.cc:1839:    case IrOpcode::kJSCreateCatchContext:
compiler/bytecode-graph-builder.cc:2349:void BytecodeGraphBuilder::VisitCreateCatchContext() {
compiler/bytecode-graph-builder.cc:2354:  const Operator* op = 
javascript()->CreateCatchContext(scope_info);
compiler/bytecode-graph-builder.cc:3748:void BytecodeGraphBuilder::VisitSetPendingMessage() {
compiler/graph-assembler.cc:337:Node* JSGraphAssembler::ClearPendingMessage() {
execution/isolate.cc:3044:void Isolate::ReportPendingMessages(bool report) {
codegen/code-stub-assembler.h:2676:  TNode<HeapObject> GetPendingMessage();
codegen/code-stub-assembler.h:2677:  void SetPendingMessage(TNode<HeapObject> message);
builtins/promise-abstract-operations.tq:613:  torque_internal::SetPendingMessage(TheHole);
compiler/turboshaft/maglev-graph-building-phase.cc:4318:  maglev::ProcessResult 
Process(maglev::SetPendingMessage* node,
compiler/opcodes.h:208:  V(JSCreateCatchContext)     \
compiler/turbofan-typer.cc:1794:Type Typer::Visitor::TypeJSCreateCatchContext(Node* node) {
compiler/graph-assembler.h:1006:  Node* ClearPendingMessage();
baseline/baseline-compiler.cc:1962:void BaselineCompiler::VisitCreateCatchContext() {
baseline/baseline-compiler.cc:2252:void BaselineCompiler::VisitSetPendingMessage() {
builtins/builtins-iterator-gen.cc:191:    TNode<HeapObject> message = GetPendingMessage();
builtins/builtins-iterator-gen.cc:192:    SetPendingMessage(TheHoleConstant());
builtins/builtins-collections-gen.cc:181:    TNode<HeapObject> message = GetPendingMessage();
builtins/builtins-collections-gen.cc:182:    SetPendingMessage(TheHoleConstant());
builtins/torque-internal.tq:382:extern macro GetPendingMessage(): TheHole|JSMessageObject;
builtins/torque-internal.tq:383:extern macro SetPendingMessage(TheHole|JSMessageObject): void;
builtins/torque-internal.tq:386:macro GetAndResetPendingMessage(): TheHole|JSMessageObject {
builtins/torque-internal.tq:387:  const message = GetPendingMessage();
builtins/torque-internal.tq:388:  SetPendingMessage(TheHole);
[Step 10: Duration 12.87 seconds| Input tokens: 195,454 | Output tokens: 5,790]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 11 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'class                                        │
│ Int32IncrementWithOverflow|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLe │
│ ngthChanged|AssertCondition', 'options': '-n --hidden --no-ignore-vcs maglev/maglev-ir.h          │
│ maglev/maglev-graph-builder.cc maglev/maglev-ir.cc'}                                              │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-ir.h maglev/maglev-graph-builder.cc maglev/maglev-ir.cc 'class Int32IncrementWithOverflow|BranchIfInt32Compare|CheckInt32Condition|UnsignedLessThanEqual|ArrayLengthChanged|AssertCondition' | head -n 1000
Observations: maglev/maglev-ir.cc:5:#include "src/maglev/maglev-ir.h"
maglev/maglev-graph-builder.cc:45:#include "src/maglev/maglev-ir.h"
[Step 11: Duration 7.28 seconds| Input tokens: 244,446 | Output tokens: 6,069]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 12 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'class                                        │
│ ThrowIfNotCallable|ThrowIfNotCallable::GenerateCode|MaxCallStackArgs', 'options': '-n --hidden    │
│ --no-ignore-vcs maglev/maglev-ir.h maglev/maglev-ir.cc'}                                          │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'class BranchIfInt32Compare|class             │
│ CheckInt32Condition|class Int32IncrementWithOverflow|class BranchIfFloat64IsHole|class            │
│ CheckMapsWithMigration|class CheckMaps', 'options': '-n --hidden --no-ignore-vcs                  │
│ maglev/maglev-ir.h'}                                                                              │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'BuildLoadJSArrayLength\\(|BuildLoadElements\\(|TryReduceArrayForEach\\(', 'options': '-n         │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-ir.h maglev/maglev-ir.cc 'class ThrowIfNotCallable|ThrowIfNotCallable::GenerateCode|MaxCallStackArgs' | head -n 1000
│ --hidden --no-ignore-vcs maglev/maglev-graph-builder.cc'}                                         │

Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-ir.h 'class BranchIfInt32Compare|class CheckInt32Condition|class Int32IncrementWithOverflow|class BranchIfFloat64IsHole|class CheckMapsWithMigration|class CheckMaps' | head -n 1000
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc',         │
│ 'section': 1}                                                                                     │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'builtins/array-foreach.tq', 'section':   │
│ 1}                                                                                                │
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/maglev-graph-builder.cc 'BuildLoadJSArrayLength\(|BuildLoadElements\(|TryReduceArrayForEach\(' | head -n 1000
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯

Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Observations: maglev/maglev-ir.cc:5:#include "src/maglev/maglev-ir.h"
Observations: rg: class BranchIfInt32Compare|class CheckInt32Condition|class 
IO error for operation on class BranchIfInt32Compare|class CheckInt32Condition|class 
Int32IncrementWithOverflow|class BranchIfFloat64IsHole|class CheckMapsWithMigration|class CheckMaps: 
No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/builtins/array-foreach.tq'
Int32IncrementWithOverflow|class BranchIfFloat64IsHole|class CheckMapsWithMigration|class CheckMaps: 
operation on BuildLoadJSArrayLength\(|BuildLoadElements\(|TryReduceArrayForEach\(: No such file or 
directory (os error 2)

Observations: rg: BuildLoadJSArrayLength\(|BuildLoadElements\(|TryReduceArrayForEach\(: IO error for 
Command: cd /Users/tarun/v8/v8/src && sed -n '1,3000p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Command: cd /Users/tarun/v8/v8/src && cat '/Users/tarun/v8/v8/src/builtins/array-foreach.tq'
Observations: // Copyright 2018 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

namespace array {
transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(
    js-implicit context: NativeContext, receiver: JSAny)(callback: JSAny,
    thisArg: JSAny, initialK: JSAny, length: JSAny): JSAny {
  // All continuation points in the optimized forEach implementation are
  // after the ToObject(O) call that ensures we are dealing with a
  // JSReceiver.
  const jsreceiver = Cast<JSReceiver>(receiver) otherwise unreachable;
  const callbackfn = Cast<Callable>(callback) otherwise unreachable;
  const numberK = Cast<Number>(initialK) otherwise unreachable;
  const numberLength = Cast<Number>(length) otherwise unreachable;

  return ArrayForEachLoopContinuation(
      jsreceiver, callbackfn, thisArg, Undefined, jsreceiver, numberK,
      numberLength, Undefined);
}

transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(
    js-implicit context: NativeContext, receiver: JSAny)(callback: JSAny,
    thisArg: JSAny, initialK: JSAny, length: JSAny, _result: JSAny): JSAny {
  // All continuation points in the optimized forEach implementation are
  // after the ToObject(O) call that ensures we are dealing with a
  // JSReceiver.
  const jsreceiver = Cast<JSReceiver>(receiver) otherwise unreachable;
  const callbackfn = Cast<Callable>(callback) otherwise unreachable;
  const numberK = Cast<Number>(initialK) otherwise unreachable;
  const numberLength = Cast<Number>(length) otherwise unreachable;

  return ArrayForEachLoopContinuation(
      jsreceiver, callbackfn, thisArg, Undefined, jsreceiver, numberK,
      numberLength, Undefined);
}

transitioning builtin ArrayForEachLoopContinuation(
    implicit context: Context)(_receiver: JSReceiver, callbackfn: Callable,
    thisArg: JSAny, _array: JSAny, o: JSReceiver, initialK: Number, len: Number,
    _to: JSAny): JSAny {
  // variables {array} and {to} are ignored.

  // 5. Let k be 0.
  // 6. Repeat, while k < len
  for (let k: Number = initialK; k < len; k = k + 1) {
    // 6a. Let Pk be ! ToString(k).
    // k is guaranteed to be a positive integer, hence ToString is
    // side-effect free and HasProperty/GetProperty do the conversion inline.

    // 6b. Let kPresent be ? HasProperty(O, Pk).
    const kPresent: Boolean = HasProperty_Inline(o, k);

    // 6c. If kPresent is true, then
    if (kPresent == True) {
      // 6c. i. Let kValue be ? Get(O, Pk).
      const kValue: JSAny = GetProperty(o, k);

      // 6c. ii. Perform ? Call(callbackfn, T, <kValue, k, O>).
      Call(context, callbackfn, thisArg, kValue, k, o);
    }

    // 6d. Increase k by 1. (done by the loop).
  }
  return Undefined;
}

transitioning macro FastArrayForEach(
    implicit context: Context)(o: JSReceiver, len: Number,
    callbackfn: Callable, thisArg: JSAny): JSAny
    labels Bailout(Smi) {
  let k: Smi = 0;
  const smiLen = Cast<Smi>(len) otherwise goto Bailout(k);
  const fastO = Cast<FastJSArray>(o) otherwise goto Bailout(k);
  let fastOW = NewFastJSArrayWitness(fastO);

  // Build a fast loop over the smi array.
  for (; k < smiLen; k++) {
    fastOW.Recheck() otherwise goto Bailout(k);

    // Ensure that we haven't walked beyond a possibly updated length.
    if (k >= fastOW.Get().length) goto Bailout(k);
    const value: JSAny = fastOW.LoadElementNoHole(k)
        otherwise continue;
    Call(context, callbackfn, thisArg, value, k, fastOW.Get());
  }
  return Undefined;
}

// https://tc39.github.io/ecma262/#sec-array.prototype.foreach
transitioning javascript builtin ArrayForEach(
    js-implicit context: NativeContext, receiver: JSAny)(...arguments): JSAny {
  try {
    RequireObjectCoercible(receiver, 'Array.prototype.forEach');

    // 1. Let O be ? ToObject(this value).
    const o: JSReceiver = ToObject_Inline(context, receiver);

    // 2. Let len be ? ToLength(? Get(O, "length")).
    const len: Number = GetLengthProperty(o);

    // 3. If IsCallable(callbackfn) is false, throw a TypeError exception.
    if (arguments.length == 0) {
      goto TypeError;
    }
    const callbackfn = Cast<Callable>(arguments|0]) otherwise TypeError;

    // 4. If thisArg is present, let T be thisArg; else let T be undefined.
    const thisArg: JSAny = arguments|1];

    // Special cases.
    let k: Number = 0;
    try {
      return FastArrayForEach(o, len, callbackfn, thisArg)
          otherwise Bailout;
    } label Bailout(kValue: Smi) deferred {
      k = kValue;
    }

    return ArrayForEachLoopContinuation(
        o, callbackfn, thisArg, Undefined, o, k, len, Undefined);
  } label TypeError deferred {
    ThrowCalledNonCallable(arguments|0]);
  }
}
}
Observations: Showing section 1/5 (lines 1-3000) of 'maglev/maglev-graph-builder.cc':
// Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/maglev/maglev-graph-builder.h"

#include <algorithm>
#include <limits>
#include <optional>
#include <utility>

#include "src/base/bounds.h"
#include "src/base/ieee754.h"
#include "src/base/logging.h"
#include "src/base/vector.h"
#include "src/builtins/builtins-constructor.h"
#include "src/builtins/builtins.h"
#include "src/codegen/cpu-features.h"
#include "src/codegen/interface-descriptors-inl.h"
#include "src/common/assert-scope.h"
#include "src/common/globals.h"
#include "src/common/message-template.h"
#include "src/compiler/access-info.h"
#include "src/compiler/bytecode-liveness-map.h"
#include "src/compiler/compilation-dependencies.h"
#include "src/compiler/feedback-source.h"
#include "src/compiler/heap-refs.h"
#include "src/compiler/js-heap-broker-inl.h"
#include "src/compiler/js-heap-broker.h"
#include "src/compiler/processed-feedback.h"
#include "src/deoptimizer/deoptimize-reason.h"
#include "src/execution/protectors.h"
#include "src/flags/flags.h"
#include "src/handles/maybe-handles-inl.h"
#include "src/ic/handler-configuration-inl.h"
#include "src/interpreter/bytecode-array-iterator.h"
#include "src/interpreter/bytecode-flags-and-tokens.h"
#include "src/interpreter/bytecode-register.h"
#include "src/interpreter/bytecodes.h"
#include "src/maglev/maglev-compilation-info.h"
#include "src/maglev/maglev-compilation-unit.h"
#include "src/maglev/maglev-graph-printer.h"
#include "src/maglev/maglev-interpreter-frame-state.h"
#include "src/maglev/maglev-ir-inl.h"
#include "src/maglev/maglev-ir.h"
#include "src/numbers/conversions.h"
#include "src/numbers/ieee754.h"
#include "src/objects/arguments.h"
#include "src/objects/elements-kind.h"
#include "src/objects/feedback-vector.h"
#include "src/objects/fixed-array.h"
#include "src/objects/heap-number-inl.h"
#include "src/objects/js-array.h"
#include "src/objects/js-function.h"
#include "src/objects/js-objects.h"
#include "src/objects/literal-objects-inl.h"
#include "src/objects/name-inl.h"
#include "src/objects/object-list-macros.h"
#include "src/objects/property-cell.h"
#include "src/objects/property-details.h"
#include "src/objects/shared-function-info.h"
#include "src/objects/slots-inl.h"
#include "src/objects/type-hints.h"
#include "src/roots/roots.h"
#include "src/utils/utils.h"
#include "src/zone/zone.h"

#ifdef V8_INTL_SUPPORT
#include "src/objects/intl-objects.h"
#endif

#define TRACE(...)                            \
  if (v8_flags.trace_maglev_graph_building) { \
    std::cout << __VA_ARGS__ << std::endl;    \
  }

#define FAIL(...)                                                         \
  TRACE("Failed " << __func__ << ":" << __LINE__ << ": " << __VA_ARGS__); \
  return ReduceResult::Fail();

namespace v8::internal::maglev {

namespace {

enum class CpuOperation {
  kFloat64Round,
};

// TODO(leszeks): Add a generic mechanism for marking nodes as optionally
// supported.
bool IsSupported(CpuOperation op) {
  switch (op) {
    case CpuOperation::kFloat64Round:
#if defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_IA32)
      return CpuFeatures::IsSupported(SSE4_1) || CpuFeatures::IsSupported(AVX);
#elif defined(V8_TARGET_ARCH_ARM)
      return CpuFeatures::IsSupported(ARMv8);
#elif defined(V8_TARGET_ARCH_ARM64) || defined(V8_TARGET_ARCH_PPC64) ||   \
    defined(V8_TARGET_ARCH_S390X) || defined(V8_TARGET_ARCH_RISCV64) ||   \
    defined(V8_TARGET_ARCH_RISCV32) || defined(V8_TARGET_ARCH_LOONG64) || \
    defined(V8_TARGET_ARCH_MIPS64)
      return true;
#else
#error "V8 does not support this architecture."
#endif
  }
}

class FunctionContextSpecialization final : public AllStatic {
 public:
  static compiler::OptionalContextRef TryToRef(
      const MaglevCompilationUnit* unit, ValueNode* context, size_t* depth) {
    DCHECK(unit->info()->specialize_to_function_context());
    if (Constant* n = context->TryCast<Constant>()) {
      return n->ref().AsContext().previous(unit->broker(), depth);
    }
    return {};
  }
};

}  // namespace

ValueNode* MaglevGraphBuilder::TryGetParentContext(ValueNode* node) {
  if (CreateFunctionContext* n = node->TryCast<CreateFunctionContext>()) {
    return n->context().node();
  }

  if (InlinedAllocation* alloc = node->TryCast<InlinedAllocation>()) {
    return alloc->object()->get(
        Context::OffsetOfElementAt(Context::PREVIOUS_INDEX));
  }

  if (CallRuntime* n = node->TryCast<CallRuntime>()) {
    switch (n->function_id()) {
      case Runtime::kPushBlockContext:
      case Runtime::kPushCatchContext:
      case Runtime::kNewFunctionContext:
        return n->context().node();
      default:
        break;
    }
  }

  return nullptr;
}

// Attempts to walk up the context chain through the graph in order to reduce
// depth and thus the number of runtime loads.
void MaglevGraphBuilder::MinimizeContextChainDepth(ValueNode** context,
                                                   size_t* depth) {
  while (*depth > 0) {
    ValueNode* parent_context = TryGetParentContext(*context);
    if (parent_context == nullptr) return;
    *context = parent_context;
    (*depth)--;
  }
}

void MaglevGraphBuilder::EscapeContext() {
  ValueNode* context = GetContext();
  if (InlinedAllocation* alloc = context->TryCast<InlinedAllocation>()) {
    alloc->ForceEscaping();
  }
}

class CallArguments {
 public:
  enum Mode {
    kDefault,
    kWithSpread,
    kWithArrayLike,
  };

  CallArguments(ConvertReceiverMode receiver_mode,
                interpreter::RegisterList reglist,
                const InterpreterFrameState& frame, Mode mode = kDefault)
      : receiver_mode_(receiver_mode),
        args_(reglist.register_count()),
        mode_(mode) {
    for (int i = 0; i < reglist.register_count(); i++) {
      args_|i] = frame.get(reglist|i]);
    }
    DCHECK_IMPLIES(args_.size() == 0,
                   receiver_mode == ConvertReceiverMode::kNullOrUndefined);
    DCHECK_IMPLIES(mode != kDefault,
                   receiver_mode == ConvertReceiverMode::kAny);
    DCHECK_IMPLIES(mode == kWithArrayLike, args_.size() == 2);
  }

  explicit CallArguments(ConvertReceiverMode receiver_mode)
      : receiver_mode_(receiver_mode), args_(), mode_(kDefault) {
    DCHECK_EQ(receiver_mode, ConvertReceiverMode::kNullOrUndefined);
  }

  CallArguments(ConvertReceiverMode receiver_mode,
                std::initializer_list<ValueNode*> args, Mode mode = kDefault)
      : receiver_mode_(receiver_mode), args_(args), mode_(mode) {
    DCHECK_IMPLIES(mode != kDefault,
                   receiver_mode == ConvertReceiverMode::kAny);
    DCHECK_IMPLIES(mode == kWithArrayLike, args_.size() == 2);
    CheckArgumentsAreNotConversionNodes();
  }

  CallArguments(ConvertReceiverMode receiver_mode,
                base::SmallVector<ValueNode*, 8>&& args, Mode mode = kDefault)
      : receiver_mode_(receiver_mode), args_(std::move(args)), mode_(mode) {
    DCHECK_IMPLIES(mode != kDefault,
                   receiver_mode == ConvertReceiverMode::kAny);
    DCHECK_IMPLIES(mode == kWithArrayLike, args_.size() == 2);
    CheckArgumentsAreNotConversionNodes();
  }

  ValueNode* receiver() const {
    if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
      return nullptr;
    }
    return args_|0];
  }

  void set_receiver(ValueNode* receiver) {
    if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
      args_.insert(args_.data(), receiver);
      receiver_mode_ = ConvertReceiverMode::kAny;
    } else {
      DCHECK(!receiver->properties().is_conversion());
      args_|0] = receiver;
    }
  }

  ValueNode* array_like_argument() {
    DCHECK_EQ(mode_, kWithArrayLike);
    DCHECK_GT(count(), 0);
    return args_|args_.size() - 1];
  }

  size_t count() const {
    if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
      return args_.size();
    }
    return args_.size() - 1;
  }

  size_t count_with_receiver() const { return count() + 1; }

  ValueNode* operator|](size_t i) const {
    if (receiver_mode_ != ConvertReceiverMode::kNullOrUndefined) {
      i++;
    }
    if (i >= args_.size()) return nullptr;
    return args_|i];
  }

  void set_arg(size_t i, ValueNode* node) {
    if (receiver_mode_ != ConvertReceiverMode::kNullOrUndefined) {
      i++;
    }
    DCHECK_LT(i, args_.size());
    DCHECK(!node->properties().is_conversion());
    args_|i] = node;
  }

  Mode mode() const { return mode_; }

  ConvertReceiverMode receiver_mode() const { return receiver_mode_; }

  void PopArrayLikeArgument() {
    DCHECK_EQ(mode_, kWithArrayLike);
    DCHECK_GT(count(), 0);
    args_.pop_back();
  }

  void PopReceiver(ConvertReceiverMode new_receiver_mode) {
    DCHECK_NE(receiver_mode_, ConvertReceiverMode::kNullOrUndefined);
    DCHECK_NE(new_receiver_mode, ConvertReceiverMode::kNullOrUndefined);
    DCHECK_GT(args_.size(), 0);  // We have at least a receiver to pop!
    // TODO(victorgomes): Do this better!
    for (size_t i = 0; i < args_.size() - 1; i++) {
      args_|i] = args_|i + 1];
    }
    args_.pop_back();

    // If there is no non-receiver argument to become the new receiver,
    // consider the new receiver to be known undefined.
    receiver_mode_ = args_.empty() ? ConvertReceiverMode::kNullOrUndefined
                                   : new_receiver_mode;
  }

 private:
  ConvertReceiverMode receiver_mode_;
  base::SmallVector<ValueNode*, 8> args_;
  Mode mode_;

  void CheckArgumentsAreNotConversionNodes() {
#ifdef DEBUG
    // Arguments can leak to the interpreter frame if the call is inlined,
    // conversions should be stored in known_node_aspects/NodeInfo.
    for (ValueNode* arg : args_) {
      DCHECK(!arg->properties().is_conversion());
    }
#endif  // DEBUG
  }
};

class V8_NODISCARD MaglevGraphBuilder::SaveCallSpeculationScope {
 public:
  explicit SaveCallSpeculationScope(
      MaglevGraphBuilder* builder,
      compiler::FeedbackSource feedback_source = compiler::FeedbackSource())
      : builder_(builder) {
    saved_ = builder_->current_speculation_feedback_;
    // Only set the current speculation feedback if speculation is allowed.
    if (IsSpeculationAllowed(builder_->broker(), feedback_source)) {
      builder->current_speculation_feedback_ = feedback_source;
    } else {
      builder->current_speculation_feedback_ = compiler::FeedbackSource();
    }
  }
  ~SaveCallSpeculationScope() {
    builder_->current_speculation_feedback_ = saved_;
  }

  const compiler::FeedbackSource& value() { return saved_; }

 private:
  MaglevGraphBuilder* builder_;
  compiler::FeedbackSource saved_;

  static bool IsSpeculationAllowed(compiler::JSHeapBroker* broker,
                                   compiler::FeedbackSource feedback_source) {
    if (!feedback_source.IsValid()) return false;
    compiler::ProcessedFeedback const& processed_feedback =
        broker->GetFeedbackForCall(feedback_source);
    if (processed_feedback.IsInsufficient()) return false;
    return processed_feedback.AsCall().speculation_mode() ==
           SpeculationMode::kAllowSpeculation;
  }
};

class V8_NODISCARD MaglevGraphBuilder::DeoptFrameScope {
 public:
  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin continuation,
                  compiler::OptionalJSFunctionRef maybe_js_target = {})
      : builder_(builder),
        parent_(builder->current_deopt_scope_),
        data_(DeoptFrame::BuiltinContinuationFrameData{
            continuation, {}, builder->GetContext(), maybe_js_target}) {
    builder_->current_interpreter_frame().virtual_objects().Snapshot();
    builder_->current_deopt_scope_ = this;
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::BuiltinContinuationFrameData>().context);
    DCHECK(data_.get<DeoptFrame::BuiltinContinuationFrameData>()
               .parameters.empty());
  }

  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin continuation,
                  compiler::OptionalJSFunctionRef maybe_js_target,
                  base::Vector<ValueNode* const> parameters)
      : builder_(builder),
        parent_(builder->current_deopt_scope_),
        data_(DeoptFrame::BuiltinContinuationFrameData{
            continuation, builder->zone()->CloneVector(parameters),
            builder->GetContext(), maybe_js_target}) {
    builder_->current_interpreter_frame().virtual_objects().Snapshot();
    builder_->current_deopt_scope_ = this;
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::BuiltinContinuationFrameData>().context);
    if (parameters.size() > 0) {
      if (InlinedAllocation* receiver =
              parameters|0]->TryCast<InlinedAllocation>()) {
        // We escape the first argument, since the builtin continuation call can
        // trigger a stack iteration, which expects the receiver to be a
        // meterialized object.
        receiver->ForceEscaping();
      }
    }
    for (ValueNode* node :
         data_.get<DeoptFrame::BuiltinContinuationFrameData>().parameters) {
      builder_->AddDeoptUse(node);
    }
  }

  DeoptFrameScope(MaglevGraphBuilder* builder, ValueNode* receiver)
      : builder_(builder),
        parent_(builder->current_deopt_scope_),
        data_(DeoptFrame::ConstructInvokeStubFrameData{
            *builder->compilation_unit(), builder->current_source_position_,
            receiver, builder->GetContext()}) {
    builder_->current_interpreter_frame().virtual_objects().Snapshot();
    builder_->current_deopt_scope_ = this;
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::ConstructInvokeStubFrameData>().receiver);
    builder_->AddDeoptUse(
        data_.get<DeoptFrame::ConstructInvokeStubFrameData>().context);
  }

  ~DeoptFrameScope() {
    builder_->current_deopt_scope_ = parent_;
    // We might have cached a checkpointed frame which includes this scope;
    // reset it just in case.
    builder_->latest_checkpointed_frame_.reset();
  }

  DeoptFrameScope* parent() const { return parent_; }

  bool IsLazyDeoptContinuationFrame() const {
    if (data_.tag() != DeoptFrame::FrameType::kBuiltinContinuationFrame) {
      return false;
    }
    switch (data_.get<DeoptFrame::FrameType::kBuiltinContinuationFrame>()
                .builtin_id) {
      case Builtin::kGetIteratorWithFeedbackLazyDeoptContinuation:
      case Builtin::kCallIteratorWithFeedbackLazyDeoptContinuation:
      case Builtin::kArrayForEachLoopLazyDeoptContinuation:
      case Builtin::kGenericLazyDeoptContinuation:
      case Builtin::kToBooleanLazyDeoptContinuation:
        return true;
      default:
        return false;
    }
  }

  DeoptFrame::FrameData& data() { return data_; }
  const DeoptFrame::FrameData& data() const { return data_; }

 private:
  MaglevGraphBuilder* builder_;
  DeoptFrameScope* parent_;
  DeoptFrame::FrameData data_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::Variable {
 public:
  explicit Variable(int index) : pseudo_register_(index) {}

 private:
  friend class MaglevSubGraphBuilder;

  // Variables pretend to be interpreter registers as far as the dummy
  // compilation unit and merge states are concerned.
  interpreter::Register pseudo_register_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::Label {
 public:
  Label(MaglevSubGraphBuilder* sub_builder, int predecessor_count)
      : predecessor_count_(predecessor_count),
        liveness_(
            sub_builder->builder_->zone()->New<compiler::BytecodeLivenessState>(
                sub_builder->compilation_unit_->register_count(),
                sub_builder->builder_->zone())) {}
  Label(MaglevSubGraphBuilder* sub_builder, int predecessor_count,
        std::initializer_list<Variable*> vars)
      : Label(sub_builder, predecessor_count) {
    for (Variable* var : vars) {
      liveness_->MarkRegisterLive(var->pseudo_register_.index());
    }
  }

 private:
  explicit Label(MergePointInterpreterFrameState* merge_state,
                 BasicBlock* basic_block)
      : merge_state_(merge_state), ref_(basic_block) {}

  friend class MaglevSubGraphBuilder;
  friend class BranchBuilder;
  MergePointInterpreterFrameState* merge_state_ = nullptr;
  int predecessor_count_ = -1;
  compiler::BytecodeLivenessState* liveness_ = nullptr;
  BasicBlockRef ref_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::LoopLabel {
 public:
 private:
  explicit LoopLabel(MergePointInterpreterFrameState* merge_state,
                     BasicBlock* loop_header)
      : merge_state_(merge_state), loop_header_(loop_header) {}

  friend class MaglevSubGraphBuilder;
  MergePointInterpreterFrameState* merge_state_ = nullptr;
  BasicBlock* loop_header_;
};

class MaglevGraphBuilder::MaglevSubGraphBuilder::
    BorrowParentKnownNodeAspectsAndVOs {
 public:
  explicit BorrowParentKnownNodeAspectsAndVOs(
      MaglevSubGraphBuilder* sub_builder)
      : sub_builder_(sub_builder) {
    sub_builder_->TakeKnownNodeAspectsAndVOsFromParent();
  }
  ~BorrowParentKnownNodeAspectsAndVOs() {
    sub_builder_->MoveKnownNodeAspectsAndVOsToParent();
  }

 private:
  MaglevSubGraphBuilder* sub_builder_;
};

void MaglevGraphBuilder::BranchBuilder::StartFallthroughBlock(
    BasicBlock* predecessor) {
  switch (mode()) {
    case kBytecodeJumpTarget: {
      auto& data = data_.bytecode_target;
      if (data.patch_accumulator_scope &&
          (data.patch_accumulator_scope->node_ == builder_->GetAccumulator())) {
        SetAccumulatorInBranch(BranchType::kBranchIfTrue);
        builder_->MergeIntoFrameState(predecessor, data.jump_target_offset);
        SetAccumulatorInBranch(BranchType::kBranchIfFalse);
        builder_->StartFallthroughBlock(data.fallthrough_offset, predecessor);
      } else {
        builder_->MergeIntoFrameState(predecessor, data.jump_target_offset);
        builder_->StartFallthroughBlock(data.fallthrough_offset, predecessor);
      }
      break;
    }
    case kLabelJumpTarget:
      auto& data = data_.label_target;
      sub_builder_->MergeIntoLabel(data.jump_label, predecessor);
      builder_->StartNewBlock(predecessor, nullptr, data.fallthrough);
      break;
  }
}

void MaglevGraphBuilder::BranchBuilder::SetAccumulatorInBranch(
    BranchType jump_type) const {
  DCHECK_EQ(mode(), kBytecodeJumpTarget);
  auto& data = data_.bytecode_target;
  if (branch_specialization_mode_ == BranchSpecializationMode::kAlwaysBoolean) {
    builder_->SetAccumulatorInBranch(builder_->GetBooleanConstant(
        data.patch_accumulator_scope->jump_type_ == jump_type));
  } else if (data.patch_accumulator_scope->jump_type_ == jump_type) {
    builder_->SetAccumulatorInBranch(
        builder_->GetRootConstant(data.patch_accumulator_scope->root_index_));
  } else {
    builder_->SetAccumulatorInBranch(data.patch_accumulator_scope->node_);
  }
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::jump_target() {
  switch (mode()) {
    case kBytecodeJumpTarget:
      return &builder_->jump_targets_|data_.bytecode_target.jump_target_offset];
    case kLabelJumpTarget:
      return &data_.label_target.jump_label->ref_;
  }
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::fallthrough() {
  switch (mode()) {
    case kBytecodeJumpTarget:
      return &builder_->jump_targets_|data_.bytecode_target.fallthrough_offset];
    case kLabelJumpTarget:
      return &data_.label_target.fallthrough;
  }
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::true_target() {
  return jump_type_ == BranchType::kBranchIfTrue ? jump_target()
                                                 : fallthrough();
}

BasicBlockRef* MaglevGraphBuilder::BranchBuilder::false_target() {
  return jump_type_ == BranchType::kBranchIfFalse ? jump_target()
                                                  : fallthrough();
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BranchBuilder::FromBool(
    bool value) const {
  switch (mode()) {
    case kBytecodeJumpTarget: {
      BranchType type_if_need_to_jump =
          (value ? BranchType::kBranchIfTrue : BranchType::kBranchIfFalse);
      builder_->MarkBranchDeadAndJumpIfNeeded(jump_type_ ==
                                              type_if_need_to_jump);
      return BranchResult::kDefault;
    }
    case kLabelJumpTarget:
      return value ? BranchResult::kAlwaysTrue : BranchResult::kAlwaysFalse;
  }
}

template <typename ControlNodeT, typename... Args>
MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BranchBuilder::Build(
    std::initializer_list<ValueNode*> control_inputs, Args&&... args) {
  static_assert(IsConditionalControlNode(Node::opcode_of<ControlNodeT>));
  BasicBlock* block = builder_->FinishBlock<ControlNodeT>(
      control_inputs, std::forward<Args>(args)..., true_target(),
      false_target());
  StartFallthroughBlock(block);
  return BranchResult::kDefault;
}

MaglevGraphBuilder::MaglevSubGraphBuilder::MaglevSubGraphBuilder(
    MaglevGraphBuilder* builder, int variable_count)
    : builder_(builder),
      compilation_unit_(MaglevCompilationUnit::NewDummy(
          builder->zone(), builder->compilation_unit(), variable_count, 0, 0)),
      pseudo_frame_(*compilation_unit_, nullptr, VirtualObject::List()) {
  // We need to set a context, since this is unconditional in the frame state,
  // so set it to the real context.
  pseudo_frame_.set(interpreter::Register::current_context(),
                    builder_->current_interpreter_frame().get(
                        interpreter::Register::current_context()));
  DCHECK_NULL(pseudo_frame_.known_node_aspects());
}

MaglevGraphBuilder::MaglevSubGraphBuilder::LoopLabel
MaglevGraphBuilder::MaglevSubGraphBuilder::BeginLoop(
    std::initializer_list<Variable*> loop_vars) {
  // Create fake liveness and loop info for the loop, with all given loop vars
  // set to be live and assigned inside the loop.
  compiler::BytecodeLivenessState* loop_header_liveness =
      builder_->zone()->New<compiler::BytecodeLivenessState>(
          compilation_unit_->register_count(), builder_->zone());
  compiler::LoopInfo* loop_info = builder_->zone()->New<compiler::LoopInfo>(
      -1, 0, kMaxInt, compilation_unit_->parameter_count(),
      compilation_unit_->register_count(), builder_->zone());
  for (Variable* var : loop_vars) {
    loop_header_liveness->MarkRegisterLive(var->pseudo_register_.index());
    loop_info->assignments().Add(var->pseudo_register_);
  }

  // Finish the current block, jumping (as a fallthrough) to the loop header.
  BasicBlockRef loop_header_ref;
  BasicBlock* loop_predecessor =
      builder_->FinishBlock<Jump>({}, &loop_header_ref);

  // Create a state for the loop header, with two predecessors (the above jump
  // and the back edge), and initialise with the current state.
  MergePointInterpreterFrameState* loop_state =
      MergePointInterpreterFrameState::NewForLoop(
          pseudo_frame_, *compilation_unit_, 0, 2, loop_header_liveness,
          loop_info);

  {
    BorrowParentKnownNodeAspectsAndVOs borrow(this);
    loop_state->Merge(builder_, *compilation_unit_, pseudo_frame_,
                      loop_predecessor);
  }

  // Start a new basic block for the loop.
  DCHECK_NULL(pseudo_frame_.known_node_aspects());
  pseudo_frame_.CopyFrom(*compilation_unit_, *loop_state);
  MoveKnownNodeAspectsAndVOsToParent();

  builder_->ProcessMergePointPredecessors(*loop_state, loop_header_ref);
  builder_->StartNewBlock(nullptr, loop_state, loop_header_ref);

  return LoopLabel{loop_state, loop_header_ref.block_ptr()};
}

template <typename ControlNodeT, typename... Args>
void MaglevGraphBuilder::MaglevSubGraphBuilder::GotoIfTrue(
    Label* true_target, std::initializer_list<ValueNode*> control_inputs,
    Args&&... args) {
  static_assert(IsConditionalControlNode(Node::opcode_of<ControlNodeT>));

  BasicBlockRef fallthrough_ref;

  // Pass through to FinishBlock, converting Labels to BasicBlockRefs and the
  // fallthrough label to the fallthrough ref.
  BasicBlock* block = builder_->FinishBlock<ControlNodeT>(
      control_inputs, std::forward<Args>(args)..., &true_target->ref_,
      &fallthrough_ref);

  MergeIntoLabel(true_target, block);

  builder_->StartNewBlock(block, nullptr, fallthrough_ref);
}

template <typename ControlNodeT, typename... Args>
void MaglevGraphBuilder::MaglevSubGraphBuilder::GotoIfFalse(
    Label* false_target, std::initializer_list<ValueNode*> control_inputs,
    Args&&... args) {
  static_assert(IsConditionalControlNode(Node::opcode_of<ControlNodeT>));

  BasicBlockRef fallthrough_ref;

  // Pass through to FinishBlock, converting Labels to BasicBlockRefs and the
  // fallthrough label to the fallthrough ref.
  BasicBlock* block = builder_->FinishBlock<ControlNodeT>(
      control_inputs, std::forward<Args>(args)..., &fallthrough_ref,
      &false_target->ref_);

  MergeIntoLabel(false_target, block);

  builder_->StartNewBlock(block, nullptr, fallthrough_ref);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::GotoOrTrim(Label* label) {
  if (builder_->current_block_ == nullptr) {
    ReducePredecessorCount(label);
    return;
  }
  Goto(label);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::Goto(Label* label) {
  CHECK_NOT_NULL(builder_->current_block_);
  BasicBlock* block = builder_->FinishBlock<Jump>({}, &label->ref_);
  MergeIntoLabel(label, block);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::ReducePredecessorCount(
    Label* label, unsigned num) {
  DCHECK_GE(label->predecessor_count_, num);
  if (num == 0) {
    return;
  }
  label->predecessor_count_ -= num;
  if (label->merge_state_ != nullptr) {
    label->merge_state_->MergeDead(*compilation_unit_, num);
  }
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::EndLoop(LoopLabel* loop_label) {
  if (builder_->current_block_ == nullptr) {
    loop_label->merge_state_->MergeDeadLoop(*compilation_unit_);
    return;
  }

  BasicBlock* block =
      builder_->FinishBlock<JumpLoop>({}, loop_label->loop_header_);
  {
    BorrowParentKnownNodeAspectsAndVOs borrow(this);
    loop_label->merge_state_->MergeLoop(builder_, *compilation_unit_,
                                        pseudo_frame_, block);
  }
  block->set_predecessor_id(loop_label->merge_state_->predecessor_count() - 1);
}

ReduceResult MaglevGraphBuilder::MaglevSubGraphBuilder::TrimPredecessorsAndBind(
    Label* label) {
  int predecessors_so_far = label->merge_state_ == nullptr
                                ? 0
                                : label->merge_state_->predecessors_so_far();
  DCHECK_LE(predecessors_so_far, label->predecessor_count_);
  builder_->current_block_ = nullptr;
  ReducePredecessorCount(label,
                         label->predecessor_count_ - predecessors_so_far);
  if (predecessors_so_far == 0) return ReduceResult::DoneWithAbort();
  Bind(label);
  return ReduceResult::Done();
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::Bind(Label* label) {
  DCHECK_NULL(builder_->current_block_);

  DCHECK_NULL(pseudo_frame_.known_node_aspects());
  pseudo_frame_.CopyFrom(*compilation_unit_, *label->merge_state_);
  MoveKnownNodeAspectsAndVOsToParent();

  CHECK_EQ(label->merge_state_->predecessors_so_far(),
           label->predecessor_count_);

  builder_->ProcessMergePointPredecessors(*label->merge_state_, label->ref_);
  builder_->StartNewBlock(nullptr, label->merge_state_, label->ref_);
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::set(Variable& var,
                                                    ValueNode* value) {
  pseudo_frame_.set(var.pseudo_register_, value);
}
ValueNode* MaglevGraphBuilder::MaglevSubGraphBuilder::get(
    const Variable& var) const {
  return pseudo_frame_.get(var.pseudo_register_);
}

template <typename FCond, typename FTrue, typename FFalse>
ReduceResult MaglevGraphBuilder::MaglevSubGraphBuilder::Branch(
    std::initializer_list<MaglevSubGraphBuilder::Variable*> vars, FCond cond,
    FTrue if_true, FFalse if_false) {
  MaglevSubGraphBuilder::Label else_branch(this, 1);
  BranchBuilder builder(builder_, this, BranchType::kBranchIfFalse,
                        &else_branch);
  BranchResult branch_result = cond(builder);
  if (branch_result == BranchResult::kAlwaysTrue) {
    return if_true();
  }
  if (branch_result == BranchResult::kAlwaysFalse) {
    return if_false();
  }
  DCHECK(branch_result == BranchResult::kDefault);
  MaglevSubGraphBuilder::Label done(this, 2, vars);
  ReduceResult result_if_true = if_true();
  CHECK(result_if_true.IsDone());
  GotoOrTrim(&done);
  Bind(&else_branch);
  ReduceResult result_if_false = if_false();
  CHECK(result_if_false.IsDone());
  if (result_if_true.IsDoneWithAbort() && result_if_false.IsDoneWithAbort()) {
    return ReduceResult::DoneWithAbort();
  }
  GotoOrTrim(&done);
  Bind(&done);
  return ReduceResult::Done();
}

template <typename FCond, typename FTrue, typename FFalse>
ValueNode* MaglevGraphBuilder::Select(FCond cond, FTrue if_true,
                                      FFalse if_false) {
  MaglevSubGraphBuilder subgraph(this, 1);
  MaglevSubGraphBuilder::Label else_branch(&subgraph, 1);
  BranchBuilder builder(this, &subgraph, BranchType::kBranchIfFalse,
                        &else_branch);
  BranchResult branch_result = cond(builder);
  if (branch_result == BranchResult::kAlwaysTrue) {
    return if_true();
  }
  if (branch_result == BranchResult::kAlwaysFalse) {
    return if_false();
  }
  DCHECK(branch_result == BranchResult::kDefault);
  MaglevSubGraphBuilder::Variable ret_val(0);
  MaglevSubGraphBuilder::Label done(&subgraph, 2, {&ret_val});
  subgraph.set(ret_val, if_true());
  subgraph.Goto(&done);
  subgraph.Bind(&else_branch);
  subgraph.set(ret_val, if_false());
  subgraph.Goto(&done);
  subgraph.Bind(&done);
  return subgraph.get(ret_val);
}

template <typename FCond, typename FTrue, typename FFalse>
ReduceResult MaglevGraphBuilder::SelectReduction(FCond cond, FTrue if_true,
                                                 FFalse if_false) {
  MaglevSubGraphBuilder subgraph(this, 1);
  MaglevSubGraphBuilder::Label else_branch(&subgraph, 1);
  BranchBuilder builder(this, &subgraph, BranchType::kBranchIfFalse,
                        &else_branch);
  BranchResult branch_result = cond(builder);
  if (branch_result == BranchResult::kAlwaysTrue) {
    return if_true();
  }
  if (branch_result == BranchResult::kAlwaysFalse) {
    return if_false();
  }
  DCHECK(branch_result == BranchResult::kDefault);
  MaglevSubGraphBuilder::Variable ret_val(0);
  MaglevSubGraphBuilder::Label done(&subgraph, 2, {&ret_val});
  ReduceResult result_if_true = if_true();
  CHECK(result_if_true.IsDone());
  if (result_if_true.IsDoneWithValue()) {
    subgraph.set(ret_val, result_if_true.value());
  }
  subgraph.GotoOrTrim(&done);
  subgraph.Bind(&else_branch);
  ReduceResult result_if_false = if_false();
  CHECK(result_if_false.IsDone());
  if (result_if_true.IsDoneWithAbort() && result_if_false.IsDoneWithAbort()) {
    return ReduceResult::DoneWithAbort();
  }
  if (result_if_false.IsDoneWithValue()) {
    subgraph.set(ret_val, result_if_false.value());
  }
  subgraph.GotoOrTrim(&done);
  subgraph.Bind(&done);
  return subgraph.get(ret_val);
}

// Known node aspects for the pseudo frame are null aside from when merging --
// before each merge, we should borrow the node aspects from the parent
// builder, and after each merge point, we should copy the node aspects back
// to the parent. This is so that the parent graph builder can update its own
// known node aspects without having to worry about this pseudo frame.
void MaglevGraphBuilder::MaglevSubGraphBuilder::
    TakeKnownNodeAspectsAndVOsFromParent() {
  DCHECK_NULL(pseudo_frame_.known_node_aspects());
  DCHECK(pseudo_frame_.virtual_objects().is_empty());
  pseudo_frame_.set_known_node_aspects(
      builder_->current_interpreter_frame_.known_node_aspects());
  pseudo_frame_.set_virtual_objects(
      builder_->current_interpreter_frame_.virtual_objects());
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::
    MoveKnownNodeAspectsAndVOsToParent() {
  DCHECK_NOT_NULL(pseudo_frame_.known_node_aspects());
  builder_->current_interpreter_frame_.set_known_node_aspects(
      pseudo_frame_.known_node_aspects());
  pseudo_frame_.clear_known_node_aspects();
  builder_->current_interpreter_frame_.set_virtual_objects(
      pseudo_frame_.virtual_objects());
  pseudo_frame_.set_virtual_objects(VirtualObject::List());
}

void MaglevGraphBuilder::MaglevSubGraphBuilder::MergeIntoLabel(
    Label* label, BasicBlock* predecessor) {
  BorrowParentKnownNodeAspectsAndVOs borrow(this);

  if (label->merge_state_ == nullptr) {
    // If there's no merge state, allocate a new one.
    label->merge_state_ = MergePointInterpreterFrameState::New(
        *compilation_unit_, pseudo_frame_, 0, label->predecessor_count_,
        predecessor, label->liveness_);
  } else {
    // If there already is a frame state, merge.
    label->merge_state_->Merge(builder_, *compilation_unit_, pseudo_frame_,
                               predecessor);
  }
}

MaglevGraphBuilder::MaglevGraphBuilder(
    LocalIsolate* local_isolate, MaglevCompilationUnit* compilation_unit,
    Graph* graph, float call_frequency, BytecodeOffset caller_bytecode_offset,
    bool caller_is_inside_loop, int inlining_id, MaglevGraphBuilder* parent)
    : local_isolate_(local_isolate),
      compilation_unit_(compilation_unit),
      parent_(parent),
      graph_(graph),
      bytecode_analysis_(bytecode().object(), zone(),
                         compilation_unit->osr_offset(), true),
      iterator_(bytecode().object()),
      source_position_iterator_(bytecode().SourcePositionTable(broker())),
      allow_loop_peeling_(v8_flags.maglev_loop_peeling),
      loop_effects_stack_(zone()),
      decremented_predecessor_offsets_(zone()),
      loop_headers_to_peel_(bytecode().length(), zone()),
      current_source_position_(SourcePosition(
          compilation_unit_->shared_function_info().StartPosition(),
          inlining_id)),
      call_frequency_(call_frequency),
      // Add an extra jump_target slot for the inline exit if needed.
      jump_targets_(zone()->AllocateArray<BasicBlockRef>(
          bytecode().length() + (is_inline() ? 1 : 0))),
      // Overallocate merge_states_ by one to allow always looking up the
      // next offset. This overallocated slot can also be used for the inline
      // exit when needed.
      merge_states_(zone()->AllocateArray<MergePointInterpreterFrameState*>(
          bytecode().length() + 1)),
      current_interpreter_frame_(
          *compilation_unit_,
          is_inline() ? parent->current_interpreter_frame_.known_node_aspects()
                      : compilation_unit_->zone()->New<KnownNodeAspects>(
                            compilation_unit_->zone()),
          is_inline() ? parent->current_interpreter_frame_.virtual_objects()
                      : VirtualObject::List()),
      caller_bytecode_offset_(caller_bytecode_offset),
      caller_is_inside_loop_(caller_is_inside_loop),
      entrypoint_(compilation_unit->is_osr()
                      ? bytecode_analysis_.osr_entry_point()
                      : 0),
      inlining_id_(inlining_id),
      catch_block_stack_(zone()),
      unobserved_context_slot_stores_(zone()) {
  memset(merge_states_, 0,
         (bytecode().length() + 1) * sizeof(InterpreterFrameState*));
  // Default construct basic block refs.
  // TODO(leszeks): This could be a memset of nullptr to ..._jump_targets_.
  for (int i = 0; i < bytecode().length(); ++i) {
    new (&jump_targets_|i]) BasicBlockRef();
  }

  if (is_inline()) {
    DCHECK_NOT_NULL(parent_);
    DCHECK_GT(compilation_unit->inlining_depth(), 0);
    // The allocation/initialisation logic here relies on inline_exit_offset
    // being the offset one past the end of the bytecode.
    DCHECK_EQ(inline_exit_offset(), bytecode().length());
    merge_states_|inline_exit_offset()] = nullptr;
    new (&jump_targets_|inline_exit_offset()]) BasicBlockRef();
    if (parent_->loop_effects_) {
      loop_effects_ = parent->loop_effects_;
      loop_effects_stack_.push_back(loop_effects_);
    }
    unobserved_context_slot_stores_ = parent_->unobserved_context_slot_stores_;
  }

  CHECK_IMPLIES(compilation_unit_->is_osr(), graph_->is_osr());
  CHECK_EQ(compilation_unit_->info()->toplevel_osr_offset() !=
               BytecodeOffset::None(),
           graph_->is_osr());
  if (compilation_unit_->is_osr()) {
    CHECK(!is_inline());

    // Make sure that we're at a valid OSR entrypoint.
    //
    // This is also a defense-in-depth check to make sure that we're not
    // compiling invalid bytecode if the OSR offset is wrong (e.g. because it
    // belongs to different bytecode).
    //
    // OSR'ing into the middle of a loop is currently not supported. There
    // should not be any issue with OSR'ing outside of loops, just we currently
    // dont do it...
    interpreter::BytecodeArrayIterator it(bytecode().object());
    it.AdvanceTo(compilation_unit_->osr_offset().ToInt());
    CHECK(it.CurrentBytecodeIsValidOSREntry());
    CHECK_EQ(entrypoint_, it.GetJumpTargetOffset());

    iterator_.AdvanceTo(entrypoint_);

    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "- Non-standard entrypoint @" << entrypoint_
                << " by OSR from @" << compilation_unit_->osr_offset().ToInt()
                << std::endl;
    }
  }
  CHECK_IMPLIES(!compilation_unit_->is_osr(), entrypoint_ == 0);

  CalculatePredecessorCounts();
}

void MaglevGraphBuilder::StartPrologue() {
  current_block_ = zone()->New<BasicBlock>(nullptr, zone());
}

BasicBlock* MaglevGraphBuilder::EndPrologue() {
  BasicBlock* first_block;
  if (!is_inline() &&
      (v8_flags.maglev_hoist_osr_value_phi_untagging && graph_->is_osr())) {
    first_block =
        FinishBlock<CheckpointedJump>({}, &jump_targets_|entrypoint_]);
  } else {
    first_block = FinishBlock<Jump>({}, &jump_targets_|entrypoint_]);
  }
  MergeIntoFrameState(first_block, entrypoint_);
  return first_block;
}

void MaglevGraphBuilder::SetArgument(int i, ValueNode* value) {
  interpreter::Register reg = interpreter::Register::FromParameterIndex(i);
  current_interpreter_frame_.set(reg, value);
}

ValueNode* MaglevGraphBuilder::GetArgument(int i) {
  DCHECK_LT(i, parameter_count());
  interpreter::Register reg = interpreter::Register::FromParameterIndex(i);
  return current_interpreter_frame_.get(reg);
}

ValueNode* MaglevGraphBuilder::GetInlinedArgument(int i) {
  DCHECK(is_inline());
  DCHECK_LT(i, argument_count());
  return inlined_arguments_|i];
}

void MaglevGraphBuilder::InitializeRegister(interpreter::Register reg,
                                            ValueNode* value) {
  current_interpreter_frame_.set(
      reg, value ? value : AddNewNode<InitialValue>({}, reg));
}

void MaglevGraphBuilder::BuildRegisterFrameInitialization(
    ValueNode* context, ValueNode* closure, ValueNode* new_target) {
  if (closure == nullptr &&
      compilation_unit_->info()->specialize_to_function_context()) {
    compiler::JSFunctionRef function = compiler::MakeRefAssumeMemoryFence(
        broker(), broker()->CanonicalPersistentHandle(
                      compilation_unit_->info()->toplevel_function()));
    closure = GetConstant(function);
    context = GetConstant(function.context(broker()));
  }
  InitializeRegister(interpreter::Register::current_context(), context);
  InitializeRegister(interpreter::Register::function_closure(), closure);

  interpreter::Register new_target_or_generator_register =
      bytecode().incoming_new_target_or_generator_register();

  int register_index = 0;

  if (compilation_unit_->is_osr()) {
    for (; register_index < register_count(); register_index++) {
      auto val =
          AddNewNode<InitialValue>({}, interpreter::Register(register_index));
      InitializeRegister(interpreter::Register(register_index), val);
      graph_->osr_values().push_back(val);
    }
    return;
  }

  // TODO(leszeks): Don't emit if not needed.
  ValueNode* undefined_value = GetRootConstant(RootIndex::kUndefinedValue);
  if (new_target_or_generator_register.is_valid()) {
    int new_target_index = new_target_or_generator_register.index();
    for (; register_index < new_target_index; register_index++) {
      current_interpreter_frame_.set(interpreter::Register(register_index),
                                     undefined_value);
    }
    current_interpreter_frame_.set(
        new_target_or_generator_register,
        new_target ? new_target
                   : GetRegisterInput(kJavaScriptCallNewTargetRegister));
    register_index++;
  }
  for (; register_index < register_count(); register_index++) {
    InitializeRegister(interpreter::Register(register_index), undefined_value);
  }
}

void MaglevGraphBuilder::BuildMergeStates() {
  auto offset_and_info = bytecode_analysis().GetLoopInfos().begin();
  auto end = bytecode_analysis().GetLoopInfos().end();
  while (offset_and_info != end && offset_and_info->first < entrypoint_) {
    ++offset_and_info;
  }
  for (; offset_and_info != end; ++offset_and_info) {
    int offset = offset_and_info->first;
    const compiler::LoopInfo& loop_info = offset_and_info->second;
    if (loop_headers_to_peel_.Contains(offset)) {
      // Peeled loops are treated like normal merges at first. We will construct
      // the proper loop header merge state when reaching the `JumpLoop` of the
      // peeled iteration.
      continue;
    }
    const compiler::BytecodeLivenessState* liveness = GetInLivenessFor(offset);
    DCHECK_NULL(merge_states_|offset]);
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "- Creating loop merge state at @" << offset << std::endl;
    }
    merge_states_|offset] = MergePointInterpreterFrameState::NewForLoop(
        current_interpreter_frame_, *compilation_unit_, offset,
        predecessor_count(offset), liveness, &loop_info);
  }

  if (bytecode().handler_table_size() > 0) {
    HandlerTable table(*bytecode().object());
    for (int i = 0; i < table.NumberOfRangeEntries(); i++) {
      const int offset = table.GetRangeHandler(i);
      const bool was_used = table.HandlerWasUsed(i);
      const interpreter::Register context_reg(table.GetRangeData(i));
      const compiler::BytecodeLivenessState* liveness =
          GetInLivenessFor(offset);
      DCHECK_EQ(predecessor_count(offset), 0);
      DCHECK_NULL(merge_states_|offset]);
      if (v8_flags.trace_maglev_graph_building) {
        std::cout << "- Creating exception merge state at @" << offset
                  << (was_used ? "" : " (never used)") << ", context register r"
                  << context_reg.index() << std::endl;
      }
      merge_states_|offset] = MergePointInterpreterFrameState::NewForCatchBlock(
          *compilation_unit_, liveness, offset, was_used, context_reg, graph_);
    }
  }
}

namespace {

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper;

// Terminal cases
template <int index>
struct GetResultLocationAndSizeHelper<index> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // TODO(leszeks): This should probably actually be "UNREACHABLE" but we have
    // lazy deopt info for interrupt budget updates at returns, not for actual
    // lazy deopts, but just for stack iteration purposes.
    return {interpreter::Register::invalid_value(), 0};
  }
  static bool HasOutputRegisterOperand() { return false; }
};

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<index, interpreter::OperandType::kRegOut,
                                      operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // We shouldn't have any other output operands than this one.
    return {iterator.GetRegisterOperand(index), 1};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<
    index, interpreter::OperandType::kRegOutPair, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // We shouldn't have any other output operands than this one.
    return {iterator.GetRegisterOperand(index), 2};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<
    index, interpreter::OperandType::kRegOutTriple, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    // We shouldn't have any other output operands than this one.
    DCHECK(!(GetResultLocationAndSizeHelper<
             index + 1, operands...>::HasOutputRegisterOperand()));
    return {iterator.GetRegisterOperand(index), 3};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

// We don't support RegOutList for lazy deopts.
template <int index, interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<
    index, interpreter::OperandType::kRegOutList, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    interpreter::RegisterList list = iterator.GetRegisterListOperand(index);
    return {list.first_register(), list.register_count()};
  }
  static bool HasOutputRegisterOperand() { return true; }
};

// Induction case.
template <int index, interpreter::OperandType operand,
          interpreter::OperandType... operands>
struct GetResultLocationAndSizeHelper<index, operand, operands...> {
  static std::pair<interpreter::Register, int> GetResultLocationAndSize(
      const interpreter::BytecodeArrayIterator& iterator) {
    return GetResultLocationAndSizeHelper<
        index + 1, operands...>::GetResultLocationAndSize(iterator);
  }
  static bool HasOutputRegisterOperand() {
    return GetResultLocationAndSizeHelper<
        index + 1, operands...>::HasOutputRegisterOperand();
  }
};

template <interpreter::Bytecode bytecode,
          interpreter::ImplicitRegisterUse implicit_use,
          interpreter::OperandType... operands>
std::pair<interpreter::Register, int> GetResultLocationAndSizeForBytecode(
    const interpreter::BytecodeArrayIterator& iterator) {
  // We don't support output registers for implicit registers.
  DCHECK(!interpreter::BytecodeOperands::WritesImplicitRegister(implicit_use));
  if (interpreter::BytecodeOperands::WritesAccumulator(implicit_use)) {
    // If we write the accumulator, we shouldn't also write an output register.
    DCHECK(!(GetResultLocationAndSizeHelper<
             0, operands...>::HasOutputRegisterOperand()));
    return {interpreter::Register::virtual_accumulator(), 1};
  }

  // Use template magic to output a the appropriate GetRegisterOperand call and
  // size for this bytecode.
  return GetResultLocationAndSizeHelper<
      0, operands...>::GetResultLocationAndSize(iterator);
}

}  // namespace

std::pair<interpreter::Register, int>
MaglevGraphBuilder::GetResultLocationAndSize() const {
  using Bytecode = interpreter::Bytecode;
  using OperandType = interpreter::OperandType;
  using ImplicitRegisterUse = interpreter::ImplicitRegisterUse;
  Bytecode bytecode = iterator_.current_bytecode();
  // TODO(leszeks): Only emit these cases for bytecodes we know can lazy deopt.
  switch (bytecode) {
#define CASE(Name, ...)                                           \
  case Bytecode::k##Name:                                         \
    return GetResultLocationAndSizeForBytecode<Bytecode::k##Name, \
                                               __VA_ARGS__>(iterator_);
    BYTECODE_LIST(CASE, CASE)
#undef CASE
  }
  UNREACHABLE();
}

#ifdef DEBUG
bool MaglevGraphBuilder::HasOutputRegister(interpreter::Register reg) const {
  interpreter::Bytecode bytecode = iterator_.current_bytecode();
  if (reg == interpreter::Register::virtual_accumulator()) {
    return interpreter::Bytecodes::WritesAccumulator(bytecode);
  }
  for (int i = 0; i < interpreter::Bytecodes::NumberOfOperands(bytecode); ++i) {
    if (interpreter::Bytecodes::IsRegisterOutputOperandType(
            interpreter::Bytecodes::GetOperandType(bytecode, i))) {
      interpreter::Register operand_reg = iterator_.GetRegisterOperand(i);
      int operand_range = iterator_.GetRegisterOperandRange(i);
      if (base::IsInRange(reg.index(), operand_reg.index(),
                          operand_reg.index() + operand_range)) {
        return true;
      }
    }
  }
  return false;
}
#endif

DeoptFrame* MaglevGraphBuilder::GetParentDeoptFrame() {
  if (parent_ == nullptr) return nullptr;
  if (parent_deopt_frame_ == nullptr) {
    // The parent resumes after the call, which is roughly equivalent to a lazy
    // deopt. Use the helper function directly so that we can mark the
    // accumulator as dead (since it'll be overwritten by this function's
    // return value anyway).
    // TODO(leszeks): This is true for our current set of
    // inlinings/continuations, but there might be cases in the future where it
    // isn't. We may need to store the relevant overwritten register in
    // LazyDeoptFrameScope.
    DCHECK(interpreter::Bytecodes::WritesAccumulator(
        parent_->iterator_.current_bytecode()));

    parent_deopt_frame_ =
        zone()->New<DeoptFrame>(parent_->GetDeoptFrameForLazyDeoptHelper(
            interpreter::Register::invalid_value(), 0,
            parent_->current_deopt_scope_, true));
    // Only create InlinedArgumentsDeoptFrame if we have a mismatch between
    // formal parameter and arguments count.
    if (HasMismatchedArgumentAndParameterCount()) {
      parent_deopt_frame_ = zone()->New<InlinedArgumentsDeoptFrame>(
          *compilation_unit_, caller_bytecode_offset_, GetClosure(),
          inlined_arguments_, parent_deopt_frame_);
      AddDeoptUse(GetClosure());
      for (ValueNode* arg :
           parent_deopt_frame_->as_inlined_arguments().arguments()) {
        AddDeoptUse(arg);
      }
    }
  }
  return parent_deopt_frame_;
}

DeoptFrame MaglevGraphBuilder::GetLatestCheckpointedFrame() {
  if (in_prologue_) {
    return GetDeoptFrameForEntryStackCheck();
  }
  if (!latest_checkpointed_frame_) {
    current_interpreter_frame_.virtual_objects().Snapshot();
    latest_checkpointed_frame_.emplace(InterpretedDeoptFrame(
        *compilation_unit_,
        zone()->New<CompactInterpreterFrameState>(
            *compilation_unit_, GetInLiveness(), current_interpreter_frame_),
        GetClosure(), BytecodeOffset(iterator_.current_offset()),
        current_source_position_, GetParentDeoptFrame()));

    latest_checkpointed_frame_->as_interpreted().frame_state()->ForEachValue(
        *compilation_unit_,
        |&](ValueNode* node, interpreter::Register) { AddDeoptUse(node); });
    AddDeoptUse(latest_checkpointed_frame_->as_interpreted().closure());

    // Skip lazy deopt builtin continuations.
    const DeoptFrameScope* deopt_scope = current_deopt_scope_;
    while (deopt_scope != nullptr &&
           deopt_scope->IsLazyDeoptContinuationFrame()) {
      deopt_scope = deopt_scope->parent();
    }

    if (deopt_scope != nullptr) {
      // Support exactly one eager deopt builtin continuation. This can be
      // expanded in the future if necessary.
      DCHECK_NULL(deopt_scope->parent());
      DCHECK_EQ(deopt_scope->data().tag(),
                DeoptFrame::FrameType::kBuiltinContinuationFrame);
#ifdef DEBUG
      if (deopt_scope->data().tag() ==
          DeoptFrame::FrameType::kBuiltinContinuationFrame) {
        const DeoptFrame::BuiltinContinuationFrameData& frame =
            deopt_scope->data().get<DeoptFrame::BuiltinContinuationFrameData>();
        if (frame.maybe_js_target) {
          int stack_parameter_count =
              Builtins::GetStackParameterCount(frame.builtin_id);
          DCHECK_EQ(stack_parameter_count, frame.parameters.length());
        } else {
          CallInterfaceDescriptor descriptor =
              Builtins::CallInterfaceDescriptorFor(frame.builtin_id);
          DCHECK_EQ(descriptor.GetParameterCount(), frame.parameters.length());
        }
      }
#endif

      // Wrap the above frame in the scope frame.
      latest_checkpointed_frame_.emplace(
          deopt_scope->data(),
          zone()->New<DeoptFrame>(*latest_checkpointed_frame_));
    }
  }
  return *latest_checkpointed_frame_;
}

DeoptFrame MaglevGraphBuilder::GetDeoptFrameForLazyDeopt(
    interpreter::Register result_location, int result_size) {
  return GetDeoptFrameForLazyDeoptHelper(result_location, result_size,
                                         current_deopt_scope_, false);
}

DeoptFrame MaglevGraphBuilder::GetDeoptFrameForLazyDeoptHelper(
    interpreter::Register result_location, int result_size,
    DeoptFrameScope* scope, bool mark_accumulator_dead) {
  if (scope == nullptr) {
    compiler::BytecodeLivenessState* liveness =
        zone()->New<compiler::BytecodeLivenessState>(*GetOutLiveness(), zone());
    // Remove result locations from liveness.
    if (result_location == interpreter::Register::virtual_accumulator()) {
      DCHECK_EQ(result_size, 1);
      liveness->MarkAccumulatorDead();
      mark_accumulator_dead = false;
    } else {
      DCHECK(!result_location.is_parameter());
      for (int i = 0; i < result_size; i++) {
        liveness->MarkRegisterDead(result_location.index() + i);
      }
    }
    // Explicitly drop the accumulator if needed.
    if (mark_accumulator_dead && liveness->AccumulatorIsLive()) {
      liveness->MarkAccumulatorDead();
    }
    current_interpreter_frame_.virtual_objects().Snapshot();
    InterpretedDeoptFrame ret(
        *compilation_unit_,
        zone()->New<CompactInterpreterFrameState>(*compilation_unit_, liveness,
                                                  current_interpreter_frame_),
        GetClosure(), BytecodeOffset(iterator_.current_offset()),
        current_source_position_, GetParentDeoptFrame());
    ret.frame_state()->ForEachValue(
        *compilation_unit_, |this](ValueNode* node, interpreter::Register reg) {
          // Receiver and closure values have to be materialized, even if
          // they don't otherwise escape.
          if (reg == interpreter::Register::receiver() ||
              reg == interpreter::Register::function_closure()) {
            node->add_use();
          } else {
            AddDeoptUse(node);
          }
        });
    AddDeoptUse(ret.closure());
    return ret;
  }

  // Currently only support builtin continuations for bytecodes that write to
  // the accumulator
  DCHECK(interpreter::Bytecodes::WritesOrClobbersAccumulator(
      iterator_.current_bytecode()));

#ifdef DEBUG
  if (scope->data().tag() == DeoptFrame::FrameType::kBuiltinContinuationFrame) {
    const DeoptFrame::BuiltinContinuationFrameData& frame =
        current_deopt_scope_->data()
            .get<DeoptFrame::BuiltinContinuationFrameData>();
    if (frame.maybe_js_target) {
      int stack_parameter_count =
          Builtins::GetStackParameterCount(frame.builtin_id);
      // The deopt input value is passed by the deoptimizer, so shouldn't be a
      // parameter here.
      DCHECK_EQ(stack_parameter_count, frame.parameters.length() + 1);
    } else {
      CallInterfaceDescriptor descriptor =
          Builtins::CallInterfaceDescriptorFor(frame.builtin_id);
      // The deopt input value is passed by the deoptimizer, so shouldn't be a
      // parameter here.
      DCHECK_EQ(descriptor.GetParameterCount(), frame.parameters.length() + 1);
      // The deopt input value is passed on the stack.
      DCHECK_GT(descriptor.GetStackParameterCount(), 0);
    }
  }
#endif

  // Mark the accumulator dead in parent frames since we know that the
  // continuation will write it.
  return DeoptFrame(scope->data(),
                    zone()->New<DeoptFrame>(GetDeoptFrameForLazyDeoptHelper(
                        result_location, result_size, scope->parent(),
                        scope->data().tag() ==
                            DeoptFrame::FrameType::kBuiltinContinuationFrame)));
}

InterpretedDeoptFrame MaglevGraphBuilder::GetDeoptFrameForEntryStackCheck() {
  if (entry_stack_check_frame_) return *entry_stack_check_frame_;
  DCHECK_EQ(iterator_.current_offset(), entrypoint_);
  DCHECK_NULL(parent_);
  entry_stack_check_frame_.emplace(
      *compilation_unit_,
      zone()->New<CompactInterpreterFrameState>(
          *compilation_unit_,
          GetInLivenessFor(graph_->is_osr() ? bailout_for_entrypoint() : 0),
          current_interpreter_frame_),
      GetClosure(), BytecodeOffset(bailout_for_entrypoint()),
      current_source_position_, nullptr);

  (*entry_stack_check_frame_)
      .frame_state()
      ->ForEachValue(
          *compilation_unit_,
          |&](ValueNode* node, interpreter::Register) { AddDeoptUse(node); });
  AddDeoptUse((*entry_stack_check_frame_).closure());
  return *entry_stack_check_frame_;
}

ValueNode* MaglevGraphBuilder::GetTaggedValue(
    ValueNode* value, UseReprHintRecording record_use_repr_hint) {
  if (V8_LIKELY(record_use_repr_hint == UseReprHintRecording::kRecord)) {
    RecordUseReprHintIfPhi(value, UseRepresentation::kTagged);
  }

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kTagged) return value;

  if (Int32Constant* as_int32_constant = value->TryCast<Int32Constant>();
      as_int32_constant && Smi::IsValid(as_int32_constant->value())) {
    return GetSmiConstant(as_int32_constant->value());
  }

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.tagged()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kInt32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagInt32>({value}));
      }
      return alternative.set_tagged(AddNewNode<Int32ToNumber>({value}));
    }
    case ValueRepresentation::kUint32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagUint32>({value}));
      }
      return alternative.set_tagged(AddNewNode<Uint32ToNumber>({value}));
    }
    case ValueRepresentation::kFloat64: {
      return alternative.set_tagged(AddNewNode<Float64ToTagged>(
          {value}, Float64ToTagged::ConversionMode::kCanonicalizeSmi));
    }
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_tagged(AddNewNode<HoleyFloat64ToTagged>(
          {value}, HoleyFloat64ToTagged::ConversionMode::kForceHeapNumber));
    }

    case ValueRepresentation::kTagged:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

ReduceResult MaglevGraphBuilder::GetSmiValue(
    ValueNode* value, UseReprHintRecording record_use_repr_hint) {
  if (V8_LIKELY(record_use_repr_hint == UseReprHintRecording::kRecord)) {
    RecordUseReprHintIfPhi(value, UseRepresentation::kTagged);
  }

  NodeInfo* node_info = GetOrCreateInfoFor(value);

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kTagged) {
    return BuildCheckSmi(value, !value->Is<Phi>());
  }

  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.tagged()) {
    // HoleyFloat64ToTagged does not canonicalize Smis by default, since it can
    // be expensive. If we are reading a Smi value, we should try to
    // canonicalize now.
    if (HoleyFloat64ToTagged* conversion_node =
            alt->TryCast<HoleyFloat64ToTagged>()) {
      conversion_node->SetMode(
          HoleyFloat64ToTagged::ConversionMode::kCanonicalizeSmi);
    }
    return BuildCheckSmi(alt, !value->Is<Phi>());
  }

  switch (representation) {
    case ValueRepresentation::kInt32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagInt32>({value}));
      }
      return alternative.set_tagged(AddNewNode<CheckedSmiTagInt32>({value}));
    }
    case ValueRepresentation::kUint32: {
      if (NodeTypeIsSmi(node_info->type())) {
        return alternative.set_tagged(AddNewNode<UnsafeSmiTagUint32>({value}));
      }
      return alternative.set_tagged(AddNewNode<CheckedSmiTagUint32>({value}));
    }
    case ValueRepresentation::kFloat64: {
      return alternative.set_tagged(AddNewNode<CheckedSmiTagFloat64>({value}));
    }
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_tagged(AddNewNode<CheckedSmiTagFloat64>({value}));
    }

    case ValueRepresentation::kTagged:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

namespace {
CheckType GetCheckType(NodeType type) {
  return NodeTypeIs(type, NodeType::kAnyHeapObject)
             ? CheckType::kOmitHeapObjectCheck
             : CheckType::kCheckHeapObject;
}
}  // namespace

ValueNode* MaglevGraphBuilder::GetInternalizedString(
    interpreter::Register reg) {
  ValueNode* node = current_interpreter_frame_.get(reg);
  NodeType old_type;
  if (CheckType(node, NodeType::kInternalizedString, &old_type)) return node;
  NodeInfo* known_info = GetOrCreateInfoFor(node);
  if (known_info->alternative().checked_value()) {
    node = known_info->alternative().checked_value();
    if (CheckType(node, NodeType::kInternalizedString, &old_type)) return node;
  }

  if (!NodeTypeIs(old_type, NodeType::kString)) {
    known_info->CombineType(NodeType::kString);
  }

  // This node may unwrap ThinStrings.
  ValueNode* maybe_unwrapping_node =
      AddNewNode<CheckedInternalizedString>({node}, GetCheckType(old_type));
  known_info->alternative().set_checked_value(maybe_unwrapping_node);

  current_interpreter_frame_.set(reg, maybe_unwrapping_node);
  return maybe_unwrapping_node;
}

ValueNode* MaglevGraphBuilder::GetTruncatedInt32ForToNumber(
    ValueNode* value, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kTruncatedInt32);

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kInt32) return value;
  if (representation == ValueRepresentation::kUint32) {
    // This node is cheap (no code gen, just a bitcast), so don't cache it.
    return AddNewNode<TruncateUint32ToInt32>({value});
  }

  // Process constants first to avoid allocating NodeInfo for them.
  switch (value->opcode()) {
    case Opcode::kConstant: {
      compiler::ObjectRef object = value->Cast<Constant>()->object();
      if (!object.IsHeapNumber()) break;
      int32_t truncated_value = DoubleToInt32(object.AsHeapNumber().value());
      if (!Smi::IsValid(truncated_value)) break;
      return GetInt32Constant(truncated_value);
    }
    case Opcode::kSmiConstant:
      return GetInt32Constant(value->Cast<SmiConstant>()->value().value());
    case Opcode::kRootConstant: {
      Tagged<Object> root_object =
          local_isolate_->root(value->Cast<RootConstant>()->index());
      if (!IsOddball(root_object, local_isolate_)) break;
      int32_t truncated_value =
          DoubleToInt32(Cast<Oddball>(root_object)->to_number_raw());
      // All oddball ToNumber truncations are valid Smis.
      DCHECK(Smi::IsValid(truncated_value));
      return GetInt32Constant(truncated_value);
    }
    case Opcode::kFloat64Constant: {
      int32_t truncated_value =
          DoubleToInt32(value->Cast<Float64Constant>()->value().get_scalar());
      if (!Smi::IsValid(truncated_value)) break;
      return GetInt32Constant(truncated_value);
    }

    // We could emit unconditional eager deopts for other kinds of constant, but
    // it's not necessary, the appropriate checking conversion nodes will deopt.
    default:
      break;
  }

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  // If there is an int32_alternative, then that works as a truncated value
  // too.
  if (ValueNode* alt = alternative.int32()) {
    return alt;
  }
  if (ValueNode* alt = alternative.truncated_int32_to_number()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kTagged: {
      NodeType old_type;
      EnsureType(value, allowed_input_type, &old_type);
      if (NodeTypeIsSmi(old_type)) {
        // Smi untagging can be cached as an int32 alternative, not just a
        // truncated alternative.
        return alternative.set_int32(BuildSmiUntag(value));
      }
      if (allowed_input_type == NodeType::kSmi) {
        return alternative.set_int32(AddNewNode<CheckedSmiUntag>({value}));
      }
      if (NodeTypeIs(old_type, allowed_input_type)) {
        return alternative.set_truncated_int32_to_number(
            AddNewNode<TruncateNumberOrOddballToInt32>({value},
                                                       conversion_type));
      }
      return alternative.set_truncated_int32_to_number(
          AddNewNode<CheckedTruncateNumberOrOddballToInt32>({value},
                                                            conversion_type));
    }
    case ValueRepresentation::kFloat64:
    // Ignore conversion_type for HoleyFloat64, and treat them like Float64.
    // ToNumber of undefined is anyway a NaN, so we'll simply truncate away
    // the NaN-ness of the hole, and don't need to do extra oddball checks so
    // we can ignore the hint (though we'll miss updating the feedback).
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_truncated_int32_to_number(
          AddNewNode<TruncateFloat64ToInt32>({value}));
    }

    case ValueRepresentation::kInt32:
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

std::optional<int32_t> MaglevGraphBuilder::TryGetInt32Constant(
    ValueNode* value) {
  switch (value->opcode()) {
    case Opcode::kInt32Constant:
      return value->Cast<Int32Constant>()->value();
    case Opcode::kUint32Constant: {
      uint32_t uint32_value = value->Cast<Uint32Constant>()->value();
      if (uint32_value <= INT32_MAX) {
        return static_cast<int32_t>(uint32_value);
      }
      return {};
    }
    case Opcode::kSmiConstant:
      return value->Cast<SmiConstant>()->value().value();
    case Opcode::kFloat64Constant: {
      double double_value =
          value->Cast<Float64Constant>()->value().get_scalar();
      if (!IsInt32Double(double_value)) return {};
      return FastD2I(value->Cast<Float64Constant>()->value().get_scalar());
    }
    default:
      break;
  }
  if (auto c = TryGetConstantAlternative(value)) {
    return TryGetInt32Constant(*c);
  }
  return {};
}

std::optional<uint32_t> MaglevGraphBuilder::TryGetUint32Constant(
    ValueNode* value) {
  switch (value->opcode()) {
    case Opcode::kInt32Constant: {
      int32_t int32_value = value->Cast<Int32Constant>()->value();
      if (int32_value >= 0) {
        return static_cast<uint32_t>(int32_value);
      }
      return {};
    }
    case Opcode::kUint32Constant:
      return value->Cast<Uint32Constant>()->value();
    case Opcode::kSmiConstant: {
      int32_t smi_value = value->Cast<SmiConstant>()->value().value();
      if (smi_value >= 0) {
        return static_cast<uint32_t>(smi_value);
      }
      return {};
    }
    case Opcode::kFloat64Constant: {
      double double_value =
          value->Cast<Float64Constant>()->value().get_scalar();
      if (!IsUint32Double(double_value)) return {};
      return FastD2UI(value->Cast<Float64Constant>()->value().get_scalar());
    }
    default:
      break;
  }
  if (auto c = TryGetConstantAlternative(value)) {
    return TryGetUint32Constant(*c);
  }
  return {};
}

ValueNode* MaglevGraphBuilder::GetInt32(ValueNode* value,
                                        bool can_be_heap_number) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kInt32);

  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kInt32) return value;

  // Process constants first to avoid allocating NodeInfo for them.
  if (auto cst = TryGetInt32Constant(value)) {
    return GetInt32Constant(cst.value());
  }
  // We could emit unconditional eager deopts for other kinds of constant, but
  // it's not necessary, the appropriate checking conversion nodes will deopt.

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.int32()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kTagged: {
      if (can_be_heap_number && !CheckType(value, NodeType::kSmi)) {
        return alternative.set_int32(AddNewNode<CheckedNumberToInt32>({value}));
      }
      return alternative.set_int32(BuildSmiUntag(value));
    }
    case ValueRepresentation::kUint32: {
      if (node_info->is_smi()) {
        return alternative.set_int32(
            AddNewNode<TruncateUint32ToInt32>({value}));
      }
      return alternative.set_int32(AddNewNode<CheckedUint32ToInt32>({value}));
    }
    case ValueRepresentation::kFloat64:
    // The check here will also work for the hole NaN, so we can treat
    // HoleyFloat64 as Float64.
    case ValueRepresentation::kHoleyFloat64: {
      return alternative.set_int32(
          AddNewNode<CheckedTruncateFloat64ToInt32>({value}));
    }

    case ValueRepresentation::kInt32:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

std::optional<double> MaglevGraphBuilder::TryGetFloat64Constant(
    ValueNode* value, TaggedToFloat64ConversionType conversion_type) {
  switch (value->opcode()) {
    case Opcode::kConstant: {
      compiler::ObjectRef object = value->Cast<Constant>()->object();
      if (object.IsHeapNumber()) {
        return object.AsHeapNumber().value();
      }
      // Oddballs should be RootConstants.
      DCHECK(!IsOddball(*object.object()));
      return {};
    }
    case Opcode::kInt32Constant:
      return value->Cast<Int32Constant>()->value();
    case Opcode::kSmiConstant:
      return value->Cast<SmiConstant>()->value().value();
    case Opcode::kFloat64Constant:
      return value->Cast<Float64Constant>()->value().get_scalar();
    case Opcode::kRootConstant: {
      Tagged<Object> root_object =
          local_isolate_->root(value->Cast<RootConstant>()->index());
      if (conversion_type == TaggedToFloat64ConversionType::kNumberOrBoolean &&
          IsBoolean(root_object)) {
        return Cast<Oddball>(root_object)->to_number_raw();
      }
      if (conversion_type == TaggedToFloat64ConversionType::kNumberOrOddball &&
          IsOddball(root_object)) {
        return Cast<Oddball>(root_object)->to_number_raw();
      }
      if (IsHeapNumber(root_object)) {
        return Cast<HeapNumber>(root_object)->value();
      }
      return {};
    }
    default:
      break;
  }
  if (auto c = TryGetConstantAlternative(value)) {
    return TryGetFloat64Constant(*c, conversion_type);
  }
  return {};
}

ValueNode* MaglevGraphBuilder::GetFloat64(ValueNode* value) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kFloat64);
  return GetFloat64ForToNumber(value, NodeType::kNumber,
                               TaggedToFloat64ConversionType::kOnlyNumber);
}

ValueNode* MaglevGraphBuilder::GetFloat64ForToNumber(
    ValueNode* value, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  ValueRepresentation representation =
      value->properties().value_representation();
  if (representation == ValueRepresentation::kFloat64) return value;

  // Process constants first to avoid allocating NodeInfo for them.
  if (auto cst = TryGetFloat64Constant(value, conversion_type)) {
    return GetFloat64Constant(cst.value());
  }
  // We could emit unconditional eager deopts for other kinds of constant, but
  // it's not necessary, the appropriate checking conversion nodes will deopt.

  NodeInfo* node_info = GetOrCreateInfoFor(value);
  auto& alternative = node_info->alternative();

  if (ValueNode* alt = alternative.float64()) {
    return alt;
  }

  switch (representation) {
    case ValueRepresentation::kTagged: {
      auto combined_type = CombineType(allowed_input_type, node_info->type());
      if (NodeTypeIs(combined_type, NodeType::kSmi)) {
        // Get the float64 value of a Smi value its int32 representation.
        return GetFloat64(GetInt32(value));
      }
      if (NodeTypeIs(combined_type, NodeType::kNumber)) {
        // Number->Float64 conversions are exact alternatives, so they can
        // also become the canonical float64_alternative.
        return alternative.set_float64(BuildNumberOrOddballToFloat64(
            value, NodeType::kNumber,
            TaggedToFloat64ConversionType::kOnlyNumber));
      }
      if (NodeTypeIs(combined_type, NodeType::kNumberOrOddball)) {
        // NumberOrOddball->Float64 conversions are not exact alternatives,
        // since they lose the information that this is an oddball, so they
        // can only become the canonical float64_alternative if they are a
        // known number (and therefore not oddball).
        return BuildNumberOrOddballToFloat64(value, combined_type,
                                             conversion_type);
      }
      // The type is impossible. We could generate an unconditional deopt here,
      // but it's too invasive. So we just generate a check which will always
      // deopt.
      return BuildNumberOrOddballToFloat64(value, allowed_input_type,
                                           conversion_type);
    }
    case ValueRepresentation::kInt32:
      return alternative.set_float64(AddNewNode<ChangeInt32ToFloat64>({value}));
    case ValueRepresentation::kUint32:
      return alternative.set_float64(
          AddNewNode<ChangeUint32ToFloat64>({value}));
    case ValueRepresentation::kHoleyFloat64: {
      switch (allowed_input_type) {
        case NodeType::kSmi:
        case NodeType::kNumber:
        case NodeType::kNumberOrBoolean:
          // Number->Float64 conversions are exact alternatives, so they can
          // also become the canonical float64_alternative. The HoleyFloat64
          // representation can represent undefined but no other oddballs, so
          // booleans cannot occur here and kNumberOrBoolean can be grouped with
          // kNumber.
          return alternative.set_float64(
              AddNewNode<CheckedHoleyFloat64ToFloat64>({value}));
        case NodeType::kNumberOrOddball:
          // NumberOrOddball->Float64 conversions are not exact alternatives,
          // since they lose the information that this is an oddball, so they
          // cannot become the canonical float64_alternative.
          return AddNewNode<HoleyFloat64ToMaybeNanFloat64>({value});
        default:
          UNREACHABLE();
      }
    }
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  UNREACHABLE();
}

ValueNode* MaglevGraphBuilder::GetHoleyFloat64ForToNumber(
    ValueNode* value, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  RecordUseReprHintIfPhi(value, UseRepresentation::kHoleyFloat64);
  ValueRepresentation representation =
      value->properties().value_representation();
  // Ignore the hint for
  if (representation == ValueRepresentation::kHoleyFloat64) return value;
  return GetFloat64ForToNumber(value, allowed_input_type, conversion_type);
}

namespace {
int32_t ClampToUint8(int32_t value) {
  if (value < 0) return 0;
  if (value > 255) return 255;
  return value;
}
}  // namespace

ValueNode* MaglevGraphBuilder::GetUint8ClampedForToNumber(ValueNode* value) {
  switch (value->properties().value_representation()) {
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kTagged: {
      if (SmiConstant* constant = value->TryCast<SmiConstant>()) {
        return GetInt32Constant(ClampToUint8(constant->value().value()));
      }
      NodeInfo* info = known_node_aspects().TryGetInfoFor(value);
      if (info && info->alternative().int32()) {
        return AddNewNode<Int32ToUint8Clamped>({info->alternative().int32()});
      }
      return AddNewNode<CheckedNumberToUint8Clamped>({value});
    }
    // HoleyFloat64 is treated like Float64. ToNumber of undefined is anyway a
    // NaN, so we'll simply truncate away the NaN-ness of the hole, and don't
    // need to do extra oddball checks (though we'll miss updating the
    // feedback).
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      // TODO(leszeks): Handle Float64Constant, which requires the correct
      // rounding for clamping.
      return AddNewNode<Float64ToUint8Clamped>({value});
    case ValueRepresentation::kInt32:
      if (Int32Constant* constant = value->TryCast<Int32Constant>()) {
        return GetInt32Constant(ClampToUint8(constant->value()));
      }
      return AddNewNode<Int32ToUint8Clamped>({value});
    case ValueRepresentation::kUint32:
      return AddNewNode<Uint32ToUint8Clamped>({value});
  }
  UNREACHABLE();
}

namespace {
template <Operation kOperation>
struct NodeForOperationHelper;

#define NODE_FOR_OPERATION_HELPER(Name)               \
  template <>                                         \
  struct NodeForOperationHelper<Operation::k##Name> { \
    using generic_type = Generic##Name;               \
  };
OPERATION_LIST(NODE_FOR_OPERATION_HELPER)
#undef NODE_FOR_OPERATION_HELPER

template <Operation kOperation>
using GenericNodeForOperation =
    typename NodeForOperationHelper<kOperation>::generic_type;

// Bitwise operations reinterprets the numeric input as Int32 bits for a
// bitwise operation, which means we want to do slightly different conversions.
template <Operation kOperation>
constexpr bool BinaryOperationIsBitwiseInt32() {
  switch (kOperation) {
    case Operation::kBitwiseNot:
    case Operation::kBitwiseAnd:
    case Operation::kBitwiseOr:
    case Operation::kBitwiseXor:
    case Operation::kShiftLeft:
    case Operation::kShiftRight:
    case Operation::kShiftRightLogical:
      return true;
    default:
      return false;
  }
}
}  // namespace

// MAP_OPERATION_TO_NODES are tuples with the following format:
// - Operation name,
// - Int32 operation node,
// - Identity of int32 operation (e.g, 0 for add/sub and 1 for mul/div), if it
//   exists, or otherwise {}.
#define MAP_BINARY_OPERATION_TO_INT32_NODE(V) \
  V(Add, Int32AddWithOverflow, 0)             \
  V(Subtract, Int32SubtractWithOverflow, 0)   \
  V(Multiply, Int32MultiplyWithOverflow, 1)   \
  V(Divide, Int32DivideWithOverflow, 1)       \
  V(Modulus, Int32ModulusWithOverflow, {})    \
  V(BitwiseAnd, Int32BitwiseAnd, ~0)          \
  V(BitwiseOr, Int32BitwiseOr, 0)             \
  V(BitwiseXor, Int32BitwiseXor, 0)           \
  V(ShiftLeft, Int32ShiftLeft, 0)             \
  V(ShiftRight, Int32ShiftRight, 0)           \
  V(ShiftRightLogical, Int32ShiftRightLogical, {})

#define MAP_UNARY_OPERATION_TO_INT32_NODE(V) \
  V(BitwiseNot, Int32BitwiseNot)             \
  V(Increment, Int32IncrementWithOverflow)   \
  V(Decrement, Int32DecrementWithOverflow)   \
  V(Negate, Int32NegateWithOverflow)

// MAP_OPERATION_TO_FLOAT64_NODE are tuples with the following format:
// (Operation name, Float64 operation node).
#define MAP_OPERATION_TO_FLOAT64_NODE(V) \
  V(Add, Float64Add)                     \
  V(Subtract, Float64Subtract)           \
  V(Multiply, Float64Multiply)           \
  V(Divide, Float64Divide)               \
  V(Modulus, Float64Modulus)             \
  V(Exponentiate, Float64Exponentiate)

template <Operation kOperation>
static constexpr std::optional<int> Int32Identity() {
  switch (kOperation) {
#define CASE(op, _, identity) \
  case Operation::k##op:      \
    return identity;
    MAP_BINARY_OPERATION_TO_INT32_NODE(CASE)
#undef CASE
    default:
      UNREACHABLE();
  }
}

namespace {
template <Operation kOperation>
struct Int32NodeForHelper;
#define SPECIALIZATION(op, OpNode, ...)         \
  template <>                                   \
  struct Int32NodeForHelper<Operation::k##op> { \
    using type = OpNode;                        \
  };
MAP_UNARY_OPERATION_TO_INT32_NODE(SPECIALIZATION)
MAP_BINARY_OPERATION_TO_INT32_NODE(SPECIALIZATION)
#undef SPECIALIZATION

template <Operation kOperation>
using Int32NodeFor = typename Int32NodeForHelper<kOperation>::type;

template <Operation kOperation>
struct Float64NodeForHelper;
#define SPECIALIZATION(op, OpNode)                \
  template <>                                     \
  struct Float64NodeForHelper<Operation::k##op> { \
    using type = OpNode;                          \
  };
MAP_OPERATION_TO_FLOAT64_NODE(SPECIALIZATION)
#undef SPECIALIZATION

template <Operation kOperation>
using Float64NodeFor = typename Float64NodeForHelper<kOperation>::type;
}  // namespace

template <Operation kOperation>
void MaglevGraphBuilder::BuildGenericUnaryOperationNode() {
  FeedbackSlot slot_index = GetSlotOperand(0);
  ValueNode* value = GetAccumulator();
  SetAccumulator(AddNewNode<GenericNodeForOperation<kOperation>>(
      {value}, compiler::FeedbackSource{feedback(), slot_index}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildGenericBinaryOperationNode() {
  ValueNode* left = LoadRegister(0);
  ValueNode* right = GetAccumulator();
  FeedbackSlot slot_index = GetSlotOperand(1);
  SetAccumulator(AddNewNode<GenericNodeForOperation<kOperation>>(
      {left, right}, compiler::FeedbackSource{feedback(), slot_index}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildGenericBinarySmiOperationNode() {
  ValueNode* left = GetAccumulator();
  int constant = iterator_.GetImmediateOperand(0);
  ValueNode* right = GetSmiConstant(constant);
  FeedbackSlot slot_index = GetSlotOperand(1);
  SetAccumulator(AddNewNode<GenericNodeForOperation<kOperation>>(
      {left, right}, compiler::FeedbackSource{feedback(), slot_index}));
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldInt32UnaryOperation(ValueNode* node) {
  auto cst = TryGetInt32Constant(node);
  if (!cst.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kBitwiseNot:
      return GetInt32Constant(~cst.value());
    case Operation::kIncrement:
      if (cst.value() < INT32_MAX) {
        return GetInt32Constant(cst.value() + 1);
      }
      return ReduceResult::Fail();
    case Operation::kDecrement:
      if (cst.value() > INT32_MIN) {
        return GetInt32Constant(cst.value() - 1);
      }
      return ReduceResult::Fail();
    case Operation::kNegate:
      if (cst.value() == 0) {
        return ReduceResult::Fail();
      }
      if (cst.value() != INT32_MIN) {
        return GetInt32Constant(-cst.value());
      }
      return ReduceResult::Fail();
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildInt32UnaryOperationNode() {
  // Use BuildTruncatingInt32BitwiseNotForToNumber with Smi input hint
  // for truncating operations.
  static_assert(!BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* value = GetAccumulator();
  PROCESS_AND_RETURN_IF_DONE(TryFoldInt32UnaryOperation<kOperation>(value),
                             SetAccumulator);
  using OpNodeT = Int32NodeFor<kOperation>;
  SetAccumulator(AddNewNode<OpNodeT>({value}));
}

void MaglevGraphBuilder::BuildTruncatingInt32BitwiseNotForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  ValueNode* value =
      GetTruncatedInt32ForToNumber(current_interpreter_frame_.accumulator(),
                                   allowed_input_type, conversion_type);
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32UnaryOperation<Operation::kBitwiseNot>(value),
      SetAccumulator);
  SetAccumulator(AddNewNode<Int32BitwiseNot>({value}));
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldInt32BinaryOperation(ValueNode* left,
                                                             ValueNode* right) {
  auto cst_right = TryGetInt32Constant(right);
  if (!cst_right.has_value()) return ReduceResult::Fail();
  return TryFoldInt32BinaryOperation<kOperation>(left, cst_right.value());
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldInt32BinaryOperation(
    ValueNode* left, int32_t cst_right) {
  auto cst_left = TryGetInt32Constant(left);
  if (!cst_left.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kAdd: {
      int64_t result = static_cast<int64_t>(cst_left.value()) +
                       static_cast<int64_t>(cst_right);
      if (result >= INT32_MIN && result <= INT32_MAX) {
        return GetInt32Constant(static_cast<int32_t>(result));
      }
      return ReduceResult::Fail();
    }
    case Operation::kSubtract: {
      int64_t result = static_cast<int64_t>(cst_left.value()) -
                       static_cast<int64_t>(cst_right);
      if (result >= INT32_MIN && result <= INT32_MAX) {
        return GetInt32Constant(static_cast<int32_t>(result));
      }
      return ReduceResult::Fail();
    }
    case Operation::kMultiply: {
      int64_t result = static_cast<int64_t>(cst_left.value()) *
                       static_cast<int64_t>(cst_right);
      if (result >= INT32_MIN && result <= INT32_MAX) {
        return GetInt32Constant(static_cast<int32_t>(result));
      }
      return ReduceResult::Fail();
    }
    case Operation::kModulus:
      // TODO(v8:7700): Constant fold mod.
      return ReduceResult::Fail();
    case Operation::kDivide:
      // TODO(v8:7700): Constant fold division.
      return ReduceResult::Fail();
    case Operation::kBitwiseAnd:
      return GetInt32Constant(cst_left.value() & cst_right);
    case Operation::kBitwiseOr:
      return GetInt32Constant(cst_left.value() | cst_right);
    case Operation::kBitwiseXor:
      return GetInt32Constant(cst_left.value() ^ cst_right);
    case Operation::kShiftLeft:
      return GetInt32Constant(cst_left.value()
                              << (static_cast<uint32_t>(cst_right) % 32));
    case Operation::kShiftRight:
      return GetInt32Constant(cst_left.value() >>
                              (static_cast<uint32_t>(cst_right) % 32));
    case Operation::kShiftRightLogical:
      return GetUint32Constant(static_cast<uint32_t>(cst_left.value()) >>
                               (static_cast<uint32_t>(cst_right) % 32));
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildInt32BinaryOperationNode() {
  // Use BuildTruncatingInt32BinaryOperationNodeForToNumber with Smi input hint
  // for truncating operations.
  static_assert(!BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left = LoadRegister(0);
  ValueNode* right = GetAccumulator();
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, right), SetAccumulator);
  using OpNodeT = Int32NodeFor<kOperation>;
  SetAccumulator(AddNewNode<OpNodeT>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildTruncatingInt32BinaryOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  static_assert(BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left;
  ValueNode* right;
  if (IsRegisterEqualToAccumulator(0)) {
    left = right = GetTruncatedInt32ForToNumber(
        current_interpreter_frame_.get(iterator_.GetRegisterOperand(0)),
        allowed_input_type, conversion_type);
  } else {
    left = GetTruncatedInt32ForToNumber(
        current_interpreter_frame_.get(iterator_.GetRegisterOperand(0)),
        allowed_input_type, conversion_type);
    right =
        GetTruncatedInt32ForToNumber(current_interpreter_frame_.accumulator(),
                                     allowed_input_type, conversion_type);
  }
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, right), SetAccumulator);
  SetAccumulator(AddNewNode<Int32NodeFor<kOperation>>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildInt32BinarySmiOperationNode() {
  // Truncating Int32 nodes treat their input as a signed int32 regardless
  // of whether it's really signed or not, so we allow Uint32 by loading a
  // TruncatedInt32 value.
  static_assert(!BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left = GetAccumulator();
  int32_t constant = iterator_.GetImmediateOperand(0);
  if (std::optional<int>(constant) == Int32Identity<kOperation>()) {
    // Deopt if {left} is not an Int32.
    EnsureInt32(left);
    // If the constant is the unit of the operation, it already has the right
    // value, so just return.
    return;
  }
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, constant), SetAccumulator);
  ValueNode* right = GetInt32Constant(constant);
  using OpNodeT = Int32NodeFor<kOperation>;
  SetAccumulator(AddNewNode<OpNodeT>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildTruncatingInt32BinarySmiOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  static_assert(BinaryOperationIsBitwiseInt32<kOperation>());
  ValueNode* left =
      GetTruncatedInt32ForToNumber(current_interpreter_frame_.accumulator(),
                                   allowed_input_type, conversion_type);
  int32_t constant = iterator_.GetImmediateOperand(0);
  if (std::optional<int>(constant) == Int32Identity<kOperation>()) {
    // If the constant is the unit of the operation, it already has the right
    // value, so use the truncated value (if not just a conversion) and return.
    if (!left->properties().is_conversion()) {
      current_interpreter_frame_.set_accumulator(left);
    }
    return;
  }
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldInt32BinaryOperation<kOperation>(left, constant), SetAccumulator);
  ValueNode* right = GetInt32Constant(constant);
  SetAccumulator(AddNewNode<Int32NodeFor<kOperation>>({left, right}));
}

ValueNode* MaglevGraphBuilder::GetNumberConstant(double constant) {
  if (IsSmiDouble(constant)) {
    return GetInt32Constant(FastD2I(constant));
  }
  return GetFloat64Constant(constant);
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldFloat64UnaryOperationForToNumber(
    TaggedToFloat64ConversionType conversion_type, ValueNode* value) {
  auto cst = TryGetFloat64Constant(value, conversion_type);
  if (!cst.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kNegate:
      return GetNumberConstant(-cst.value());
    case Operation::kIncrement:
      return GetNumberConstant(cst.value() + 1);
    case Operation::kDecrement:
      return GetNumberConstant(cst.value() - 1);
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldFloat64BinaryOperationForToNumber(
    TaggedToFloat64ConversionType conversion_type, ValueNode* left,
    ValueNode* right) {
  auto cst_right = TryGetFloat64Constant(right, conversion_type);
  if (!cst_right.has_value()) return ReduceResult::Fail();
  return TryFoldFloat64BinaryOperationForToNumber<kOperation>(
      conversion_type, left, cst_right.value());
}

template <Operation kOperation>
ReduceResult MaglevGraphBuilder::TryFoldFloat64BinaryOperationForToNumber(
    TaggedToFloat64ConversionType conversion_type, ValueNode* left,
    double cst_right) {
  auto cst_left = TryGetFloat64Constant(left, conversion_type);
  if (!cst_left.has_value()) return ReduceResult::Fail();
  switch (kOperation) {
    case Operation::kAdd:
      return GetNumberConstant(cst_left.value() + cst_right);
    case Operation::kSubtract:
      return GetNumberConstant(cst_left.value() - cst_right);
    case Operation::kMultiply:
      return GetNumberConstant(cst_left.value() * cst_right);
    case Operation::kDivide:
      return GetNumberConstant(cst_left.value() / cst_right);
    case Operation::kModulus:
      // TODO(v8:7700): Constant fold mod.
      return ReduceResult::Fail();
    case Operation::kExponentiate:
      return GetNumberConstant(math::pow(cst_left.value(), cst_right));
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildFloat64BinarySmiOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  // TODO(v8:7700): Do constant identity folding. Make sure to normalize
  // HoleyFloat64 nodes if folded.
  ValueNode* left = GetAccumulatorHoleyFloat64ForToNumber(allowed_input_type,
                                                          conversion_type);
  double constant = static_cast<double>(iterator_.GetImmediateOperand(0));
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldFloat64BinaryOperationForToNumber<kOperation>(conversion_type,
                                                           left, constant),
      SetAccumulator);
  ValueNode* right = GetFloat64Constant(constant);
  SetAccumulator(AddNewNode<Float64NodeFor<kOperation>>({left, right}));
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildFloat64UnaryOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  // TODO(v8:7700): Do constant identity folding. Make sure to normalize
  // HoleyFloat64 nodes if folded.
  ValueNode* value = GetAccumulatorHoleyFloat64ForToNumber(allowed_input_type,
                                                           conversion_type);
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldFloat64UnaryOperationForToNumber<kOperation>(conversion_type,
                                                          value),
      SetAccumulator);
  switch (kOperation) {
    case Operation::kNegate:
      SetAccumulator(AddNewNode<Float64Negate>({value}));
      break;
    case Operation::kIncrement:
      SetAccumulator(AddNewNode<Float64Add>({value, GetFloat64Constant(1)}));
      break;
    case Operation::kDecrement:
      SetAccumulator(
          AddNewNode<Float64Subtract>({value, GetFloat64Constant(1)}));
      break;
    default:
      UNREACHABLE();
  }
}

template <Operation kOperation>
void MaglevGraphBuilder::BuildFloat64BinaryOperationNodeForToNumber(
    NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  // TODO(v8:7700): Do constant identity folding. Make sure to normalize
  // HoleyFloat64 nodes if folded.
  ValueNode* left = LoadRegisterHoleyFloat64ForToNumber(0, allowed_input_type,
                                                        conversion_type);
  ValueNode* right = GetAccumulatorHoleyFloat64ForToNumber(allowed_input_type,
                                                           conversion_type);
  PROCESS_AND_RETURN_IF_DONE(
      TryFoldFloat64BinaryOperationForToNumber<kOperation>(conversion_type,
                                                           left, right),
      SetAccumulator);
  SetAccumulator(AddNewNode<Float64NodeFor<kOperation>>({left, right}));
}

namespace {
std::tuple<NodeType, TaggedToFloat64ConversionType>
BinopHintToNodeTypeAndConversionType(BinaryOperationHint hint) {
  switch (hint) {
    case BinaryOperationHint::kSignedSmall:
      return std::make_tuple(NodeType::kSmi,
                             TaggedToFloat64ConversionType::kOnlyNumber);
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
      return std::make_tuple(NodeType::kNumber,
                             TaggedToFloat64ConversionType::kOnlyNumber);
    case BinaryOperationHint::kNumberOrOddball:
      return std::make_tuple(NodeType::kNumberOrOddball,
                             TaggedToFloat64ConversionType::kNumberOrOddball);
    case BinaryOperationHint::kNone:
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      UNREACHABLE();
  }
}
}  // namespace

template <Operation kOperation>
void MaglevGraphBuilder::VisitUnaryOperation() {
  FeedbackNexus nexus = FeedbackNexusForOperand(0);
  BinaryOperationHint feedback_hint = nexus.GetBinaryOperationFeedback();
  switch (feedback_hint) {
    case BinaryOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForBinaryOperation));
    case BinaryOperationHint::kSignedSmall:
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kNumberOrOddball: {
      auto |allowed_input_type, conversion_type] =
          BinopHintToNodeTypeAndConversionType(feedback_hint);
      if constexpr (BinaryOperationIsBitwiseInt32<kOperation>()) {
        static_assert(kOperation == Operation::kBitwiseNot);
        return BuildTruncatingInt32BitwiseNotForToNumber(allowed_input_type,
                                                         conversion_type);
      } else if (feedback_hint == BinaryOperationHint::kSignedSmall) {
        return BuildInt32UnaryOperationNode<kOperation>();
      }
      return BuildFloat64UnaryOperationNodeForToNumber<kOperation>(
          allowed_input_type, conversion_type);
      break;
    }
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      // Fallback to generic node.
      break;
  }
  BuildGenericUnaryOperationNode<kOperation>();
}

template <Operation kOperation>
void MaglevGraphBuilder::VisitBinaryOperation() {
  FeedbackNexus nexus = FeedbackNexusForOperand(1);
  BinaryOperationHint feedback_hint = nexus.GetBinaryOperationFeedback();
  switch (feedback_hint) {
    case BinaryOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForBinaryOperation));
    case BinaryOperationHint::kSignedSmall:
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kNumberOrOddball: {
      auto |allowed_input_type, conversion_type] =
          BinopHintToNodeTypeAndConversionType(feedback_hint);
      if constexpr (BinaryOperationIsBitwiseInt32<kOperation>()) {
        return BuildTruncatingInt32BinaryOperationNodeForToNumber<kOperation>(
            allowed_input_type, conversion_type);
      } else if (feedback_hint == BinaryOperationHint::kSignedSmall) {
        if constexpr (kOperation == Operation::kExponentiate) {
          // Exponentiate never updates the feedback to be a Smi.
          UNREACHABLE();
        } else {
          return BuildInt32BinaryOperationNode<kOperation>();
        }
      } else {
        return BuildFloat64BinaryOperationNodeForToNumber<kOperation>(
            allowed_input_type, conversion_type);
      }
      break;
    }
    case BinaryOperationHint::kString:
      if constexpr (kOperation == Operation::kAdd) {
        ValueNode* left = LoadRegister(0);
        ValueNode* right = GetAccumulator();
        if (RootConstant* root_constant = left->TryCast<RootConstant>()) {
          if (root_constant->index() == RootIndex::kempty_string) {
            BuildCheckString(right);
            // The right side is already in the accumulator register.
            return;
          }
        }
        if (RootConstant* root_constant = right->TryCast<RootConstant>()) {
          if (root_constant->index() == RootIndex::kempty_string) {
            BuildCheckString(left);
            MoveNodeBetweenRegisters(
                iterator_.GetRegisterOperand(0),
                interpreter::Register::virtual_accumulator());
            return;
          }
        }
        BuildCheckString(left);
        BuildCheckString(right);
        SetAccumulator(AddNewNode<StringConcat>({left, right}));
        return;
      }
      break;
    case BinaryOperationHint::kStringOrStringWrapper:
      if constexpr (kOperation == Operation::kAdd) {
        if (broker()
                ->dependencies()
                ->DependOnStringWrapperToPrimitiveProtector()) {
          ValueNode* left = LoadRegister(0);
          ValueNode* right = GetAccumulator();
          BuildCheckStringOrStringWrapper(left);
          BuildCheckStringOrStringWrapper(right);
          SetAccumulator(AddNewNode<StringWrapperConcat>({left, right}));
          return;
        }
      }
      ||fallthrough]];
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      // Fallback to generic node.
      break;
  }
  BuildGenericBinaryOperationNode<kOperation>();
}

template <Operation kOperation>
void MaglevGraphBuilder::VisitBinarySmiOperation() {
  FeedbackNexus nexus = FeedbackNexusForOperand(1);
  BinaryOperationHint feedback_hint = nexus.GetBinaryOperationFeedback();
  switch (feedback_hint) {
    case BinaryOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForBinaryOperation));
    case BinaryOperationHint::kSignedSmall:
    case BinaryOperationHint::kSignedSmallInputs:
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kNumberOrOddball: {
      const auto |allowed_input_type, conversion_type] =
          BinopHintToNodeTypeAndConversionType(feedback_hint);
      if constexpr (BinaryOperationIsBitwiseInt32<kOperation>()) {
        return BuildTruncatingInt32BinarySmiOperationNodeForToNumber<
            kOperation>(allowed_input_type, conversion_type);
      } else if (feedback_hint == BinaryOperationHint::kSignedSmall) {
        if constexpr (kOperation == Operation::kExponentiate) {
          // Exponentiate never updates the feedback to be a Smi.
          UNREACHABLE();
        } else {
          return BuildInt32BinarySmiOperationNode<kOperation>();
        }
      } else {
        return BuildFloat64BinarySmiOperationNodeForToNumber<kOperation>(
            allowed_input_type, conversion_type);
      }
      break;
    }
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
    case BinaryOperationHint::kAny:
      // Fallback to generic node.
      break;
  }
  BuildGenericBinarySmiOperationNode<kOperation>();
}

template <Operation kOperation, typename type>
bool OperationValue(type left, type right) {
  switch (kOperation) {
    case Operation::kEqual:
    case Operation::kStrictEqual:
      return left == right;
    case Operation::kLessThan:
      return left < right;
    case Operation::kLessThanOrEqual:
      return left <= right;
    case Operation::kGreaterThan:
      return left > right;
    case Operation::kGreaterThanOrEqual:
      return left >= right;
  }
}

// static
compiler::OptionalHeapObjectRef MaglevGraphBuilder::TryGetConstant(
    compiler::JSHeapBroker* broker, LocalIsolate* isolate, ValueNode* node) {
  if (Constant* c = node->TryCast<Constant>()) {
    return c->object();
  }
  if (RootConstant* c = node->TryCast<RootConstant>()) {
    return MakeRef(broker, isolate->root_handle(c->index())).AsHeapObject();
  }
  return {};
}

compiler::OptionalHeapObjectRef MaglevGraphBuilder::TryGetConstant(
    ValueNode* node, ValueNode** constant_node) {
  if (auto result = TryGetConstant(broker(), local_isolate(), node)) {
    if (constant_node) *constant_node = node;
    return result;
  }
  if (auto c = TryGetConstantAlternative(node)) {
    return TryGetConstant(*c, constant_node);
  }
  return {};
}

std::optional<ValueNode*> MaglevGraphBuilder::TryGetConstantAlternative(
    ValueNode* node) {
  const NodeInfo* info = known_node_aspects().TryGetInfoFor(node);
  if (info) {
    if (auto c = info->alternative().checked_value()) {
      if (IsConstantNode(c->opcode())) {
        return c;
      }
    }
  }
  return {};
}

template <Operation kOperation>
bool MaglevGraphBuilder::TryReduceCompareEqualAgainstConstant() {
  if (kOperation != Operation::kStrictEqual && kOperation != Operation::kEqual)
    return false;

  ValueNode* left = LoadRegister(0);
  ValueNode* right = GetAccumulator();

  ValueNode* other = right;
  compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(left);
  if (!maybe_constant) {
    maybe_constant = TryGetConstant(right);
    other = left;
  }
  if (!maybe_constant) return false;

  if (CheckType(other, NodeType::kBoolean)) {
    auto CompareOtherWith = |&](bool constant) {
      compiler::OptionalHeapObjectRef const_other = TryGetConstant(other);
      if (const_other) {
        auto bool_other = const_other->TryGetBooleanValue(broker());
        if (bool_other.has_value()) {
          SetAccumulator(GetBooleanConstant(constant == *bool_other));
          return;
        }
      }
      if (constant) {
        SetAccumulator(other);
      } else {
        SetAccumulator(AddNewNode<LogicalNot>({other}));
      }
    };

    if (maybe_constant.equals(broker_->true_value())) {
      CompareOtherWith(true);
      return true;
    } else if (maybe_constant.equals(broker_->false_value())) {
      CompareOtherWith(false);
      return true;
    } else if (kOperation == Operation::kEqual) {
      // For `bool == num` we can convert the actual comparison `ToNumber(bool)
      // == num` into `(num == 1) ? bool : ((num == 0) ? !bool : false)`,
      std::optional<double> val = {};
      if (maybe_constant.value().IsSmi()) {
        val = maybe_constant.value().AsSmi();
      } else if (maybe_constant.value().IsHeapNumber()) {
        val = maybe_constant.value().AsHeapNumber().value();
      }
      if (val) {
        if (*val == 0) {
          CompareOtherWith(false);
        } else if (*val == 1) {
          CompareOtherWith(true);
        } else {
          // The constant number is neither equal to `ToNumber(true)` nor
          // `ToNumber(false)`.
          SetAccumulator(GetBooleanConstant(false));
        }
        return true;
      }
    }
  }

  if (kOperation != Operation::kStrictEqual) return false;

  InstanceType type = maybe_constant.value().map(broker()).instance_type();
  if (!InstanceTypeChecker::IsReferenceComparable(type)) return false;

  // If the constant is the undefined value, we can compare it
  // against holey floats.
  if (maybe_constant->IsUndefined()) {
    ValueNode* holey_float = nullptr;
    if (left->properties().value_representation() ==
        ValueRepresentation::kHoleyFloat64) {
      holey_float = left;
    } else if (right->properties().value_representation() ==
               ValueRepresentation::kHoleyFloat64) {
      holey_float = right;
    }
    if (holey_float) {
      SetAccumulator(AddNewNode<HoleyFloat64IsHole>({holey_float}));
      return true;
    }
  }

  if (left->properties().value_representation() !=
          ValueRepresentation::kTagged ||
      right->properties().value_representation() !=
          ValueRepresentation::kTagged) {
    SetAccumulator(GetBooleanConstant(false));
  } else {
    SetAccumulator(BuildTaggedEqual(left, right));
  }
  return true;
}

template <Operation kOperation>
void MaglevGraphBuilder::VisitCompareOperation() {
  if (TryReduceCompareEqualAgainstConstant<kOperation>()) return;

  // Compare opcodes are not always commutative. We sort the ones which are for
  // better CSE coverage.
  auto SortCommute = |](ValueNode*& left, ValueNode*& right) {
    if (!v8_flags.maglev_cse) return;
    if (kOperation != Operation::kEqual &&
        kOperation != Operation::kStrictEqual) {
      return;
    }
    if (left > right) {
      std::swap(left, right);
    }
  };

  auto TryConstantFoldInt32 = |&](ValueNode* left, ValueNode* right) {
    if (left->Is<Int32Constant>() && right->Is<Int32Constant>()) {
      int left_value = left->Cast<Int32Constant>()->value();
      int right_value = right->Cast<Int32Constant>()->value();
      SetAccumulator(GetBooleanConstant(
          OperationValue<kOperation>(left_value, right_value)));
      return true;
    }
    return false;
  };

  auto TryConstantFoldEqual = |&](ValueNode* left, ValueNode* right) {
    if (left == right) {
      SetAccumulator(
          GetBooleanConstant(kOperation == Operation::kEqual ||
                             kOperation == Operation::kStrictEqual ||
                             kOperation == Operation::kLessThanOrEqual ||
                             kOperation == Operation::kGreaterThanOrEqual));
      return true;
    }
    return false;
  };

  auto MaybeOddballs = |&]() {
    auto MaybeOddball = |&](ValueNode* value) {
      ValueRepresentation rep = value->value_representation();
      switch (rep) {
        case ValueRepresentation::kInt32:
        case ValueRepresentation::kUint32:
        case ValueRepresentation::kFloat64:
          return false;
        default:
          break;
      }
      return !CheckType(value, NodeType::kNumber);
    };
    return MaybeOddball(LoadRegister(0)) || MaybeOddball(GetAccumulator());
  };

  FeedbackNexus nexus = FeedbackNexusForOperand(1);
  switch (nexus.GetCompareOperationFeedback()) {
    case CompareOperationHint::kNone:
      RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
          DeoptimizeReason::kInsufficientTypeFeedbackForCompareOperation));

    case CompareOperationHint::kSignedSmall: {
      // TODO(victorgomes): Add a smart equality operator, that compares for
      // constants in different representations.
      ValueNode* left = GetInt32(LoadRegister(0));
      ValueNode* right = GetInt32(GetAccumulator());
      if (TryConstantFoldEqual(left, right)) return;
      if (TryConstantFoldInt32(left, right)) return;
      SortCommute(left, right);
      SetAccumulator(AddNewNode<Int32Compare>({left, right}, kOperation));
      return;
    }
    case CompareOperationHint::kNumberOrOddball:
      // TODO(leszeks): we could support all kNumberOrOddball with
      // BranchIfFloat64Compare, but we'd need to special case comparing
      // oddballs with NaN value (e.g. undefined) against themselves.
      if (MaybeOddballs()) {
        break;
      }
      ||fallthrough]];
    case CompareOperationHint::kNumberOrBoolean:
      if (kOperation == Operation::kStrictEqual && MaybeOddballs()) {
        break;
      }
      ||fallthrough]];
    case CompareOperationHint::kNumber: {
      ValueNode* left = LoadRegister(0);
      ValueNode* right = GetAccumulator();
      if (left->value_representation() == ValueRepresentation::kInt32 &&
          right->value_representation() == ValueRepresentation::kInt32) {
        if (TryConstantFoldEqual(left, right)) return;
        if (TryConstantFoldInt32(left, right)) return;
        SortCommute(left, right);
        SetAccumulator(AddNewNode<Int32Compare>({left, right}, kOperation));
        return;
      }
      // In compare operations, booleans should be converted to Float64 but
      // non-boolean oddballs shouldn't. Even if the feedback type was
      // kNumberOrOddball, we'd still pass
      // TaggedToFloat64ConversionType::kNumberOrBoolean.
      NodeType allowed_input_type;
      TaggedToFloat64ConversionType conversion_type;
      if (nexus.GetCompareOperationFeedback() ==
          CompareOperationHint::kNumberOrBoolean) {
        allowed_input_type = NodeType::kNumberOrBoolean;
        conversion_type = TaggedToFloat64ConversionType::kNumberOrBoolean;
      } else {
        allowed_input_type = NodeType::kNumber;
        conversion_type = TaggedToFloat64ConversionType::kOnlyNumber;
      }
      left = GetFloat64ForToNumber(left, allowed_input_type, conversion_type);
      right = GetFloat64ForToNumber(right, allowed_input_type, conversion_type);
      if (left->Is<Float64Constant>() && right->Is<Float64Constant>()) {
        double left_value = left->Cast<Float64Constant>()->value().get_scalar();
        double right_value =
            right->Cast<Float64Constant>()->value().get_scalar();
        SetAccumulator(GetBooleanConstant(
            OperationValue<kOperation>(left_value, right_value)));
        return;
      }
      SortCommute(left, right);
      SetAccumulator(AddNewNode<Float64Compare>({left, right}, kOperation));
      return;
    }
    case CompareOperationHint::kInternalizedString: {
      DCHECK(kOperation == Operation::kEqual ||
             kOperation == Operation::kStrictEqual);
      ValueNode *left, *right;
      if (IsRegisterEqualToAccumulator(0)) {
        left = right = GetInternalizedString(iterator_.GetRegisterOperand(0));
        SetAccumulator(GetRootConstant(RootIndex::kTrueValue));
        return;
      }
      left = GetInternalizedString(iterator_.GetRegisterOperand(0));
      right =
          GetInternalizedString(interpreter::Register::virtual_accumulator());
      if (TryConstantFoldEqual(left, right)) return;
      SetAccumulator(BuildTaggedEqual(left, right));
      return;
    }
    case CompareOperationHint::kSymbol: {
      DCHECK(kOperation == Operation::kEqual ||
             kOperation == Operation::kStrictEqual);

      ValueNode* left = LoadRegister(0);
      ValueNode* right = GetAccumulator();
      BuildCheckSymbol(left);
      BuildCheckSymbol(right);
      if (TryConstantFoldEqual(left, right)) return;
      SetAccumulator(BuildTaggedEqual(left, right));
      return;
    }
    case CompareOperationHint::kString: {
      ValueNode* left = LoadRegister(0);
      ValueNode* right = GetAccumulator();
      BuildCheckString(left);
      BuildCheckString(right);

      ValueNode* result;
      if (TryConstantFoldEqual(left, right)) return;
      ValueNode* tagged_left = GetTaggedValue(left);
      ValueNode* tagged_right = GetTaggedValue(right);
      switch (kOperation) {
        case Operation::kEqual:
        case Operation::kStrictEqual:
          result = AddNewNode<StringEqual>({tagged_left, tagged_right});
          break;
        case Operation::kLessThan:
          result = BuildCallBuiltin<Builtin::kStringLessThan>(
              {tagged_left, tagged_right});
          break;
        case Operation::kLessThanOrEqual:
          result = BuildCallBuiltin<Builtin::kStringLessThanOrEqual>(
              {tagged_left, tagged_right});
          break;
        case Operation::kGreaterThan:
          result = BuildCallBuiltin<Builtin::kStringGreaterThan>(
              {tagged_left, tagged_right});
          break;
        case Operation::kGreaterThanOrEqual:
          result = BuildCallBuiltin<Builtin::kStringGreaterThanOrEqual>(
              {tagged_left, tagged_right});
          break;
      }

      SetAccumulator(result);
      return;
    }
    case CompareOperationHint::kAny:
    case CompareOperationHint::kBigInt64:
    case CompareOperationHint::kBigInt:
    case CompareOperationHint::kReceiverOrNullOrUndefined:
      break;
    case CompareOperationHint::kReceiver: {
      DCHECK(kOperation == Operation::kEqual ||
             kOperation == Operation::kStrictEqual);

      ValueNode* left = LoadRegister(0);
[Step 12: Duration 29.33 seconds| Input tokens: 293,576 | Output tokens: 7,096]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 13 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'BuildLoadJSArrayLength\\(', 'options': '-n   │
│ --hidden --no-ignore-vcs'}                                                                        │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern':                                               │
│ 'UnsafeSmiUntag|UnsafeSmiTag|CheckedSmiUntag', 'options': '-n --hidden --no-ignore-vcs maglev/'}  │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc',         │
│ 'section': 5}                                                                                     │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc',         │
│ 'section': 4}                                                                                     │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-ir.cc', 'section': 2}      │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs maglev/ 'UnsafeSmiUntag|UnsafeSmiTag|CheckedSmiUntag' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
╭───────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-ir.cc'Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-ir.h', 'section': 4}       │
Observations: rg: UnsafeSmiUntag|UnsafeSmiTag|CheckedSmiUntag: IO error for operation on 
UnsafeSmiUntag|UnsafeSmiTag|CheckedSmiUntag: No such file or directory (os error 2)
Command: cd /Users/tarun/v8/v8/src && rg -n --hidden --no-ignore-vcs 'BuildLoadJSArrayLength\(' | head -n 1000
Observations: maglev/maglev-graph-builder.cc:5402:ValueNode* 

maglev/maglev-graph-builder.cc:6183:  ValueNode* length = is_jsarray ? 
Command: cd /Users/tarun/v8/v8/src && sed -n '9001,12000p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
maglev/maglev-graph-builder.cc:6269:      length = GetInt32(BuildLoadJSArrayLength(object));

maglev/maglev-graph-builder.cc:8311:    ValueNode* current_length = BuildLoadJSArrayLength(receiver);
MaglevGraphBuilder::BuildLoadJSArrayLength(ValueNode* js_array,
maglev/maglev-graph-builder.cc:8123:  ValueNode* original_length = BuildLoadJSArrayLength(receiver);
GetInt32(BuildLoadJSArrayLength(object))
GetSmiValue(BuildLoadJSArrayLength(receiver)));
maglev/maglev-graph-builder.cc:9161:                     
GetSmiValue(BuildLoadJSArrayLength(receiver)));
maglev/maglev-graph-builder.h:2276:  ValueNode* BuildLoadJSArrayLength(ValueNode* js_array,
GetUint32ElementIndex(BuildLoadJSArrayLength(
maglev/maglev-graph-builder.cc:9009:                     
maglev/maglev-graph-builder.cc:8410:                     
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-ir.h'
Observations: Showing section 4/5 (lines 9001-12000) of 'maglev/maglev-graph-builder.cc':

  std::optional<MaglevSubGraphBuilder::Label> do_return;
  if (unique_kind_count > 1) {
    do_return.emplace(&sub_graph, unique_kind_count);
  }

  ValueNode* old_array_length_smi;
  GET_VALUE_OR_ABORT(old_array_length_smi,
                     GetSmiValue(BuildLoadJSArrayLength(receiver)));
  ValueNode* old_array_length =
      AddNewNode<UnsafeSmiUntag>({old_array_length_smi});
  ValueNode* new_array_length_smi =
      AddNewNode<CheckedSmiIncrement>({old_array_length_smi});

  ValueNode* elements_array = BuildLoadElements(receiver);
  ValueNode* elements_array_length = BuildLoadFixedArrayLength(elements_array);

  auto build_array_push = |&](ElementsKind kind) {
    ValueNode* value;
    GET_VALUE_OR_ABORT(value, ConvertForStoring(args|0], kind));

    ValueNode* writable_elements_array = AddNewNode<MaybeGrowFastElements>(
        {elements_array, receiver, old_array_length, elements_array_length},
        kind);

    AddNewNode<StoreTaggedFieldNoWriteBarrier>({receiver, new_array_length_smi},
                                               JSArray::kLengthOffset,
                                               StoreTaggedMode::kDefault);

    // Do the store
    if (IsDoubleElementsKind(kind)) {
      BuildStoreFixedDoubleArrayElement(writable_elements_array,
                                        old_array_length, value);
    } else {
      DCHECK(IsSmiElementsKind(kind) || IsObjectElementsKind(kind));
      BuildStoreFixedArrayElement(writable_elements_array, old_array_length,
                                  value);
    }
    return ReduceResult::Done();
  };

  RETURN_IF_ABORT(BuildJSArrayBuiltinMapSwitchOnElementsKind(
      receiver, map_kinds, sub_graph, do_return, unique_kind_count,
      index_to_elements_kind, build_array_push));

  if (do_return.has_value()) {
    sub_graph.Bind(&*do_return);
  }
  RecordKnownProperty(receiver, broker()->length_string(), new_array_length_smi,
                      false, compiler::AccessMode::kStore);
  return new_array_length_smi;
}

ReduceResult MaglevGraphBuilder::TryReduceArrayPrototypePop(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  // We can't reduce Function#call when there is no receiver function.
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.pop - no receiver"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  ValueNode* receiver = GetValueOrUndefined(args.receiver());

  auto node_info = known_node_aspects().TryGetInfoFor(receiver);
  // If the map set is not found, then we don't know anything about the map of
  // the receiver, so bail.
  if (!node_info || !node_info->possible_maps_are_known()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout
          << "  ! Failed to reduce Array.prototype.pop - unknown receiver map"
          << std::endl;
    }
    return ReduceResult::Fail();
  }

  const PossibleMaps& possible_maps = node_info->possible_maps();

  // If the set of possible maps is empty, then there's no possible map for this
  // receiver, therefore this path is unreachable at runtime. We're unlikely to
  // ever hit this case, BuildCheckMaps should already unconditionally deopt,
  // but check it in case another checking operation fails to statically
  // unconditionally deopt.
  if (possible_maps.is_empty()) {
    // TODO(leszeks): Add an unreachable assert here.
    return ReduceResult::DoneWithAbort();
  }

  if (!broker()->dependencies()->DependOnNoElementsProtector()) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.pop - "
                   "NoElementsProtector invalidated"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  constexpr int max_kind_count = 4;
  std::array<SmallZoneVector<compiler::MapRef, 2>, max_kind_count> map_kinds = {
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone()),
      SmallZoneVector<compiler::MapRef, 2>(zone())};
  // Smi and Object elements kinds are treated as identical for pop, so we can
  // group them together without differentiation.
  // ElementsKind is mapped to an index in the 4 element array using:
  //   - Bit 2 (Only set for double in the fast element range) is mapped to bit
  //   - Bit 0 (packedness)
  // +-------+----------------------------------------------+
  // | Index |    ElementsKinds                             |
  // +-------+----------------------------------------------+
  // |   0   |    PACKED_SMI_ELEMENTS and PACKED_ELEMENTS   |
  // |   1   |    HOLEY_SMI_ELEMENETS and HOLEY_ELEMENTS    |
  // |   2   |    PACKED_DOUBLE_ELEMENTS                    |
  // |   3   |    HOLEY_DOUBLE_ELEMENTS                     |
  // +-------+----------------------------------------------+
  auto elements_kind_to_index = |&](ElementsKind kind) {
    uint8_t kind_int = static_cast<uint8_t>(kind);
    uint8_t kind_index = ((kind_int & 0x4) >> 1) | (kind_int & 0x1);
    DCHECK_LT(kind_index, max_kind_count);
    return kind_index;
  };
  auto index_to_elements_kind = |&](uint8_t kind_index) {
    uint8_t kind_int;
    kind_int = ((kind_index & 0x2) << 1) | (kind_index & 0x1);
    return static_cast<ElementsKind>(kind_int);
  };

  int unique_kind_count;
  if (!CanInlineArrayResizingBuiltin(broker(), possible_maps, map_kinds,
                                     elements_kind_to_index, &unique_kind_count,
                                     true)) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  ! Failed to reduce Array.prototype.pop - Map doesn't "
                   "support fast resizing"
                << std::endl;
    }
    return ReduceResult::Fail();
  }

  MaglevSubGraphBuilder sub_graph(this, 2);
  MaglevSubGraphBuilder::Variable var_value(0);
  MaglevSubGraphBuilder::Variable var_new_array_length(1);

  std::optional<MaglevSubGraphBuilder::Label> do_return =
      std::make_optional<MaglevSubGraphBuilder::Label>(
          &sub_graph, unique_kind_count + 1,
          std::initializer_list<MaglevSubGraphBuilder::Variable*>{
              &var_value, &var_new_array_length});
  MaglevSubGraphBuilder::Label empty_array(&sub_graph, 1);

  ValueNode* old_array_length_smi;
  GET_VALUE_OR_ABORT(old_array_length_smi,
                     GetSmiValue(BuildLoadJSArrayLength(receiver)));

  // If the array is empty, skip the pop and return undefined.
  sub_graph.GotoIfTrue<BranchIfReferenceEqual>(
      &empty_array, {old_array_length_smi, GetSmiConstant(0)});

  ValueNode* elements_array = BuildLoadElements(receiver);
  ValueNode* new_array_length_smi =
      AddNewNode<CheckedSmiDecrement>({old_array_length_smi});
  ValueNode* new_array_length =
      AddNewNode<UnsafeSmiUntag>({new_array_length_smi});
  sub_graph.set(var_new_array_length, new_array_length_smi);

  auto build_array_pop = |&](ElementsKind kind) {
    // Handle COW if needed.
    ValueNode* writable_elements_array =
        IsSmiOrObjectElementsKind(kind)
            ? AddNewNode<EnsureWritableFastElements>({elements_array, receiver})
            : elements_array;

    // Store new length.
    AddNewNode<StoreTaggedFieldNoWriteBarrier>({receiver, new_array_length_smi},
                                               JSArray::kLengthOffset,
                                               StoreTaggedMode::kDefault);

    // Load the value and store the hole in it's place.
    ValueNode* value;
    if (IsDoubleElementsKind(kind)) {
      value = BuildLoadFixedDoubleArrayElement(writable_elements_array,
                                               new_array_length);
      BuildStoreFixedDoubleArrayElement(
          writable_elements_array, new_array_length,
          GetFloat64Constant(Float64::FromBits(kHoleNanInt64)));
    } else {
      DCHECK(IsSmiElementsKind(kind) || IsObjectElementsKind(kind));
      value =
          BuildLoadFixedArrayElement(writable_elements_array, new_array_length);
      BuildStoreFixedArrayElement(writable_elements_array, new_array_length,
                                  GetRootConstant(RootIndex::kTheHoleValue));
    }

    if (IsHoleyElementsKind(kind)) {
      value = AddNewNode<ConvertHoleToUndefined>({value});
    }
    sub_graph.set(var_value, value);
    return ReduceResult::Done();
  };

  RETURN_IF_ABORT(BuildJSArrayBuiltinMapSwitchOnElementsKind(
      receiver, map_kinds, sub_graph, do_return, unique_kind_count,
      index_to_elements_kind, build_array_pop));

  sub_graph.Bind(&empty_array);
  sub_graph.set(var_new_array_length, GetSmiConstant(0));
  sub_graph.set(var_value, GetRootConstant(RootIndex::kUndefinedValue));
  sub_graph.Goto(&*do_return);

  sub_graph.Bind(&*do_return);
  RecordKnownProperty(receiver, broker()->length_string(),
                      sub_graph.get(var_new_array_length), false,
                      compiler::AccessMode::kStore);
  return sub_graph.get(var_value);
}

ReduceResult MaglevGraphBuilder::TryReduceFunctionPrototypeHasInstance(
    compiler::JSFunctionRef target, CallArguments& args) {
  // We can't reduce Function#hasInstance when there is no receiver function.
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    return ReduceResult::Fail();
  }
  if (args.count() != 1) {
    return ReduceResult::Fail();
  }
  compiler::OptionalHeapObjectRef maybe_receiver_constant =
      TryGetConstant(args.receiver());
  if (!maybe_receiver_constant) {
    return ReduceResult::Fail();
  }
  compiler::HeapObjectRef receiver_object = maybe_receiver_constant.value();
  if (!receiver_object.IsJSObject() ||
      !receiver_object.map(broker()).is_callable()) {
    return ReduceResult::Fail();
  }
  return BuildOrdinaryHasInstance(args|0], receiver_object.AsJSObject(),
                                  nullptr);
}

ReduceResult MaglevGraphBuilder::TryReduceObjectPrototypeHasOwnProperty(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    return ReduceResult::Fail();
  }

  // We can constant-fold the {receiver.hasOwnProperty(name)} builtin call to
  // the {True} node in this case:

  //   for (name in receiver) {
  //     if (receiver.hasOwnProperty(name)) {
  //        ...
  //     }
  //   }

  if (args.count() != 1 || args|0] != current_for_in_state.key) {
    return ReduceResult::Fail();
  }
  ValueNode* receiver = args.receiver();
  if (receiver == current_for_in_state.receiver) {
    if (current_for_in_state.receiver_needs_map_check) {
      auto* receiver_map =
          BuildLoadTaggedField(receiver, HeapObject::kMapOffset);
      AddNewNode<CheckDynamicValue>(
          {receiver_map, current_for_in_state.cache_type});
      current_for_in_state.receiver_needs_map_check = false;
    }
    return GetRootConstant(RootIndex::kTrueValue);
  }

  // We can also optimize for this case below:

  // receiver(is a heap constant with fast map)
  //  ^
  //  |    object(all keys are enumerable)
  //  |      ^
  //  |      |
  //  |   JSForInNext
  //  |      ^
  //  +----+ |
  //       | |
  //  JSCall|hasOwnProperty]

  // We can replace the {JSCall} with several internalized string
  // comparisons.

  compiler::OptionalMapRef maybe_receiver_map;
  compiler::OptionalHeapObjectRef receiver_ref = TryGetConstant(receiver);
  if (receiver_ref.has_value()) {
    compiler::HeapObjectRef receiver_object = receiver_ref.value();
    compiler::MapRef receiver_map = receiver_object.map(broker());
    maybe_receiver_map = receiver_map;
  } else {
    NodeInfo* known_info = GetOrCreateInfoFor(receiver);
    if (known_info->possible_maps_are_known()) {
      compiler::ZoneRefSet<Map> possible_maps = known_info->possible_maps();
      if (possible_maps.size() == 1) {
        compiler::MapRef receiver_map = *(possible_maps.begin());
        maybe_receiver_map = receiver_map;
      }
    }
  }
  if (!maybe_receiver_map.has_value()) {
    return ReduceResult::Fail();
  }

  compiler::MapRef receiver_map = maybe_receiver_map.value();
  InstanceType instance_type = receiver_map.instance_type();
  int const nof = receiver_map.NumberOfOwnDescriptors();
  // We set a heuristic value to limit the compare instructions number.
  if (nof > 4 || IsSpecialReceiverInstanceType(instance_type) ||
      receiver_map.is_dictionary_map()) {
    return ReduceResult::Fail();
  }
  RETURN_IF_ABORT(BuildCheckMaps(receiver, base::VectorOf({receiver_map})));
  //  Replace builtin call with several internalized string comparisons.
  MaglevSubGraphBuilder sub_graph(this, 1);
  MaglevSubGraphBuilder::Variable var_result(0);
  MaglevSubGraphBuilder::Label done(
      &sub_graph, nof + 1,
      std::initializer_list<MaglevSubGraphBuilder::Variable*>{&var_result});
  const compiler::DescriptorArrayRef descriptor_array =
      receiver_map.instance_descriptors(broker());
  for (InternalIndex key_index : InternalIndex::Range(nof)) {
    compiler::NameRef receiver_key =
        descriptor_array.GetPropertyKey(broker(), key_index);
    ValueNode* lhs = GetConstant(receiver_key);
    sub_graph.set(var_result, GetRootConstant(RootIndex::kTrueValue));
    sub_graph.GotoIfTrue<BranchIfReferenceEqual>(&done, {lhs, args|0]});
  }
  sub_graph.set(var_result, GetRootConstant(RootIndex::kFalseValue));
  sub_graph.Goto(&done);
  sub_graph.Bind(&done);
  return sub_graph.get(var_result);
}

ReduceResult MaglevGraphBuilder::TryReduceGetProto(ValueNode* object) {
  NodeInfo* info = known_node_aspects().TryGetInfoFor(object);
  if (!info || !info->possible_maps_are_known()) {
    return ReduceResult::Fail();
  }
  auto& possible_maps = info->possible_maps();
  if (possible_maps.is_empty()) {
    return ReduceResult::DoneWithAbort();
  }
  auto it = possible_maps.begin();
  compiler::MapRef map = *it;
  if (IsSpecialReceiverInstanceType(map.instance_type())) {
    return ReduceResult::Fail();
  }
  DCHECK(!map.IsPrimitiveMap() && map.IsJSReceiverMap());
  compiler::HeapObjectRef proto = map.prototype(broker());
  ++it;
  for (; it != possible_maps.end(); ++it) {
    map = *it;
    if (IsSpecialReceiverInstanceType(map.instance_type()) ||
        !proto.equals(map.prototype(broker()))) {
      return ReduceResult::Fail();
    }
    DCHECK(!map.IsPrimitiveMap() && map.IsJSReceiverMap());
  }
  return GetConstant(proto);
}

ReduceResult MaglevGraphBuilder::TryReduceObjectPrototypeGetProto(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() != 0) {
    return ReduceResult::Fail();
  }
  return TryReduceGetProto(args.receiver());
}

ReduceResult MaglevGraphBuilder::TryReduceObjectGetPrototypeOf(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() != 1) {
    return ReduceResult::Fail();
  }
  return TryReduceGetProto(args|0]);
}

ReduceResult MaglevGraphBuilder::TryReduceReflectGetPrototypeOf(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryReduceObjectGetPrototypeOf(target, args);
}

ReduceResult MaglevGraphBuilder::TryReduceMathRound(
    compiler::JSFunctionRef target, CallArguments& args) {
  // The complete mapping:
}
  //   1)
ReduceResult MaglevGraphBuilder::TryReduceNumberParseInt(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) {
    return GetRootConstant(RootIndex::kNanValue);
  }
  if (args.count() != 1) {
    if (RootConstant* c = args|1]->TryCast<RootConstant>()) {
      if (c->index() != RootIndex::kUndefinedValue) {
        return ReduceResult::Fail();
      }
    } else if (SmiConstant* c = args|1]->TryCast<SmiConstant>()) {
      if (c->value().value() != 10 && c->value().value() != 0) {
        return ReduceResult::Fail();
      }
    } else {
      return ReduceResult::Fail();
    }
  }

  ValueNode* arg = args|0];

  switch (arg->value_representation()) {
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kInt32:
      return arg;
    case ValueRepresentation::kTagged:
      switch (CheckTypes(arg, {NodeType::kSmi})) {
        case NodeType::kSmi:
          return arg;
        default:
          // TODO(verwaest): Support actually parsing strings, converting
          // doubles to ints, ...
          return ReduceResult::Fail();
      }
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      return ReduceResult::Fail();
  }
}

ReduceResult MaglevGraphBuilder::TryReduceMathAbs(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) {
    return GetRootConstant(RootIndex::kNanValue);
  }
  ValueNode* arg = args|0];

  switch (arg->value_representation()) {
    case ValueRepresentation::kUint32:
      return arg;
    case ValueRepresentation::kInt32:
      if (!CanSpeculateCall()) {
        return ReduceResult::Fail();
      }
      return AddNewNode<Int32AbsWithOverflow>({arg});
    case ValueRepresentation::kTagged:
      switch (CheckTypes(arg, {NodeType::kSmi, NodeType::kNumberOrOddball})) {
        case NodeType::kSmi:
          if (!CanSpeculateCall()) return ReduceResult::Fail();
          return AddNewNode<Int32AbsWithOverflow>({arg});
        case NodeType::kNumberOrOddball:
          return AddNewNode<Float64Abs>({GetHoleyFloat64ForToNumber(
              arg, NodeType::kNumberOrOddball,
              TaggedToFloat64ConversionType::kNumberOrOddball)});
        // TODO(verwaest): Add support for ToNumberOrNumeric and deopt.
        default:
          break;
      }
      break;
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      return AddNewNode<Float64Abs>({arg});
  }
  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::TryReduceMathFloor(
    compiler::JSFunctionRef target, CallArguments& args) {
  return DoTryReduceMathRound(args, Float64Round::Kind::kFloor);
}

ReduceResult MaglevGraphBuilder::TryReduceMathCeil(
    compiler::JSFunctionRef target, CallArguments& args) {
  return DoTryReduceMathRound(args, Float64Round::Kind::kCeil);
}

ReduceResult MaglevGraphBuilder::DoTryReduceMathRound(CallArguments& args,
                                                      Float64Round::Kind kind) {
  if (args.count() == 0) {
    return GetRootConstant(RootIndex::kNanValue);
  }
  ValueNode* arg = args|0];
  auto arg_repr = arg->value_representation();
  if (arg_repr == ValueRepresentation::kInt32 ||
      arg_repr == ValueRepresentation::kUint32) {
    return arg;
  }
  if (CheckType(arg, NodeType::kSmi)) return arg;
  if (!IsSupported(CpuOperation::kFloat64Round)) {
    return ReduceResult::Fail();
  }
  if (arg_repr == ValueRepresentation::kFloat64 ||
      arg_repr == ValueRepresentation::kHoleyFloat64) {
    return AddNewNode<Float64Round>({arg}, kind);
  }
  DCHECK_EQ(arg_repr, ValueRepresentation::kTagged);
  if (CheckType(arg, NodeType::kNumberOrOddball)) {
    return AddNewNode<Float64Round>(
        {GetHoleyFloat64ForToNumber(
            arg, NodeType::kNumberOrOddball,
            TaggedToFloat64ConversionType::kNumberOrOddball)},
        kind);
  }
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  DeoptFrameScope continuation_scope(this, Float64Round::continuation(kind));
  ToNumberOrNumeric* conversion =
      AddNewNode<ToNumberOrNumeric>({arg}, Object::Conversion::kToNumber);
  ValueNode* float64_value = AddNewNode<UncheckedNumberOrOddballToFloat64>(
      {conversion}, TaggedToFloat64ConversionType::kOnlyNumber);
  return AddNewNode<Float64Round>({float64_value}, kind);
}

ReduceResult MaglevGraphBuilder::TryReduceArrayConstructor(
    compiler::JSFunctionRef target, CallArguments& args) {
  return TryReduceConstructArrayConstructor(target, args);
}

ReduceResult MaglevGraphBuilder::TryReduceStringConstructor(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() == 0) {
    return GetRootConstant(RootIndex::kempty_string);
  }

  return BuildToString(args|0], ToString::kConvertSymbol);
}

ReduceResult MaglevGraphBuilder::TryReduceMathPow(
    compiler::JSFunctionRef target, CallArguments& args) {
  if (args.count() < 2) {
    // For < 2 args, we'll be calculating Math.Pow(arg|0], undefined), which is
    // ToNumber(arg|0]) ** NaN == NaN. So we can just return NaN.
    // However, if there is a single argument and it's tagged, we have to call
    // ToNumber on it before returning NaN, for side effects. This call could
    // lazy deopt, which would mean we'd need a continuation to actually set
    // the NaN return value... it's easier to just bail out, this should be
    // an uncommon case anyway.
    if (args.count() == 1 && args|0]->properties().is_tagged()) {
      return ReduceResult::Fail();
    }
    return GetRootConstant(RootIndex::kNanValue);
  }
  if (!CanSpeculateCall()) {
    return ReduceResult::Fail();
  }
  // If both arguments are tagged, it is cheaper to call Math.Pow builtin,
  // instead of Float64Exponentiate, since we are still making a call and we
  // don't need to unbox both inputs. See https://crbug.com/1393643.
  if (args|0]->properties().is_tagged() && args|1]->properties().is_tagged()) {
    // The Math.pow call will be created in CallKnownJSFunction reduction.
    return ReduceResult::Fail();
  }
  ValueNode* left = GetHoleyFloat64ForToNumber(
      args|0], NodeType::kNumber, TaggedToFloat64ConversionType::kOnlyNumber);
  ValueNode* right = GetHoleyFloat64ForToNumber(
      args|1], NodeType::kNumber, TaggedToFloat64ConversionType::kOnlyNumber);
  return AddNewNode<Float64Exponentiate>({left, right});
}

#define MATH_UNARY_IEEE_BUILTIN_REDUCER(MathName, ExtName, EnumName)          \
  ReduceResult MaglevGraphBuilder::TryReduce##MathName(                       \
      compiler::JSFunctionRef target, CallArguments& args) {                  \
    if (args.count() < 1) {                                                   \
      return GetRootConstant(RootIndex::kNanValue);                           \
    }                                                                         \
    if (!CanSpeculateCall()) {                                                \
      ValueRepresentation rep = args|0]->properties().value_representation(); \
      if (rep == ValueRepresentation::kTagged ||                              \
          rep == ValueRepresentation::kHoleyFloat64) {                        \
        return ReduceResult::Fail();                                          \
      }                                                                       \
    }                                                                         \
    ValueNode* value =                                                        \
        GetFloat64ForToNumber(args|0], NodeType::kNumber,                     \
                              TaggedToFloat64ConversionType::kOnlyNumber);    \
    return AddNewNode<Float64Ieee754Unary>(                                   \
        {value}, Float64Ieee754Unary::Ieee754Function::k##EnumName);          \
  }

IEEE_754_UNARY_LIST(MATH_UNARY_IEEE_BUILTIN_REDUCER)
#undef MATH_UNARY_IEEE_BUILTIN_REDUCER

ReduceResult MaglevGraphBuilder::TryReduceBuiltin(
    compiler::JSFunctionRef target, compiler::SharedFunctionInfoRef shared,
    CallArguments& args, const compiler::FeedbackSource& feedback_source) {
  if (args.mode() != CallArguments::kDefault) {
    // TODO(victorgomes): Maybe inline the spread stub? Or call known function
    // directly if arguments list is an array.
    return ReduceResult::Fail();
  }
  SaveCallSpeculationScope speculate(this, feedback_source);
  if (!shared.HasBuiltinId()) {
    return ReduceResult::Fail();
  }
  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "  ! Trying to reduce builtin "
              << Builtins::name(shared.builtin_id()) << std::endl;
  }
  switch (shared.builtin_id()) {
#define CASE(Name, ...)  \
  case Builtin::k##Name: \
    return TryReduce##Name(target, args);
    MAGLEV_REDUCED_BUILTIN(CASE)
#undef CASE
    default:
      // TODO(v8:7700): Inline more builtins.
      return ReduceResult::Fail();
  }
}

ValueNode* MaglevGraphBuilder::GetConvertReceiver(
    compiler::SharedFunctionInfoRef shared, const CallArguments& args) {
  if (shared.native() || shared.language_mode() == LanguageMode::kStrict) {
    if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
      return GetRootConstant(RootIndex::kUndefinedValue);
    } else {
      return args.receiver();
    }
  }
  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
    return GetConstant(

  return DoTryReduceMathRound(args, Float64Round::Kind::kNearest);
  ValueNode* receiver = args.receiver();
  if (CheckType(receiver, NodeType::kJSReceiver)) return receiver;
  if (compiler::OptionalHeapObjectRef maybe_constant =
          TryGetConstant(receiver)) {
    compiler::HeapObjectRef constant = maybe_constant.value();
    if (constant.IsNullOrUndefined()) {
      return GetConstant(
          broker()->target_native_context().global_proxy_object(broker()));
    }
  }
  return AddNewNode<ConvertReceiver>(
      {receiver}, broker()->target_native_context(), args.receiver_mode());
}

template <typename CallNode, typename... Args>
CallNode* MaglevGraphBuilder::AddNewCallNode(const CallArguments& args,
                                             Args&&... extra_args) {
  size_t input_count = args.count_with_receiver() + CallNode::kFixedInputCount;
  return AddNewNode<CallNode>(
      input_count,
      |&](CallNode* call) {
        int arg_index = 0;
        call->set_arg(arg_index++,
                      GetTaggedValue(GetValueOrUndefined(args.receiver())));
        for (size_t i = 0; i < args.count(); ++i) {
          call->set_arg(arg_index++, GetTaggedValue(args|i]));
        }
      },
      std::forward<Args>(extra_args)...);
}

ValueNode* MaglevGraphBuilder::BuildGenericCall(ValueNode* target,
                                                Call::TargetType target_type,
                                                const CallArguments& args) {
  // TODO(victorgomes): We do not collect call feedback from optimized/inlined
  // calls. In order to be consistent, we don't pass the feedback_source to the
  // IR, so that we avoid collecting for generic calls as well. We might want to
  // revisit this in the future.
  switch (args.mode()) {
    case CallArguments::kDefault:
      return AddNewCallNode<Call>(args, args.receiver_mode(), target_type,
                                  GetTaggedValue(target),
                                  GetTaggedValue(GetContext()));
    case CallArguments::kWithSpread:
      DCHECK_EQ(args.receiver_mode(), ConvertReceiverMode::kAny);
      return AddNewCallNode<CallWithSpread>(args, GetTaggedValue(target),
                                            GetTaggedValue(GetContext()));
    case CallArguments::kWithArrayLike:
      DCHECK_EQ(args.receiver_mode(), ConvertReceiverMode::kAny);
      // We don't use AddNewCallNode here, because the number of required
      // arguments is known statically.
      return AddNewNode<CallWithArrayLike>(
          {target, GetValueOrUndefined(args.receiver()), args|0],
           GetContext()});
  }
}

ValueNode* MaglevGraphBuilder::BuildCallSelf(
    ValueNode* context, ValueNode* function, ValueNode* new_target,
    compiler::SharedFunctionInfoRef shared, CallArguments& args) {
  ValueNode* receiver = GetConvertReceiver(shared, args);
  size_t input_count = args.count() + CallSelf::kFixedInputCount;
  graph()->set_has_recursive_calls(true);
  DCHECK_EQ(
      compilation_unit_->info()->toplevel_compilation_unit()->parameter_count(),
      shared.internal_formal_parameter_count_with_receiver());
  return AddNewNode<CallSelf>(
      input_count,
      |&](CallSelf* call) {
        for (int i = 0; i < static_cast<int>(args.count()); i++) {
          call->set_arg(i, GetTaggedValue(args|i]));
        }
      },
      compilation_unit_->info()->toplevel_compilation_unit()->parameter_count(),
      GetTaggedValue(function), GetTaggedValue(context),
      GetTaggedValue(receiver), GetTaggedValue(new_target));
}

bool MaglevGraphBuilder::TargetIsCurrentCompilingUnit(
    compiler::JSFunctionRef target) {
  if (compilation_unit_->info()->specialize_to_function_context()) {
    return target.object().equals(
        compilation_unit_->info()->toplevel_function());
  }
  return target.object()->shared() ==
         compilation_unit_->info()->toplevel_function()->shared();
}

ReduceResult MaglevGraphBuilder::ReduceCallForApiFunction(
    compiler::FunctionTemplateInfoRef api_callback,
    compiler::OptionalSharedFunctionInfoRef maybe_shared,
    compiler::OptionalJSObjectRef api_holder, CallArguments& args) {
  if (args.mode() != CallArguments::kDefault) {
    // TODO(victorgomes): Maybe inline the spread stub? Or call known function
    // directly if arguments list is an array.
    return ReduceResult::Fail();
  }
  // Check if the function has an associated C++ code to execute.
  compiler::OptionalObjectRef maybe_callback_data =
      api_callback.callback_data(broker());
  if (!maybe_callback_data.has_value()) {
    // TODO(ishell): consider generating "return undefined" for empty function
    // instead of failing.
    return ReduceResult::Fail();
  }

  size_t input_count = args.count() + CallKnownApiFunction::kFixedInputCount;
  ValueNode* receiver;
  if (maybe_shared.has_value()) {
    receiver = GetConvertReceiver(maybe_shared.value(), args);
  } else {
    receiver = args.receiver();
    CHECK_NOT_NULL(receiver);
  }

  CallKnownApiFunction::Mode mode =
      broker()->dependencies()->DependOnNoProfilingProtector()
          ? (v8_flags.maglev_inline_api_calls
                 ? CallKnownApiFunction::kNoProfilingInlined
                 : CallKnownApiFunction::kNoProfiling)
          : CallKnownApiFunction::kGeneric;

  return AddNewNode<CallKnownApiFunction>(
      input_count,
      |&](CallKnownApiFunction* call) {
        for (int i = 0; i < static_cast<int>(args.count()); i++) {
          call->set_arg(i, GetTaggedValue(args|i]));
        }
      },
      mode, api_callback, api_holder, GetTaggedValue(GetContext()),
      GetTaggedValue(receiver));
}

ReduceResult MaglevGraphBuilder::TryBuildCallKnownApiFunction(
    compiler::JSFunctionRef function, compiler::SharedFunctionInfoRef shared,
    CallArguments& args) {
  compiler::OptionalFunctionTemplateInfoRef maybe_function_template_info =
      shared.function_template_info(broker());
  if (!maybe_function_template_info.has_value()) {
    // Not an Api function.
    return ReduceResult::Fail();
  }

  // See if we can optimize this API call.
  compiler::FunctionTemplateInfoRef function_template_info =
      maybe_function_template_info.value();

  compiler::HolderLookupResult api_holder;
  if (function_template_info.accept_any_receiver() &&
      function_template_info.is_signature_undefined(broker())) {
    // We might be able to optimize the API call depending on the
    // {function_template_info}.
    // If the API function accepts any kind of {receiver}, we only need to
    // ensure that the {receiver} is actually a JSReceiver at this point,
    // and also pass that as the {holder}. There are two independent bits
    // here:
    //
    //  a. When the "accept any receiver" bit is set, it means we don't
    //     need to perform access checks, even if the {receiver}'s map
        broker()->target_native_context().global_proxy_object(broker()));
  }
    //     need to do the compatible receiver check, since all receivers
    //     are considered compatible at that point, and the {receiver}
    //     will be pass as the {holder}.

    api_holder =
        compiler::HolderLookupResult{CallOptimization::kHolderIsReceiver};
  } else {
    // Try to infer API holder from the known aspects of the {receiver}.
    api_holder =
        TryInferApiHolderValue(function_template_info, args.receiver());
  }

  switch (api_holder.lookup) {
    case CallOptimization::kHolderIsReceiver:
    case CallOptimization::kHolderFound:
      return ReduceCallForApiFunction(function_template_info, shared,
                                      api_holder.holder, args);

    case CallOptimization::kHolderNotFound:
      break;
  }

  // We don't have enough information to eliminate the access check
  // and/or the compatible receiver check, so use the generic builtin
  // that does those checks dynamically. This is still significantly
  // faster than the generic call sequence.
  Builtin builtin_name;
  // TODO(ishell): create no-profiling versions of kCallFunctionTemplate
  // builtins and use them here based on DependOnNoProfilingProtector()
  // dependency state.
  if (function_template_info.accept_any_receiver()) {
    DCHECK(!function_template_info.is_signature_undefined(broker()));
    builtin_name = Builtin::kCallFunctionTemplate_CheckCompatibleReceiver;
  } else if (function_template_info.is_signature_undefined(broker())) {
    builtin_name = Builtin::kCallFunctionTemplate_CheckAccess;
  } else {
    builtin_name =
        Builtin::kCallFunctionTemplate_CheckAccessAndCompatibleReceiver;
  }

  // The CallFunctionTemplate builtin requires the {receiver} to be
  // an actual JSReceiver, so make sure we do the proper conversion
  // first if necessary.
  ValueNode* receiver = GetConvertReceiver(shared, args);
  int kContext = 1;
  int kFunctionTemplateInfo = 1;
  int kArgc = 1;
  return AddNewNode<CallBuiltin>(
      kFunctionTemplateInfo + kArgc + kContext + args.count_with_receiver(),
      |&](CallBuiltin* call_builtin) {
        int arg_index = 0;
        call_builtin->set_arg(arg_index++, GetConstant(function_template_info));
        call_builtin->set_arg(
            arg_index++,
            GetInt32Constant(JSParameterCount(static_cast<int>(args.count()))));

        call_builtin->set_arg(arg_index++, GetTaggedValue(receiver));
        for (int i = 0; i < static_cast<int>(args.count()); i++) {
          call_builtin->set_arg(arg_index++, GetTaggedValue(args|i]));
        }
      },
      builtin_name, GetTaggedValue(GetContext()));
}

ReduceResult MaglevGraphBuilder::TryBuildCallKnownJSFunction(
    compiler::JSFunctionRef function, ValueNode* new_target,
    CallArguments& args, const compiler::FeedbackSource& feedback_source) {
  // Don't inline CallFunction stub across native contexts.
  if (function.native_context(broker()) != broker()->target_native_context()) {
    return ReduceResult::Fail();
  }
  compiler::SharedFunctionInfoRef shared = function.shared(broker());
  RETURN_IF_DONE(TryBuildCallKnownApiFunction(function, shared, args));

  ValueNode* closure = GetConstant(function);
  compiler::ContextRef context = function.context(broker());
  ValueNode* context_node = GetConstant(context);
  ReduceResult res;
  if (MaglevIsTopTier() && TargetIsCurrentCompilingUnit(function) &&
      !graph_->is_osr()) {
    DCHECK(!shared.HasBuiltinId());
    res = BuildCallSelf(context_node, closure, new_target, shared, args);
  } else {
    res = TryBuildCallKnownJSFunction(
        context_node, closure, new_target,
#ifdef V8_ENABLE_LEAPTIERING
        function.dispatch_handle(),
#endif
        shared, function.feedback_vector(broker()), args, feedback_source);
  }
  return res;
}

ReduceResult MaglevGraphBuilder::TryBuildCallKnownJSFunction(
    ValueNode* context, ValueNode* function, ValueNode* new_target,
#ifdef V8_ENABLE_LEAPTIERING
    JSDispatchHandle dispatch_handle,
#endif
    compiler::SharedFunctionInfoRef shared,
    compiler::OptionalFeedbackVectorRef feedback_vector, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  if (v8_flags.maglev_inlining) {
    RETURN_IF_DONE(TryBuildInlinedCall(context, function, new_target, shared,
                                       feedback_vector, args, feedback_source));
  }
  ValueNode* receiver = GetConvertReceiver(shared, args);
  size_t input_count = args.count() + CallKnownJSFunction::kFixedInputCount;
  return AddNewNode<CallKnownJSFunction>(
      input_count,
      |&](CallKnownJSFunction* call) {
        for (int i = 0; i < static_cast<int>(args.count()); i++) {
          call->set_arg(i, GetTaggedValue(args|i]));
        }
      },
#ifdef V8_ENABLE_LEAPTIERING
      dispatch_handle,
#endif
      shared, GetTaggedValue(function), GetTaggedValue(context),
      GetTaggedValue(receiver), GetTaggedValue(new_target));
}

ReduceResult MaglevGraphBuilder::BuildCheckValue(ValueNode* node,
                                                 compiler::HeapObjectRef ref) {
  DCHECK(!ref.IsSmi());
  DCHECK(!ref.IsHeapNumber());

  if (!IsInstanceOfNodeType(ref.map(broker()), GetType(node), broker())) {
    return EmitUnconditionalDeopt(DeoptimizeReason::kValueMismatch);
  }
  if (compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(node)) {
    if (maybe_constant.value().equals(ref)) {
      return ReduceResult::Done();
    }
    return EmitUnconditionalDeopt(DeoptimizeReason::kUnknown);
  }
  if (ref.IsString()) {
    DCHECK(ref.IsInternalizedString());
    AddNewNode<CheckValueEqualsString>({node}, ref.AsInternalizedString());
    SetKnownValue(node, ref, NodeType::kString);
  } else {
    AddNewNode<CheckValue>({node}, ref);
    SetKnownValue(node, ref, StaticTypeForConstant(broker(), ref));
  }

  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::BuildCheckValue(ValueNode* node,
                                                 compiler::ObjectRef ref) {
  if (ref.IsHeapObject() && !ref.IsHeapNumber()) {
    return BuildCheckValue(node, ref.AsHeapObject());
  }
  return BuildCheckNumericalValue(node, ref);
}

ReduceResult MaglevGraphBuilder::BuildCheckNumericalValue(
    ValueNode* node, compiler::ObjectRef ref) {
  DCHECK(ref.IsSmi() || ref.IsHeapNumber());
  if (ref.IsSmi()) {
    int ref_value = ref.AsSmi();
    if (IsConstantNode(node->opcode())) {
      if (node->Is<SmiConstant>() &&
          node->Cast<SmiConstant>()->value().value() == ref_value) {
        return ReduceResult::Done();
      }
      if (node->Is<Int32Constant>() &&
          node->Cast<Int32Constant>()->value() == ref_value) {
        return ReduceResult::Done();
      }
      return EmitUnconditionalDeopt(DeoptimizeReason::kUnknown);
    //     has the "needs access check" bit set.
      return EmitUnconditionalDeopt(DeoptimizeReason::kValueMismatch);
    //  b. When the {function_template_info} has no signature, we don't
    AddNewNode<CheckValueEqualsInt32>({node}, ref_value);
  } else {
    DCHECK(ref.IsHeapNumber());
    Float64 ref_value = Float64::FromBits(ref.AsHeapNumber().value_as_bits());
    DCHECK(!ref_value.is_hole_nan());
    if (node->Is<Float64Constant>()) {
      Float64 f64 = node->Cast<Float64Constant>()->value();
      DCHECK(!f64.is_hole_nan());
      if (f64 == ref_value) {
        return ReduceResult::Done();
      }
      return EmitUnconditionalDeopt(DeoptimizeReason::kUnknown);
    } else if (compiler::OptionalHeapObjectRef constant =
                   TryGetConstant(node)) {
      if (constant.value().IsHeapNumber()) {
        Float64 f64 =
            Float64::FromBits(constant.value().AsHeapNumber().value_as_bits());
        DCHECK(!f64.is_hole_nan());
        if (f64 == ref_value) {
          return ReduceResult::Done();
        }
      }
      return EmitUnconditionalDeopt(DeoptimizeReason::kUnknown);
    }
    if (!NodeTypeIs(NodeType::kNumber, GetType(node))) {
      return EmitUnconditionalDeopt(DeoptimizeReason::kValueMismatch);
    }
    if (ref_value.is_nan()) {
      AddNewNode<CheckFloat64IsNan>({node});
    } else {
      AddNewNode<CheckValueEqualsFloat64>({node}, ref_value);
    }
  }

  SetKnownValue(node, ref, NodeType::kNumber);
  return ReduceResult::Done();
}

ValueNode* MaglevGraphBuilder::BuildConvertHoleToUndefined(ValueNode* node) {
  if (!node->is_tagged()) return node;
  compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(node);
  if (maybe_constant) {
    return maybe_constant.value().IsTheHole()
               ? GetRootConstant(RootIndex::kUndefinedValue)
               : node;
  }
  return AddNewNode<ConvertHoleToUndefined>({node});
}

ReduceResult MaglevGraphBuilder::BuildCheckNotHole(ValueNode* node) {
  if (!node->is_tagged()) return ReduceResult::Done();
  compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(node);
  if (maybe_constant) {
    if (maybe_constant.value().IsTheHole()) {
      return EmitUnconditionalDeopt(DeoptimizeReason::kHole);
    }
    return ReduceResult::Done();
  }
  AddNewNode<CheckNotHole>({node});
  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::ReduceCallForConstant(
    compiler::JSFunctionRef target, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  if (args.mode() != CallArguments::kDefault) {
    // TODO(victorgomes): Maybe inline the spread stub? Or call known function
    // directly if arguments list is an array.
    return ReduceResult::Fail();
  }
  compiler::SharedFunctionInfoRef shared = target.shared(broker());
  ValueNode* target_node = GetConstant(target);
  // Do not reduce calls to functions with break points.
  if (!shared.HasBreakInfo(broker())) {
    if (IsClassConstructor(shared.kind())) {
      // If we have a class constructor, we should raise an exception.
      return BuildCallRuntime(Runtime::kThrowConstructorNonCallableError,
                              {target_node});
    }
    DCHECK(IsCallable(*target.object()));
    RETURN_IF_DONE(TryReduceBuiltin(target, shared, args, feedback_source));
    RETURN_IF_DONE(TryBuildCallKnownJSFunction(
        target, GetRootConstant(RootIndex::kUndefinedValue), args,
        feedback_source));
  }
  return BuildGenericCall(target_node, Call::TargetType::kJSFunction, args);
}

compiler::HolderLookupResult MaglevGraphBuilder::TryInferApiHolderValue(
    compiler::FunctionTemplateInfoRef function_template_info,
    ValueNode* receiver) {
  const compiler::HolderLookupResult not_found;

  auto receiver_info = known_node_aspects().TryGetInfoFor(receiver);
  if (!receiver_info || !receiver_info->possible_maps_are_known()) {
    // No info about receiver, can't infer API holder.
    return not_found;
  }
  DCHECK(!receiver_info->possible_maps().is_empty());
    }

    }
    if (NodeTypeIs(GetType(node), NodeType::kAnyHeapObject)) {
      function_template_info.LookupHolderOfExpectedType(broker(),
                                                        first_receiver_map);
  if (api_holder.lookup == CallOptimization::kHolderNotFound) {
    // Can't infer API holder.
    return not_found;
  }

  // Check that all {receiver_maps} are actually JSReceiver maps and
  // that the {function_template_info} accepts them without access
  // checks (even if "access check needed" is set for {receiver}).
  //
  // API holder might be a receivers's hidden prototype (i.e. the receiver is
  // a global proxy), so in this case the map check or stability dependency on
  // the receiver guard us from detaching a global object from global proxy.
  CHECK(first_receiver_map.IsJSReceiverMap());
  CHECK(!first_receiver_map.is_access_check_needed() ||
        function_template_info.accept_any_receiver());

  for (compiler::MapRef receiver_map : receiver_info->possible_maps()) {
    compiler::HolderLookupResult holder_i =
        function_template_info.LookupHolderOfExpectedType(broker(),
                                                          receiver_map);

    if (api_holder.lookup != holder_i.lookup) {
      // Different API holders, dynamic lookup is required.
      return not_found;
    }
    DCHECK(holder_i.lookup == CallOptimization::kHolderFound ||
           holder_i.lookup == CallOptimization::kHolderIsReceiver);
    if (holder_i.lookup == CallOptimization::kHolderFound) {
      DCHECK(api_holder.holder.has_value() && holder_i.holder.has_value());
      if (!api_holder.holder->equals(*holder_i.holder)) {
        // Different API holders, dynamic lookup is required.
        return not_found;
      }
    }

    CHECK(receiver_map.IsJSReceiverMap());
    CHECK(!receiver_map.is_access_check_needed() ||
          function_template_info.accept_any_receiver());
  }
  return api_holder;
}

ReduceResult MaglevGraphBuilder::ReduceCallForTarget(
    ValueNode* target_node, compiler::JSFunctionRef target, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  RETURN_IF_ABORT(BuildCheckValue(target_node, target));
  return ReduceCallForConstant(target, args, feedback_source);
}

ReduceResult MaglevGraphBuilder::ReduceCallForNewClosure(
    ValueNode* target_node, ValueNode* target_context,
#ifdef V8_ENABLE_LEAPTIERING
    JSDispatchHandle dispatch_handle,
#endif
    compiler::SharedFunctionInfoRef shared,
    compiler::OptionalFeedbackVectorRef feedback_vector, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  // Do not reduce calls to functions with break points.
  if (args.mode() != CallArguments::kDefault) {
    // TODO(victorgomes): Maybe inline the spread stub? Or call known function
    // directly if arguments list is an array.
    return ReduceResult::Fail();
  }
  if (!shared.HasBreakInfo(broker())) {
    if (IsClassConstructor(shared.kind())) {
      // If we have a class constructor, we should raise an exception.
      return BuildCallRuntime(Runtime::kThrowConstructorNonCallableError,
                              {target_node});
    }
    RETURN_IF_DONE(TryBuildCallKnownJSFunction(
        target_context, target_node,
        GetRootConstant(RootIndex::kUndefinedValue),
#ifdef V8_ENABLE_LEAPTIERING
        dispatch_handle,
#endif
        shared, feedback_vector, args, feedback_source));
  }
  return BuildGenericCall(target_node, Call::TargetType::kJSFunction, args);
}

ReduceResult MaglevGraphBuilder::ReduceFunctionPrototypeApplyCallWithReceiver(
    compiler::OptionalHeapObjectRef maybe_receiver, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  if (args.mode() != CallArguments::kDefault) return ReduceResult::Fail();

  ValueNode* function = GetValueOrUndefined(args.receiver());
  if (maybe_receiver.has_value()) {
    RETURN_IF_ABORT(BuildCheckValue(function, maybe_receiver.value()));
    function = GetConstant(maybe_receiver.value());
  }

  SaveCallSpeculationScope saved(this);
  if (args.count() == 0) {
    CallArguments empty_args(ConvertReceiverMode::kNullOrUndefined);
    return ReduceCall(function, empty_args, feedback_source);
  }
  auto build_call_only_with_new_receiver = |&] {
    CallArguments new_args(ConvertReceiverMode::kAny, {args|0]});
    return ReduceCall(function, new_args, feedback_source);
  };
  if (args.count() == 1 || IsNullValue(args|1]) || IsUndefinedValue(args|1])) {
    return build_call_only_with_new_receiver();
  }
  compiler::MapRef first_receiver_map = receiver_info->possible_maps()|0];
  // See if we can constant-fold the compatible receiver checks.
  compiler::HolderLookupResult api_holder =
    return ReduceCallWithArrayLike(function, new_args, feedback_source);
  };
  if (!MayBeNullOrUndefined(args|1])) {
    return build_call_with_array_like();
  }
  return SelectReduction(
      |&](auto& builder) {
        return BuildBranchIfUndefinedOrNull(builder, args|1]);
      },
      build_call_only_with_new_receiver, build_call_with_array_like);
}

void MaglevGraphBuilder::BuildCallWithFeedback(
    ValueNode* target_node, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  const compiler::ProcessedFeedback& processed_feedback =
      broker()->GetFeedbackForCall(feedback_source);
  if (processed_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForCall));
  }

  DCHECK_EQ(processed_feedback.kind(), compiler::ProcessedFeedback::kCall);
  const compiler::CallFeedback& call_feedback = processed_feedback.AsCall();

  if (call_feedback.target().has_value() &&
      call_feedback.target()->IsJSFunction()) {
    CallFeedbackContent content = call_feedback.call_feedback_content();
    compiler::JSFunctionRef feedback_target =
        call_feedback.target()->AsJSFunction();
    if (content == CallFeedbackContent::kReceiver) {
      compiler::NativeContextRef native_context =
          broker()->target_native_context();
      compiler::JSFunctionRef apply_function =
          native_context.function_prototype_apply(broker());
      RETURN_VOID_IF_ABORT(BuildCheckValue(target_node, apply_function));
      PROCESS_AND_RETURN_IF_DONE(ReduceFunctionPrototypeApplyCallWithReceiver(
                                     feedback_target, args, feedback_source),
                                 SetAccumulator);
      feedback_target = apply_function;
    } else {
      DCHECK_EQ(CallFeedbackContent::kTarget, content);
    }
    RETURN_VOID_IF_ABORT(BuildCheckValue(target_node, feedback_target));
  }

  PROCESS_AND_RETURN_IF_DONE(ReduceCall(target_node, args, feedback_source),
                             SetAccumulator);
}

ReduceResult MaglevGraphBuilder::ReduceCallWithArrayLikeForArgumentsObject(
    ValueNode* target_node, CallArguments& args,
    VirtualObject* arguments_object,
    const compiler::FeedbackSource& feedback_source) {
  DCHECK_EQ(args.mode(), CallArguments::kWithArrayLike);
  DCHECK(arguments_object->map().IsJSArgumentsObjectMap() ||
         arguments_object->map().IsJSArrayMap());
  args.PopArrayLikeArgument();
  ValueNode* elements_value =
      arguments_object->get(JSArgumentsObject::kElementsOffset);
  if (elements_value->Is<ArgumentsElements>()) {
    Call::TargetType target_type = Call::TargetType::kAny;
    // TODO(victorgomes): Add JSFunction node type in KNA and use the info here.
    if (compiler::OptionalHeapObjectRef maybe_constant =
            TryGetConstant(target_node)) {
      if (maybe_constant->IsJSFunction()) {
        compiler::SharedFunctionInfoRef shared =
            maybe_constant->AsJSFunction().shared(broker());
        if (!IsClassConstructor(shared.kind())) {
          target_type = Call::TargetType::kJSFunction;
        }
      }
    }
    int start_index = 0;
    if (elements_value->Cast<ArgumentsElements>()->type() ==
        CreateArgumentsType::kRestParameter) {
      start_index =
          elements_value->Cast<ArgumentsElements>()->formal_parameter_count();
    }
    return AddNewCallNode<CallForwardVarargs>(args, GetTaggedValue(target_node),
                                              GetTaggedValue(GetContext()),
                                              start_index, target_type);
  }

  if (elements_value->Is<RootConstant>()) {
    // It is a RootConstant, Elements can only be the empty fixed array.
    DCHECK_EQ(elements_value->Cast<RootConstant>()->index(),
              RootIndex::kEmptyFixedArray);
    CallArguments new_args(ConvertReceiverMode::kAny, {args.receiver()});
    return ReduceCall(target_node, new_args, feedback_source);
  }

  if (Constant* constant_value = elements_value->TryCast<Constant>()) {
    DCHECK(constant_value->object().IsFixedArray());
    compiler::FixedArrayRef elements = constant_value->object().AsFixedArray();
    base::SmallVector<ValueNode*, 8> arg_list;
    DCHECK_NOT_NULL(args.receiver());
    arg_list.push_back(args.receiver());
    for (int i = 0; i < static_cast<int>(args.count()); i++) {
      arg_list.push_back(args|i]);
    }
    for (uint32_t i = 0; i < elements.length(); i++) {
      arg_list.push_back(GetConstant(*elements.TryGet(broker(), i)));
    }
    CallArguments new_args(ConvertReceiverMode::kAny, std::move(arg_list));
    return ReduceCall(target_node, new_args, feedback_source);
  }

  DCHECK(elements_value->Is<InlinedAllocation>());
  InlinedAllocation* allocation = elements_value->Cast<InlinedAllocation>();
  VirtualObject* elements = allocation->object();

  base::SmallVector<ValueNode*, 8> arg_list;
  DCHECK_NOT_NULL(args.receiver());
  arg_list.push_back(args.receiver());
  for (int i = 0; i < static_cast<int>(args.count()); i++) {
    arg_list.push_back(args|i]);
  }
  DCHECK(elements->get(offsetof(FixedArray, length_))->Is<Int32Constant>());
  int length = elements->get(offsetof(FixedArray, length_))
                   ->Cast<Int32Constant>()
                   ->value();
  for (int i = 0; i < length; i++) {
    arg_list.push_back(elements->get(FixedArray::OffsetOfElementAt(i)));
  }
  CallArguments new_args(ConvertReceiverMode::kAny, std::move(arg_list));
  return ReduceCall(target_node, new_args, feedback_source);
}

namespace {
bool IsSloppyMappedArgumentsObject(compiler::JSHeapBroker* broker,
                                   compiler::MapRef map) {
  return broker->target_native_context()
      .fast_aliased_arguments_map(broker)
      .equals(map);
}
}  // namespace

std::optional<VirtualObject*>
MaglevGraphBuilder::TryGetNonEscapingArgumentsObject(ValueNode* value) {
  if (!value->Is<InlinedAllocation>()) return {};
  InlinedAllocation* alloc = value->Cast<InlinedAllocation>();
  // Although the arguments object has not been changed so far, since it is not
  // escaping, it could be modified after this bytecode if it is inside a loop.
  if (IsInsideLoop()) {
    if (!is_loop_effect_tracking() ||
        !loop_effects_->allocations.contains(alloc)) {
      return {};
    }
  }
  // TODO(victorgomes): We can probably loosen the IsNotEscaping requirement if
  // we keep track of the arguments object changes so far.
  if (alloc->IsEscaping()) return {};
  VirtualObject* object = alloc->object();
  // TODO(victorgomes): Support simple JSArray forwarding.
  compiler::MapRef map = object->map();
  // It is a rest parameter, if it is an array with ArgumentsElements node as
  // the elements array.
  if (map.IsJSArrayMap() && object->get(JSArgumentsObject::kElementsOffset)
                                ->Is<ArgumentsElements>()) {
    return object;
  }
  // TODO(victorgomes): We can loosen the IsSloppyMappedArgumentsObject
  // requirement if there is no stores to  the mapped arguments.
  if (map.IsJSArgumentsObjectMap() &&
      !IsSloppyMappedArgumentsObject(broker(), map)) {
    return object;
  }
  return {};
}
                           CallArguments::kWithArrayLike);
    CallArguments new_args(ConvertReceiverMode::kAny, {args|0], args|1]},
  auto build_call_with_array_like = |&] {
    const compiler::FeedbackSource& feedback_source) {
  DCHECK_EQ(args.mode(), CallArguments::kWithArrayLike);

  // TODO(victorgomes): Add the case for JSArrays and Rest parameter.
  if (std::optional<VirtualObject*> arguments_object =
          TryGetNonEscapingArgumentsObject(args.array_like_argument())) {
    RETURN_IF_DONE(ReduceCallWithArrayLikeForArgumentsObject(
        target_node, args, *arguments_object, feedback_source));
  }

  // On fallthrough, create a generic call.
  return BuildGenericCall(target_node, Call::TargetType::kAny, args);
}

ReduceResult MaglevGraphBuilder::ReduceCall(
    ValueNode* target_node, CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  if (compiler::OptionalHeapObjectRef maybe_constant =
          TryGetConstant(target_node)) {
    if (maybe_constant->IsJSFunction()) {
      ReduceResult result = ReduceCallForTarget(
          target_node, maybe_constant->AsJSFunction(), args, feedback_source);
      RETURN_IF_DONE(result);
    }
  }

  // If the implementation here becomes more complex, we could probably
  // deduplicate the code for FastCreateClosure and CreateClosure by using
  // templates or giving them a shared base class.
  if (FastCreateClosure* create_closure =
          target_node->TryCast<FastCreateClosure>()) {
    ReduceResult result = ReduceCallForNewClosure(
        create_closure, create_closure->context().node(),
#ifdef V8_ENABLE_LEAPTIERING
        create_closure->feedback_cell().dispatch_handle(),
#endif
        create_closure->shared_function_info(),
        create_closure->feedback_cell().feedback_vector(broker()), args,
        feedback_source);
    RETURN_IF_DONE(result);
  } else if (CreateClosure* create_closure =
                 target_node->TryCast<CreateClosure>()) {
    ReduceResult result = ReduceCallForNewClosure(
        create_closure, create_closure->context().node(),
#ifdef V8_ENABLE_LEAPTIERING
        create_closure->feedback_cell().dispatch_handle(),
#endif
        create_closure->shared_function_info(),
        create_closure->feedback_cell().feedback_vector(broker()), args,
        feedback_source);
    RETURN_IF_DONE(result);
  }

  // On fallthrough, create a generic call.
  return BuildGenericCall(target_node, Call::TargetType::kAny, args);
}

void MaglevGraphBuilder::BuildCallFromRegisterList(
    ConvertReceiverMode receiver_mode) {
  ValueNode* target = LoadRegister(0);
  interpreter::RegisterList reg_list = iterator_.GetRegisterListOperand(1);
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source(feedback(), slot);
  CallArguments args(receiver_mode, reg_list, current_interpreter_frame_);
  BuildCallWithFeedback(target, args, feedback_source);
}

void MaglevGraphBuilder::BuildCallFromRegisters(
    int arg_count, ConvertReceiverMode receiver_mode) {
  ValueNode* target = LoadRegister(0);
  const int receiver_count =
      (receiver_mode == ConvertReceiverMode::kNullOrUndefined) ? 0 : 1;
  const int reg_count = arg_count + receiver_count;
  FeedbackSlot slot = GetSlotOperand(reg_count + 1);
  compiler::FeedbackSource feedback_source(feedback(), slot);
  switch (reg_count) {
    case 0: {
      DCHECK_EQ(receiver_mode, ConvertReceiverMode::kNullOrUndefined);
      CallArguments args(receiver_mode);
      BuildCallWithFeedback(target, args, feedback_source);
      break;
    }
    case 1: {
      CallArguments args(receiver_mode, {LoadRegister(1)});
      BuildCallWithFeedback(target, args, feedback_source);
      break;
    }
    case 2: {
      CallArguments args(receiver_mode, {LoadRegister(1), LoadRegister(2)});
      BuildCallWithFeedback(target, args, feedback_source);
      break;
    }
    case 3: {
      CallArguments args(receiver_mode,
                         {LoadRegister(1), LoadRegister(2), LoadRegister(3)});
      BuildCallWithFeedback(target, args, feedback_source);
      break;
    }
    default:
      UNREACHABLE();
  }
}

void MaglevGraphBuilder::VisitCallAnyReceiver() {
  BuildCallFromRegisterList(ConvertReceiverMode::kAny);
}
void MaglevGraphBuilder::VisitCallProperty() {
  BuildCallFromRegisterList(ConvertReceiverMode::kNotNullOrUndefined);
}
void MaglevGraphBuilder::VisitCallProperty0() {
  BuildCallFromRegisters(0, ConvertReceiverMode::kNotNullOrUndefined);
}
void MaglevGraphBuilder::VisitCallProperty1() {
  BuildCallFromRegisters(1, ConvertReceiverMode::kNotNullOrUndefined);
}
void MaglevGraphBuilder::VisitCallProperty2() {
  BuildCallFromRegisters(2, ConvertReceiverMode::kNotNullOrUndefined);
}

    ValueNode* target_node, CallArguments& args,
}
ReduceResult MaglevGraphBuilder::ReduceCallWithArrayLike(
  BuildCallFromRegisters(0, ConvertReceiverMode::kNullOrUndefined);
}
void MaglevGraphBuilder::VisitCallUndefinedReceiver1() {
  BuildCallFromRegisters(1, ConvertReceiverMode::kNullOrUndefined);
}
void MaglevGraphBuilder::VisitCallUndefinedReceiver2() {
  BuildCallFromRegisters(2, ConvertReceiverMode::kNullOrUndefined);
}

void MaglevGraphBuilder::VisitCallWithSpread() {
  ValueNode* function = LoadRegister(0);
  interpreter::RegisterList reglist = iterator_.GetRegisterListOperand(1);
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source(feedback(), slot);
  CallArguments args(ConvertReceiverMode::kAny, reglist,
                     current_interpreter_frame_, CallArguments::kWithSpread);
  BuildCallWithFeedback(function, args, feedback_source);
}

void MaglevGraphBuilder::VisitCallRuntime() {
  Runtime::FunctionId function_id = iterator_.GetRuntimeIdOperand(0);
  interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
  ValueNode* context = GetContext();
  size_t input_count = args.register_count() + CallRuntime::kFixedInputCount;
  CallRuntime* call_runtime = AddNewNode<CallRuntime>(
      input_count,
      |&](CallRuntime* call_runtime) {
        for (int i = 0; i < args.register_count(); ++i) {
          call_runtime->set_arg(i, GetTaggedValue(args|i]));
        }
      },
      function_id, context);
  SetAccumulator(call_runtime);

  if (RuntimeFunctionCanThrow(function_id)) {
    RETURN_VOID_IF_DONE(BuildAbort(AbortReason::kUnexpectedReturnFromThrow));
    UNREACHABLE();
  }
}

void MaglevGraphBuilder::VisitCallJSRuntime() {
  // Get the function to call from the native context.
  compiler::NativeContextRef native_context = broker()->target_native_context();
  ValueNode* context = GetConstant(native_context);
  uint32_t slot = iterator_.GetNativeContextIndexOperand(0);
  ValueNode* callee =
      LoadAndCacheContextSlot(context, slot, kMutable, ContextKind::kDefault);
  // Call the function.
  interpreter::RegisterList reglist = iterator_.GetRegisterListOperand(1);
  CallArguments args(ConvertReceiverMode::kNullOrUndefined, reglist,
                     current_interpreter_frame_);
  SetAccumulator(BuildGenericCall(callee, Call::TargetType::kJSFunction, args));
}

void MaglevGraphBuilder::VisitCallRuntimeForPair() {
  Runtime::FunctionId function_id = iterator_.GetRuntimeIdOperand(0);
  interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
  ValueNode* context = GetContext();

  size_t input_count = args.register_count() + CallRuntime::kFixedInputCount;
  CallRuntime* call_runtime = AddNewNode<CallRuntime>(
      input_count,
      |&](CallRuntime* call_runtime) {
        for (int i = 0; i < args.register_count(); ++i) {
          call_runtime->set_arg(i, GetTaggedValue(args|i]));
        }
      },
      function_id, context);
  auto result = iterator_.GetRegisterPairOperand(3);
  StoreRegisterPair(result, call_runtime);
}

void MaglevGraphBuilder::VisitInvokeIntrinsic() {
  // InvokeIntrinsic <function_id> <first_arg> <arg_count>
  Runtime::FunctionId intrinsic_id = iterator_.GetIntrinsicIdOperand(0);
  interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
  switch (intrinsic_id) {
#define CASE(Name, _, arg_count)                                         \
  case Runtime::kInline##Name:                                           \
    DCHECK_IMPLIES(arg_count != -1, arg_count == args.register_count()); \
    VisitIntrinsic##Name(args);                                          \
    break;
    INTRINSICS_LIST(CASE)
#undef CASE
    default:
      UNREACHABLE();
  }
}

void MaglevGraphBuilder::VisitIntrinsicCopyDataProperties(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kCopyDataProperties>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::
    VisitIntrinsicCopyDataPropertiesWithExcludedPropertiesOnStack(
        interpreter::RegisterList args) {
  SmiConstant* excluded_property_count =
      GetSmiConstant(args.register_count() - 1);
  int kContext = 1;
  int kExcludedPropertyCount = 1;
  CallBuiltin* call_builtin = AddNewNode<CallBuiltin>(
      args.register_count() + kContext + kExcludedPropertyCount,
      |&](CallBuiltin* call_builtin) {
        int arg_index = 0;
        call_builtin->set_arg(arg_index++, GetTaggedValue(args|0]));
        call_builtin->set_arg(arg_index++, excluded_property_count);
        for (int i = 1; i < args.register_count(); i++) {
          call_builtin->set_arg(arg_index++, GetTaggedValue(args|i]));
        }
      },
      Builtin::kCopyDataPropertiesWithExcludedProperties,
      GetTaggedValue(GetContext()));
  SetAccumulator(call_builtin);
}

void MaglevGraphBuilder::VisitIntrinsicCreateIterResultObject(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  ValueNode* value = current_interpreter_frame_.get(args|0]);
  ValueNode* done = current_interpreter_frame_.get(args|1]);
  compiler::MapRef map =
      broker()->target_native_context().iterator_result_map(broker());
  VirtualObject* iter_result = CreateJSIteratorResult(map, value, done);
  ValueNode* allocation =
      BuildInlinedAllocation(iter_result, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  SetAccumulator(allocation);
}

void MaglevGraphBuilder::VisitIntrinsicCreateAsyncFromSyncIterator(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 1);
  SetAccumulator(
      BuildCallBuiltin<Builtin::kCreateAsyncFromSyncIteratorBaseline>(
          {GetTaggedValue(args|0])}));
}

void MaglevGraphBuilder::VisitIntrinsicCreateJSGeneratorObject(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  ValueNode* closure = current_interpreter_frame_.get(args|0]);
  ValueNode* receiver = current_interpreter_frame_.get(args|1]);
  PROCESS_AND_RETURN_IF_DONE(
      TryBuildAndAllocateJSGeneratorObject(closure, receiver), SetAccumulator);
  SetAccumulator(BuildCallBuiltin<Builtin::kCreateGeneratorObject>(
      {GetTaggedValue(closure), GetTaggedValue(receiver)}));
}

void MaglevGraphBuilder::VisitIntrinsicGeneratorGetResumeMode(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 1);
  ValueNode* generator = current_interpreter_frame_.get(args|0]);
  SetAccumulator(
      BuildLoadTaggedField(generator, JSGeneratorObject::kResumeModeOffset));
}

void MaglevGraphBuilder::VisitIntrinsicGeneratorClose(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 1);
  ValueNode* generator = current_interpreter_frame_.get(args|0]);
  ValueNode* value = GetSmiConstant(JSGeneratorObject::kGeneratorClosed);
  BuildStoreTaggedFieldNoWriteBarrier(generator, value,
                                      JSGeneratorObject::kContinuationOffset,
                                      StoreTaggedMode::kDefault);
  SetAccumulator(GetRootConstant(RootIndex::kUndefinedValue));
}

void MaglevGraphBuilder::VisitIntrinsicGetImportMetaObject(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 0);
  SetAccumulator(BuildCallRuntime(Runtime::kGetImportMetaObject, {}).value());
}

void MaglevGraphBuilder::VisitIntrinsicAsyncFunctionAwait(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncFunctionAwait>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncFunctionEnter(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncFunctionEnter>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncFunctionReject(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncFunctionReject>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncFunctionResolve(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncFunctionResolve>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncGeneratorAwait(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncGeneratorAwait>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncGeneratorReject(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncGeneratorReject>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncGeneratorResolve(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 3);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncGeneratorResolve>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1]),
       GetTaggedValue(args|2])}));
}

void MaglevGraphBuilder::VisitIntrinsicAsyncGeneratorYieldWithAwait(
    interpreter::RegisterList args) {
  DCHECK_EQ(args.register_count(), 2);
  SetAccumulator(BuildCallBuiltin<Builtin::kAsyncGeneratorYieldWithAwait>(
      {GetTaggedValue(args|0]), GetTaggedValue(args|1])}));
}

ValueNode* MaglevGraphBuilder::BuildGenericConstruct(
    ValueNode* target, ValueNode* new_target, ValueNode* context,
    const CallArguments& args,
    const compiler::FeedbackSource& feedback_source) {
  size_t input_count = args.count_with_receiver() + Construct::kFixedInputCount;
  DCHECK_EQ(args.receiver_mode(), ConvertReceiverMode::kNullOrUndefined);
  return AddNewNode<Construct>(
      input_count,
      |&](Construct* construct) {
        int arg_index = 0;
        // Add undefined receiver.
        construct->set_arg(arg_index++,
                           GetRootConstant(RootIndex::kUndefinedValue));
        for (size_t i = 0; i < args.count(); i++) {
          construct->set_arg(arg_index++, GetTaggedValue(args|i]));
        }
      },
      feedback_source, GetTaggedValue(target), GetTaggedValue(new_target),
      GetTaggedValue(context));
}

ValueNode* MaglevGraphBuilder::BuildAndAllocateKeyValueArray(ValueNode* key,
                                                             ValueNode* value) {
  VirtualObject* elements = CreateFixedArray(broker()->fixed_array_map(), 2);
  elements->set(FixedArray::OffsetOfElementAt(0), key);
  elements->set(FixedArray::OffsetOfElementAt(1), value);
  compiler::MapRef map =
      broker()->target_native_context().js_array_packed_elements_map(broker());
  VirtualObject* array =
      CreateJSArray(map, map.instance_size(), GetInt32Constant(2));
  array->set(JSArray::kElementsOffset, elements);
  ValueNode* allocation = BuildInlinedAllocation(array, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

ValueNode* MaglevGraphBuilder::BuildAndAllocateJSArray(
    compiler::MapRef map, ValueNode* length, ValueNode* elements,
    const compiler::SlackTrackingPrediction& slack_tracking_prediction,
    AllocationType allocation_type) {
  VirtualObject* array =
      CreateJSArray(map, slack_tracking_prediction.instance_size(), length);
  array->set(JSArray::kElementsOffset, elements);
  for (int i = 0; i < slack_tracking_prediction.inobject_property_count();
       i++) {
    array->set(map.GetInObjectPropertyOffset(i),
               GetRootConstant(RootIndex::kUndefinedValue));
  }
  ValueNode* allocation = BuildInlinedAllocation(array, allocation_type);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  BuildCallFromRegisterList(ConvertReceiverMode::kNullOrUndefined);
  ClearCurrentAllocationBlock();
void MaglevGraphBuilder::VisitCallUndefinedReceiver() {
void MaglevGraphBuilder::VisitCallUndefinedReceiver0() {

ValueNode* MaglevGraphBuilder::BuildAndAllocateJSArrayIterator(
    ValueNode* array, IterationKind iteration_kind) {
  compiler::MapRef map =
      broker()->target_native_context().initial_array_iterator_map(broker());
  VirtualObject* iterator = CreateJSArrayIterator(map, array, iteration_kind);
  ValueNode* allocation =
      BuildInlinedAllocation(iterator, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

ReduceResult MaglevGraphBuilder::TryBuildAndAllocateJSGeneratorObject(
    ValueNode* closure, ValueNode* receiver) {
  compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(closure);
  if (!maybe_constant.has_value()) return ReduceResult::Fail();
  if (!maybe_constant->IsJSFunction()) return ReduceResult::Fail();
  compiler::JSFunctionRef function = maybe_constant->AsJSFunction();
  if (!function.has_initial_map(broker())) return ReduceResult::Fail();

  // Create the register file.
  compiler::SharedFunctionInfoRef shared = function.shared(broker());
  DCHECK(shared.HasBytecodeArray());
  compiler::BytecodeArrayRef bytecode_array = shared.GetBytecodeArray(broker());
  int parameter_count_no_receiver = bytecode_array.parameter_count() - 1;
  int length = parameter_count_no_receiver + bytecode_array.register_count();
  if (FixedArray::SizeFor(length) > kMaxRegularHeapObjectSize) {
    return ReduceResult::Fail();
  }
  auto undefined = GetRootConstant(RootIndex::kUndefinedValue);
  VirtualObject* register_file =
      CreateFixedArray(broker()->fixed_array_map(), length);
  for (int i = 0; i < length; i++) {
    register_file->set(FixedArray::OffsetOfElementAt(i), undefined);
  }

  // Create the JS|Async]GeneratorObject instance.
  compiler::SlackTrackingPrediction slack_tracking_prediction =
      broker()->dependencies()->DependOnInitialMapInstanceSizePrediction(
          function);
  compiler::MapRef initial_map = function.initial_map(broker());
  VirtualObject* generator = CreateJSGeneratorObject(
      initial_map, slack_tracking_prediction.instance_size(), GetContext(),
      closure, receiver, register_file);

  // Handle in-object properties.
  for (int i = 0; i < slack_tracking_prediction.inobject_property_count();
       i++) {
    generator->set(initial_map.GetInObjectPropertyOffset(i), undefined);
  }

  ValueNode* allocation =
      BuildInlinedAllocation(generator, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the
  // next side effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

namespace {

compiler::OptionalMapRef GetArrayConstructorInitialMap(
    compiler::JSHeapBroker* broker, compiler::JSFunctionRef array_function,
    ElementsKind elements_kind, size_t argc, std::optional<int> maybe_length) {
  compiler::MapRef initial_map = array_function.initial_map(broker);
  if (argc == 1 && (!maybe_length.has_value() || *maybe_length > 0)) {
    // Constructing an Array via new Array(N) where N is an unsigned
    // integer, always creates a holey backing store.
    elements_kind = GetHoleyElementsKind(elements_kind);
  }
  return initial_map.AsElementsKind(broker, elements_kind);
}

}  // namespace

ValueNode* MaglevGraphBuilder::BuildElementsArray(int length) {
  if (length == 0) {
    return GetRootConstant(RootIndex::kEmptyFixedArray);
  }
  VirtualObject* elements =
      CreateFixedArray(broker()->fixed_array_map(), length);
  auto hole = GetRootConstant(RootIndex::kTheHoleValue);
  for (int i = 0; i < length; i++) {
    elements->set(FixedArray::OffsetOfElementAt(i), hole);
  }
  return elements;
}

ReduceResult MaglevGraphBuilder::TryReduceConstructArrayConstructor(
  return allocation;
    compiler::OptionalAllocationSiteRef maybe_allocation_site) {
  // next side effect clear it.
}
          ? maybe_allocation_site->GetElementsKind()
          : array_function.initial_map(broker()).elements_kind();
  // TODO(victorgomes): Support double elements array.
  if (IsDoubleElementsKind(elements_kind)) return ReduceResult::Fail();
  DCHECK(IsFastElementsKind(elements_kind));

  std::optional<int> maybe_length;
  if (args.count() == 1) {
    maybe_length = TryGetInt32Constant(args|0]);
  }
  compiler::OptionalMapRef maybe_initial_map = GetArrayConstructorInitialMap(
      broker(), array_function, elements_kind, args.count(), maybe_length);
  if (!maybe_initial_map.has_value()) return ReduceResult::Fail();
  compiler::MapRef initial_map = maybe_initial_map.value();
  compiler::SlackTrackingPrediction slack_tracking_prediction =
      broker()->dependencies()->DependOnInitialMapInstanceSizePrediction(
          array_function);

  // Tells whether we are protected by either the {site} or a
  // protector cell to do certain speculative optimizations.
  bool can_inline_call = false;
  AllocationType allocation_type = AllocationType::kYoung;

  if (maybe_allocation_site) {
    can_inline_call = maybe_allocation_site->CanInlineCall();
    allocation_type =
        broker()->dependencies()->DependOnPretenureMode(*maybe_allocation_site);
    broker()->dependencies()->DependOnElementsKind(*maybe_allocation_site);
  } else {
    compiler::PropertyCellRef array_constructor_protector = MakeRef(
        broker(), local_isolate()->factory()->array_constructor_protector());
    array_constructor_protector.CacheAsProtector(broker());
    can_inline_call = array_constructor_protector.value(broker()).AsSmi() ==
                      Protectors::kProtectorValid;
  }

  if (args.count() == 0) {
    return BuildAndAllocateJSArray(
        initial_map, GetSmiConstant(0),
        BuildElementsArray(JSArray::kPreallocatedArrayElements),
        slack_tracking_prediction, allocation_type);
  }

  if (maybe_length.has_value() && *maybe_length >= 0 &&
      *maybe_length < JSArray::kInitialMaxFastElementArray) {
    return BuildAndAllocateJSArray(initial_map, GetSmiConstant(*maybe_length),
                                   BuildElementsArray(*maybe_length),
                                   slack_tracking_prediction, allocation_type);
  }

  // TODO(victorgomes): If we know the argument cannot be a number, we should
  // allocate an array with one element.
  // We don't know anything about the length, so we rely on the allocation
  // site to avoid deopt loops.
  if (args.count() == 1 && can_inline_call) {
    return SelectReduction(
        |&](auto& builder) {
          return BuildBranchIfInt32Compare(builder,
                                           Operation::kGreaterThanOrEqual,
                                           args|0], GetInt32Constant(0));
        },
        |&] {
          ValueNode* elements =
              AddNewNode<AllocateElementsArray>({args|0]}, allocation_type);
          return BuildAndAllocateJSArray(initial_map, args|0], elements,
                                         slack_tracking_prediction,
                                         allocation_type);
        },
        |&] {
          ValueNode* error = GetSmiConstant(
              static_cast<int>(MessageTemplate::kInvalidArrayLength));
          return BuildCallRuntime(Runtime::kThrowRangeError, {error});
        });
  }

  // TODO(victorgomes): Support the constructor with argument count larger
  // than 1.
  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::TryReduceConstructBuiltin(
    compiler::JSFunctionRef builtin,
    compiler::SharedFunctionInfoRef shared_function_info, ValueNode* target,
    CallArguments& args) {
  // TODO(victorgomes): specialize more known constants builtin targets.
  switch (shared_function_info.builtin_id()) {
    case Builtin::kArrayConstructor: {
      RETURN_IF_DONE(TryReduceConstructArrayConstructor(builtin, args));
      break;
    }
    case Builtin::kObjectConstructor: {
      // If no value is passed, we can immediately lower to a simple
      // constructor.
      if (args.count() == 0) {
        RETURN_IF_ABORT(BuildCheckValue(target, builtin));
        ValueNode* result = BuildInlinedAllocation(CreateJSConstructor(builtin),
                                                   AllocationType::kYoung);
        // TODO(leszeks): Don't eagerly clear the raw allocation, have the
        // next side effect clear it.
        ClearCurrentAllocationBlock();
        return result;
      maybe_allocation_site.has_value()
    compiler::JSFunctionRef array_function, CallArguments& args,
  ElementsKind elements_kind =
    default:
      break;
  }
  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::TryReduceConstructGeneric(
    compiler::JSFunctionRef function,
    compiler::SharedFunctionInfoRef shared_function_info, ValueNode* target,
    ValueNode* new_target, CallArguments& args,
    compiler::FeedbackSource& feedback_source) {
  RETURN_IF_ABORT(BuildCheckValue(target, function));

  int construct_arg_count = static_cast<int>(args.count());
  base::Vector<ValueNode*> construct_arguments_without_receiver =
      zone()->AllocateVector<ValueNode*>(construct_arg_count);
  for (int i = 0; i < construct_arg_count; i++) {
    construct_arguments_without_receiver|i] = args|i];
  }

  if (IsDerivedConstructor(shared_function_info.kind())) {
    ValueNode* implicit_receiver = GetRootConstant(RootIndex::kTheHoleValue);
    args.set_receiver(implicit_receiver);
    ValueNode* call_result;
    {
      DeoptFrameScope construct(this, implicit_receiver);
      ReduceResult result = TryBuildCallKnownJSFunction(function, new_target,
                                                        args, feedback_source);
      RETURN_IF_ABORT(result);
      call_result = result.value();
    }
    if (CheckType(call_result, NodeType::kJSReceiver)) return call_result;
    ValueNode* constant_node;
    if (compiler::OptionalHeapObjectRef maybe_constant =
            TryGetConstant(call_result, &constant_node)) {
      compiler::HeapObjectRef constant = maybe_constant.value();
      if (constant.IsJSReceiver()) return constant_node;
    }
    if (!call_result->properties().is_tagged()) {
      return BuildCallRuntime(Runtime::kThrowConstructorReturnedNonObject, {});
    }
    return AddNewNode<CheckDerivedConstructResult>({call_result});
  }

  // We do not create a construct stub lazy deopt frame, since
  // FastNewObject cannot fail if target is a JSFunction.
  ValueNode* implicit_receiver = nullptr;
  if (function.has_initial_map(broker())) {
    compiler::MapRef map = function.initial_map(broker());
    if (map.GetConstructor(broker()).equals(function)) {
      implicit_receiver = BuildInlinedAllocation(CreateJSConstructor(function),
                                                 AllocationType::kYoung);
      // TODO(leszeks): Don't eagerly clear the raw allocation, have the
      // next side effect clear it.
      ClearCurrentAllocationBlock();
    }
  }
  if (implicit_receiver == nullptr) {
    implicit_receiver = BuildCallBuiltin<Builtin::kFastNewObject>(
        {GetTaggedValue(target), GetTaggedValue(new_target)});
  }
  EnsureType(implicit_receiver, NodeType::kJSReceiver);

  args.set_receiver(implicit_receiver);
  ValueNode* call_result;
  {
    DeoptFrameScope construct(this, implicit_receiver);
    ReduceResult result = TryBuildCallKnownJSFunction(function, new_target,
                                                      args, feedback_source);
    RETURN_IF_ABORT(result);
    call_result = result.value();
  }
  if (CheckType(call_result, NodeType::kJSReceiver)) return call_result;
  if (!call_result->properties().is_tagged()) return implicit_receiver;
  ValueNode* constant_node;
  if (compiler::OptionalHeapObjectRef maybe_constant =
          TryGetConstant(call_result, &constant_node)) {
    compiler::HeapObjectRef constant = maybe_constant.value();
    DCHECK(CheckType(implicit_receiver, NodeType::kJSReceiver));
    if (constant.IsJSReceiver()) return constant_node;
    return implicit_receiver;
  }
  return AddNewNode<CheckConstructResult>({call_result, implicit_receiver});
}

ReduceResult MaglevGraphBuilder::TryReduceConstruct(
    compiler::HeapObjectRef feedback_target, ValueNode* target,
    ValueNode* new_target, CallArguments& args,
    compiler::FeedbackSource& feedback_source) {
  DCHECK(!feedback_target.IsAllocationSite());
  if (!feedback_target.map(broker()).is_constructor()) {
    // TODO(victorgomes): Deal the case where target is not a constructor.
    return ReduceResult::Fail();
  }

  if (target != new_target) return ReduceResult::Fail();

  // TODO(v8:7700): Add fast paths for other callables.
  if (!feedback_target.IsJSFunction()) return ReduceResult::Fail();
  compiler::JSFunctionRef function = feedback_target.AsJSFunction();

  // Do not inline constructors with break points.
  compiler::SharedFunctionInfoRef shared_function_info =
      function.shared(broker());
  if (shared_function_info.HasBreakInfo(broker())) {
    return ReduceResult::Fail();
  }

  // Do not inline cross natives context.
  if (function.native_context(broker()) != broker()->target_native_context()) {
    return ReduceResult::Fail();
  }

  if (args.mode() != CallArguments::kDefault) {
    // TODO(victorgomes): Maybe inline the spread stub? Or call known
    // function directly if arguments list is an array.
    return ReduceResult::Fail();
  }

  if (shared_function_info.HasBuiltinId()) {
    RETURN_IF_DONE(TryReduceConstructBuiltin(function, shared_function_info,
                                             target, args));
  }

  if (shared_function_info.construct_as_builtin()) {
    // TODO(victorgomes): Inline JSBuiltinsConstructStub.
    return ReduceResult::Fail();
  }

  return TryReduceConstructGeneric(function, shared_function_info, target,
                                   new_target, args, feedback_source);
}

void MaglevGraphBuilder::BuildConstruct(
    ValueNode* target, ValueNode* new_target, CallArguments& args,
    compiler::FeedbackSource& feedback_source) {
  compiler::ProcessedFeedback const& processed_feedback =
      broker()->GetFeedbackForCall(feedback_source);
  if (processed_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForConstruct));
  }

  DCHECK_EQ(processed_feedback.kind(), compiler::ProcessedFeedback::kCall);
  compiler::OptionalHeapObjectRef feedback_target =
      processed_feedback.AsCall().target();
  if (feedback_target.has_value() && feedback_target->IsAllocationSite()) {
    // The feedback is an AllocationSite, which means we have called the
    // Array function and collected transition (and pretenuring) feedback
    // for the resulting arrays.
    compiler::JSFunctionRef array_function =
        broker()->target_native_context().array_function(broker());
    RETURN_VOID_IF_ABORT(BuildCheckValue(target, array_function));
    PROCESS_AND_RETURN_IF_DONE(
        TryReduceConstructArrayConstructor(array_function, args,
                                           feedback_target->AsAllocationSite()),
        SetAccumulator);
  } else {
    if (feedback_target.has_value()) {
      PROCESS_AND_RETURN_IF_DONE(
          TryReduceConstruct(feedback_target.value(), target, new_target, args,
                             feedback_source),
          SetAccumulator);
    }
    if (compiler::OptionalHeapObjectRef maybe_constant =
            TryGetConstant(target)) {
      PROCESS_AND_RETURN_IF_DONE(
          TryReduceConstruct(maybe_constant.value(), target, new_target, args,
                             feedback_source),
          SetAccumulator);
    }
  }
  ValueNode* context = GetContext();
  SetAccumulator(BuildGenericConstruct(target, new_target, context, args,
                                       feedback_source));
}

void MaglevGraphBuilder::VisitConstruct() {
  ValueNode* new_target = GetAccumulator();
  ValueNode* target = LoadRegister(0);
  interpreter::RegisterList reg_list = iterator_.GetRegisterListOperand(1);
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  CallArguments args(ConvertReceiverMode::kNullOrUndefined, reg_list,
                     current_interpreter_frame_);
  BuildConstruct(target, new_target, args, feedback_source);
}

void MaglevGraphBuilder::VisitConstructWithSpread() {
  ValueNode* new_target = GetAccumulator();
  ValueNode* constructor = LoadRegister(0);
  interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
  ValueNode* context = GetContext();
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source(feedback(), slot);

  int kReceiver = 1;
  size_t input_count =
      args.register_count() + kReceiver + ConstructWithSpread::kFixedInputCount;
  ConstructWithSpread* construct = AddNewNode<ConstructWithSpread>(
      input_count,
      |&](ConstructWithSpread* construct) {
        int arg_index = 0;
        // Add undefined receiver.
        construct->set_arg(arg_index++,
                           GetRootConstant(RootIndex::kUndefinedValue));
        for (int i = 0; i < args.register_count(); i++) {
          construct->set_arg(arg_index++, GetTaggedValue(args|i]));
        }
      },
      feedback_source, GetTaggedValue(constructor), GetTaggedValue(new_target),
      GetTaggedValue(context));
  SetAccumulator(construct);
}
      break;
void MaglevGraphBuilder::VisitConstructForwardAllArgs() {
    }
      }
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  if (is_inline()) {
    base::SmallVector<ValueNode*, 8> forwarded_args(argument_count());
    for (int i = 1 /* skip receiver */; i < argument_count(); ++i) {
      forwarded_args|i] = GetInlinedArgument(i);
    }
    CallArguments args(ConvertReceiverMode::kNullOrUndefined,
                       std::move(forwarded_args));
    BuildConstruct(target, new_target, args, feedback_source);
  } else {
    // TODO(syg): Add ConstructForwardAllArgs reductions and support inlining.
    SetAccumulator(
        BuildCallBuiltin<Builtin::kConstructForwardAllArgs_WithFeedback>(
            {GetTaggedValue(target), GetTaggedValue(new_target)},
            feedback_source));
  }
}

void MaglevGraphBuilder::VisitTestEqual() {
  VisitCompareOperation<Operation::kEqual>();
}
void MaglevGraphBuilder::VisitTestEqualStrict() {
  VisitCompareOperation<Operation::kStrictEqual>();
}
void MaglevGraphBuilder::VisitTestLessThan() {
  VisitCompareOperation<Operation::kLessThan>();
}
void MaglevGraphBuilder::VisitTestLessThanOrEqual() {
  VisitCompareOperation<Operation::kLessThanOrEqual>();
}
void MaglevGraphBuilder::VisitTestGreaterThan() {
  VisitCompareOperation<Operation::kGreaterThan>();
}
void MaglevGraphBuilder::VisitTestGreaterThanOrEqual() {
  VisitCompareOperation<Operation::kGreaterThanOrEqual>();
}

MaglevGraphBuilder::InferHasInPrototypeChainResult
MaglevGraphBuilder::InferHasInPrototypeChain(
    ValueNode* receiver, compiler::HeapObjectRef prototype) {
  auto node_info = known_node_aspects().TryGetInfoFor(receiver);
  // If the map set is not found, then we don't know anything about the map of
  // the receiver, so bail.
  if (!node_info || !node_info->possible_maps_are_known()) {
    return kMayBeInPrototypeChain;
  }

  // If the set of possible maps is empty, then there's no possible map for this
  // receiver, therefore this path is unreachable at runtime. We're unlikely to
  // ever hit this case, BuildCheckMaps should already unconditionally deopt,
  // but check it in case another checking operation fails to statically
  // unconditionally deopt.
  if (node_info->possible_maps().is_empty()) {
    // TODO(leszeks): Add an unreachable assert here.
    return kIsNotInPrototypeChain;
  }

  ZoneVector<compiler::MapRef> receiver_map_refs(zone());

  // Try to determine either that all of the {receiver_maps} have the given
  // {prototype} in their chain, or that none do. If we can't tell, return
  // kMayBeInPrototypeChain.
  bool all = true;
  bool none = true;
  for (compiler::MapRef map : node_info->possible_maps()) {
    receiver_map_refs.push_back(map);
    while (true) {
      if (IsSpecialReceiverInstanceType(map.instance_type())) {
        return kMayBeInPrototypeChain;
      }
      if (!map.IsJSObjectMap()) {
        all = false;
        break;
      }
      compiler::HeapObjectRef map_prototype = map.prototype(broker());
      if (map_prototype.equals(prototype)) {
        none = false;
        break;
      }
      map = map_prototype.map(broker());
      // TODO(v8:11457) Support dictionary mode protoypes here.
      if (!map.is_stable() || map.is_dictionary_map()) {
        return kMayBeInPrototypeChain;
      }
      if (map.oddball_type(broker()) == compiler::OddballType::kNull) {
        all = false;
        break;
      }
    }
  }
  DCHECK(!receiver_map_refs.empty());
  DCHECK_IMPLIES(all, !none);
  if (!all && !none) return kMayBeInPrototypeChain;

  {
    compiler::OptionalJSObjectRef last_prototype;
    if (all) {
      // We don't need to protect the full chain if we found the prototype, we
      // can stop at {prototype}.  In fact we could stop at the one before
      // {prototype} but since we're dealing with multiple receiver maps this
      // might be a different object each time, so it's much simpler to include
      // {prototype}. That does, however, mean that we must check {prototype}'s
      // map stability.
      if (!prototype.IsJSObject() || !prototype.map(broker()).is_stable()) {
        return kMayBeInPrototypeChain;
      }
      last_prototype = prototype.AsJSObject();
    }
    broker()->dependencies()->DependOnStablePrototypeChains(
        receiver_map_refs, kStartAtPrototype, last_prototype);
  }

  DCHECK_EQ(all, !none);
  return all ? kIsInPrototypeChain : kIsNotInPrototypeChain;
}

ReduceResult MaglevGraphBuilder::TryBuildFastHasInPrototypeChain(
    ValueNode* object, compiler::HeapObjectRef prototype) {
  auto in_prototype_chain = InferHasInPrototypeChain(object, prototype);
  if (in_prototype_chain == kMayBeInPrototypeChain) return ReduceResult::Fail();

  return GetBooleanConstant(in_prototype_chain == kIsInPrototypeChain);
}

ReduceResult MaglevGraphBuilder::BuildHasInPrototypeChain(
    ValueNode* object, compiler::HeapObjectRef prototype) {
  RETURN_IF_DONE(TryBuildFastHasInPrototypeChain(object, prototype));
  return AddNewNode<HasInPrototypeChain>({object}, prototype);
}

ReduceResult MaglevGraphBuilder::TryBuildFastOrdinaryHasInstance(
  ValueNode* new_target = GetAccumulator();
  ValueNode* target = LoadRegister(0);
  const bool is_constant = callable_node_if_not_constant == nullptr;
  if (!is_constant) return ReduceResult::Fail();

  if (callable.IsJSBoundFunction()) {
    // OrdinaryHasInstance on bound functions turns into a recursive
    // invocation of the instanceof operator again.
    compiler::JSBoundFunctionRef function = callable.AsJSBoundFunction();
    compiler::JSReceiverRef bound_target_function =
        function.bound_target_function(broker());

    if (bound_target_function.IsJSObject()) {
      RETURN_IF_DONE(TryBuildFastInstanceOf(
          object, bound_target_function.AsJSObject(), nullptr));
    }

    // If we can't build a fast instance-of, build a slow one with the
    // partial optimisation of using the bound target function constant.
    return BuildCallBuiltin<Builtin::kInstanceOf>(
        {GetTaggedValue(object), GetConstant(bound_target_function)});
  }

  if (callable.IsJSFunction()) {
    // Optimize if we currently know the "prototype" property.
    compiler::JSFunctionRef function = callable.AsJSFunction();

    // TODO(v8:7700): Remove the has_prototype_slot condition once the broker
    // is always enabled.
    if (!function.map(broker()).has_prototype_slot() ||
        !function.has_instance_prototype(broker()) ||
        function.PrototypeRequiresRuntimeLookup(broker())) {
      return ReduceResult::Fail();
    }

    compiler::HeapObjectRef prototype =
        broker()->dependencies()->DependOnPrototypeProperty(function);
    return BuildHasInPrototypeChain(object, prototype);
  }

  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::BuildOrdinaryHasInstance(
    ValueNode* object, compiler::JSObjectRef callable,
    ValueNode* callable_node_if_not_constant) {
  RETURN_IF_DONE(TryBuildFastOrdinaryHasInstance(
      object, callable, callable_node_if_not_constant));

  return BuildCallBuiltin<Builtin::kOrdinaryHasInstance>(
      {callable_node_if_not_constant
           ? GetTaggedValue(callable_node_if_not_constant)
           : GetConstant(callable),
       GetTaggedValue(object)});
}

ReduceResult MaglevGraphBuilder::TryBuildFastInstanceOf(
    ValueNode* object, compiler::JSObjectRef callable,
    ValueNode* callable_node_if_not_constant) {
  compiler::MapRef receiver_map = callable.map(broker());
  compiler::NameRef name = broker()->has_instance_symbol();
  compiler::PropertyAccessInfo access_info = broker()->GetPropertyAccessInfo(
      receiver_map, name, compiler::AccessMode::kLoad);

  // TODO(v8:11457) Support dictionary mode holders here.
  if (access_info.IsInvalid() || access_info.HasDictionaryHolder()) {
    return ReduceResult::Fail();
  }
  access_info.RecordDependencies(broker()->dependencies());

  if (access_info.IsNotFound()) {
    // If there's no @@hasInstance handler, the OrdinaryHasInstance operation
    // takes over, but that requires the constructor to be callable.
    if (!receiver_map.is_callable()) {
      return ReduceResult::Fail();
    }

    broker()->dependencies()->DependOnStablePrototypeChains(
        access_info.lookup_start_object_maps(), kStartAtPrototype);

    // Monomorphic property access.
    if (callable_node_if_not_constant) {
    ValueNode* object, compiler::JSObjectRef callable,

    ValueNode* callable_node_if_not_constant) {
    } else {
      // Even if we have a constant receiver, we still have to make sure its
      // map is correct, in case it migrates.
      if (receiver_map.is_stable()) {
        broker()->dependencies()->DependOnStableMap(receiver_map);
      } else {
        RETURN_IF_ABORT(BuildCheckMaps(
            GetConstant(callable),
            base::VectorOf(access_info.lookup_start_object_maps())));
      }
    }

    return BuildOrdinaryHasInstance(object, callable,
                                    callable_node_if_not_constant);
  }

  if (access_info.IsFastDataConstant()) {
    compiler::OptionalJSObjectRef holder = access_info.holder();
    bool found_on_proto = holder.has_value();
    compiler::JSObjectRef holder_ref =
        found_on_proto ? holder.value() : callable;
    if (access_info.field_representation().IsDouble()) {
      return ReduceResult::Fail();
    }
    compiler::OptionalObjectRef has_instance_field =
        holder_ref.GetOwnFastConstantDataProperty(
            broker(), access_info.field_representation(),
            access_info.field_index(), broker()->dependencies());
    if (!has_instance_field.has_value() ||
        !has_instance_field->IsHeapObject() ||
        !has_instance_field->AsHeapObject().map(broker()).is_callable()) {
      return ReduceResult::Fail();
    }

    if (found_on_proto) {
      broker()->dependencies()->DependOnStablePrototypeChains(
          access_info.lookup_start_object_maps(), kStartAtPrototype,
          holder.value());
    }

    ValueNode* callable_node;
    if (callable_node_if_not_constant) {
      // Check that {callable_node_if_not_constant} is actually {callable}.
      RETURN_IF_ABORT(BuildCheckValue(callable_node_if_not_constant, callable));
      callable_node = callable_node_if_not_constant;
    } else {
      callable_node = GetConstant(callable);
    }
    RETURN_IF_ABORT(BuildCheckMaps(
        callable_node, base::VectorOf(access_info.lookup_start_object_maps())));

    // Special case the common case, where @@hasInstance is
    // Function.p.hasInstance. In this case we don't need to call ToBoolean (or
    // use the continuation), since OrdinaryHasInstance is guaranteed to return
    // a boolean.
    if (has_instance_field->IsJSFunction()) {
      compiler::SharedFunctionInfoRef shared =
          has_instance_field->AsJSFunction().shared(broker());
      if (shared.HasBuiltinId() &&
          shared.builtin_id() == Builtin::kFunctionPrototypeHasInstance) {
        return BuildOrdinaryHasInstance(object, callable,
                                        callable_node_if_not_constant);
      }
    }

    // Call @@hasInstance
    CallArguments args(ConvertReceiverMode::kNotNullOrUndefined,
                       {callable_node, object});
    ValueNode* call_result;
    {
      // Make sure that a lazy deopt after the @@hasInstance call also performs
      // ToBoolean before returning to the interpreter.
      DeoptFrameScope continuation_scope(
          this, Builtin::kToBooleanLazyDeoptContinuation);

      if (has_instance_field->IsJSFunction()) {
        SaveCallSpeculationScope saved(this);
        GET_VALUE_OR_ABORT(
            call_result,
            ReduceCallForConstant(has_instance_field->AsJSFunction(), args));
      } else {
        call_result = BuildGenericCall(GetConstant(*has_instance_field),
                                       Call::TargetType::kAny, args);
      }
      // TODO(victorgomes): Propagate the case if we need to soft deopt.
    }

    return BuildToBoolean(call_result);
  }

  return ReduceResult::Fail();
}

template <bool flip>
ValueNode* MaglevGraphBuilder::BuildToBoolean(ValueNode* value) {
  if (IsConstantNode(value->opcode())) {
    return GetBooleanConstant(FromConstantToBool(local_isolate(), value) ^
                              flip);
  }

  switch (value->value_representation()) {
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      // The ToBoolean of both the_hole and NaN is false, so we can use the
      // same operation for HoleyFloat64 and Float64.
      return AddNewNode<Float64ToBoolean>({value}, flip);

    case ValueRepresentation::kUint32:
      // Uint32 has the same logic as Int32 when converting ToBoolean, namely
      // comparison against zero, so we can cast it and ignore the signedness.
      value = AddNewNode<TruncateUint32ToInt32>({value});
      ||fallthrough]];
    case ValueRepresentation::kInt32:
      return AddNewNode<Int32ToBoolean>({value}, flip);

    case ValueRepresentation::kIntPtr:
      UNREACHABLE();

    case ValueRepresentation::kTagged:
      break;
  }

  NodeInfo* node_info = known_node_aspects().TryGetInfoFor(value);
  if (node_info) {
    if (ValueNode* as_int32 = node_info->alternative().int32()) {
      return AddNewNode<Int32ToBoolean>({as_int32}, flip);
    }
    if (ValueNode* as_float64 = node_info->alternative().float64()) {
      return AddNewNode<Float64ToBoolean>({as_float64}, flip);
    }
  }

  NodeType value_type;
  if (CheckType(value, NodeType::kJSReceiver, &value_type)) {
    ValueNode* result = BuildTestUndetectable(value);
    // TODO(victorgomes): Check if it is worth to create
    // TestUndetectableLogicalNot or to remove ToBooleanLogicalNot, since we
    // already optimize LogicalNots by swapping the branches.
    if constexpr (!flip) {
      result = BuildLogicalNot(result);
    }
    return result;
  }
  ValueNode* falsy_value = nullptr;
  if (CheckType(value, NodeType::kString)) {
    falsy_value = GetRootConstant(RootIndex::kempty_string);
  } else if (CheckType(value, NodeType::kSmi)) {
    falsy_value = GetSmiConstant(0);
  }
  if (falsy_value != nullptr) {
    return AddNewNode<std::conditional_t<flip, TaggedEqual, TaggedNotEqual>>(
        {value, falsy_value});
  }
  if (CheckType(value, NodeType::kBoolean)) {
    if constexpr (flip) {
      value = BuildLogicalNot(value);
    }
    return value;
  }
  return AddNewNode<std::conditional_t<flip, ToBooleanLogicalNot, ToBoolean>>(
      {value}, GetCheckType(value_type));
}

ReduceResult MaglevGraphBuilder::TryBuildFastInstanceOfWithFeedback(
    ValueNode* object, ValueNode* callable,
    compiler::FeedbackSource feedback_source) {
  compiler::ProcessedFeedback const& feedback =
      broker()->GetFeedbackForInstanceOf(feedback_source);

  if (feedback.IsInsufficient()) {
    return EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForInstanceOf);
  }

  // Check if the right hand side is a known receiver, or
  // we have feedback from the InstanceOfIC.
  compiler::OptionalHeapObjectRef maybe_constant;
  if ((maybe_constant = TryGetConstant(callable)) &&
      maybe_constant.value().IsJSObject()) {
    compiler::JSObjectRef callable_ref = maybe_constant.value().AsJSObject();
    return TryBuildFastInstanceOf(object, callable_ref, nullptr);
  }
  if (feedback_source.IsValid()) {
    compiler::OptionalJSObjectRef callable_from_feedback =
        feedback.AsInstanceOf().value();
    if (callable_from_feedback) {
      return TryBuildFastInstanceOf(object, *callable_from_feedback, callable);
    }
  }
  return ReduceResult::Fail();
}

void MaglevGraphBuilder::VisitTestInstanceOf() {
  // TestInstanceOf <src> <feedback_slot>
  ValueNode* object = LoadRegister(0);
  ValueNode* callable = GetAccumulator();
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  ReduceResult result =
      RETURN_IF_ABORT(BuildCheckMaps(
  PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);
          callable_node_if_not_constant,
          base::VectorOf(access_info.lookup_start_object_maps())));
  SetAccumulator(
      AddNewNode<TestInstanceOf>({context, object, callable}, feedback_source));
}

void MaglevGraphBuilder::VisitTestIn() {
  // TestIn <src> <feedback_slot>
  ValueNode* object = GetAccumulator();
  ValueNode* name = LoadRegister(0);
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  // TODO(victorgomes): Create fast path using feedback.
  USE(feedback_source);

  SetAccumulator(BuildCallBuiltin<Builtin::kKeyedHasIC>(
      {GetTaggedValue(object), GetTaggedValue(name)}, feedback_source));
}

void MaglevGraphBuilder::VisitToName() {
  // ToObject <dst>
  if (!CheckType(GetAccumulator(), NodeType::kName)) {
    SetAccumulator(AddNewNode<ToName>({GetContext(), GetAccumulator()}));
  }
}

ValueNode* MaglevGraphBuilder::BuildToString(ValueNode* value,
                                             ToString::ConversionMode mode) {
  if (CheckType(value, NodeType::kString)) return value;
  // TODO(victorgomes): Add fast path for constant primitives.
  if (CheckType(value, NodeType::kNumber)) {
    // TODO(verwaest): Float64ToString if float.
    return AddNewNode<NumberToString>({value});
  }
  return AddNewNode<ToString>({GetContext(), value}, mode);
}

void MaglevGraphBuilder::BuildToNumberOrToNumeric(Object::Conversion mode) {
  ValueNode* value = GetAccumulator();
  switch (value->value_representation()) {
    case ValueRepresentation::kInt32:
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kFloat64:
      return;

    case ValueRepresentation::kHoleyFloat64: {
      SetAccumulator(AddNewNode<HoleyFloat64ToMaybeNanFloat64>({value}));
      return;
    }

    case ValueRepresentation::kTagged:
      // We'll insert the required checks depending on the feedback.
      break;

    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }

  FeedbackSlot slot = GetSlotOperand(0);
  switch (broker()->GetFeedbackForBinaryOperation(
      compiler::FeedbackSource(feedback(), slot))) {
    case BinaryOperationHint::kSignedSmall:
      RETURN_VOID_IF_ABORT(BuildCheckSmi(value));
      break;
    case BinaryOperationHint::kSignedSmallInputs:
      UNREACHABLE();
    case BinaryOperationHint::kNumber:
    case BinaryOperationHint::kBigInt:
    case BinaryOperationHint::kBigInt64:
      if (mode == Object::Conversion::kToNumber &&
          EnsureType(value, NodeType::kNumber)) {
        return;
      }
      AddNewNode<CheckNumber>({value}, mode);
      break;
    case BinaryOperationHint::kNone:
    // TODO(leszeks): Faster ToNumber for kNumberOrOddball
    case BinaryOperationHint::kNumberOrOddball:
    case BinaryOperationHint::kString:
    case BinaryOperationHint::kStringOrStringWrapper:
    case BinaryOperationHint::kAny:
      if (CheckType(value, NodeType::kNumber)) return;
      SetAccumulator(AddNewNode<ToNumberOrNumeric>({value}, mode));
      break;
  }
}

void MaglevGraphBuilder::VisitToNumber() {
  BuildToNumberOrToNumeric(Object::Conversion::kToNumber);
}
void MaglevGraphBuilder::VisitToNumeric() {
  BuildToNumberOrToNumeric(Object::Conversion::kToNumeric);
}

void MaglevGraphBuilder::VisitToObject() {
  // ToObject <dst>
  ValueNode* value = GetAccumulator();
  interpreter::Register destination = iterator_.GetRegisterOperand(0);
  NodeType old_type;
  if (CheckType(value, NodeType::kJSReceiver, &old_type)) {
    MoveNodeBetweenRegisters(interpreter::Register::virtual_accumulator(),
                             destination);
  } else {
    StoreRegister(destination, AddNewNode<ToObject>({GetContext(), value},
                                                    GetCheckType(old_type)));
  }
}

void MaglevGraphBuilder::VisitToString() {
  // ToString
  SetAccumulator(BuildToString(GetAccumulator(), ToString::kThrowOnSymbol));
}

void MaglevGraphBuilder::VisitToBoolean() {
  SetAccumulator(BuildToBoolean(GetAccumulator()));
}

void MaglevGraphBuilder::VisitCreateRegExpLiteral() {
  // CreateRegExpLiteral <pattern_idx> <literal_idx> <flags>
  compiler::StringRef pattern = GetRefOperand<String>(0);
  FeedbackSlot slot = GetSlotOperand(1);
  uint32_t flags = GetFlag16Operand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  compiler::ProcessedFeedback const& processed_feedback =
      broker()->GetFeedbackForRegExpLiteral(feedback_source);
      TryBuildFastInstanceOfWithFeedback(object, callable, feedback_source);
    compiler::RegExpBoilerplateDescriptionRef literal =

  ValueNode* context = GetContext();
        broker()->target_native_context();
    compiler::MapRef map =
        native_context.regexp_function(broker()).initial_map(broker());
    SetAccumulator(BuildInlinedAllocation(
        CreateRegExpLiteralObject(map, literal), AllocationType::kYoung));
    // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
    // effect clear it.
    ClearCurrentAllocationBlock();
    return;
  }
  // Fallback.
  SetAccumulator(
      AddNewNode<CreateRegExpLiteral>({}, pattern, feedback_source, flags));
}

void MaglevGraphBuilder::VisitCreateArrayLiteral() {
  compiler::HeapObjectRef constant_elements = GetRefOperand<HeapObject>(0);
  FeedbackSlot slot_index = GetSlotOperand(1);
  int bytecode_flags = GetFlag8Operand(2);
  int literal_flags =
      interpreter::CreateArrayLiteralFlags::FlagsBits::decode(bytecode_flags);
  compiler::FeedbackSource feedback_source(feedback(), slot_index);

  compiler::ProcessedFeedback const& processed_feedback =
      broker()->GetFeedbackForArrayOrObjectLiteral(feedback_source);

  if (processed_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForArrayLiteral));
  }

  ReduceResult result =
      TryBuildFastCreateObjectOrArrayLiteral(processed_feedback.AsLiteral());
  PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);

  if (interpreter::CreateArrayLiteralFlags::FastCloneSupportedBit::decode(
          bytecode_flags)) {
    // TODO(victorgomes): CreateShallowArrayLiteral should not need the
    // boilerplate descriptor. However the current builtin checks that the
    // feedback exists and fallsback to CreateArrayLiteral if it doesn't.
    SetAccumulator(AddNewNode<CreateShallowArrayLiteral>(
        {}, constant_elements, feedback_source, literal_flags));
  } else {
    SetAccumulator(AddNewNode<CreateArrayLiteral>(
        {}, constant_elements, feedback_source, literal_flags));
  }
}

void MaglevGraphBuilder::VisitCreateArrayFromIterable() {
  ValueNode* iterable = GetAccumulator();
  SetAccumulator(BuildCallBuiltin<Builtin::kIterableToListWithSymbolLookup>(
      {GetTaggedValue(iterable)}));
}

void MaglevGraphBuilder::VisitCreateEmptyArrayLiteral() {
  FeedbackSlot slot_index = GetSlotOperand(0);
  compiler::FeedbackSource feedback_source(feedback(), slot_index);
  compiler::ProcessedFeedback const& processed_feedback =
      broker()->GetFeedbackForArrayOrObjectLiteral(feedback_source);
  if (processed_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForArrayLiteral));
  }
  compiler::AllocationSiteRef site = processed_feedback.AsLiteral().value();

  broker()->dependencies()->DependOnElementsKind(site);
  ElementsKind kind = site.GetElementsKind();

  compiler::NativeContextRef native_context = broker()->target_native_context();
  compiler::MapRef map = native_context.GetInitialJSArrayMap(broker(), kind);
  // Initial JSArray map shouldn't have any in-object properties.
  SBXCHECK_EQ(map.GetInObjectProperties(), 0);
  SetAccumulator(BuildInlinedAllocation(
      CreateJSArray(map, map.instance_size(), GetSmiConstant(0)),
      AllocationType::kYoung));
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
}

std::optional<VirtualObject*>
MaglevGraphBuilder::TryReadBoilerplateForFastLiteral(
    compiler::JSObjectRef boilerplate, AllocationType allocation, int max_depth,
    int* max_properties) {
  DCHECK_GE(max_depth, 0);
  DCHECK_GE(*max_properties, 0);

  if (max_depth == 0) return {};

  // Prevent concurrent migrations of boilerplate objects.
  compiler::JSHeapBroker::BoilerplateMigrationGuardIfNeeded
      boilerplate_access_guard(broker());

  // Now that we hold the migration lock, get the current map.
  compiler::MapRef boilerplate_map = boilerplate.map(broker());
  // Protect against concurrent changes to the boilerplate object by checking
  // for an identical value at the end of the compilation.
  broker()->dependencies()->DependOnObjectSlotValue(
      boilerplate, HeapObject::kMapOffset, boilerplate_map);
  {
    compiler::OptionalMapRef current_boilerplate_map =
    compiler::NativeContextRef native_context =
    if (!current_boilerplate_map.has_value() ||
  if (!processed_feedback.IsInsufficient()) {
        processed_feedback.AsRegExpLiteral().value();
      // re-learn the boilerplate. This will be easier once we get rid of the
      // two-pass approach, since we'll be able to create the eager deopt here
      // and return a ReduceResult::DoneWithAbort().
      return {};
    }
  }

  // Bail out if the boilerplate map has been deprecated.  The map could of
  // course be deprecated at some point after the line below, but it's not a
  // correctness issue -- it only means the literal won't be created with the
  // most up to date map(s).
  if (boilerplate_map.is_deprecated()) return {};

  // We currently only support in-object properties.
  if (boilerplate.map(broker()).elements_kind() == DICTIONARY_ELEMENTS ||
      boilerplate.map(broker()).is_dictionary_map() ||
      !boilerplate.raw_properties_or_hash(broker()).has_value()) {
    return {};
  }
  {
    compiler::ObjectRef properties =
        *boilerplate.raw_properties_or_hash(broker());
    bool const empty =
        properties.IsSmi() ||
        properties.equals(MakeRef(
            broker(), local_isolate()->factory()->empty_fixed_array())) ||
        properties.equals(MakeRef(
            broker(),
            Cast<Object>(local_isolate()->factory()->empty_property_array())));
    if (!empty) return {};
  }

  compiler::OptionalFixedArrayBaseRef maybe_elements =
      boilerplate.elements(broker(), kRelaxedLoad);
  if (!maybe_elements.has_value()) return {};
  compiler::FixedArrayBaseRef boilerplate_elements = maybe_elements.value();
  broker()->dependencies()->DependOnObjectSlotValue(
      boilerplate, JSObject::kElementsOffset, boilerplate_elements);
  const uint32_t elements_length = boilerplate_elements.length();

  VirtualObject* fast_literal =
      boilerplate_map.IsJSArrayMap()
          ? CreateJSArray(
                boilerplate_map, boilerplate_map.instance_size(),
                GetConstant(
                    boilerplate.AsJSArray().GetBoilerplateLength(broker())))
          : CreateJSObject(boilerplate_map);

  int inobject_properties = boilerplate_map.GetInObjectProperties();

  // Compute the in-object properties to store first.
  int index = 0;
  for (InternalIndex i :
       InternalIndex::Range(boilerplate_map.NumberOfOwnDescriptors())) {
    PropertyDetails const property_details =
        boilerplate_map.GetPropertyDetails(broker(), i);
    if (property_details.location() != PropertyLocation::kField) continue;
    DCHECK_EQ(PropertyKind::kData, property_details.kind());
    if ((*max_properties)-- == 0) return {};

    int offset = boilerplate_map.GetInObjectPropertyOffset(index);
#ifdef DEBUG
    FieldIndex field_index =
        FieldIndex::ForDetails(*boilerplate_map.object(), property_details);
    DCHECK(field_index.is_inobject());
    DCHECK_EQ(index, field_index.property_index());
    DCHECK_EQ(field_index.offset(), offset);
#endif

    // The index is derived from the in-sandbox `NumberOfOwnDescriptors` value,
    // but the access is out-of-sandbox fast_literal fields.
    SBXCHECK_LT(index, inobject_properties);

    // Note: the use of RawInobjectPropertyAt (vs. the higher-level
    // GetOwnFastConstantDataProperty) here is necessary, since the underlying
    // value may be `uninitialized`, which the latter explicitly does not
    // support.
    compiler::OptionalObjectRef maybe_boilerplate_value =
        boilerplate.RawInobjectPropertyAt(
            broker(),
            FieldIndex::ForInObjectOffset(offset, FieldIndex::kTagged));
    if (!maybe_boilerplate_value.has_value()) return {};

    // Note: We don't need to take a compilation dependency verifying the value
    // of `boilerplate_value`, since boilerplate properties are constant after
    // initialization modulo map migration. We protect against concurrent map
    // migrations (other than elements kind transition, which don't affect us)
    // via the boilerplate_migration_access lock.
    compiler::ObjectRef boilerplate_value = maybe_boilerplate_value.value();

    if (boilerplate_value.IsJSObject()) {
      compiler::JSObjectRef boilerplate_object = boilerplate_value.AsJSObject();
      std::optional<VirtualObject*> maybe_object_value =
          TryReadBoilerplateForFastLiteral(boilerplate_object, allocation,
                                           max_depth - 1, max_properties);
      if (!maybe_object_value.has_value()) return {};
      fast_literal->set(offset, maybe_object_value.value());
    } else if (property_details.representation().IsDouble()) {
      fast_literal->set(offset,
                        CreateHeapNumber(Float64::FromBits(
                            boilerplate_value.AsHeapNumber().value_as_bits())));
    } else {
      // It's fine to store the 'uninitialized' Oddball into a Smi field since
      // it will get overwritten anyway.
      DCHECK_IMPLIES(property_details.representation().IsSmi() &&
                         !boilerplate_value.IsSmi(),
                     IsUninitialized(*boilerplate_value.object()));
      fast_literal->set(offset, GetConstant(boilerplate_value));
    }
    index++;
  }

  // Fill slack at the end of the boilerplate object with filler maps.
  for (; index < inobject_properties; ++index) {
    DCHECK(!V8_MAP_PACKING_BOOL);
    // TODO(wenyuzhao): Fix incorrect MachineType when V8_MAP_PACKING is
    // enabled.
    int offset = boilerplate_map.GetInObjectPropertyOffset(index);
    fast_literal->set(offset, GetRootConstant(RootIndex::kOnePointerFillerMap));
  }

  DCHECK_EQ(JSObject::kElementsOffset, JSArray::kElementsOffset);
  // Empty or copy-on-write elements just store a constant.
  compiler::MapRef elements_map = boilerplate_elements.map(broker());
  // Protect against concurrent changes to the boilerplate object by checking
  // for an identical value at the end of the compilation.
  broker()->dependencies()->DependOnObjectSlotValue(
      boilerplate_elements, HeapObject::kMapOffset, elements_map);
  if (boilerplate_elements.length() == 0 ||
      elements_map.IsFixedCowArrayMap(broker())) {
    if (allocation == AllocationType::kOld &&
        !boilerplate.IsElementsTenured(boilerplate_elements)) {
      return {};
    }
    fast_literal->set(JSObject::kElementsOffset,
                      GetConstant(boilerplate_elements));
  } else {
    // Compute the elements to store first (might have effects).
    if (boilerplate_elements.IsFixedDoubleArray()) {
      int const size = FixedDoubleArray::SizeFor(elements_length);
      if (size > kMaxRegularHeapObjectSize) return {};
      fast_literal->set(
          JSObject::kElementsOffset,
          CreateDoubleFixedArray(elements_length,
                                 boilerplate_elements.AsFixedDoubleArray()));
    } else {
      int const size = FixedArray::SizeFor(elements_length);
      if (size > kMaxRegularHeapObjectSize) return {};
      VirtualObject* elements =
          CreateFixedArray(broker()->fixed_array_map(), elements_length);
      compiler::FixedArrayRef boilerplate_elements_as_fixed_array =
          boilerplate_elements.AsFixedArray();
      for (uint32_t i = 0; i < elements_length; ++i) {
        if ((*max_properties)-- == 0) return {};
        compiler::OptionalObjectRef element_value =
            boilerplate_elements_as_fixed_array.TryGet(broker(), i);
        if (!element_value.has_value()) return {};
        if (element_value->IsJSObject()) {
Command: cd /Users/tarun/v8/v8/src && sed -n '3001,6000p' '/Users/tarun/v8/v8/src/maglev/maglev-ir.cc'Command: cd /Users/tarun/v8/v8/src && sed -n '12001,14107p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'Command: cd /Users/tarun/v8/v8/src && sed -n '9001,10563p' '/Users/tarun/v8/v8/src/maglev/maglev-ir.h'
      // TODO(leszeks): Emit an eager deopt for this case, so that we can
        boilerplate.map_direct_read(broker());

        !current_boilerplate_map->equals(boilerplate_map)) {


              TryReadBoilerplateForFastLiteral(element_value->AsJSObject(),
                                               allocation, max_depth - 1,
                                               max_properties);
          if (!object.has_value()) return {};
          elements->set(FixedArray::OffsetOfElementAt(i), *object);
        } else {
          elements->set(FixedArray::OffsetOfElementAt(i),
                        GetConstant(*element_value));
        }
      }

      fast_literal->set(JSObject::kElementsOffset, elements);
    }
  }

  return fast_literal;
}

VirtualObject* MaglevGraphBuilder::DeepCopyVirtualObject(VirtualObject* old) {
  CHECK_EQ(old->type(), VirtualObject::kDefault);
  ValueNode** slots = zone()->AllocateArray<ValueNode*>(old->slot_count());
  VirtualObject* vobject = NodeBase::New<VirtualObject>(
      zone(), 0, old->map(), NewObjectId(), old->slot_count(), slots);
  current_interpreter_frame_.add_object(vobject);
  for (int i = 0; i < static_cast<int>(old->slot_count()); i++) {
    vobject->set_by_index(i, old->get_by_index(i));
  }
  vobject->set_allocation(old->allocation());
  old->allocation()->UpdateObject(vobject);
  return vobject;
}

VirtualObject* MaglevGraphBuilder::CreateVirtualObjectForMerge(
    compiler::MapRef map, uint32_t slot_count) {
  ValueNode** slots = zone()->AllocateArray<ValueNode*>(slot_count);
  VirtualObject* vobject = NodeBase::New<VirtualObject>(
      zone(), 0, map, NewObjectId(), slot_count, slots);
  return vobject;
}

VirtualObject* MaglevGraphBuilder::CreateVirtualObject(
    compiler::MapRef map, uint32_t slot_count_including_map) {
  // VirtualObjects are not added to the Maglev graph.
  DCHECK_GT(slot_count_including_map, 0);
  uint32_t slot_count = slot_count_including_map - 1;
  ValueNode** slots = zone()->AllocateArray<ValueNode*>(slot_count);
  VirtualObject* vobject = NodeBase::New<VirtualObject>(
      zone(), 0, map, NewObjectId(), slot_count, slots);
  return vobject;
}

VirtualObject* MaglevGraphBuilder::CreateHeapNumber(Float64 value) {
  // VirtualObjects are not added to the Maglev graph.
  VirtualObject* vobject = NodeBase::New<VirtualObject>(
      zone(), 0, broker()->heap_number_map(), NewObjectId(), value);
  return vobject;
}

VirtualObject* MaglevGraphBuilder::CreateDoubleFixedArray(
    uint32_t elements_length, compiler::FixedDoubleArrayRef elements) {
  // VirtualObjects are not added to the Maglev graph.
  VirtualObject* vobject = NodeBase::New<VirtualObject>(
      zone(), 0, broker()->fixed_double_array_map(), NewObjectId(),
      elements_length, elements);
  return vobject;
}

VirtualObject* MaglevGraphBuilder::CreateJSObject(compiler::MapRef map) {
  DCHECK(!map.is_dictionary_map());
  DCHECK(!map.IsInobjectSlackTrackingInProgress());
  int slot_count = map.instance_size() / kTaggedSize;
  SBXCHECK_GE(slot_count, 3);
  VirtualObject* object = CreateVirtualObject(map, slot_count);
  object->set(JSObject::kPropertiesOrHashOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSObject::kElementsOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->ClearSlots(JSObject::kElementsOffset,
                     GetRootConstant(RootIndex::kOnePointerFillerMap));
  return object;
}

VirtualObject* MaglevGraphBuilder::CreateJSArray(compiler::MapRef map,
                                                 int instance_size,
                                                 ValueNode* length) {
  int slot_count = instance_size / kTaggedSize;
  SBXCHECK_GE(slot_count, 4);
  VirtualObject* object = CreateVirtualObject(map, slot_count);
  object->set(JSArray::kPropertiesOrHashOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSArray::kElementsOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSArray::kLengthOffset, length);
  object->ClearSlots(JSArray::kLengthOffset,
                     GetRootConstant(RootIndex::kOnePointerFillerMap));
  return object;
}

VirtualObject* MaglevGraphBuilder::CreateJSArrayIterator(
    compiler::MapRef map, ValueNode* iterated_object, IterationKind kind) {
  int slot_count = map.instance_size() / kTaggedSize;
  SBXCHECK_EQ(slot_count, 6);
  VirtualObject* object = CreateVirtualObject(map, slot_count);
  object->set(JSArrayIterator::kPropertiesOrHashOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSArrayIterator::kElementsOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSArrayIterator::kIteratedObjectOffset, iterated_object);
  object->set(JSArrayIterator::kNextIndexOffset, GetInt32Constant(0));
  object->set(JSArrayIterator::kKindOffset,
              GetInt32Constant(static_cast<int>(kind)));
  return object;
}

VirtualObject* MaglevGraphBuilder::CreateJSConstructor(
    compiler::JSFunctionRef constructor) {
  compiler::SlackTrackingPrediction prediction =
      broker()->dependencies()->DependOnInitialMapInstanceSizePrediction(
          constructor);
  int slot_count = prediction.instance_size() / kTaggedSize;
  VirtualObject* object =
      CreateVirtualObject(constructor.initial_map(broker()), slot_count);
  SBXCHECK_GE(slot_count, 3);
  object->set(JSObject::kPropertiesOrHashOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSObject::kElementsOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->ClearSlots(JSObject::kElementsOffset,
                     GetRootConstant(RootIndex::kOnePointerFillerMap));
  return object;
}

VirtualObject* MaglevGraphBuilder::CreateFixedArray(compiler::MapRef map,
                                                    int length) {
  int slot_count = FixedArray::SizeFor(length) / kTaggedSize;
  VirtualObject* array = CreateVirtualObject(map, slot_count);
  array->set(offsetof(FixedArray, length_), GetInt32Constant(length));
  array->ClearSlots(offsetof(FixedArray, length_),
                    GetRootConstant(RootIndex::kOnePointerFillerMap));
  return array;
}

VirtualObject* MaglevGraphBuilder::CreateContext(
    compiler::MapRef map, int length, compiler::ScopeInfoRef scope_info,
    ValueNode* previous_context, std::optional<ValueNode*> extension) {
  int slot_count = FixedArray::SizeFor(length) / kTaggedSize;
  VirtualObject* context = CreateVirtualObject(map, slot_count);
  context->set(Context::kLengthOffset, GetInt32Constant(length));
  context->set(Context::OffsetOfElementAt(Context::SCOPE_INFO_INDEX),
               GetConstant(scope_info));
  context->set(Context::OffsetOfElementAt(Context::PREVIOUS_INDEX),
               previous_context);
  int index = Context::PREVIOUS_INDEX + 1;
  if (extension.has_value()) {
    context->set(Context::OffsetOfElementAt(Context::EXTENSION_INDEX),
                 extension.value());
    index++;
  }
  for (; index < length; index++) {
    context->set(Context::OffsetOfElementAt(index),
                 GetRootConstant(RootIndex::kUndefinedValue));
  }
  return context;
}

VirtualObject* MaglevGraphBuilder::CreateArgumentsObject(
    compiler::MapRef map, ValueNode* length, ValueNode* elements,
    std::optional<ValueNode*> callee) {
  DCHECK_EQ(JSSloppyArgumentsObject::kLengthOffset, JSArray::kLengthOffset);
  DCHECK_EQ(JSStrictArgumentsObject::kLengthOffset, JSArray::kLengthOffset);
  int slot_count = map.instance_size() / kTaggedSize;
  SBXCHECK_EQ(slot_count, callee.has_value() ? 5 : 4);
  VirtualObject* arguments = CreateVirtualObject(map, slot_count);
  arguments->set(JSArray::kPropertiesOrHashOffset,
                 GetRootConstant(RootIndex::kEmptyFixedArray));
  arguments->set(JSArray::kElementsOffset, elements);
  arguments->set(JSArray::kLengthOffset, length);
  if (callee.has_value()) {
    arguments->set(JSSloppyArgumentsObject::kCalleeOffset, callee.value());
          std::optional<VirtualObject*> object =
  DCHECK(arguments->map().IsJSArgumentsObjectMap() ||
Observations: Showing section 5/5 (lines 12001-14107) of 'maglev/maglev-graph-builder.cc':
  return arguments;
}

VirtualObject* MaglevGraphBuilder::CreateMappedArgumentsElements(
    compiler::MapRef map, int mapped_count, ValueNode* context,
    ValueNode* unmapped_elements) {
  int slot_count = SloppyArgumentsElements::SizeFor(mapped_count) / kTaggedSize;
  VirtualObject* elements = CreateVirtualObject(map, slot_count);
  elements->set(offsetof(SloppyArgumentsElements, length_),
                GetInt32Constant(mapped_count));
  elements->set(offsetof(SloppyArgumentsElements, context_), context);
  elements->set(offsetof(SloppyArgumentsElements, arguments_),
                unmapped_elements);
  return elements;
}

VirtualObject* MaglevGraphBuilder::CreateRegExpLiteralObject(
    compiler::MapRef map, compiler::RegExpBoilerplateDescriptionRef literal) {
  DCHECK_EQ(JSRegExp::Size(), JSRegExp::kLastIndexOffset + kTaggedSize);
  int slot_count = JSRegExp::Size() / kTaggedSize;
  VirtualObject* regexp = CreateVirtualObject(map, slot_count);
  regexp->set(JSRegExp::kPropertiesOrHashOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  regexp->set(JSRegExp::kElementsOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  regexp->set(JSRegExp::kDataOffset,
              GetTrustedConstant(literal.data(broker()),
                                 kRegExpDataIndirectPointerTag));
  regexp->set(JSRegExp::kSourceOffset, GetConstant(literal.source(broker())));
  regexp->set(JSRegExp::kFlagsOffset, GetInt32Constant(literal.flags()));
  regexp->set(JSRegExp::kLastIndexOffset,
              GetInt32Constant(JSRegExp::kInitialLastIndexValue));
  return regexp;
}

VirtualObject* MaglevGraphBuilder::CreateJSGeneratorObject(
    compiler::MapRef map, int instance_size, ValueNode* context,
    ValueNode* closure, ValueNode* receiver, ValueNode* register_file) {
  int slot_count = instance_size / kTaggedSize;
  InstanceType instance_type = map.instance_type();
  DCHECK(instance_type == JS_GENERATOR_OBJECT_TYPE ||
         instance_type == JS_ASYNC_GENERATOR_OBJECT_TYPE);
  SBXCHECK_GE(slot_count, instance_type == JS_GENERATOR_OBJECT_TYPE ? 10 : 12);
  VirtualObject* object = CreateVirtualObject(map, slot_count);
  object->set(JSGeneratorObject::kPropertiesOrHashOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSGeneratorObject::kElementsOffset,
              GetRootConstant(RootIndex::kEmptyFixedArray));
  object->set(JSGeneratorObject::kContextOffset, context);
  object->set(JSGeneratorObject::kFunctionOffset, closure);
  object->set(JSGeneratorObject::kReceiverOffset, receiver);
  object->set(JSGeneratorObject::kInputOrDebugPosOffset,
              GetRootConstant(RootIndex::kUndefinedValue));
  object->set(JSGeneratorObject::kResumeModeOffset,
              GetInt32Constant(JSGeneratorObject::kNext));
  object->set(JSGeneratorObject::kContinuationOffset,
              GetInt32Constant(JSGeneratorObject::kGeneratorExecuting));
  object->set(JSGeneratorObject::kParametersAndRegistersOffset, register_file);
  if (instance_type == JS_ASYNC_GENERATOR_OBJECT_TYPE) {
    object->set(JSAsyncGeneratorObject::kQueueOffset,
                GetRootConstant(RootIndex::kUndefinedValue));
    object->set(JSAsyncGeneratorObject::kIsAwaitingOffset, GetInt32Constant(0));
  }
  return object;
}

VirtualObject* MaglevGraphBuilder::CreateJSIteratorResult(compiler::MapRef map,
                                                          ValueNode* value,
                                                          ValueNode* done) {
  static_assert(JSIteratorResult::kSize == 5 * kTaggedSize);
  int slot_count = JSIteratorResult::kSize / kTaggedSize;
  VirtualObject* iter_result = CreateVirtualObject(map, slot_count);
  iter_result->set(JSIteratorResult::kPropertiesOrHashOffset,
                   GetRootConstant(RootIndex::kEmptyFixedArray));
  iter_result->set(JSIteratorResult::kElementsOffset,
                   GetRootConstant(RootIndex::kEmptyFixedArray));
  iter_result->set(JSIteratorResult::kValueOffset, value);
  iter_result->set(JSIteratorResult::kDoneOffset, done);
  return iter_result;
}

VirtualObject* MaglevGraphBuilder::CreateJSStringIterator(compiler::MapRef map,
                                                          ValueNode* string) {
  static_assert(JSStringIterator::kHeaderSize == 5 * kTaggedSize);
  int slot_count = JSStringIterator::kHeaderSize / kTaggedSize;
  VirtualObject* string_iter = CreateVirtualObject(map, slot_count);
  string_iter->set(JSStringIterator::kPropertiesOrHashOffset,
                   GetRootConstant(RootIndex::kEmptyFixedArray));
  string_iter->set(JSStringIterator::kElementsOffset,
                   GetRootConstant(RootIndex::kEmptyFixedArray));
  string_iter->set(JSStringIterator::kStringOffset, string);
  string_iter->set(JSStringIterator::kIndexOffset, GetInt32Constant(0));
  return string_iter;
}

InlinedAllocation* MaglevGraphBuilder::ExtendOrReallocateCurrentAllocationBlock(
    AllocationType allocation_type, VirtualObject* vobject) {
  DCHECK_LE(vobject->size(), kMaxRegularHeapObjectSize);
  if (!current_allocation_block_ ||
      current_allocation_block_->allocation_type() != allocation_type ||
      !v8_flags.inline_new ||
      compilation_unit()->info()->for_turboshaft_frontend()) {
    current_allocation_block_ =
        AddNewNode<AllocationBlock>({}, allocation_type);
  }

  int current_size = current_allocation_block_->size();
  if (current_size + vobject->size() > kMaxRegularHeapObjectSize) {
    current_allocation_block_ =
        AddNewNode<AllocationBlock>({}, allocation_type);
  }

  DCHECK_GE(current_size, 0);
  InlinedAllocation* allocation =
      AddNewNode<InlinedAllocation>({current_allocation_block_}, vobject);
  graph()->allocations_escape_map().emplace(allocation, zone());
  current_allocation_block_->Add(allocation);
  vobject->set_allocation(allocation);
  return allocation;
}

void MaglevGraphBuilder::ClearCurrentAllocationBlock() {
  current_allocation_block_ = nullptr;
}

void MaglevGraphBuilder::AddNonEscapingUses(InlinedAllocation* allocation,
                                            int use_count) {
  if (!v8_flags.maglev_escape_analysis) return;
  allocation->AddNonEscapingUses(use_count);
}

void MaglevGraphBuilder::AddDeoptUse(VirtualObject* vobject) {
  if (vobject->type() != VirtualObject::kDefault) return;
  for (uint32_t i = 0; i < vobject->slot_count(); i++) {
    ValueNode* value = vobject->get_by_index(i);
    if (InlinedAllocation* nested_allocation =
            value->TryCast<InlinedAllocation>()) {
      VirtualObject* nested_object =
          current_interpreter_frame_.virtual_objects().FindAllocatedWith(
              nested_allocation);
      CHECK_NOT_NULL(nested_object);
      AddDeoptUse(nested_object);
    } else if (!IsConstantNode(value->opcode()) &&
               value->opcode() != Opcode::kArgumentsElements &&
               value->opcode() != Opcode::kArgumentsLength &&
               value->opcode() != Opcode::kRestLength) {
      AddDeoptUse(value);
    }
  }
}

ValueNode* MaglevGraphBuilder::BuildInlinedAllocationForHeapNumber(
    VirtualObject* vobject, AllocationType allocation_type) {
  DCHECK(vobject->map().IsHeapNumberMap());
  InlinedAllocation* allocation =
      ExtendOrReallocateCurrentAllocationBlock(allocation_type, vobject);
  AddNonEscapingUses(allocation, 2);
  }
         arguments->map().IsJSArrayMap());
  AddNewNode<StoreFloat64>({allocation, GetFloat64Constant(vobject->number())},
                           static_cast<int>(offsetof(HeapNumber, value_)));
  return allocation;
}

ValueNode* MaglevGraphBuilder::BuildInlinedAllocationForDoubleFixedArray(
    VirtualObject* vobject, AllocationType allocation_type) {
  DCHECK(vobject->map().IsFixedDoubleArrayMap());
  InlinedAllocation* allocation =
      ExtendOrReallocateCurrentAllocationBlock(allocation_type, vobject);
  int length = vobject->double_elements_length();
  AddNonEscapingUses(allocation, length + 2);
  BuildStoreMap(allocation, broker()->fixed_double_array_map(),
                StoreMap::initializing_kind(allocation_type));
  AddNewNode<StoreTaggedFieldNoWriteBarrier>(
      {allocation, GetSmiConstant(length)},
      static_cast<int>(offsetof(FixedDoubleArray, length_)),
      StoreTaggedMode::kDefault);
  for (int i = 0; i < length; ++i) {
    AddNewNode<StoreFloat64>(
        {allocation,
         GetFloat64Constant(
             vobject->double_elements().GetFromImmutableFixedDoubleArray(i))},
        FixedDoubleArray::OffsetOfElementAt(i));
  }
  return allocation;
}

ValueNode* MaglevGraphBuilder::BuildInlinedAllocation(
    VirtualObject* vobject, AllocationType allocation_type) {
  current_interpreter_frame_.add_object(vobject);
  if (vobject->type() == VirtualObject::kHeapNumber) {
    return BuildInlinedAllocationForHeapNumber(vobject, allocation_type);
  }
  if (vobject->type() == VirtualObject::kFixedDoubleArray) {
    return BuildInlinedAllocationForDoubleFixedArray(vobject, allocation_type);
  }
  SmallZoneVector<ValueNode*, 8> values(vobject->slot_count(), zone());
  for (uint32_t i = 0; i < vobject->slot_count(); i++) {
    ValueNode* node = vobject->get_by_index(i);
    if (node->Is<VirtualObject>()) {
      VirtualObject* nested = node->Cast<VirtualObject>();
      node = BuildInlinedAllocation(nested, allocation_type);
      vobject->set_by_index(i, node);
    } else if (node->Is<Float64Constant>()) {
      node = BuildInlinedAllocationForHeapNumber(
          CreateHeapNumber(node->Cast<Float64Constant>()->value()),
          allocation_type);
    } else {
      node = GetTaggedValue(node);
    }
    values|i] = node;
  }
  InlinedAllocation* allocation =
      ExtendOrReallocateCurrentAllocationBlock(allocation_type, vobject);
  AddNonEscapingUses(allocation, vobject->slot_count() + 1);
  BuildStoreMap(allocation, vobject->map(),
                StoreMap::initializing_kind(allocation_type));
  for (uint32_t i = 0; i < vobject->slot_count(); i++) {
    BuildInitializeStore(allocation, values|i], (i + 1) * kTaggedSize);
  }
  if (is_loop_effect_tracking()) {
    loop_effects_->allocations.insert(allocation);
  }
  return allocation;
}

ValueNode* MaglevGraphBuilder::BuildInlinedArgumentsElements(int start_index,
                                                             int length) {
  DCHECK(is_inline());
  if (length == 0) {
    return GetRootConstant(RootIndex::kEmptyFixedArray);
  }
  VirtualObject* elements =
      CreateFixedArray(broker()->fixed_array_map(), length);
  for (int i = 0; i < length; i++) {
    elements->set(FixedArray::OffsetOfElementAt(i),
                  inlined_arguments_|i + start_index + 1]);
  }
  return elements;
}

ValueNode* MaglevGraphBuilder::BuildInlinedUnmappedArgumentsElements(
    int mapped_count) {
  int length = argument_count_without_receiver();
  if (length == 0) {
    return GetRootConstant(RootIndex::kEmptyFixedArray);
                StoreMap::initializing_kind(allocation_type));
  VirtualObject* unmapped_elements =
  BuildStoreMap(allocation, broker()->heap_number_map(),
  int i = 0;
  for (; i < mapped_count; i++) {
    unmapped_elements->set(FixedArray::OffsetOfElementAt(i),
                           GetRootConstant(RootIndex::kTheHoleValue));
  }
  for (; i < length; i++) {
    unmapped_elements->set(FixedArray::OffsetOfElementAt(i),
                           inlined_arguments_|i + 1]);
  }
  return unmapped_elements;
}

template <CreateArgumentsType type>
VirtualObject* MaglevGraphBuilder::BuildVirtualArgumentsObject() {
  switch (type) {
    case CreateArgumentsType::kMappedArguments:
      if (parameter_count_without_receiver() == 0) {
        // If there is no aliasing, the arguments object elements are not
        // special in any way, we can just return an unmapped backing store.
        if (is_inline()) {
          int length = argument_count_without_receiver();
          ValueNode* elements = BuildInlinedArgumentsElements(0, length);
          return CreateArgumentsObject(
              broker()->target_native_context().sloppy_arguments_map(broker()),
              GetInt32Constant(length), elements, GetClosure());
        } else {
          ArgumentsLength* length = AddNewNode<ArgumentsLength>({});
          EnsureType(length, NodeType::kSmi);
          ArgumentsElements* elements = AddNewNode<ArgumentsElements>(
              {length}, CreateArgumentsType::kUnmappedArguments,
              parameter_count_without_receiver());
          return CreateArgumentsObject(
              broker()->target_native_context().sloppy_arguments_map(broker()),
              length, elements, GetClosure());
        }
      } else {
        // If the parameter count is zero, we should have used the unmapped
        // backing store.
        int param_count = parameter_count_without_receiver();
        DCHECK_GT(param_count, 0);
        DCHECK(CanAllocateSloppyArgumentElements());
        int param_idx_in_ctxt = compilation_unit_->shared_function_info()
                                    .context_parameters_start() +
                                param_count - 1;
        // The {unmapped_elements} correspond to the extra arguments
        // (overapplication) that do not need be "mapped" to the actual
        // arguments. Mapped arguments are accessed via the context, whereas
        // unmapped arguments are simply accessed via this fixed array. See
        // SloppyArgumentsElements in src/object/arguments.h.
        if (is_inline()) {
          int length = argument_count_without_receiver();
          int mapped_count = std::min(param_count, length);
          ValueNode* unmapped_elements =
              BuildInlinedUnmappedArgumentsElements(mapped_count);
          VirtualObject* elements = CreateMappedArgumentsElements(
              broker()->sloppy_arguments_elements_map(), mapped_count,
              GetContext(), unmapped_elements);
          for (int i = 0; i < mapped_count; i++, param_idx_in_ctxt--) {
            elements->set(SloppyArgumentsElements::OffsetOfElementAt(i),
                          GetInt32Constant(param_idx_in_ctxt));
          }
          return CreateArgumentsObject(
              broker()->target_native_context().fast_aliased_arguments_map(
                  broker()),
              GetInt32Constant(length), elements, GetClosure());
        } else {
          ArgumentsLength* length = AddNewNode<ArgumentsLength>({});
          EnsureType(length, NodeType::kSmi);
          ArgumentsElements* unmapped_elements = AddNewNode<ArgumentsElements>(
              {length}, CreateArgumentsType::kMappedArguments, param_count);
          VirtualObject* elements = CreateMappedArgumentsElements(
              broker()->sloppy_arguments_elements_map(), param_count,
              GetContext(), unmapped_elements);
          ValueNode* the_hole_value = GetConstant(broker()->the_hole_value());
          for (int i = 0; i < param_count; i++, param_idx_in_ctxt--) {
            ValueNode* value = Select(
                |&](auto& builder) {
                  return BuildBranchIfInt32Compare(builder,
                                                   Operation::kLessThan,
                                                   GetInt32Constant(i), length);
                },
                |&] { return GetSmiConstant(param_idx_in_ctxt); },
                |&] { return the_hole_value; });
            elements->set(SloppyArgumentsElements::OffsetOfElementAt(i), value);
          }
          return CreateArgumentsObject(
              broker()->target_native_context().fast_aliased_arguments_map(
                  broker()),
              length, elements, GetClosure());
        }
      }
    case CreateArgumentsType::kUnmappedArguments:
      if (is_inline()) {
        int length = argument_count_without_receiver();
        ValueNode* elements = BuildInlinedArgumentsElements(0, length);
        return CreateArgumentsObject(
            broker()->target_native_context().strict_arguments_map(broker()),
            GetInt32Constant(length), elements);
      } else {
        ArgumentsLength* length = AddNewNode<ArgumentsLength>({});
        EnsureType(length, NodeType::kSmi);
        ArgumentsElements* elements = AddNewNode<ArgumentsElements>(
            {length}, CreateArgumentsType::kUnmappedArguments,
            parameter_count_without_receiver());
        return CreateArgumentsObject(
            broker()->target_native_context().strict_arguments_map(broker()),
            length, elements);
      }
    case CreateArgumentsType::kRestParameter:
  }
      CreateFixedArray(broker()->fixed_array_map(), length);
        int length =
            std::max(0, argument_count_without_receiver() - start_index);
        ValueNode* elements =
            BuildInlinedArgumentsElements(start_index, length);
        return CreateArgumentsObject(
            broker()->target_native_context().js_array_packed_elements_map(
                broker()),
            GetInt32Constant(length), elements);
      } else {
        ArgumentsLength* length = AddNewNode<ArgumentsLength>({});
        EnsureType(length, NodeType::kSmi);
        ArgumentsElements* elements = AddNewNode<ArgumentsElements>(
            {length}, CreateArgumentsType::kRestParameter,
            parameter_count_without_receiver());
        RestLength* rest_length =
            AddNewNode<RestLength>({}, parameter_count_without_receiver());
        return CreateArgumentsObject(
            broker()->target_native_context().js_array_packed_elements_map(
                broker()),
            rest_length, elements);
      }
  }
}

template <CreateArgumentsType type>
ValueNode* MaglevGraphBuilder::BuildAndAllocateArgumentsObject() {
  auto arguments = BuildVirtualArgumentsObject<type>();
  ValueNode* allocation =
      BuildInlinedAllocation(arguments, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
  return allocation;
}

ReduceResult MaglevGraphBuilder::TryBuildFastCreateObjectOrArrayLiteral(
    const compiler::LiteralFeedback& feedback) {
  compiler::AllocationSiteRef site = feedback.value();
  if (!site.boilerplate(broker()).has_value()) return ReduceResult::Fail();
  AllocationType allocation_type =
      broker()->dependencies()->DependOnPretenureMode(site);

  // First try to extract out the shape and values of the boilerplate, bailing
  // out on complex boilerplates.
  int max_properties = compiler::kMaxFastLiteralProperties;
  std::optional<VirtualObject*> maybe_value = TryReadBoilerplateForFastLiteral(
      *site.boilerplate(broker()), allocation_type,
      compiler::kMaxFastLiteralDepth, &max_properties);
  if (!maybe_value.has_value()) return ReduceResult::Fail();

  // Then, use the collected information to actually create nodes in the graph.
  // TODO(leszeks): Add support for unwinding graph modifications, so that we
  // can get rid of this two pass approach.
  broker()->dependencies()->DependOnElementsKinds(site);
  ReduceResult result = BuildInlinedAllocation(*maybe_value, allocation_type);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
  return result;
}

void MaglevGraphBuilder::VisitCreateObjectLiteral() {
  compiler::ObjectBoilerplateDescriptionRef boilerplate_desc =
      GetRefOperand<ObjectBoilerplateDescription>(0);
  FeedbackSlot slot_index = GetSlotOperand(1);
  int bytecode_flags = GetFlag8Operand(2);
  int literal_flags =
      interpreter::CreateObjectLiteralFlags::FlagsBits::decode(bytecode_flags);
  compiler::FeedbackSource feedback_source(feedback(), slot_index);

  compiler::ProcessedFeedback const& processed_feedback =
      broker()->GetFeedbackForArrayOrObjectLiteral(feedback_source);
  if (processed_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForObjectLiteral));
  }

  ReduceResult result =
      TryBuildFastCreateObjectOrArrayLiteral(processed_feedback.AsLiteral());
  PROCESS_AND_RETURN_IF_DONE(result, SetAccumulator);

  if (interpreter::CreateObjectLiteralFlags::FastCloneSupportedBit::decode(
          bytecode_flags)) {
    // TODO(victorgomes): CreateShallowObjectLiteral should not need the
    // boilerplate descriptor. However the current builtin checks that the
    // feedback exists and fallsback to CreateObjectLiteral if it doesn't.
    SetAccumulator(AddNewNode<CreateShallowObjectLiteral>(
        {}, boilerplate_desc, feedback_source, literal_flags));
  } else {
    SetAccumulator(AddNewNode<CreateObjectLiteral>(
        {}, boilerplate_desc, feedback_source, literal_flags));
  }
}

void MaglevGraphBuilder::VisitCreateEmptyObjectLiteral() {
  compiler::NativeContextRef native_context = broker()->target_native_context();
  compiler::MapRef map =
      native_context.object_function(broker()).initial_map(broker());
  DCHECK(!map.is_dictionary_map());
  DCHECK(!map.IsInobjectSlackTrackingInProgress());
  SetAccumulator(
      BuildInlinedAllocation(CreateJSObject(map), AllocationType::kYoung));
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
}

void MaglevGraphBuilder::VisitCloneObject() {
  // CloneObject <source_idx> <flags> <feedback_slot>
  ValueNode* source = LoadRegister(0);
  ValueNode* flags =
      GetSmiConstant(interpreter::CreateObjectLiteralFlags::FlagsBits::decode(
          GetFlag8Operand(1)));
  FeedbackSlot slot = GetSlotOperand(2);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  SetAccumulator(BuildCallBuiltin<Builtin::kCloneObjectIC>(
      {GetTaggedValue(source), flags}, feedback_source));
}

void MaglevGraphBuilder::VisitGetTemplateObject() {
  // GetTemplateObject <descriptor_idx> <literal_idx>
  compiler::SharedFunctionInfoRef shared_function_info =
      compilation_unit_->shared_function_info();
  ValueNode* description = GetConstant(GetRefOperand<HeapObject>(0));
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& feedback =
      broker()->GetFeedbackForTemplateObject(feedback_source);
  if (feedback.IsInsufficient()) {
    return SetAccumulator(AddNewNode<GetTemplateObject>(
        {description}, shared_function_info, feedback_source));
  }
  compiler::JSArrayRef template_object = feedback.AsTemplateObject().value();
  SetAccumulator(GetConstant(template_object));
}

void MaglevGraphBuilder::VisitCreateClosure() {
  compiler::SharedFunctionInfoRef shared_function_info =
      GetRefOperand<SharedFunctionInfo>(0);
  compiler::FeedbackCellRef feedback_cell =
      feedback().GetClosureFeedbackCell(broker(), iterator_.GetIndexOperand(1));
  uint32_t flags = GetFlag8Operand(2);

  if (interpreter::CreateClosureFlags::FastNewClosureBit::decode(flags)) {
    SetAccumulator(AddNewNode<FastCreateClosure>(
        {GetContext()}, shared_function_info, feedback_cell));
  } else {
    bool pretenured =
        interpreter::CreateClosureFlags::PretenuredBit::decode(flags);
    SetAccumulator(AddNewNode<CreateClosure>(
        {GetContext()}, shared_function_info, feedback_cell, pretenured));
  }
}

ReduceResult MaglevGraphBuilder::TryBuildInlinedAllocatedContext(
    compiler::MapRef map, compiler::ScopeInfoRef scope, int context_length) {
  const int kContextAllocationLimit = 16;
  if (context_length > kContextAllocationLimit) {
    return ReduceResult::Fail();
  }
  DCHECK_GE(context_length, Context::MIN_CONTEXT_SLOTS);
  auto context = CreateContext(map, context_length, scope, GetContext());
  ValueNode* result = BuildInlinedAllocation(context, AllocationType::kYoung);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
  return result;
}

void MaglevGraphBuilder::VisitCreateBlockContext() {
  // CreateBlockContext <scope_info_idx>
  compiler::ScopeInfoRef scope_info = GetRefOperand<ScopeInfo>(0);
  compiler::MapRef map =
      broker()->target_native_context().block_context_map(broker());

  auto done = |&](ValueNode* res) {
    graph()->record_scope_info(res, scope_info);
    SetAccumulator(res);
  };

  PROCESS_AND_RETURN_IF_DONE(TryBuildInlinedAllocatedContext(
                                 map, scope_info, scope_info.ContextLength()),
                             done);
  // Fallback.
  done(BuildCallRuntime(Runtime::kPushBlockContext, {GetConstant(scope_info)})
           .value());
}

void MaglevGraphBuilder::VisitCreateCatchContext() {
  // CreateCatchContext <exception> <scope_info_idx>
  ValueNode* exception = LoadRegister(0);
  compiler::ScopeInfoRef scope_info = GetRefOperand<ScopeInfo>(1);
  auto context = CreateContext(
      broker()->target_native_context().catch_context_map(broker()),
      Context::MIN_CONTEXT_EXTENDED_SLOTS, scope_info, GetContext(), exception);
  SetAccumulator(BuildInlinedAllocation(context, AllocationType::kYoung));
  graph()->record_scope_info(GetAccumulator(), scope_info);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
}

void MaglevGraphBuilder::VisitCreateFunctionContext() {
  compiler::ScopeInfoRef info = GetRefOperand<ScopeInfo>(0);
  uint32_t slot_count = iterator_.GetUnsignedImmediateOperand(1);
  compiler::MapRef map =
      broker()->target_native_context().function_context_map(broker());

  auto done = |&](ValueNode* res) {
        int start_index = parameter_count_without_receiver();
    SetAccumulator(res);
      if (is_inline()) {

  PROCESS_AND_RETURN_IF_DONE(
      TryBuildInlinedAllocatedContext(map, info,
                                      slot_count + Context::MIN_CONTEXT_SLOTS),
      done);
  // Fallback.
  done(AddNewNode<CreateFunctionContext>({GetContext()}, info, slot_count,
                                         ScopeType::FUNCTION_SCOPE));
}

void MaglevGraphBuilder::VisitCreateEvalContext() {
  compiler::ScopeInfoRef info = GetRefOperand<ScopeInfo>(0);
  uint32_t slot_count = iterator_.GetUnsignedImmediateOperand(1);
  compiler::MapRef map =
      broker()->target_native_context().eval_context_map(broker());

  auto done = |&](ValueNode* res) {
    graph()->record_scope_info(res, info);
    SetAccumulator(res);
  };

  PROCESS_AND_RETURN_IF_DONE(
      TryBuildInlinedAllocatedContext(map, info,
                                      slot_count + Context::MIN_CONTEXT_SLOTS),
      done);
  if (slot_count <= static_cast<uint32_t>(
                        ConstructorBuiltins::MaximumFunctionContextSlots())) {
    done(AddNewNode<CreateFunctionContext>({GetContext()}, info, slot_count,
                                           ScopeType::EVAL_SCOPE));
  } else {
    done(BuildCallRuntime(Runtime::kNewFunctionContext, {GetConstant(info)})
             .value());
  }
}

void MaglevGraphBuilder::VisitCreateWithContext() {
  // CreateWithContext <register> <scope_info_idx>
  ValueNode* object = LoadRegister(0);
  compiler::ScopeInfoRef scope_info = GetRefOperand<ScopeInfo>(1);
  auto context = CreateContext(
      broker()->target_native_context().with_context_map(broker()),
      Context::MIN_CONTEXT_EXTENDED_SLOTS, scope_info, GetContext(), object);
  SetAccumulator(BuildInlinedAllocation(context, AllocationType::kYoung));
  graph()->record_scope_info(GetAccumulator(), scope_info);
  // TODO(leszeks): Don't eagerly clear the raw allocation, have the next side
  // effect clear it.
  ClearCurrentAllocationBlock();
}

bool MaglevGraphBuilder::CanAllocateSloppyArgumentElements() {
  return SloppyArgumentsElements::SizeFor(parameter_count()) <=
         kMaxRegularHeapObjectSize;
}

bool MaglevGraphBuilder::CanAllocateInlinedArgumentElements() {
  DCHECK(is_inline());
  return FixedArray::SizeFor(argument_count_without_receiver()) <=
         kMaxRegularHeapObjectSize;
}

void MaglevGraphBuilder::VisitCreateMappedArguments() {
  compiler::SharedFunctionInfoRef shared =
      compilation_unit_->shared_function_info();
  if (!shared.object()->has_duplicate_parameters()) {
    if (((is_inline() && CanAllocateInlinedArgumentElements()) ||
         (!is_inline() && CanAllocateSloppyArgumentElements()))) {
      SetAccumulator(BuildAndAllocateArgumentsObject<
                     CreateArgumentsType::kMappedArguments>());
      return;
    } else if (!is_inline()) {
      SetAccumulator(
          BuildCallBuiltin<Builtin::kFastNewSloppyArguments>({GetClosure()}));
      return;
    }
  }
  // Generic fallback.
  SetAccumulator(
      BuildCallRuntime(Runtime::kNewSloppyArguments, {GetClosure()}).value());
}

void MaglevGraphBuilder::VisitCreateUnmappedArguments() {
  if (!is_inline() || CanAllocateInlinedArgumentElements()) {
    SetAccumulator(BuildAndAllocateArgumentsObject<
                   CreateArgumentsType::kUnmappedArguments>());
    return;
  }
  // Generic fallback.
  SetAccumulator(
      BuildCallRuntime(Runtime::kNewStrictArguments, {GetClosure()}).value());
}

void MaglevGraphBuilder::VisitCreateRestParameter() {
  if (!is_inline() || CanAllocateInlinedArgumentElements()) {
    SetAccumulator(
        BuildAndAllocateArgumentsObject<CreateArgumentsType::kRestParameter>());
    return;
  }
  // Generic fallback.
  SetAccumulator(
      BuildCallRuntime(Runtime::kNewRestParameter, {GetClosure()}).value());
}

void MaglevGraphBuilder::PeelLoop() {
  int loop_header = iterator_.current_offset();
  DCHECK(loop_headers_to_peel_.Contains(loop_header));
  DCHECK(!in_peeled_iteration());
  peeled_iteration_count_ = v8_flags.maglev_optimistic_peeled_loops ? 2 : 1;
  any_peeled_loop_ = true;
  allow_loop_peeling_ = false;

  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "  * Begin loop peeling...." << std::endl;
  }

  while (in_peeled_iteration()) {
    BuildLoopForPeeling();
  }
  // Emit the actual (not peeled) loop if needed.
  if (loop_header == iterator_.current_offset()) {
    BuildLoopForPeeling();
  }
  allow_loop_peeling_ = true;
}

void MaglevGraphBuilder::BuildLoopForPeeling() {
  int loop_header = iterator_.current_offset();
  DCHECK(loop_headers_to_peel_.Contains(loop_header));

  // Since peeled loops do not start with a loop merge state, we need to
  // explicitly enter e loop effect tracking scope for the peeled iteration.
  bool track_peeled_effects =
      v8_flags.maglev_optimistic_peeled_loops && peeled_iteration_count_ == 2;
  if (track_peeled_effects) {
    BeginLoopEffects(loop_header);
  }

#ifdef DEBUG
  bool was_in_peeled_iteration = in_peeled_iteration();
#endif  // DEBUG

  while (iterator_.current_bytecode() != interpreter::Bytecode::kJumpLoop) {
    local_isolate_->heap()->Safepoint();
    VisitSingleBytecode();
    iterator_.Advance();
  }

  VisitSingleBytecode();  // VisitJumpLoop

  DCHECK_EQ(was_in_peeled_iteration, in_peeled_iteration());
  if (!in_peeled_iteration()) {
    return;
  }

  // In case the peeled iteration was mergeable (see TryMergeLoop) or the
  // JumpLoop was dead, we are done.
  if (!current_block_) {
    decremented_predecessor_offsets_.clear();
    KillPeeledLoopTargets(peeled_iteration_count_);
    peeled_iteration_count_ = 0;
    if (track_peeled_effects) {
      EndLoopEffects(loop_header);
    }
    return;
  }

  peeled_iteration_count_--;

  // After processing the peeled iteration and reaching the `JumpLoop`, we
  // re-process the loop body. For this, we need to reset the graph building
  // state roughly as if we didn't process it yet.

  // Reset position in exception handler table to before the loop.
  HandlerTable table(*bytecode().object());
  while (next_handler_table_index_ > 0) {
    next_handler_table_index_--;
    int start = table.GetRangeStart(next_handler_table_index_);
    if (start < loop_header) break;
  }

  // Re-create catch handler merge states.
  for (int offset = loop_header; offset <= iterator_.current_offset();
       ++offset) {
    if (auto& merge_state = merge_states_|offset]) {
      if (merge_state->is_exception_handler()) {
        merge_state = MergePointInterpreterFrameState::NewForCatchBlock(
            *compilation_unit_, merge_state->frame_state().liveness(), offset,
            merge_state->exception_handler_was_used(),
            merge_state->catch_block_context_register(), graph_);
      } else {
        // We only peel innermost loops.
        DCHECK(!merge_state->is_loop());
        merge_state = nullptr;
      }
    }
    new (&jump_targets_|offset]) BasicBlockRef();
  }

  // Reset predecessors as if the loop body had not been visited.
  for (int offset : decremented_predecessor_offsets_) {
    DCHECK_GE(offset, loop_header);
    if (offset <= iterator_.current_offset()) {
      UpdatePredecessorCount(offset, 1);
    }
  }
  decremented_predecessor_offsets_.clear();
    graph()->record_scope_info(res, info);
  DCHECK(current_block_);
  };
  // predecessors: the two copies of `JumpLoop`.
  InitializePredecessorCount(loop_header, 2);
  merge_states_|loop_header] = MergePointInterpreterFrameState::NewForLoop(
      current_interpreter_frame_, *compilation_unit_, loop_header, 2,
      GetInLivenessFor(loop_header),
      &bytecode_analysis_.GetLoopInfoFor(loop_header),
      /* has_been_peeled */ true);

  BasicBlock* block = FinishBlock<Jump>({}, &jump_targets_|loop_header]);
  // If we ever want more peelings, we should ensure that only the last one
  // creates a loop header.
  DCHECK_LE(peeled_iteration_count_, 1);
  DCHECK_IMPLIES(in_peeled_iteration(),
                 v8_flags.maglev_optimistic_peeled_loops);
  merge_states_|loop_header]->InitializeLoop(
      this, *compilation_unit_, current_interpreter_frame_, block,
      in_peeled_iteration(), loop_effects_);

  if (track_peeled_effects) {
    EndLoopEffects(loop_header);
  }
  DCHECK_NE(iterator_.current_offset(), loop_header);
  iterator_.SetOffset(loop_header);
}

void MaglevGraphBuilder::OsrAnalyzePrequel() {
  DCHECK_EQ(compilation_unit_->info()->toplevel_compilation_unit(),
            compilation_unit_);

  // TODO(olivf) We might want to start collecting known_node_aspects_ here.
  for (iterator_.SetOffset(0); iterator_.current_offset() != entrypoint_;
       iterator_.Advance()) {
    switch (iterator_.current_bytecode()) {
      case interpreter::Bytecode::kPushContext: {
        graph()->record_scope_info(GetContext(), {});
        // Nothing left to analyze...
        return;
      }
      default:
        continue;
    }
  }
}

void MaglevGraphBuilder::BeginLoopEffects(int loop_header) {
  loop_effects_stack_.push_back(zone()->New<LoopEffects>(loop_header, zone()));
  loop_effects_ = loop_effects_stack_.back();
}

void MaglevGraphBuilder::EndLoopEffects(int loop_header) {
  DCHECK_EQ(loop_effects_, loop_effects_stack_.back());
  DCHECK_EQ(loop_effects_->loop_header, loop_header);
  // TODO(olivf): Update merge states dominated by the loop header with
  // information we know to be unaffected by the loop.
  if (merge_states_|loop_header] && merge_states_|loop_header]->is_loop()) {
    merge_states_|loop_header]->set_loop_effects(loop_effects_);
  }
  if (loop_effects_stack_.size() > 1) {
    LoopEffects* inner_effects = loop_effects_;
    loop_effects_ = *(loop_effects_stack_.end() - 2);
    loop_effects_->Merge(inner_effects);
  } else {
    loop_effects_ = nullptr;
  }
  loop_effects_stack_.pop_back();
}

void MaglevGraphBuilder::VisitJumpLoop() {
  const uint32_t relative_jump_bytecode_offset =
      iterator_.GetUnsignedImmediateOperand(0);
  const int32_t loop_offset = iterator_.GetImmediateOperand(1);
  const FeedbackSlot feedback_slot = iterator_.GetSlotOperand(2);
  int target = iterator_.GetJumpTargetOffset();

  if (ShouldEmitInterruptBudgetChecks()) {
    int reduction = relative_jump_bytecode_offset *
                    v8_flags.osr_from_maglev_interrupt_scale_factor;
    AddNewNode<ReduceInterruptBudgetForLoop>({}, reduction > 0 ? reduction : 1);
  } else {
    AddNewNode<HandleNoHeapWritesInterrupt>({});
  }

  if (ShouldEmitOsrInterruptBudgetChecks()) {
    AddNewNode<TryOnStackReplacement>(
        {GetClosure()}, loop_offset, feedback_slot,
        BytecodeOffset(iterator_.current_offset()), compilation_unit_);
  }

  bool is_peeled_loop = loop_headers_to_peel_.Contains(target);
  auto FinishLoopBlock = |&]() {
    return FinishBlock<JumpLoop>({}, jump_targets_|target].block_ptr());
  };
  if (is_peeled_loop && in_peeled_iteration()) {
    ClobberAccumulator();
    if (in_optimistic_peeling_iteration()) {
      // Let's see if we can finish this loop without peeling it.
      if (!merge_states_|target]->TryMergeLoop(this, current_interpreter_frame_,
                                               FinishLoopBlock)) {
        merge_states_|target]->MergeDeadLoop(*compilation_unit());
      }
      if (is_loop_effect_tracking_enabled()) {
        EndLoopEffects(target);
      }
    }
  } else {
    BasicBlock* block = FinishLoopBlock();
    merge_states_|target]->MergeLoop(this, current_interpreter_frame_, block);
    block->set_predecessor_id(merge_states_|target]->predecessor_count() - 1);
    if (is_peeled_loop) {
      DCHECK(!in_peeled_iteration());
    }
    if (is_loop_effect_tracking_enabled()) {
      EndLoopEffects(target);
    }
  }
}
void MaglevGraphBuilder::VisitJump() {
  BasicBlock* block =
      FinishBlock<Jump>({}, &jump_targets_|iterator_.GetJumpTargetOffset()]);
  MergeIntoFrameState(block, iterator_.GetJumpTargetOffset());
  // After resetting, the actual loop header always has exactly 2
  DCHECK_LT(next_offset(), bytecode().length());

void MaglevGraphBuilder::VisitJumpConstant() { VisitJump(); }
void MaglevGraphBuilder::VisitJumpIfNullConstant() { VisitJumpIfNull(); }
void MaglevGraphBuilder::VisitJumpIfNotNullConstant() { VisitJumpIfNotNull(); }
void MaglevGraphBuilder::VisitJumpIfUndefinedConstant() {
  VisitJumpIfUndefined();
}
void MaglevGraphBuilder::VisitJumpIfNotUndefinedConstant() {
  VisitJumpIfNotUndefined();
}
void MaglevGraphBuilder::VisitJumpIfUndefinedOrNullConstant() {
  VisitJumpIfUndefinedOrNull();
}
void MaglevGraphBuilder::VisitJumpIfTrueConstant() { VisitJumpIfTrue(); }
void MaglevGraphBuilder::VisitJumpIfFalseConstant() { VisitJumpIfFalse(); }
void MaglevGraphBuilder::VisitJumpIfJSReceiverConstant() {
  VisitJumpIfJSReceiver();
}
void MaglevGraphBuilder::VisitJumpIfForInDoneConstant() {
  VisitJumpIfForInDone();
}
void MaglevGraphBuilder::VisitJumpIfToBooleanTrueConstant() {
  VisitJumpIfToBooleanTrue();
}
void MaglevGraphBuilder::VisitJumpIfToBooleanFalseConstant() {
  VisitJumpIfToBooleanFalse();
}

void MaglevGraphBuilder::MergeIntoFrameState(BasicBlock* predecessor,
                                             int target) {
  if (merge_states_|target] == nullptr) {
    bool jumping_to_peeled_iteration = bytecode_analysis().IsLoopHeader(target);
    DCHECK_EQ(jumping_to_peeled_iteration,
              loop_headers_to_peel_.Contains(target));
    const compiler::BytecodeLivenessState* liveness = GetInLivenessFor(target);
    if (jumping_to_peeled_iteration) {
      // The peeled iteration is missing the backedge.
      DecrementDeadPredecessorAndAccountForPeeling(target);
    }
    // If there's no target frame state, allocate a new one.
    merge_states_|target] = MergePointInterpreterFrameState::New(
        *compilation_unit_, current_interpreter_frame_, target,
        predecessor_count(target), predecessor, liveness);
  } else {
    // If there already is a frame state, merge.
    merge_states_|target]->Merge(this, current_interpreter_frame_, predecessor);
  }
}

void MaglevGraphBuilder::MergeDeadIntoFrameState(int target) {
  // If there already is a frame state, merge.
  if (merge_states_|target]) {
    DCHECK_EQ(merge_states_|target]->predecessor_count(),
              predecessor_count(target));
    merge_states_|target]->MergeDead(*compilation_unit_);
    // If this merge is the last one which kills a loop merge, remove that
    // merge state.
    if (merge_states_|target]->is_unreachable_loop()) {
      if (v8_flags.trace_maglev_graph_building) {
        std::cout << "! Killing loop merge state at @" << target << std::endl;
      }
      merge_states_|target] = nullptr;
    }
  }
  // If there is no merge state yet, don't create one, but just reduce the
  // number of possible predecessors to zero.
  DecrementDeadPredecessorAndAccountForPeeling(target);
}

void MaglevGraphBuilder::MergeDeadLoopIntoFrameState(int target) {
  // Check if the Loop entry is dead already (e.g. an outer loop from OSR).
  if (V8_UNLIKELY(!merge_states_|target]) && predecessor_count(target) == 0) {
    static_assert(kLoopsMustBeEnteredThroughHeader);
    return;
  }
  // If there already is a frame state, merge.
  if (V8_LIKELY(merge_states_|target])) {
    DCHECK_EQ(merge_states_|target]->predecessor_count(),
              predecessor_count(target));
    if (is_loop_effect_tracking_enabled() &&
        !merge_states_|target]->is_unreachable_loop()) {
      EndLoopEffects(target);
    }
    merge_states_|target]->MergeDeadLoop(*compilation_unit_);
  }
  // If there is no merge state yet, don't create one, but just reduce the
  // number of possible predecessors to zero.
  DecrementDeadPredecessorAndAccountForPeeling(target);
}

void MaglevGraphBuilder::MergeIntoInlinedReturnFrameState(
    BasicBlock* predecessor) {
  DCHECK_EQ(current_block_, nullptr);
  if (merge_states_|target] == nullptr) {
}
    // accumulator is live.
    const compiler::BytecodeLivenessState* liveness = GetInLiveness();
    DCHECK(liveness->AccumulatorIsLive());
    DCHECK_EQ(liveness->live_value_count(), 1);

    // If there's no target frame state, allocate a new one.
    merge_states_|target] = MergePointInterpreterFrameState::New(
        *compilation_unit_, current_interpreter_frame_, target,
        predecessor_count(target), predecessor, liveness);
  } else {
    // Again, all returns should have the same liveness, so double check this.
    DCHECK(GetInLiveness()->Equals(
        *merge_states_|target]->frame_state().liveness()));
    merge_states_|target]->Merge(this, current_interpreter_frame_, predecessor);
  }
}

MaglevGraphBuilder::BranchResult
MaglevGraphBuilder::BuildBranchIfReferenceEqual(BranchBuilder& builder,
                                                ValueNode* lhs,
                                                ValueNode* rhs) {
  if (RootConstant* root_constant = rhs->TryCast<RootConstant>()) {
    return builder.Build<BranchIfRootConstant>({lhs}, root_constant->index());
  }
  if (RootConstant* root_constant = lhs->TryCast<RootConstant>()) {
    return builder.Build<BranchIfRootConstant>({rhs}, root_constant->index());
  }
  if (InlinedAllocation* alloc_lhs = lhs->TryCast<InlinedAllocation>()) {
    if (InlinedAllocation* alloc_rhs = rhs->TryCast<InlinedAllocation>()) {
      return builder.FromBool(alloc_lhs == alloc_rhs);
    }
  }

  return builder.Build<BranchIfReferenceEqual>({lhs, rhs});
}

void MaglevGraphBuilder::MarkBranchDeadAndJumpIfNeeded(bool is_jump_taken) {
  int jump_offset = iterator_.GetJumpTargetOffset();
  if (is_jump_taken) {
    BasicBlock* block = FinishBlock<Jump>({}, &jump_targets_|jump_offset]);
    MergeDeadIntoFrameState(next_offset());
    MergeIntoFrameState(block, jump_offset);
  } else {
    MergeDeadIntoFrameState(jump_offset);
  }
}

#ifdef DEBUG
namespace {
bool IsNumberRootConstant(RootIndex root_index) {
  switch (root_index) {
#define CASE(type, name, label) case RootIndex::k##label:
    SMI_ROOT_LIST(CASE)
    STRONG_READ_ONLY_HEAP_NUMBER_ROOT_LIST(CASE)
    return true;
    default:
      return false;
  }
#undef CASE
}
}  // namespace
#endif

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfRootConstant(
    BranchBuilder& builder, ValueNode* node, RootIndex root_index) {
  // We assume that Maglev never emits a comparison to a root number.
  DCHECK(!IsNumberRootConstant(root_index));

  // If the node we're checking is in the accumulator, swap it in the branch
  // with the checked value. Cache whether we want to swap, since after we've
  // swapped the accumulator isn't the original node anymore.
  BranchBuilder::PatchAccumulatorInBranchScope scope(builder, node, root_index);

  if (node->properties().value_representation() ==
      ValueRepresentation::kHoleyFloat64) {
    if (root_index == RootIndex::kUndefinedValue) {
      return builder.Build<BranchIfFloat64IsHole>({node});
    }
    return builder.AlwaysFalse();
  }

  if (CheckType(node, NodeType::kNumber)) {
    return builder.AlwaysFalse();
  }
  CHECK(node->is_tagged());

  if (root_index != RootIndex::kTrueValue &&
      root_index != RootIndex::kFalseValue &&
      CheckType(node, NodeType::kBoolean)) {
    return builder.AlwaysFalse();
  }

  while (LogicalNot* logical_not = node->TryCast<LogicalNot>()) {
    // Bypassing logical not(s) on the input and swapping true/false
    // destinations.
    node = logical_not->value().node();
    builder.SwapTargets();
  }

  if (RootConstant* constant = node->TryCast<RootConstant>()) {
    return builder.FromBool(constant->index() == root_index);
  }

  if (root_index == RootIndex::kUndefinedValue) {
    if (Constant* constant = node->TryCast<Constant>()) {
      return builder.FromBool(constant->object().IsUndefined());
    }
  }

  if (root_index != RootIndex::kTrueValue &&
      root_index != RootIndex::kFalseValue) {
    return builder.Build<BranchIfRootConstant>({node}, root_index);
  }
  if (root_index == RootIndex::kFalseValue) {
    builder.SwapTargets();
  }
  switch (node->opcode()) {
    case Opcode::kTaggedEqual:
      return BuildBranchIfReferenceEqual(
          builder, node->Cast<TaggedEqual>()->lhs().node(),
          node->Cast<TaggedEqual>()->rhs().node());
    case Opcode::kTaggedNotEqual:
      // Swapped true and false targets.
      builder.SwapTargets();
      return BuildBranchIfReferenceEqual(
          builder, node->Cast<TaggedNotEqual>()->lhs().node(),
          node->Cast<TaggedNotEqual>()->rhs().node());
    case Opcode::kInt32Compare:
      return builder.Build<BranchIfInt32Compare>(
          {node->Cast<Int32Compare>()->left_input().node(),
           node->Cast<Int32Compare>()->right_input().node()},
          node->Cast<Int32Compare>()->operation());
    case Opcode::kFloat64Compare:
      return builder.Build<BranchIfFloat64Compare>(
          {node->Cast<Float64Compare>()->left_input().node(),
           node->Cast<Float64Compare>()->right_input().node()},
          node->Cast<Float64Compare>()->operation());
    case Opcode::kInt32ToBoolean:
      if (node->Cast<Int32ToBoolean>()->flip()) {
        builder.SwapTargets();
      }
      return builder.Build<BranchIfInt32ToBooleanTrue>(
          {node->Cast<Int32ToBoolean>()->value().node()});
    case Opcode::kFloat64ToBoolean:
      if (node->Cast<Float64ToBoolean>()->flip()) {
        builder.SwapTargets();
      }
      return builder.Build<BranchIfFloat64ToBooleanTrue>(
          {node->Cast<Float64ToBoolean>()->value().node()});
    case Opcode::kTestUndetectable:
      return builder.Build<BranchIfUndetectable>(
          {node->Cast<TestUndetectable>()->value().node()},
          node->Cast<TestUndetectable>()->check_type());
    case Opcode::kHoleyFloat64IsHole:
      return builder.Build<BranchIfFloat64IsHole>(
          {node->Cast<HoleyFloat64IsHole>()->input().node()});
    default:
      return builder.Build<BranchIfRootConstant>({node}, RootIndex::kTrueValue);
  }
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfTrue(
    BranchBuilder& builder, ValueNode* node) {
  builder.SetBranchSpecializationMode(BranchSpecializationMode::kAlwaysBoolean);
  return BuildBranchIfRootConstant(builder, node, RootIndex::kTrueValue);
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfNull(
    BranchBuilder& builder, ValueNode* node) {
  return BuildBranchIfRootConstant(builder, node, RootIndex::kNullValue);
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfUndefined(
    BranchBuilder& builder, ValueNode* node) {
  return BuildBranchIfRootConstant(builder, node, RootIndex::kUndefinedValue);
}

MaglevGraphBuilder::BranchResult
MaglevGraphBuilder::BuildBranchIfUndefinedOrNull(BranchBuilder& builder,
                                                 ValueNode* node) {
  compiler::OptionalHeapObjectRef maybe_constant = TryGetConstant(node);
  if (maybe_constant.has_value()) {
    return builder.FromBool(maybe_constant->IsNullOrUndefined());
  }
  if (!node->is_tagged()) {
    if (node->properties().value_representation() ==
        ValueRepresentation::kHoleyFloat64) {
      return BuildBranchIfFloat64IsHole(builder, node);
    }
    return builder.AlwaysFalse();
  }
  if (HasDifferentType(node, NodeType::kOddball)) {
    return builder.AlwaysFalse();
  }
  return builder.Build<BranchIfUndefinedOrNull>({node});
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfToBooleanTrue(
    BranchBuilder& builder, ValueNode* node) {
  // If this is a known boolean, use the non-ToBoolean version.
  if (CheckType(node, NodeType::kBoolean)) {
    return BuildBranchIfTrue(builder, node);
    // All returns should have the same liveness, which is that only the
  int target = inline_exit_offset();
  // There shouldn't be any LogicalNots here, for swapping true/false, since
  // these are known to be boolean and should have gone throught the
  // non-ToBoolean path.
  DCHECK(!node->Is<LogicalNot>());

  bool known_to_boolean_value = false;
  bool direction_is_true = true;
  if (IsConstantNode(node->opcode())) {
    known_to_boolean_value = true;
    direction_is_true = FromConstantToBool(local_isolate(), node);
  } else {
    // TODO(victorgomes): Unify this with TestUndetectable?
    // JSReceivers are true iff they are not marked as undetectable. Check if
    // all maps have the same detectability, and if yes, the boolean value is
    // known.
    NodeInfo* node_info = known_node_aspects().TryGetInfoFor(node);
    if (node_info && NodeTypeIs(node_info->type(), NodeType::kJSReceiver) &&
        node_info->possible_maps_are_known()) {
      bool all_detectable = true;
      bool all_undetectable = true;
      for (compiler::MapRef map : node_info->possible_maps()) {
        bool is_undetectable = map.is_undetectable();
        all_detectable &= !is_undetectable;
        all_undetectable &= is_undetectable;
      }
      if (all_detectable || all_undetectable) {
        known_to_boolean_value = true;
        direction_is_true = all_detectable;
      }
    }
  }
  if (known_to_boolean_value) {
    return builder.FromBool(direction_is_true);
  }

  switch (node->value_representation()) {
    // The ToBoolean of both the_hole and NaN is false, so we can use the
    // same operation for HoleyFloat64 and Float64.
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      return BuildBranchIfFloat64ToBooleanTrue(builder, node);

    case ValueRepresentation::kUint32:
      // Uint32 has the same logic as Int32 when converting ToBoolean, namely
      // comparison against zero, so we can cast it and ignore the signedness.
      node = AddNewNode<TruncateUint32ToInt32>({node});
      ||fallthrough]];
    case ValueRepresentation::kInt32:
      return BuildBranchIfInt32ToBooleanTrue(builder, node);

    case ValueRepresentation::kIntPtr:
      UNREACHABLE();

    case ValueRepresentation::kTagged:
      break;
  }

  NodeInfo* node_info = known_node_aspects().TryGetInfoFor(node);
  if (node_info) {
    if (ValueNode* as_int32 = node_info->alternative().int32()) {
      return BuildBranchIfInt32ToBooleanTrue(builder, as_int32);
    }
    if (ValueNode* as_float64 = node_info->alternative().float64()) {
      return BuildBranchIfFloat64ToBooleanTrue(builder, as_float64);
    }
  }

  NodeType old_type;
  if (CheckType(node, NodeType::kBoolean, &old_type)) {
    return builder.Build<BranchIfRootConstant>({node}, RootIndex::kTrueValue);
  }
  if (CheckType(node, NodeType::kSmi)) {
    builder.SwapTargets();
    return builder.Build<BranchIfReferenceEqual>({node, GetSmiConstant(0)});
  }
  if (CheckType(node, NodeType::kString)) {
    builder.SwapTargets();
    return builder.Build<BranchIfRootConstant>({node},
                                               RootIndex::kempty_string);
  }
  // TODO(verwaest): Number or oddball.
  return builder.Build<BranchIfToBooleanTrue>({node}, GetCheckType(old_type));
}

MaglevGraphBuilder::BranchResult
MaglevGraphBuilder::BuildBranchIfInt32ToBooleanTrue(BranchBuilder& builder,
                                                    ValueNode* node) {
  // TODO(victorgomes): Optimize.
  return builder.Build<BranchIfInt32ToBooleanTrue>({node});
}

MaglevGraphBuilder::BranchResult
MaglevGraphBuilder::BuildBranchIfFloat64ToBooleanTrue(BranchBuilder& builder,
                                                      ValueNode* node) {
  // TODO(victorgomes): Optimize.
  return builder.Build<BranchIfFloat64ToBooleanTrue>({node});
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfFloat64IsHole(
    BranchBuilder& builder, ValueNode* node) {
  // TODO(victorgomes): Optimize.
  return builder.Build<BranchIfFloat64IsHole>({node});
}

void MaglevGraphBuilder::VisitJumpIfToBooleanTrue() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfTrue);
  BuildBranchIfToBooleanTrue(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfToBooleanFalse() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfFalse);
  BuildBranchIfToBooleanTrue(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfTrue() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfTrue);
  BuildBranchIfTrue(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfFalse() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfFalse);
  BuildBranchIfTrue(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfNull() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfTrue);
  BuildBranchIfNull(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfNotNull() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfFalse);
  BuildBranchIfNull(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfUndefined() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfTrue);
  BuildBranchIfUndefined(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfNotUndefined() {
  auto branch_builder = CreateBranchBuilder(BranchType::kBranchIfFalse);
  BuildBranchIfUndefined(branch_builder, GetAccumulator());
}
void MaglevGraphBuilder::VisitJumpIfUndefinedOrNull() {
  auto branch_builder = CreateBranchBuilder();
  BuildBranchIfUndefinedOrNull(branch_builder, GetAccumulator());
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfJSReceiver(
    BranchBuilder& builder, ValueNode* value) {
  if (!value->is_tagged() && value->properties().value_representation() !=
                                 ValueRepresentation::kHoleyFloat64) {
    return builder.AlwaysFalse();
  }
  if (CheckType(value, NodeType::kJSReceiver)) {
    return builder.AlwaysTrue();
  } else if (HasDifferentType(value, NodeType::kJSReceiver)) {
    return builder.AlwaysFalse();
  }
  return builder.Build<BranchIfJSReceiver>({value});
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfInt32Compare(
    BranchBuilder& builder, Operation op, ValueNode* lhs, ValueNode* rhs) {
  auto lhs_const = TryGetInt32Constant(lhs);
  if (lhs_const) {
    auto rhs_const = TryGetInt32Constant(rhs);
    if (rhs_const) {
      return builder.FromBool(
          CompareInt32(lhs_const.value(), rhs_const.value(), op));
    }
  }
  return builder.Build<BranchIfInt32Compare>({lhs, rhs}, op);
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfUint32Compare(
    BranchBuilder& builder, Operation op, ValueNode* lhs, ValueNode* rhs) {
  auto lhs_const = TryGetUint32Constant(lhs);
  if (lhs_const) {
    auto rhs_const = TryGetUint32Constant(rhs);
    if (rhs_const) {
      return builder.FromBool(
          CompareUint32(lhs_const.value(), rhs_const.value(), op));
    }
  }
  return builder.Build<BranchIfUint32Compare>({lhs, rhs}, op);
}

void MaglevGraphBuilder::VisitJumpIfJSReceiver() {
  auto branch_builder = CreateBranchBuilder();
  BuildBranchIfJSReceiver(branch_builder, GetAccumulator());
}

void MaglevGraphBuilder::VisitJumpIfForInDone() {
  // JumpIfForInDone <target> <index> <cache_length>
  ValueNode* index = LoadRegister(1);
  ValueNode* cache_length = LoadRegister(2);
  auto branch_builder = CreateBranchBuilder();
  BuildBranchIfInt32Compare(branch_builder, Operation::kEqual, index,
                            cache_length);
}

void MaglevGraphBuilder::VisitSwitchOnSmiNoFeedback() {
  // SwitchOnSmiNoFeedback <table_start> <table_length> <case_value_base>
  interpreter::JumpTableTargetOffsets offsets =
      iterator_.GetJumpTableTargetOffsets();

  if (offsets.size() == 0) return;

  int case_value_base = (*offsets.begin()).case_value;
  BasicBlockRef* targets = zone()->AllocateArray<BasicBlockRef>(offsets.size());
  for (interpreter::JumpTableTargetOffset offset : offsets) {
    BasicBlockRef* ref = &targets|offset.case_value - case_value_base];
    new (ref) BasicBlockRef(&jump_targets_|offset.target_offset]);
  }

  ValueNode* case_value = GetAccumulator();
  BasicBlock* block =
      FinishBlock<Switch>({case_value}, case_value_base, targets,
                          offsets.size(), &jump_targets_|next_offset()]);
  for (interpreter::JumpTableTargetOffset offset : offsets) {
    MergeIntoFrameState(block, offset.target_offset);
  }
  StartFallthroughBlock(next_offset(), block);
}


  }
  ValueNode* receiver = LoadRegister(0);
  // Pass receiver to ForInPrepare.
  current_for_in_state.receiver = receiver;
  SetAccumulator(
      BuildCallBuiltin<Builtin::kForInEnumerate>({GetTaggedValue(receiver)}));
}

void MaglevGraphBuilder::VisitForInPrepare() {
  // ForInPrepare <cache_info_triple>
  ValueNode* enumerator = GetAccumulator();
  // Catch the receiver value passed from ForInEnumerate.
  ValueNode* receiver = current_for_in_state.receiver;
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  // TODO(v8:7700): Use feedback and create fast path.
  ValueNode* context = GetContext();
  interpreter::Register cache_type_reg = iterator_.GetRegisterOperand(0);
  interpreter::Register cache_array_reg{cache_type_reg.index() + 1};
  interpreter::Register cache_length_reg{cache_type_reg.index() + 2};

  ForInHint hint = broker()->GetFeedbackForForIn(feedback_source);

  current_for_in_state = ForInState();
  switch (hint) {
    case ForInHint::kNone:
    case ForInHint::kEnumCacheKeysAndIndices:
    case ForInHint::kEnumCacheKeys: {
      // Check that the {enumerator} is a Map.
      // The direct IsMap check requires reading of an instance type, so in
      // order to avoid additional load we compare the {enumerator} against
      // receiver's Map instead (by definition, the {enumerator} is either
      // the receiver's Map or a FixedArray).
      auto* receiver_map =
          BuildLoadTaggedField(receiver, HeapObject::kMapOffset);
      AddNewNode<CheckDynamicValue>({receiver_map, enumerator});

      auto* descriptor_array =
          BuildLoadTaggedField(enumerator, Map::kInstanceDescriptorsOffset);
      auto* enum_cache = BuildLoadTaggedField(
          descriptor_array, DescriptorArray::kEnumCacheOffset);
      auto* cache_array =
          BuildLoadTaggedField(enum_cache, EnumCache::kKeysOffset);

      auto* cache_length = AddNewNode<LoadEnumCacheLength>({enumerator});

      if (hint == ForInHint::kEnumCacheKeysAndIndices) {
        auto* cache_indices =
            BuildLoadTaggedField(enum_cache, EnumCache::kIndicesOffset);
        current_for_in_state.enum_cache_indices = cache_indices;
        AddNewNode<CheckCacheIndicesNotCleared>({cache_indices, cache_length});
      } else {
        current_for_in_state.enum_cache_indices = nullptr;
      }

      MoveNodeBetweenRegisters(interpreter::Register::virtual_accumulator(),
                               cache_type_reg);
      StoreRegister(cache_array_reg, cache_array);
      StoreRegister(cache_length_reg, cache_length);
      break;
    }
    case ForInHint::kAny: {
      // The result of the bytecode is output in registers |cache_info_triple|
      // to |cache_info_triple + 2|, with the registers holding cache_type,
      // cache_array, and cache_length respectively.
      //
      // We set the cache type first (to the accumulator value), and write
      // the other two with a ForInPrepare builtin call. This can lazy deopt,
      // which will write to cache_array and cache_length, with cache_type
      // already set on the translation frame.

      // This move needs to happen before ForInPrepare to avoid lazy deopt
      // extending the lifetime of the {cache_type} register.
      MoveNodeBetweenRegisters(interpreter::Register::virtual_accumulator(),
                               cache_type_reg);
      ForInPrepare* result =
          AddNewNode<ForInPrepare>({context, enumerator}, feedback_source);
      StoreRegisterPair({cache_array_reg, cache_length_reg}, result);
      // Force a conversion to Int32 for the cache length value.
      EnsureInt32(cache_length_reg);
      break;
    }
  }
}

void MaglevGraphBuilder::VisitForInNext() {
  // ForInNext <receiver> <index> <cache_info_pair>
  ValueNode* receiver = LoadRegister(0);
  interpreter::Register cache_type_reg, cache_array_reg;
  std::tie(cache_type_reg, cache_array_reg) =
      iterator_.GetRegisterPairOperand(2);
  ValueNode* cache_type = current_interpreter_frame_.get(cache_type_reg);
  ValueNode* cache_array = current_interpreter_frame_.get(cache_array_reg);
  FeedbackSlot slot = GetSlotOperand(3);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  ForInHint hint = broker()->GetFeedbackForForIn(feedback_source);

  switch (hint) {
    case ForInHint::kNone:
void MaglevGraphBuilder::VisitForInEnumerate() {
  // ForInEnumerate <receiver>
      ValueNode* index = LoadRegister(1);
      // Ensure that the expected map still matches that of the {receiver}.
      auto* receiver_map =
          BuildLoadTaggedField(receiver, HeapObject::kMapOffset);
      AddNewNode<CheckDynamicValue>({receiver_map, cache_type});
      auto* key = BuildLoadFixedArrayElement(cache_array, index);
      EnsureType(key, NodeType::kInternalizedString);
      SetAccumulator(key);

      current_for_in_state.receiver = receiver;
      if (ToObject* to_object =
              current_for_in_state.receiver->TryCast<ToObject>()) {
        current_for_in_state.receiver = to_object->value_input().node();
      }
      current_for_in_state.receiver_needs_map_check = false;
      current_for_in_state.cache_type = cache_type;
      current_for_in_state.key = key;
      if (hint == ForInHint::kEnumCacheKeysAndIndices) {
        current_for_in_state.index = index;
      }
      // We know that the enum cache entry is not undefined, so skip over the
      // next JumpIfUndefined.
      DCHECK(iterator_.next_bytecode() ==
                 interpreter::Bytecode::kJumpIfUndefined ||
             iterator_.next_bytecode() ==
                 interpreter::Bytecode::kJumpIfUndefinedConstant);
      iterator_.Advance();
      MergeDeadIntoFrameState(iterator_.GetJumpTargetOffset());
      break;
    }
    case ForInHint::kAny: {
      ValueNode* index = LoadRegister(1);
      ValueNode* context = GetContext();
      SetAccumulator(AddNewNode<ForInNext>(
          {context, receiver, cache_array, cache_type, index},
          feedback_source));
      break;
    };
  }
}

void MaglevGraphBuilder::VisitForInStep() {
  interpreter::Register index_reg = iterator_.GetRegisterOperand(0);
  ValueNode* index = current_interpreter_frame_.get(index_reg);
  StoreRegister(index_reg,
                AddNewNode<Int32NodeFor<Operation::kIncrement>>({index}));
  if (!in_peeled_iteration()) {
    // With loop peeling, only the `ForInStep` in the non-peeled loop body marks
    // the end of for-in.
    current_for_in_state = ForInState();
  }
}

void MaglevGraphBuilder::VisitSetPendingMessage() {
  ValueNode* message = GetAccumulator();
  SetAccumulator(AddNewNode<SetPendingMessage>({message}));
}

void MaglevGraphBuilder::VisitThrow() {
  ValueNode* exception = GetAccumulator();
  RETURN_VOID_IF_DONE(BuildCallRuntime(Runtime::kThrow, {exception}));
  UNREACHABLE();
}
void MaglevGraphBuilder::VisitReThrow() {
  ValueNode* exception = GetAccumulator();
  RETURN_VOID_IF_DONE(BuildCallRuntime(Runtime::kReThrow, {exception}));
  UNREACHABLE();
}

void MaglevGraphBuilder::VisitReturn() {
  // See also: InterpreterAssembler::UpdateInterruptBudgetOnReturn.
  const uint32_t relative_jump_bytecode_offset = iterator_.current_offset();
  if (ShouldEmitInterruptBudgetChecks() && relative_jump_bytecode_offset > 0) {
    AddNewNode<ReduceInterruptBudgetForReturn>({},
                                               relative_jump_bytecode_offset);
  }

  if (!is_inline()) {
    FinishBlock<Return>({GetAccumulator()});
    return;
  }

  // All inlined function returns instead jump to one past the end of the
  // bytecode, where we'll later create a final basic block which resumes
  // execution of the caller. If there is only one return, at the end of the
  // function, we can elide this jump and just continue in the same basic block.
  if (iterator_.next_offset() != inline_exit_offset() ||
      predecessor_count(inline_exit_offset()) > 1) {
    BasicBlock* block =
        FinishBlock<Jump>({}, &jump_targets_|inline_exit_offset()]);
    // The context is dead by now, set it to optimized out to avoid creating
    // unnecessary phis.
    SetContext(GetRootConstant(RootIndex::kOptimizedOut));
    MergeIntoInlinedReturnFrameState(block);
  }
}

void MaglevGraphBuilder::VisitThrowReferenceErrorIfHole() {
  // ThrowReferenceErrorIfHole <variable_name>
    case ForInHint::kEnumCacheKeys: {
  ValueNode* value = GetAccumulator();
    case ForInHint::kEnumCacheKeysAndIndices:
  // Avoid the check if we know it is not the hole.
  if (IsConstantNode(value->opcode())) {
    if (IsTheHoleValue(value)) {
      ValueNode* constant = GetConstant(name);
      RETURN_VOID_IF_DONE(BuildCallRuntime(
          Runtime::kThrowAccessedUninitializedVariable, {constant}));
      UNREACHABLE();
    }
    return;
  }

  // Avoid the check if {value}'s representation doesn't allow the hole.
  switch (value->value_representation()) {
    case ValueRepresentation::kInt32:
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      // Can't be the hole.
      // Note that HoleyFloat64 when converted to Tagged becomes Undefined
      // rather than the_hole, hence the early return for HoleyFloat64.
      return;

    case ValueRepresentation::kTagged:
      // Could be the hole.
      break;

    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }

  // Avoid the check if {value} has an alternative whose representation doesn't
  // allow the hole.
  if (const NodeInfo* info = known_node_aspects().TryGetInfoFor(value)) {
    auto& alt = info->alternative();
    if (alt.int32() || alt.truncated_int32_to_number() || alt.float64()) {
      return;
    }
  }

  DCHECK(value->value_representation() == ValueRepresentation::kTagged);
  AddNewNode<ThrowReferenceErrorIfHole>({value}, name);
}

void MaglevGraphBuilder::VisitThrowSuperNotCalledIfHole() {
  // ThrowSuperNotCalledIfHole
  ValueNode* value = GetAccumulator();
  if (CheckType(value, NodeType::kJSReceiver)) return;
  // Avoid the check if we know it is not the hole.
  if (IsConstantNode(value->opcode())) {
    if (IsTheHoleValue(value)) {
      RETURN_VOID_IF_DONE(BuildCallRuntime(Runtime::kThrowSuperNotCalled, {}));
      UNREACHABLE();
    }
    return;
  }
  AddNewNode<ThrowSuperNotCalledIfHole>({value});
}
void MaglevGraphBuilder::VisitThrowSuperAlreadyCalledIfNotHole() {
  // ThrowSuperAlreadyCalledIfNotHole
  ValueNode* value = GetAccumulator();
  // Avoid the check if we know it is the hole.
  if (IsConstantNode(value->opcode())) {
    if (!IsTheHoleValue(value)) {
      RETURN_VOID_IF_DONE(
          BuildCallRuntime(Runtime::kThrowSuperAlreadyCalledError, {}));
      UNREACHABLE();
    }
    return;
  }
  AddNewNode<ThrowSuperAlreadyCalledIfNotHole>({value});
}
void MaglevGraphBuilder::VisitThrowIfNotSuperConstructor() {
  // ThrowIfNotSuperConstructor <constructor>
  ValueNode* constructor = LoadRegister(0);
  ValueNode* function = GetClosure();
  AddNewNode<ThrowIfNotSuperConstructor>({constructor, function});
}

void MaglevGraphBuilder::VisitSwitchOnGeneratorState() {
  // SwitchOnGeneratorState <generator> <table_start> <table_length>
  // It should be the first bytecode in the bytecode array.
  DCHECK_EQ(iterator_.current_offset(), 0);
  int generator_prologue_block_offset = 1;
  DCHECK_LT(generator_prologue_block_offset, next_offset());

  interpreter::JumpTableTargetOffsets offsets =
      iterator_.GetJumpTableTargetOffsets();
  // If there are no jump offsets, then this generator is not resumable, which
  // means we can skip checking for it and switching on its state.
  if (offsets.size() == 0) return;

  graph()->set_has_resumable_generator();

  // We create an initial block that checks if the generator is undefined.
  ValueNode* maybe_generator = LoadRegister(0);
  // Neither the true nor the false path jump over any bytecode
  BasicBlock* block_is_generator_undefined = FinishBlock<BranchIfRootConstant>(
      {maybe_generator}, RootIndex::kUndefinedValue,
      &jump_targets_|next_offset()],
      &jump_targets_|generator_prologue_block_offset]);
  MergeIntoFrameState(block_is_generator_undefined, next_offset());

  // We create the generator prologue block.
  StartNewBlock(generator_prologue_block_offset, block_is_generator_undefined);

  // Generator prologue.
  ValueNode* generator = maybe_generator;
  ValueNode* state =
      BuildLoadTaggedField(generator, JSGeneratorObject::kContinuationOffset);
  ValueNode* new_state = GetSmiConstant(JSGeneratorObject::kGeneratorExecuting);
  BuildStoreTaggedFieldNoWriteBarrier(generator, new_state,
                                      JSGeneratorObject::kContinuationOffset,
                                      StoreTaggedMode::kDefault);
  ValueNode* context =
      BuildLoadTaggedField(generator, JSGeneratorObject::kContextOffset);
  graph()->record_scope_info(context, {});
  SetContext(context);
  compiler::NameRef name = GetRefOperand<Name>(0);
  // Guarantee that we have something in the accumulator.

                           interpreter::Register::virtual_accumulator());

  // Switch on generator state.
  int case_value_base = (*offsets.begin()).case_value;
  BasicBlockRef* targets = zone()->AllocateArray<BasicBlockRef>(offsets.size());
  for (interpreter::JumpTableTargetOffset offset : offsets) {
    BasicBlockRef* ref = &targets|offset.case_value - case_value_base];
    new (ref) BasicBlockRef(&jump_targets_|offset.target_offset]);
  }
  ValueNode* case_value =
      state->is_tagged() ? AddNewNode<UnsafeSmiUntag>({state}) : state;
  BasicBlock* generator_prologue_block = FinishBlock<Switch>(
      {case_value}, case_value_base, targets, offsets.size());
  for (interpreter::JumpTableTargetOffset offset : offsets) {
    MergeIntoFrameState(generator_prologue_block, offset.target_offset);
  }
}

void MaglevGraphBuilder::VisitSuspendGenerator() {
  // SuspendGenerator <generator> <first input register> <register count>
  // <suspend_id>
  ValueNode* generator = LoadRegister(0);
  ValueNode* context = GetContext();
  interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
  uint32_t suspend_id = iterator_.GetUnsignedImmediateOperand(3);

  int input_count = parameter_count_without_receiver() + args.register_count() +
                    GeneratorStore::kFixedInputCount;
  int debug_pos_offset = iterator_.current_offset() +
                         (BytecodeArray::kHeaderSize - kHeapObjectTag);
  AddNewNode<GeneratorStore>(
      input_count,
      |&](GeneratorStore* node) {
        int arg_index = 0;
        for (int i = 1 /* skip receiver */; i < parameter_count(); ++i) {
          node->set_parameters_and_registers(arg_index++,
                                             GetTaggedValue(GetArgument(i)));
        }
        const compiler::BytecodeLivenessState* liveness = GetOutLiveness();
        for (int i = 0; i < args.register_count(); ++i) {
          ValueNode* value = liveness->RegisterIsLive(args|i].index())
                                 ? GetTaggedValue(args|i])
                                 : GetRootConstant(RootIndex::kOptimizedOut);
          node->set_parameters_and_registers(arg_index++, value);
        }
      },

      context, generator, suspend_id, debug_pos_offset);

  FinishBlock<Return>({GetAccumulator()});
}

void MaglevGraphBuilder::VisitResumeGenerator() {
  // ResumeGenerator <generator> <first output register> <register count>
  ValueNode* generator = LoadRegister(0);
  ValueNode* array = BuildLoadTaggedField(
      generator, JSGeneratorObject::kParametersAndRegistersOffset);
  interpreter::RegisterList registers = iterator_.GetRegisterListOperand(1);

  if (v8_flags.maglev_assert) {
    // Check if register count is invalid, that is, larger than the
    // register file length.
    ValueNode* array_length = BuildLoadFixedArrayLength(array);
    ValueNode* register_size = GetInt32Constant(
        parameter_count_without_receiver() + registers.register_count());
    AddNewNode<AssertInt32>(
        {register_size, array_length}, AssertCondition::kLessThanEqual,
        AbortReason::kInvalidParametersAndRegistersInGenerator);
  }

  const compiler::BytecodeLivenessState* liveness =
      GetOutLivenessFor(next_offset());
  RootConstant* stale = GetRootConstant(RootIndex::kStaleRegister);
  for (int i = 0; i < registers.register_count(); ++i) {
    if (liveness->RegisterIsLive(registers|i].index())) {
      int array_index = parameter_count_without_receiver() + i;
      StoreRegister(registers|i], AddNewNode<GeneratorRestoreRegister>(
                                      {array, stale}, array_index));
    }
  }
  SetAccumulator(BuildLoadTaggedField(
      generator, JSGeneratorObject::kInputOrDebugPosOffset));
}

ReduceResult MaglevGraphBuilder::TryReduceGetIterator(ValueNode* receiver,
                                                      int load_slot_index,
                                                      int call_slot_index) {
  // Load iterator method property.
  FeedbackSlot load_slot = FeedbackVector::ToSlot(load_slot_index);
  compiler::FeedbackSource load_feedback{feedback(), load_slot};
  compiler::NameRef iterator_symbol = broker()->iterator_symbol();
  ValueNode* iterator_method;
  {
    DeoptFrameScope deopt_continuation(
        this, Builtin::kGetIteratorWithFeedbackLazyDeoptContinuation, {},
        base::VectorOf<ValueNode*>({receiver, GetSmiConstant(call_slot_index),
                                    GetConstant(feedback())}));
    ReduceResult result_load =
        TryBuildLoadNamedProperty(receiver, iterator_symbol, load_feedback);
    if (result_load.IsDoneWithAbort() || result_load.IsFail()) {
      return result_load;
    }
    DCHECK(result_load.IsDoneWithValue());
    iterator_method = result_load.value();
  }
  auto throw_iterator_error = |&] {
    return BuildCallRuntime(Runtime::kThrowIteratorError, {receiver});
  };
  if (!iterator_method->is_tagged()) {
    return throw_iterator_error();
  }
  auto throw_symbol_iterator_invalid = |&] {
    return BuildCallRuntime(Runtime::kThrowSymbolIteratorInvalid, {});
  };
  auto call_iterator_method = |&] {
    DeoptFrameScope deopt_continuation(
        this, Builtin::kCallIteratorWithFeedbackLazyDeoptContinuation);

    FeedbackSlot call_slot = FeedbackVector::ToSlot(call_slot_index);
    compiler::FeedbackSource call_feedback{feedback(), call_slot};
    CallArguments args(ConvertReceiverMode::kAny, {receiver});
    ReduceResult result_call = ReduceCall(iterator_method, args, call_feedback);

    if (result_call.IsDoneWithAbort()) return result_call;
    DCHECK(result_call.IsDoneWithValue());
    return SelectReduction(
        |&](auto& builder) {
          return BuildBranchIfJSReceiver(builder, result_call.value());
        },
        |&] { return result_call; }, throw_symbol_iterator_invalid);
  };
  // Check if the iterator_method is undefined and call the method otherwise.
  return SelectReduction(
      |&](auto& builder) {
        return BuildBranchIfUndefined(builder, iterator_method);
      },
      throw_iterator_error, call_iterator_method);
}

void MaglevGraphBuilder::VisitGetIterator() {
  // GetIterator <object>
  ValueNode* receiver = LoadRegister(0);
  int load_slot = iterator_.GetIndexOperand(1);
  int call_slot = iterator_.GetIndexOperand(2);
  PROCESS_AND_RETURN_IF_DONE(
      TryReduceGetIterator(receiver, load_slot, call_slot), SetAccumulator);
  // Fallback to the builtin.
  ValueNode* context = GetContext();
  SetAccumulator(AddNewNode<GetIterator>({context, receiver}, load_slot,
                                         call_slot, feedback()));
}

void MaglevGraphBuilder::VisitDebugger() {
  CHECK(BuildCallRuntime(Runtime::kHandleDebuggerStatement, {}).IsDone());
}

void MaglevGraphBuilder::VisitIncBlockCounter() {
  ValueNode* closure = GetClosure();
  ValueNode* coverage_array_slot = GetSmiConstant(iterator_.GetIndexOperand(0));
  BuildCallBuiltin<Builtin::kIncBlockCounter>(
      {GetTaggedValue(closure), coverage_array_slot});
}

void MaglevGraphBuilder::VisitAbort() {
  AbortReason reason = static_cast<AbortReason>(GetFlag8Operand(0));
  RETURN_VOID_IF_DONE(BuildAbort(reason));
  UNREACHABLE();
}

void MaglevGraphBuilder::VisitWide() { UNREACHABLE(); }
void MaglevGraphBuilder::VisitExtraWide() { UNREACHABLE(); }
#define DEBUG_BREAK(Name, ...) \
  void MaglevGraphBuilder::Visit##Name() { UNREACHABLE(); }
DEBUG_BREAK_BYTECODE_LIST(DEBUG_BREAK)
#undef DEBUG_BREAK
void MaglevGraphBuilder::VisitIllegal() { UNREACHABLE(); }

}  // namespace v8::internal::maglev
  MoveNodeBetweenRegisters(iterator_.GetRegisterOperand(0),

Observations: Showing section 4/4 (lines 9001-10563) of 'maglev/maglev-ir.h':
  const Input& new_target() const { return input(kNewTargetIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }

  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }

  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  Builtin builtin_;
};

class CallForwardVarargs : public ValueNodeT<CallForwardVarargs> {
  using Base = ValueNodeT<CallForwardVarargs>;

 public:
  static constexpr int kFunctionIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kFixedInputCount = 2;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallForwardVarargs(uint64_t bitfield, ValueNode* function, ValueNode* context,
                     int start_index, Call::TargetType target_type)
      : Base(bitfield), start_index_(start_index), target_type_(target_type) {
    set_input(kFunctionIndex, function);
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& function() { return input(kFunctionIndex); }
  const Input& function() const { return input(kFunctionIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  int start_index() const { return start_index_; }
  Call::TargetType target_type() const { return target_type_; }

 private:
  int start_index_;
  Call::TargetType target_type_;
};

class CallRuntime : public ValueNodeT<CallRuntime> {
  using Base = ValueNodeT<CallRuntime>;

 public:
  // We assume the context as fixed input.
  static constexpr int kContextIndex = 0;
  static constexpr int kFixedInputCount = 1;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallRuntime(uint64_t bitfield, Runtime::FunctionId function_id,
              ValueNode* context)
      : Base(bitfield), function_id_(function_id) {
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Runtime::FunctionId function_id() const { return function_id_; }

  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  int ReturnCount() const {
    return Runtime::FunctionForId(function_id())->result_size;
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  Runtime::FunctionId function_id_;
};

class CallWithSpread : public ValueNodeT<CallWithSpread> {
  using Base = ValueNodeT<CallWithSpread>;

 public:
  // We assume function and context as fixed inputs.
  static constexpr int kFunctionIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kFixedInputCount = 2;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallWithSpread(uint64_t bitfield, ValueNode* function, ValueNode* context)
      : Base(bitfield) {
    set_input(kFunctionIndex, function);
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& function() { return input(kFunctionIndex); }
  const Input& function() const { return input(kFunctionIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  int num_args_no_spread() const {
    DCHECK_GT(num_args(), 0);
    return num_args() - 1;
  }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args_no_spread() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args_no_spread() - 1)));
  }
  Input& spread() {
    // Spread is the last argument/input.
    return input(input_count() - 1);
  }
  Input& receiver() { return arg(0); }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CallWithArrayLike : public FixedInputValueNodeT<4, CallWithArrayLike> {
  using Base = FixedInputValueNodeT<4, CallWithArrayLike>;

 public:
  // We assume function and context as fixed inputs.
  static constexpr int kFunctionIndex = 0;
  static constexpr int kReceiverIndex = 1;
  static constexpr int kArgumentsListIndex = 2;
  static constexpr int kContextIndex = 3;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  explicit CallWithArrayLike(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties = OpProperties::JSCall();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged,
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& function() { return input(kFunctionIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  Input& arguments_list() { return input(kArgumentsListIndex); }
  Input& context() { return input(kContextIndex); }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CallSelf : public ValueNodeT<CallSelf> {
  using Base = ValueNodeT<CallSelf>;

 public:
  static constexpr int kClosureIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kReceiverIndex = 2;
  static constexpr int kNewTargetIndex = 3;
  static constexpr int kFixedInputCount = 4;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallSelf(uint64_t bitfield, int expected_parameter_count, ValueNode* closure,
           ValueNode* context, ValueNode* receiver, ValueNode* new_target)
      : Base(bitfield), expected_parameter_count_(expected_parameter_count) {
    set_input(kClosureIndex, closure);
    set_input(kContextIndex, context);
    set_input(kReceiverIndex, receiver);
    set_input(kNewTargetIndex, new_target);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& closure() { return input(kClosureIndex); }
  const Input& closure() const { return input(kClosureIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  const Input& receiver() const { return input(kReceiverIndex); }
  Input& new_target() { return input(kNewTargetIndex); }
  const Input& new_target() const { return input(kNewTargetIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  int expected_parameter_count_;
};

class CallKnownJSFunction : public ValueNodeT<CallKnownJSFunction> {
  using Base = ValueNodeT<CallKnownJSFunction>;

 public:
  static constexpr int kClosureIndex = 0;
  static constexpr int kContextIndex = 1;
  static constexpr int kReceiverIndex = 2;
  static constexpr int kNewTargetIndex = 3;
  static constexpr int kFixedInputCount = 4;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  inline CallKnownJSFunction(
      uint64_t bitfield,
#ifdef V8_ENABLE_LEAPTIERING
      JSDispatchHandle dispatch_handle,
#endif
      compiler::SharedFunctionInfoRef shared_function_info, ValueNode* closure,
      ValueNode* context, ValueNode* receiver, ValueNode* new_target);

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& closure() { return input(kClosureIndex); }
  const Input& closure() const { return input(kClosureIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  const Input& receiver() const { return input(kReceiverIndex); }
  Input& new_target() { return input(kNewTargetIndex); }
  const Input& new_target() const { return input(kNewTargetIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  compiler::SharedFunctionInfoRef shared_function_info() const {
    return shared_function_info_;
  }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  int expected_parameter_count() const { return expected_parameter_count_; }

 private:
#ifdef V8_ENABLE_LEAPTIERING
  JSDispatchHandle dispatch_handle_;
#endif
  const compiler::SharedFunctionInfoRef shared_function_info_;
  // Cache the expected parameter count so that we can access it in
  // MaxCallStackArgs without needing to unpark the local isolate.
  int expected_parameter_count_;
};

class CallKnownApiFunction : public ValueNodeT<CallKnownApiFunction> {
  using Base = ValueNodeT<CallKnownApiFunction>;

 public:
  enum Mode {
    // Use Builtin::kCallApiCallbackOptimizedNoProfiling.
    kNoProfiling,
    // Inline API call sequence into the generated code.
    kNoProfilingInlined,
    // Use Builtin::kCallApiCallbackOptimized.
    kGeneric,
  };

  static constexpr int kContextIndex = 0;
  static constexpr int kReceiverIndex = 1;
  static constexpr int kFixedInputCount = 2;

  // We need enough inputs to have these fixed inputs plus the maximum arguments
  // to a function call.
  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  CallKnownApiFunction(uint64_t bitfield, Mode mode,
                       compiler::FunctionTemplateInfoRef function_template_info,
                       compiler::OptionalJSObjectRef api_holder,
                       ValueNode* context, ValueNode* receiver)
      : Base(bitfield | ModeField::encode(mode)),
        function_template_info_(function_template_info),
        api_holder_(api_holder) {
    set_input(kContextIndex, context);
    set_input(kReceiverIndex, receiver);
  }

  // TODO(ishell): introduce JSApiCall() which will take C++ ABI into account
  // when deciding which registers to splill.
  static constexpr OpProperties kProperties = OpProperties::JSCall();

  // Input& closure() { return input(kClosureIndex); }
  // const Input& closure() const { return input(kClosureIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  Input& receiver() { return input(kReceiverIndex); }
  const Input& receiver() const { return input(kReceiverIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  auto args() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args() - 1)));
  }

  Mode mode() const { return ModeField::decode(bitfield()); }

  compiler::FunctionTemplateInfoRef function_template_info() const {
    return function_template_info_;
  }
  compiler::OptionalJSObjectRef api_holder() const { return api_holder_; }

  bool inline_builtin() const { return mode() == kNoProfilingInlined; }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  using ModeField = NextBitField<Mode, 2>;

  void GenerateCallApiCallbackOptimizedInline(MaglevAssembler* masm,
                                              const ProcessingState& state);

  const compiler::FunctionTemplateInfoRef function_template_info_;
  const compiler::OptionalJSObjectRef api_holder_;
};

class ConstructWithSpread : public ValueNodeT<ConstructWithSpread> {
  using Base = ValueNodeT<ConstructWithSpread>;

 public:
  // We assume function and context as fixed inputs.
  static constexpr int kFunctionIndex = 0;
  static constexpr int kNewTargetIndex = 1;
  static constexpr int kContextIndex = 2;
  static constexpr int kFixedInputCount = 3;

  // This ctor is used when for variable input counts.
  // Inputs must be initialized manually.
  ConstructWithSpread(uint64_t bitfield, compiler::FeedbackSource feedback,
                      ValueNode* function, ValueNode* new_target,
                      ValueNode* context)
      : Base(bitfield), feedback_(feedback) {
    set_input(kFunctionIndex, function);
    set_input(kNewTargetIndex, new_target);
    set_input(kContextIndex, context);
  }

  static constexpr OpProperties kProperties = OpProperties::JSCall();

  Input& function() { return input(kFunctionIndex); }
  const Input& function() const { return input(kFunctionIndex); }
  Input& new_target() { return input(kNewTargetIndex); }
  const Input& new_target() const { return input(kNewTargetIndex); }
  Input& context() { return input(kContextIndex); }
  const Input& context() const { return input(kContextIndex); }
  int num_args() const { return input_count() - kFixedInputCount; }
  int num_args_no_spread() const {
    DCHECK_GT(num_args(), 0);
    return num_args() - 1;
  }
  Input& arg(int i) { return input(i + kFixedInputCount); }
  void set_arg(int i, ValueNode* node) {
    set_input(i + kFixedInputCount, node);
  }
  Input& spread() {
    // Spread is the last argument/input.
    return input(input_count() - 1);
  }
  auto args_no_spread() {
    return base::make_iterator_range(
        std::make_reverse_iterator(&arg(-1)),
        std::make_reverse_iterator(&arg(num_args_no_spread() - 1)));
  }
  compiler::FeedbackSource feedback() const { return feedback_; }

  void VerifyInputs(MaglevGraphLabeller* graph_labeller) const;
#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing();
#endif
  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  const compiler::FeedbackSource feedback_;
};

class ConvertReceiver : public FixedInputValueNodeT<1, ConvertReceiver> {
  using Base = FixedInputValueNodeT<1, ConvertReceiver>;

 public:
  explicit ConvertReceiver(uint64_t bitfield,
                           compiler::NativeContextRef native_context,
                           ConvertReceiverMode mode)
      : Base(bitfield), native_context_(native_context), mode_(mode) {}

  Input& receiver_input() { return input(0); }

  // The implementation currently calls runtime.
  static constexpr OpProperties kProperties =
      OpProperties::Call() | OpProperties::NotIdempotent();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  auto options() const { return std::tuple{native_context_, mode_}; }

  compiler::NativeContextRef native_context() const { return native_context_; }
  ConvertReceiverMode mode() const { return mode_; }

 private:
  const compiler::NativeContextRef native_context_;
  ConvertReceiverMode mode_;
};

class CheckConstructResult
    : public FixedInputValueNodeT<2, CheckConstructResult> {
  using Base = FixedInputValueNodeT<2, CheckConstructResult>;

 public:
  explicit CheckConstructResult(uint64_t bitfield) : Base(bitfield) {}

  Input& construct_result_input() { return input(0); }
  Input& implicit_receiver_input() { return input(1); }

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CheckDerivedConstructResult
    : public FixedInputValueNodeT<1, CheckDerivedConstructResult> {
  using Base = FixedInputValueNodeT<1, CheckDerivedConstructResult>;

 public:
  explicit CheckDerivedConstructResult(uint64_t bitfield) : Base(bitfield) {}

  Input& construct_result_input() { return input(0); }

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  bool for_derived_constructor();

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class CheckNotHole : public FixedInputNodeT<1, CheckNotHole> {
  using Base = FixedInputNodeT<1, CheckNotHole>;

 public:
  explicit CheckNotHole(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties = OpProperties::EagerDeopt();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& object_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ConvertHoleToUndefined
    : public FixedInputValueNodeT<1, ConvertHoleToUndefined> {
  using Base = FixedInputValueNodeT<1, ConvertHoleToUndefined>;

 public:
  explicit ConvertHoleToUndefined(uint64_t bitfield) : Base(bitfield) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& object_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class HandleNoHeapWritesInterrupt
    : public FixedInputNodeT<0, HandleNoHeapWritesInterrupt> {
  using Base = FixedInputNodeT<0, HandleNoHeapWritesInterrupt>;

 public:
  explicit HandleNoHeapWritesInterrupt(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties = OpProperties::DeferredCall() |
                                              OpProperties::LazyDeopt() |
                                              OpProperties::NotIdempotent();

  void SetValueLocationConstraints() {}
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
  int MaxCallStackArgs() const { return 0; }
};

class ReduceInterruptBudgetForLoop
    : public FixedInputNodeT<0, ReduceInterruptBudgetForLoop> {
  using Base = FixedInputNodeT<0, ReduceInterruptBudgetForLoop>;

 public:
  explicit ReduceInterruptBudgetForLoop(uint64_t bitfield, int amount)
      : Base(bitfield), amount_(amount) {
    DCHECK_GT(amount, 0);
  }

  static constexpr OpProperties kProperties = OpProperties::DeferredCall() |
                                              OpProperties::LazyDeopt() |
                                              OpProperties::NotIdempotent();

  int amount() const { return amount_; }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  const int amount_;
};

class ReduceInterruptBudgetForReturn
    : public FixedInputNodeT<0, ReduceInterruptBudgetForReturn> {
  using Base = FixedInputNodeT<0, ReduceInterruptBudgetForReturn>;

 public:
  explicit ReduceInterruptBudgetForReturn(uint64_t bitfield, int amount)
      : Base(bitfield), amount_(amount) {
    DCHECK_GT(amount, 0);
  }

  static constexpr OpProperties kProperties =
      OpProperties::DeferredCall() | OpProperties::NotIdempotent();

  int amount() const { return amount_; }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  const int amount_;
};

class ThrowReferenceErrorIfHole
    : public FixedInputNodeT<1, ThrowReferenceErrorIfHole> {
  using Base = FixedInputNodeT<1, ThrowReferenceErrorIfHole>;

 public:
  explicit ThrowReferenceErrorIfHole(uint64_t bitfield,
                                     const compiler::NameRef name)
      : Base(bitfield), name_(name) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  compiler::NameRef name() const { return name_; }

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  auto options() const { return std::tuple{name_}; }

 private:
  const compiler::NameRef name_;
};

class ThrowSuperNotCalledIfHole
    : public FixedInputNodeT<1, ThrowSuperNotCalledIfHole> {
  using Base = FixedInputNodeT<1, ThrowSuperNotCalledIfHole>;

 public:
  explicit ThrowSuperNotCalledIfHole(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ThrowSuperAlreadyCalledIfNotHole
    : public FixedInputNodeT<1, ThrowSuperAlreadyCalledIfNotHole> {
  using Base = FixedInputNodeT<1, ThrowSuperAlreadyCalledIfNotHole>;

 public:
  explicit ThrowSuperAlreadyCalledIfNotHole(uint64_t bitfield)
      : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ThrowIfNotCallable : public FixedInputNodeT<1, ThrowIfNotCallable> {
  using Base = FixedInputNodeT<1, ThrowIfNotCallable>;

 public:
  explicit ThrowIfNotCallable(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value() { return Node::input(0); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class ThrowIfNotSuperConstructor
    : public FixedInputNodeT<2, ThrowIfNotSuperConstructor> {
  using Base = FixedInputNodeT<2, ThrowIfNotSuperConstructor>;

 public:
  explicit ThrowIfNotSuperConstructor(uint64_t bitfield) : Base(bitfield) {}

  static constexpr OpProperties kProperties =
      OpProperties::CanThrow() | OpProperties::DeferredCall();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& constructor() { return Node::input(0); }
  Input& function() { return Node::input(1); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class TransitionElementsKind
    : public FixedInputValueNodeT<2, TransitionElementsKind> {
  using Base = FixedInputValueNodeT<2, TransitionElementsKind>;

 public:
  explicit TransitionElementsKind(
      uint64_t bitfield, const ZoneVector<compiler::MapRef>& transition_sources,
      compiler::MapRef transition_target)
      : Base(bitfield),
        transition_sources_(transition_sources),
        transition_target_(transition_target) {}

  // TODO(leszeks): Special case the case where all transitions are fast.
  static constexpr OpProperties kProperties =
      OpProperties::AnySideEffects() | OpProperties::DeferredCall();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& object_input() { return input(0); }
  Input& map_input() { return input(1); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  const ZoneVector<compiler::MapRef>& transition_sources() const {
    return transition_sources_;
  }
  const compiler::MapRef transition_target() const {
    return transition_target_;
  }

 private:
  ZoneVector<compiler::MapRef> transition_sources_;
  const compiler::MapRef transition_target_;
};

class TransitionElementsKindOrCheckMap
    : public FixedInputNodeT<2, TransitionElementsKindOrCheckMap> {
  using Base = FixedInputNodeT<2, TransitionElementsKindOrCheckMap>;

 public:
  explicit TransitionElementsKindOrCheckMap(
      uint64_t bitfield, const ZoneVector<compiler::MapRef>& transition_sources,
      compiler::MapRef transition_target)
      : Base(bitfield),
        transition_sources_(transition_sources),
        transition_target_(transition_target) {}

  // TODO(leszeks): Special case the case where all transitions are fast.
  static constexpr OpProperties kProperties = OpProperties::AnySideEffects() |
                                              OpProperties::DeferredCall() |
                                              OpProperties::EagerDeopt();
  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

  Input& object_input() { return Node::input(0); }
  Input& map_input() { return Node::input(1); }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  const ZoneVector<compiler::MapRef>& transition_sources() const {
    return transition_sources_;
  }
  const compiler::MapRef transition_target() const {
    return transition_target_;
  }

 private:
  ZoneVector<compiler::MapRef> transition_sources_;
  const compiler::MapRef transition_target_;
};

class GetContinuationPreservedEmbedderData
    : public FixedInputValueNodeT<0, GetContinuationPreservedEmbedderData> {
  using Base = FixedInputValueNodeT<0, GetContinuationPreservedEmbedderData>;

 public:
  explicit GetContinuationPreservedEmbedderData(uint64_t bitfield)
      : Base(bitfield) {}

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  static constexpr OpProperties kProperties =
      OpProperties::CanRead() | OpProperties::TaggedValue();
};

class SetContinuationPreservedEmbedderData
    : public FixedInputNodeT<1, SetContinuationPreservedEmbedderData> {
  using Base = FixedInputNodeT<1, SetContinuationPreservedEmbedderData>;

 public:
  explicit SetContinuationPreservedEmbedderData(uint64_t bitfield)
      : Base(bitfield) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& data_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  static constexpr OpProperties kProperties = OpProperties::CanWrite();
};

class ControlNode : public NodeBase {
 public:
  // A "hole" in control flow is a control node that unconditionally interrupts
  // linear control flow (either by jumping or by exiting).
  //
  // A "post-dominating" hole is a hole that is guaranteed to be be reached in
  // control flow after this node (i.e. it is a hole that is a post-dominator
  // of this node).
  ControlNode* next_post_dominating_hole() const {
    return next_post_dominating_hole_;
  }
  void set_next_post_dominating_hole(ControlNode* node) {
    DCHECK_IMPLIES(node != nullptr, node->Is<UnconditionalControlNode>() ||
                                        node->Is<TerminalControlNode>() ||
                                        node->Is<Switch>());
    next_post_dominating_hole_ = node;
  }

 protected:
  using NodeBase::NodeBase;

 private:
  ControlNode* next_post_dominating_hole_ = nullptr;
};

class UnconditionalControlNode : public ControlNode {
 public:
  BasicBlock* target() const { return target_.block_ptr(); }
  int predecessor_id() const { return predecessor_id_; }
  void set_predecessor_id(int id) { predecessor_id_ = id; }

  void set_target(BasicBlock* block) { target_.set_block_ptr(block); }

 protected:
  explicit UnconditionalControlNode(uint64_t bitfield,
                                    BasicBlockRef* target_refs)
      : ControlNode(bitfield), target_(target_refs) {}
  explicit UnconditionalControlNode(uint64_t bitfield, BasicBlock* target)
      : ControlNode(bitfield), target_(target) {}

 private:
  BasicBlockRef target_;
  int predecessor_id_ = 0;
};

template <class Derived>
class UnconditionalControlNodeT
    : public FixedInputNodeTMixin<0, UnconditionalControlNode, Derived> {
  static_assert(IsUnconditionalControlNode(NodeBase::opcode_of<Derived>));

 protected:
  explicit UnconditionalControlNodeT(uint64_t bitfield,
                                     BasicBlockRef* target_refs)
      : FixedInputNodeTMixin<0, UnconditionalControlNode, Derived>(
            bitfield, target_refs) {}
  explicit UnconditionalControlNodeT(uint64_t bitfield, BasicBlock* target)
      : FixedInputNodeTMixin<0, UnconditionalControlNode, Derived>(bitfield,
                                                                   target) {}
};

class ConditionalControlNode : public ControlNode {
 public:
  explicit ConditionalControlNode(uint64_t bitfield) : ControlNode(bitfield) {}
};

class BranchControlNode : public ConditionalControlNode {
 public:
  BranchControlNode(uint64_t bitfield, BasicBlockRef* if_true_refs,
                    BasicBlockRef* if_false_refs)
      : ConditionalControlNode(bitfield),
        if_true_(if_true_refs),
        if_false_(if_false_refs) {}

  BasicBlock* if_true() const { return if_true_.block_ptr(); }
  BasicBlock* if_false() const { return if_false_.block_ptr(); }

  void set_if_true(BasicBlock* block) { if_true_.set_block_ptr(block); }
  void set_if_false(BasicBlock* block) { if_false_.set_block_ptr(block); }

 private:
  BasicBlockRef if_true_;
  BasicBlockRef if_false_;
};

class TerminalControlNode : public ControlNode {
 protected:
  explicit TerminalControlNode(uint64_t bitfield) : ControlNode(bitfield) {}
};

template <size_t InputCount, class Derived>
class TerminalControlNodeT
    : public FixedInputNodeTMixin<InputCount, TerminalControlNode, Derived> {
  static_assert(IsTerminalControlNode(NodeBase::opcode_of<Derived>));

 protected:
  explicit TerminalControlNodeT(uint64_t bitfield)
      : FixedInputNodeTMixin<InputCount, TerminalControlNode, Derived>(
            bitfield) {}
};

template <size_t InputCount, class Derived>
class BranchControlNodeT
    : public FixedInputNodeTMixin<InputCount, BranchControlNode, Derived> {
  static_assert(IsBranchControlNode(NodeBase::opcode_of<Derived>));

 protected:
  explicit BranchControlNodeT(uint64_t bitfield, BasicBlockRef* if_true_refs,
                              BasicBlockRef* if_false_refs)
      : FixedInputNodeTMixin<InputCount, BranchControlNode, Derived>(
            bitfield, if_true_refs, if_false_refs) {}
};

class Jump : public UnconditionalControlNodeT<Jump> {
  using Base = UnconditionalControlNodeT<Jump>;

 public:
  Jump(uint64_t bitfield, BasicBlockRef* target_refs)
      : Base(bitfield, target_refs) {}

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

// TODO(olivf): Unify implementation with Jump.
class CheckpointedJump : public UnconditionalControlNodeT<CheckpointedJump> {
  using Base = UnconditionalControlNodeT<CheckpointedJump>;

 public:
  CheckpointedJump(uint64_t bitfield, BasicBlockRef* target_refs)
      : Base(bitfield, target_refs) {}

  static constexpr OpProperties kProperties =
      OpProperties::DeoptCheckpoint() | Base::kProperties;

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class JumpLoop : public UnconditionalControlNodeT<JumpLoop> {
  using Base = UnconditionalControlNodeT<JumpLoop>;

 public:
  explicit JumpLoop(uint64_t bitfield, BasicBlock* target)
      : Base(bitfield, target) {}

  explicit JumpLoop(uint64_t bitfield, BasicBlockRef* ref)
      : Base(bitfield, ref) {}

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

  base::Vector<Input> used_nodes() { return used_node_locations_; }
  void set_used_nodes(base::Vector<Input> locations) {
    used_node_locations_ = locations;
  }

 private:
  base::Vector<Input> used_node_locations_;
};

class Abort : public TerminalControlNodeT<0, Abort> {
  using Base = TerminalControlNodeT<0, Abort>;

 public:
  explicit Abort(uint64_t bitfield, AbortReason reason)
      : Base(bitfield), reason_(reason) {
    DCHECK_EQ(NodeBase::opcode(), opcode_of<Abort>);
  }

  static constexpr OpProperties kProperties = OpProperties::Call();

  AbortReason reason() const { return reason_; }

  int MaxCallStackArgs() const;
  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  const AbortReason reason_;
};

class Return : public TerminalControlNodeT<1, Return> {
  using Base = TerminalControlNodeT<1, Return>;

 public:
  explicit Return(uint64_t bitfield) : Base(bitfield) {
    DCHECK_EQ(NodeBase::opcode(), opcode_of<Return>);
  }

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& value_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class Deopt : public TerminalControlNodeT<0, Deopt> {
  using Base = TerminalControlNodeT<0, Deopt>;

 public:
  explicit Deopt(uint64_t bitfield, DeoptimizeReason reason)
      : Base(bitfield), reason_(reason) {
    DCHECK_EQ(NodeBase::opcode(), opcode_of<Deopt>);
  }

  static constexpr OpProperties kProperties = OpProperties::EagerDeopt();

  DeoptimizeReason reason() const { return reason_; }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  DeoptimizeReason reason_;
};

class Switch : public FixedInputNodeTMixin<1, ConditionalControlNode, Switch> {
  using Base = FixedInputNodeTMixin<1, ConditionalControlNode, Switch>;

 public:
  explicit Switch(uint64_t bitfield, int value_base, BasicBlockRef* targets,
                  int size)
      : Base(bitfield),
        value_base_(value_base),
        targets_(targets),
        size_(size),
        fallthrough_() {}

  explicit Switch(uint64_t bitfield, int value_base, BasicBlockRef* targets,
                  int size, BasicBlockRef* fallthrough)
      : Base(bitfield),
        value_base_(value_base),
        targets_(targets),
        size_(size),
        fallthrough_(fallthrough) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kInt32};

  int value_base() const { return value_base_; }
  BasicBlockRef* targets() const { return targets_; }
  int size() const { return size_; }

  bool has_fallthrough() const { return fallthrough_.has_value(); }
  BasicBlock* fallthrough() const {
    DCHECK(has_fallthrough());
    return fallthrough_.value().block_ptr();
  }

  void set_fallthrough(BasicBlock* fallthrough) {
    DCHECK(has_fallthrough());
    fallthrough_.value().set_block_ptr(fallthrough);
  }

  Input& value() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  const int value_base_;
  BasicBlockRef* targets_;
  const int size_;
  std::optional<BasicBlockRef> fallthrough_;
};

class BranchIfSmi : public BranchControlNodeT<1, BranchIfSmi> {
  using Base = BranchControlNodeT<1, BranchIfSmi>;

 public:
  explicit BranchIfSmi(uint64_t bitfield, BasicBlockRef* if_true_refs,
                       BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfRootConstant
    : public BranchControlNodeT<1, BranchIfRootConstant> {
  using Base = BranchControlNodeT<1, BranchIfRootConstant>;

 public:
  explicit BranchIfRootConstant(uint64_t bitfield, RootIndex root_index,
                                BasicBlockRef* if_true_refs,
                                BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), root_index_(root_index) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  RootIndex root_index() { return root_index_; }
  Input& condition_input() { return input(0); }

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  RootIndex root_index_;
};

class BranchIfUndefinedOrNull
    : public BranchControlNodeT<1, BranchIfUndefinedOrNull> {
  using Base = BranchControlNodeT<1, BranchIfUndefinedOrNull>;

 public:
  explicit BranchIfUndefinedOrNull(uint64_t bitfield,
                                   BasicBlockRef* if_true_refs,
                                   BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfUndetectable
    : public BranchControlNodeT<1, BranchIfUndetectable> {
  using Base = BranchControlNodeT<1, BranchIfUndetectable>;

 public:
  explicit BranchIfUndetectable(uint64_t bitfield, CheckType check_type,
                                BasicBlockRef* if_true_refs,
                                BasicBlockRef* if_false_refs)
      : Base(CheckTypeBitField::update(bitfield, check_type), if_true_refs,
             if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }
  CheckType check_type() const { return CheckTypeBitField::decode(bitfield()); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  using CheckTypeBitField = NextBitField<CheckType, 1>;
};

class BranchIfJSReceiver : public BranchControlNodeT<1, BranchIfJSReceiver> {
  using Base = BranchControlNodeT<1, BranchIfJSReceiver>;

 public:
  explicit BranchIfJSReceiver(uint64_t bitfield, BasicBlockRef* if_true_refs,
                              BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfToBooleanTrue
    : public BranchControlNodeT<1, BranchIfToBooleanTrue> {
  using Base = BranchControlNodeT<1, BranchIfToBooleanTrue>;

 public:
  explicit BranchIfToBooleanTrue(uint64_t bitfield, CheckType check_type,
                                 BasicBlockRef* if_true_refs,
                                 BasicBlockRef* if_false_refs)
      : Base(CheckTypeBitField::update(bitfield, check_type), if_true_refs,
             if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  Input& condition_input() { return input(0); }
  CheckType check_type() const { return CheckTypeBitField::decode(bitfield()); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}

 private:
  using CheckTypeBitField = NextBitField<CheckType, 1>;
};

class BranchIfInt32ToBooleanTrue
    : public BranchControlNodeT<1, BranchIfInt32ToBooleanTrue> {
  using Base = BranchControlNodeT<1, BranchIfInt32ToBooleanTrue>;

 public:
  explicit BranchIfInt32ToBooleanTrue(uint64_t bitfield,
                                      BasicBlockRef* if_true_refs,
                                      BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kInt32};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfFloat64ToBooleanTrue
    : public BranchControlNodeT<1, BranchIfFloat64ToBooleanTrue> {
  using Base = BranchControlNodeT<1, BranchIfFloat64ToBooleanTrue>;

 public:
  explicit BranchIfFloat64ToBooleanTrue(uint64_t bitfield,
                                        BasicBlockRef* if_true_refs,
                                        BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kHoleyFloat64};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfFloat64IsHole
    : public BranchControlNodeT<1, BranchIfFloat64IsHole> {
  using Base = BranchControlNodeT<1, BranchIfFloat64IsHole>;

 public:
  explicit BranchIfFloat64IsHole(uint64_t bitfield, BasicBlockRef* if_true_refs,
                                 BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kHoleyFloat64};

  Input& condition_input() { return input(0); }

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfInt32Compare
    : public BranchControlNodeT<2, BranchIfInt32Compare> {
  using Base = BranchControlNodeT<2, BranchIfInt32Compare>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfInt32Compare(uint64_t bitfield, Operation operation,
                                BasicBlockRef* if_true_refs,
                                BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kInt32, ValueRepresentation::kInt32};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Operation operation() const { return operation_; }

 private:
  Operation operation_;
};

class BranchIfUint32Compare
    : public BranchControlNodeT<2, BranchIfUint32Compare> {
  using Base = BranchControlNodeT<2, BranchIfUint32Compare>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfUint32Compare(uint64_t bitfield, Operation operation,
                                 BasicBlockRef* if_true_refs,
                                 BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kUint32, ValueRepresentation::kUint32};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Operation operation() const { return operation_; }

 private:
  Operation operation_;
};

class BranchIfFloat64Compare
    : public BranchControlNodeT<2, BranchIfFloat64Compare> {
  using Base = BranchControlNodeT<2, BranchIfFloat64Compare>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfFloat64Compare(uint64_t bitfield, Operation operation,
                                  BasicBlockRef* if_true_refs,
                                  BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kFloat64, ValueRepresentation::kFloat64};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

  Operation operation() const { return operation_; }

 private:
  Operation operation_;
};

class BranchIfReferenceEqual
    : public BranchControlNodeT<2, BranchIfReferenceEqual> {
  using Base = BranchControlNodeT<2, BranchIfReferenceEqual>;

 public:
  static constexpr int kLeftIndex = 0;
  static constexpr int kRightIndex = 1;
  Input& left_input() { return NodeBase::input(kLeftIndex); }
  Input& right_input() { return NodeBase::input(kRightIndex); }

  explicit BranchIfReferenceEqual(uint64_t bitfield,
                                  BasicBlockRef* if_true_refs,
                                  BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs) {}

  static constexpr typename Base::InputTypes kInputTypes{
      ValueRepresentation::kTagged, ValueRepresentation::kTagged};

#ifdef V8_COMPRESS_POINTERS
  void MarkTaggedInputsAsDecompressing() {
    // Don't need to decompress values to reference compare.
  }
#endif

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
};

class BranchIfTypeOf : public BranchControlNodeT<1, BranchIfTypeOf> {
  using Base = BranchControlNodeT<1, BranchIfTypeOf>;

 public:
  static constexpr int kValueIndex = 0;
  Input& value_input() { return NodeBase::input(kValueIndex); }

  explicit BranchIfTypeOf(uint64_t bitfield,
                          interpreter::TestTypeOfFlags::LiteralFlag literal,
                          BasicBlockRef* if_true_refs,
                          BasicBlockRef* if_false_refs)
      : Base(bitfield, if_true_refs, if_false_refs), literal_(literal) {}

  static constexpr
      typename Base::InputTypes kInputTypes{ValueRepresentation::kTagged};

  void SetValueLocationConstraints();
  void GenerateCode(MaglevAssembler*, const ProcessingState&);
  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;

 private:
  interpreter::TestTypeOfFlags::LiteralFlag literal_;
};

constexpr inline OpProperties StaticPropertiesForOpcode(Opcode opcode) {
  switch (opcode) {
#define CASE(op)      \
  case Opcode::k##op: \
    return op::kProperties;
    NODE_BASE_LIST(CASE)
#undef CASE
  }
}

template <typename Function>
inline void NodeBase::ForAllInputsInRegallocAssignmentOrder(Function&& f) {
  auto iterate_inputs = |&](InputAllocationPolicy category) {
    for (Input& input : *this) {
      switch (compiler::UnallocatedOperand::cast(input.operand())
                  .extended_policy()) {
        case compiler::UnallocatedOperand::MUST_HAVE_REGISTER:
          if (category == InputAllocationPolicy::kArbitraryRegister)
            f(category, &input);
          break;

        case compiler::UnallocatedOperand::REGISTER_OR_SLOT_OR_CONSTANT:
          if (category == InputAllocationPolicy::kAny) f(category, &input);
          break;

        case compiler::UnallocatedOperand::FIXED_REGISTER:
        case compiler::UnallocatedOperand::FIXED_FP_REGISTER:
          if (category == InputAllocationPolicy::kFixedRegister)
            f(category, &input);
          break;

        case compiler::UnallocatedOperand::REGISTER_OR_SLOT:
        case compiler::UnallocatedOperand::SAME_AS_INPUT:
        case compiler::UnallocatedOperand::NONE:
        case compiler::UnallocatedOperand::MUST_HAVE_SLOT:
          UNREACHABLE();
      }
    }
  };

  iterate_inputs(InputAllocationPolicy::kFixedRegister);
  iterate_inputs(InputAllocationPolicy::kArbitraryRegister);
  iterate_inputs(InputAllocationPolicy::kAny);
}

NodeType StaticTypeForNode(compiler::JSHeapBroker* broker,
                           LocalIsolate* isolate, ValueNode* node);

}  // namespace maglev
}  // namespace internal
}  // namespace v8

#endif  // V8_MAGLEV_MAGLEV_IR_H_
Observations: Showing section 2/3 (lines 3001-6000) of 'maglev/maglev-ir.cc':

  int element_size = compiler::ExternalArrayElementSize(type_);

  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register data_pointer = temps.Acquire();

  // We need to make sure we don't clobber is_little_endian_input by writing to
  // the result register.
  Register reg_with_result = result_reg;
  if (type_ != ExternalArrayType::kExternalInt8Array &&
      !is_little_endian_constant() &&
      result_reg == ToRegister(is_little_endian_input())) {
    reg_with_result = data_pointer;
  }

  // Load data pointer.
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));
  MemOperand element_address = __ DataViewElementOperand(data_pointer, index);
  __ LoadSignedField(reg_with_result, element_address, element_size);

  // We ignore little endian argument if type is a byte size.
  if (type_ != ExternalArrayType::kExternalInt8Array) {
    if (is_little_endian_constant()) {
      if (!V8_TARGET_BIG_ENDIAN_BOOL &&
          !FromConstantToBool(masm, is_little_endian_input().node())) {
        DCHECK_EQ(reg_with_result, result_reg);
        __ ReverseByteOrder(result_reg, element_size);
      }
    } else {
      ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
      DCHECK_NE(reg_with_result, ToRegister(is_little_endian_input()));
      __ ToBoolean(
          ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
          V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
          V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order,
          false);
      __ bind(*reverse_byte_order);
      __ ReverseByteOrder(reg_with_result, element_size);
      __ bind(*keep_byte_order);
      if (reg_with_result != result_reg) {
        __ Move(result_reg, reg_with_result);
      }
    }
  }
}

void StoreSignedIntDataViewElement::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(index_input());
  if (compiler::ExternalArrayElementSize(type_) > 1) {
    UseAndClobberRegister(value_input());
  } else {
    UseRegister(value_input());
  }
  if (is_little_endian_constant() ||
      type_ == ExternalArrayType::kExternalInt8Array) {
    UseAny(is_little_endian_input());
  } else {
    UseRegister(is_little_endian_input());
  }
  set_temporaries_needed(1);
}
void StoreSignedIntDataViewElement::GenerateCode(MaglevAssembler* masm,
                                                 const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  Register value = ToRegister(value_input());

  if (v8_flags.debug_code) {
    __ AssertObjectTypeInRange(object,
                               FIRST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               LAST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               AbortReason::kUnexpectedValue);
  }

  int element_size = compiler::ExternalArrayElementSize(type_);

  // We ignore little endian argument if type is a byte size.
  if (element_size > 1) {
    if (is_little_endian_constant()) {
      if (!V8_TARGET_BIG_ENDIAN_BOOL &&
          !FromConstantToBool(masm, is_little_endian_input().node())) {
        __ ReverseByteOrder(value, element_size);
      }
    } else {
      ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
      __ ToBoolean(
          ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
          V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
          V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order,
          false);
      __ bind(*reverse_byte_order);
      __ ReverseByteOrder(value, element_size);
      __ bind(*keep_byte_order);
    }
  }

  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register data_pointer = temps.Acquire();
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));
  MemOperand element_address = __ DataViewElementOperand(data_pointer, index);
  __ StoreField(element_address, value, element_size);
}

void LoadDoubleDataViewElement::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(index_input());
  if (is_little_endian_constant()) {
    UseAny(is_little_endian_input());
  } else {
    UseRegister(is_little_endian_input());
  }
  set_temporaries_needed(1);
  DefineAsRegister(this);
}
void LoadDoubleDataViewElement::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  DoubleRegister result_reg = ToDoubleRegister(result());
  Register data_pointer = temps.Acquire();

  if (v8_flags.debug_code) {
    __ AssertObjectTypeInRange(object,
                               FIRST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               LAST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               AbortReason::kUnexpectedValue);
  }

  // Load data pointer.
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));

  if (is_little_endian_constant()) {
    if (!V8_TARGET_BIG_ENDIAN_BOOL &&
        FromConstantToBool(masm, is_little_endian_input().node())) {
      __ LoadUnalignedFloat64(result_reg, data_pointer, index);
    } else {
      __ LoadUnalignedFloat64AndReverseByteOrder(result_reg, data_pointer,
                                                 index);
    }
  } else {
    Label done;
    ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
    // TODO(leszeks): We're likely to be calling this on an existing boolean --
    // maybe that's a case we should fast-path here and re-use that boolean
    // value?
    __ ToBoolean(
        ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
        V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
        V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order, true);
    __ bind(*keep_byte_order);
    __ LoadUnalignedFloat64(result_reg, data_pointer, index);
    __ Jump(&done);
    // We should swap the bytes if big endian.
    __ bind(*reverse_byte_order);
    __ LoadUnalignedFloat64AndReverseByteOrder(result_reg, data_pointer, index);
    __ bind(&done);
  }
}

void StoreDoubleDataViewElement::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(index_input());
  UseRegister(value_input());
  if (is_little_endian_constant()) {
    UseAny(is_little_endian_input());
  } else {
    UseRegister(is_little_endian_input());
  }
  set_temporaries_needed(1);
}
void StoreDoubleDataViewElement::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  DoubleRegister value = ToDoubleRegister(value_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register data_pointer = temps.Acquire();

  if (v8_flags.debug_code) {
    __ AssertObjectTypeInRange(object,
                               FIRST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               LAST_JS_DATA_VIEW_OR_RAB_GSAB_DATA_VIEW_TYPE,
                               AbortReason::kUnexpectedValue);
  }

  // Load data pointer.
  __ LoadExternalPointerField(
      data_pointer, FieldMemOperand(object, JSDataView::kDataPointerOffset));

  if (is_little_endian_constant()) {
    if (!V8_TARGET_BIG_ENDIAN_BOOL &&
        FromConstantToBool(masm, is_little_endian_input().node())) {
      __ StoreUnalignedFloat64(data_pointer, index, value);
    } else {
      __ ReverseByteOrderAndStoreUnalignedFloat64(data_pointer, index, value);
    }
  } else {
    Label done;
    ZoneLabelRef keep_byte_order(masm), reverse_byte_order(masm);
    // TODO(leszeks): We're likely to be calling this on an existing boolean --
    // maybe that's a case we should fast-path here and re-use that boolean
    // value?
    __ ToBoolean(
        ToRegister(is_little_endian_input()), CheckType::kCheckHeapObject,
        V8_TARGET_BIG_ENDIAN_BOOL ? reverse_byte_order : keep_byte_order,
        V8_TARGET_BIG_ENDIAN_BOOL ? keep_byte_order : reverse_byte_order, true);
    __ bind(*keep_byte_order);
    __ StoreUnalignedFloat64(data_pointer, index, value);
    __ Jump(&done);
    // We should swap the bytes if big endian.
    __ bind(*reverse_byte_order);
    __ ReverseByteOrderAndStoreUnalignedFloat64(data_pointer, index, value);
    __ bind(&done);
  }
}


void LoadEnumCacheLength::SetValueLocationConstraints() {
  UseRegister(map_input());
  DefineAsRegister(this);
}
void LoadEnumCacheLength::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register map = ToRegister(map_input());
  Register result_reg = ToRegister(result());
  __ AssertMap(map);
  __ LoadBitField<Map::Bits3::EnumLengthBits>(
      result_reg, FieldMemOperand(map, Map::kBitField3Offset));
}

int LoadGlobal::MaxCallStackArgs() const {
  if (typeof_mode() == TypeofMode::kNotInside) {
    using D = CallInterfaceDescriptorFor<Builtin::kLoadGlobalIC>::type;
    return D::GetStackParameterCount();
  } else {
    using D =
        CallInterfaceDescriptorFor<Builtin::kLoadGlobalICInsideTypeof>::type;
    return D::GetStackParameterCount();
  }
}
void LoadGlobal::SetValueLocationConstraints() {
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}
void LoadGlobal::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  if (typeof_mode() == TypeofMode::kNotInside) {
    __ CallBuiltin<Builtin::kLoadGlobalIC>(
        context(),                                    // context
        name().object(),                              // name
        TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
        feedback().vector                             // feedback vector
    );
  } else {
    DCHECK_EQ(typeof_mode(), TypeofMode::kInside);
    __ CallBuiltin<Builtin::kLoadGlobalICInsideTypeof>(
        context(),                                    // context
        name().object(),                              // name
        TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
        feedback().vector                             // feedback vector
    );
  }

  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int StoreGlobal::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreGlobalIC>::type;
  return D::GetStackParameterCount();
}
void StoreGlobal::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreGlobalIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void StoreGlobal::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStoreGlobalIC>(
      context(),                                    // context
      name().object(),                              // name
      value(),                                      // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void CheckValue::SetValueLocationConstraints() { UseRegister(target_input()); }
void CheckValue::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  Register target = ToRegister(target_input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  __ CompareTaggedAndJumpIf(target, value().object(), kNotEqual, fail);
}

void CheckValueEqualsInt32::SetValueLocationConstraints() {
  UseRegister(target_input());
}
void CheckValueEqualsInt32::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  Register target = ToRegister(target_input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  __ CompareInt32AndJumpIf(target, value(), kNotEqual, fail);
}

void CheckValueEqualsFloat64::SetValueLocationConstraints() {
  UseRegister(target_input());
  set_double_temporaries_needed(1);
}
void CheckValueEqualsFloat64::GenerateCode(MaglevAssembler* masm,
                                           const ProcessingState& state) {
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  DoubleRegister scratch = temps.AcquireDouble();
  DoubleRegister target = ToDoubleRegister(target_input());
  __ Move(scratch, value());
  __ CompareFloat64AndJumpIf(scratch, target, kNotEqual, fail, fail);
}

void CheckFloat64IsNan::SetValueLocationConstraints() {
  UseRegister(target_input());
}
void CheckFloat64IsNan::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  DoubleRegister target = ToDoubleRegister(target_input());
  __ JumpIfNotNan(target, fail);
}

void CheckValueEqualsString::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStringEqual>::type;
  UseFixed(target_input(), D::GetRegisterParameter(D::kLeft));
  RequireSpecificTemporary(D::GetRegisterParameter(D::kLength));
}
void CheckValueEqualsString::GenerateCode(MaglevAssembler* masm,
                                          const ProcessingState& state) {
  using D = CallInterfaceDescriptorFor<Builtin::kStringEqual>::type;

  ZoneLabelRef end(masm);
  DCHECK_EQ(D::GetRegisterParameter(D::kLeft), ToRegister(target_input()));
  Register target = D::GetRegisterParameter(D::kLeft);
  // Maybe the string is internalized already, do a fast reference check first.
  __ CompareTaggedAndJumpIf(target, value().object(), kEqual, *end,
                            Label::kNear);

  __ EmitEagerDeoptIfSmi(this, target, DeoptimizeReason::kWrongValue);
  __ JumpIfString(
      target,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, CheckValueEqualsString* node,
             ZoneLabelRef end) {
            Register target = D::GetRegisterParameter(D::kLeft);
            Register string_length = D::GetRegisterParameter(D::kLength);
            __ StringLength(string_length, target);
            Label* fail = __ GetDeoptLabel(node, DeoptimizeReason::kWrongValue);
            __ CompareInt32AndJumpIf(string_length, node->value().length(),
                                     kNotEqual, fail);
            RegisterSnapshot snapshot = node->register_snapshot();
            {
              SaveRegisterStateForCall save_register_state(masm, snapshot);
              __ CallBuiltin<Builtin::kStringEqual>(
                  node->target_input(),    // left
                  node->value().object(),  // right
                  string_length            // length
              );
              save_register_state.DefineSafepoint();
              // Compare before restoring registers, so that the deopt below has
              // the correct register set.
              __ CompareRoot(kReturnRegister0, RootIndex::kTrueValue);
            }
            __ EmitEagerDeoptIf(kNotEqual, DeoptimizeReason::kWrongValue, node);
            __ Jump(*end);
          },
          this, end));

  __ EmitEagerDeopt(this, DeoptimizeReason::kWrongValue);

  __ bind(*end);
}

void CheckDynamicValue::SetValueLocationConstraints() {
  UseRegister(first_input());
  UseRegister(second_input());
}
void CheckDynamicValue::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  Register first = ToRegister(first_input());
  Register second = ToRegister(second_input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kWrongValue);
  __ CompareTaggedAndJumpIf(first, second, kNotEqual, fail);
}

void CheckSmi::SetValueLocationConstraints() { UseRegister(receiver_input()); }
void CheckSmi::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  __ EmitEagerDeoptIfNotSmi(this, object, DeoptimizeReason::kNotASmi);
}

void CheckHeapObject::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckHeapObject::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kSmi);
}

void CheckSymbol::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckSymbol::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kNotASymbol);
  }
  __ JumpIfNotObjectType(object, SYMBOL_TYPE,
                         __ GetDeoptLabel(this, DeoptimizeReason::kNotASymbol));
}

void CheckInstanceType::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckInstanceType::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kWrongInstanceType);
  }
  if (first_instance_type_ == last_instance_type_) {
    __ JumpIfNotObjectType(
        object, first_instance_type_,
        __ GetDeoptLabel(this, DeoptimizeReason::kWrongInstanceType));
  } else {
    __ JumpIfObjectTypeNotInRange(
        object, first_instance_type_, last_instance_type_,
        __ GetDeoptLabel(this, DeoptimizeReason::kWrongInstanceType));
  }
}

void CheckCacheIndicesNotCleared::SetValueLocationConstraints() {
  UseRegister(indices_input());
  UseRegister(length_input());
}
void CheckCacheIndicesNotCleared::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  Register indices = ToRegister(indices_input());
  Register length = ToRegister(length_input());
  __ AssertNotSmi(indices);

  if (v8_flags.debug_code) {
    __ AssertObjectType(indices, FIXED_ARRAY_TYPE,
                        AbortReason::kOperandIsNotAFixedArray);
  }
  Label done;
  // If the cache length is zero, we don't have any indices, so we know this is
  // ok even though the indices are the empty array.
  __ CompareInt32AndJumpIf(length, 0, kEqual, &done);
  // Otherwise, an empty array with non-zero required length is not valid.
  __ JumpIfRoot(indices, RootIndex::kEmptyFixedArray,
                __ GetDeoptLabel(this, DeoptimizeReason::kWrongEnumIndices));
  __ bind(&done);
}

void CheckTypedArrayBounds::SetValueLocationConstraints() {
  UseRegister(index_input());
  UseRegister(length_input());
}
void CheckTypedArrayBounds::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  Register index = ToRegister(index_input());
  Register length = ToRegister(length_input());
  // The index must be a zero-extended Uint32 for this to work.
#ifdef V8_TARGET_ARCH_RISCV64
  // All Word32 values are been signed-extended in Register in RISCV.
  __ ZeroExtendWord(index, index);
#endif
  __ AssertZeroExtended(index);
  __ CompareIntPtrAndJumpIf(
      index, length, kUnsignedGreaterThanEqual,
      __ GetDeoptLabel(this, DeoptimizeReason::kOutOfBounds));
}

void CheckInt32Condition::SetValueLocationConstraints() {
  UseRegister(left_input());
  UseRegister(right_input());
}
void CheckInt32Condition::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Label* fail = __ GetDeoptLabel(this, reason());
  __ CompareInt32AndJumpIf(ToRegister(left_input()), ToRegister(right_input()),
                           NegateCondition(ToCondition(condition())), fail);
}

int StoreScriptContextSlotWithWriteBarrier::MaxCallStackArgs() const {
  return WriteBarrierDescriptor::GetStackParameterCount();
}

void StoreScriptContextSlotWithWriteBarrier::SetValueLocationConstraints() {
  UseFixed(context_input(), WriteBarrierDescriptor::ObjectRegister());
  UseRegister(new_value_input());
  set_temporaries_needed(2);
  set_double_temporaries_needed(1);
}

void StoreScriptContextSlotWithWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  __ RecordComment("StoreScriptContextSlotWithWriteBarrier");
  ZoneLabelRef done(masm);
  ZoneLabelRef do_normal_store(masm);

  // TODO(leszeks): Consider making this an arbitrary register and push/popping
  // in the deferred path.
  Register context = WriteBarrierDescriptor::ObjectRegister();
  Register new_value = ToRegister(new_value_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  Register old_value = temps.Acquire();

  __ AssertObjectType(context, SCRIPT_CONTEXT_TYPE,
                      AbortReason::kUnexpectedInstanceType);

  __ LoadTaggedField(old_value, context, offset());
  __ CompareTaggedAndJumpIf(old_value, new_value, kEqual, *done);

  // Load property.
  // TODO(victorgomes): Should we hoist the side_table?
  __ LoadTaggedField(
      scratch, context,
      Context::OffsetOfElementAt(Context::CONTEXT_SIDE_TABLE_PROPERTY_INDEX));
  __ LoadTaggedField(scratch, scratch,
                     FixedArray::OffsetOfElementAt(
                         index() - Context::MIN_CONTEXT_EXTENDED_SLOTS));

  __ CompareTaggedAndJumpIf(
      scratch, ContextSidePropertyCell::Other(), kNotEqual,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register context, Register old_value,
             Register new_value, Register property,
             StoreScriptContextSlotWithWriteBarrier* node, ZoneLabelRef done,
             ZoneLabelRef do_normal_store) {
            Label check_smi, check_mutable_int32, mutable_heap_number;
            __ CompareRootAndEmitEagerDeoptIf(
                property, RootIndex::kUndefinedValue, kEqual,
                DeoptimizeReason::kWrongValue, node);
            __ JumpIfSmi(property, &check_smi);
            __ AssertObjectType(property, CONTEXT_SIDE_PROPERTY_CELL_TYPE,
                                AbortReason::kUnexpectedInstanceType);
            __ LoadTaggedField(
                property, property,
                ContextSidePropertyCell::kPropertyDetailsRawOffset);
            __ bind(&check_smi);

            // Check for const case.
            __ CompareTaggedAndJumpIf(
                property, ContextSidePropertyCell::Const(), kEqual,
                __ GetDeoptLabel(node, DeoptimizeReason::kWrongValue));

            if (v8_flags.script_context_mutable_heap_number) {
              // Check for smi case
              __ CompareTaggedAndJumpIf(property,
                                        ContextSidePropertyCell::SmiMarker(),
                                        kNotEqual, &check_mutable_int32);
              __ EmitEagerDeoptIfNotSmi(node, new_value,
                                        DeoptimizeReason::kWrongValue);
              __ Jump(*do_normal_store);

              MaglevAssembler::TemporaryRegisterScope temps(masm);
              DoubleRegister double_scratch = temps.AcquireDouble();

              // Check mutable int32 case.
              __ bind(&check_mutable_int32);
              if (v8_flags.script_context_mutable_heap_int32) {
                __ CompareTaggedAndJumpIf(
                    property, ContextSidePropertyCell::MutableInt32(),
                    kNotEqual, &mutable_heap_number);
                {
                  Label new_value_is_not_smi;
                  Register new_value_int32 = property;
                  __ JumpIfNotSmi(new_value, &new_value_is_not_smi);
                  __ SmiUntag(new_value_int32, new_value);
                  __ StoreHeapInt32Value(new_value_int32, old_value);
                  __ Jump(*done);

                  __ bind(&new_value_is_not_smi);
                  __ CompareMapWithRoot(new_value, RootIndex::kHeapNumberMap,
                                        property);
                  __ EmitEagerDeoptIf(kNotEqual, DeoptimizeReason::kWrongValue,
                                      node);

                  __ LoadHeapNumberValue(double_scratch, new_value);
                  __ TryTruncateDoubleToInt32(
                      new_value_int32, double_scratch,
                      __ GetDeoptLabel(node, DeoptimizeReason::kWrongValue));
                  __ StoreHeapInt32Value(new_value_int32, old_value);
                  __ Jump(*done);
                }
              }

              // Check mutable heap number case.
              __ bind(&mutable_heap_number);
              {
                Label new_value_is_not_smi;
                Register new_value_int32 = property;
                __ JumpIfNotSmi(new_value, &new_value_is_not_smi);
                __ SmiUntag(new_value_int32, new_value);
                __ Int32ToDouble(double_scratch, new_value_int32);
                __ StoreHeapNumberValue(double_scratch, old_value);
                __ Jump(*done);

                __ bind(&new_value_is_not_smi);
                __ CompareMapWithRoot(new_value, RootIndex::kHeapNumberMap,
                                      property);
                __ EmitEagerDeoptIf(kNotEqual, DeoptimizeReason::kWrongValue,
                                    node);
                __ LoadHeapNumberValue(double_scratch, new_value);
                __ StoreHeapNumberValue(double_scratch, old_value);
                __ Jump(*done);
              }
            } else {
              __ Jump(*do_normal_store);
            }
          },
          context, old_value, new_value, scratch, this, done, do_normal_store));

  __ bind(*do_normal_store);
  __ StoreTaggedFieldWithWriteBarrier(
      context, offset(), new_value, register_snapshot(),
      new_value_input().node()->decompresses_tagged_result()
          ? MaglevAssembler::kValueIsDecompressed
          : MaglevAssembler::kValueIsCompressed,
      MaglevAssembler::kValueCanBeSmi);

  __ bind(*done);
}

void CheckString::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckString::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kNotAString);
  }
  __ JumpIfNotString(object,
                     __ GetDeoptLabel(this, DeoptimizeReason::kNotAString));
}

void CheckStringOrStringWrapper::SetValueLocationConstraints() {
  UseRegister(receiver_input());
  set_temporaries_needed(1);
}

void CheckStringOrStringWrapper::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  Register object = ToRegister(receiver_input());

  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object,
                           DeoptimizeReason::kNotAStringOrStringWrapper);
  }

  auto deopt =
      __ GetDeoptLabel(this, DeoptimizeReason::kNotAStringOrStringWrapper);
  Label done;

  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();

  __ JumpIfString(object, &done);
  __ JumpIfNotObjectType(object, InstanceType::JS_PRIMITIVE_WRAPPER_TYPE,
                         deopt);
  __ LoadMap(scratch, object);
  __ LoadBitField<Map::Bits2::ElementsKindBits>(
      scratch, FieldMemOperand(scratch, Map::kBitField2Offset));
  static_assert(FAST_STRING_WRAPPER_ELEMENTS + 1 ==
                SLOW_STRING_WRAPPER_ELEMENTS);
  __ CompareInt32AndJumpIf(scratch, FAST_STRING_WRAPPER_ELEMENTS, kLessThan,
                           deopt);
  __ CompareInt32AndJumpIf(scratch, SLOW_STRING_WRAPPER_ELEMENTS, kGreaterThan,
                           deopt);
  __ Jump(&done);
  __ bind(&done);
}

void CheckDetectableCallable::SetValueLocationConstraints() {
  UseRegister(receiver_input());
  set_temporaries_needed(1);
}

void CheckDetectableCallable::GenerateCode(MaglevAssembler* masm,
                                           const ProcessingState& state) {
  Register object = ToRegister(receiver_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  auto deopt = __ GetDeoptLabel(this, DeoptimizeReason::kNotDetectableReceiver);
  __ JumpIfNotCallable(object, scratch, check_type(), deopt);
  __ JumpIfUndetectable(object, scratch, CheckType::kOmitHeapObjectCheck,
                        deopt);
}

void CheckNotHole::SetValueLocationConstraints() {
  UseRegister(object_input());
}
void CheckNotHole::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  __ CompareRootAndEmitEagerDeoptIf(ToRegister(object_input()),
                                    RootIndex::kTheHoleValue, kEqual,
                                    DeoptimizeReason::kHole, this);
}

void ConvertHoleToUndefined::SetValueLocationConstraints() {
  UseRegister(object_input());
  DefineSameAsFirst(this);
}
void ConvertHoleToUndefined::GenerateCode(MaglevAssembler* masm,
                                          const ProcessingState& state) {
  Label done;
  DCHECK_EQ(ToRegister(object_input()), ToRegister(result()));
  __ JumpIfNotRoot(ToRegister(object_input()), RootIndex::kTheHoleValue, &done);
  __ LoadRoot(ToRegister(result()), RootIndex::kUndefinedValue);
  __ bind(&done);
}

int ConvertReceiver::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  return D::GetStackParameterCount();
}
void ConvertReceiver::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  static_assert(D::GetRegisterParameter(D::kInput) == kReturnRegister0);
  UseFixed(receiver_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void ConvertReceiver::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  Label convert_to_object, done;
  Register receiver = ToRegister(receiver_input());
  __ JumpIfSmi(
      receiver, &convert_to_object,
      v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);

  // If {receiver} is not primitive, no need to move it to {result}, since
  // they share the same register.
  DCHECK_EQ(receiver, ToRegister(result()));
  __ JumpIfJSAnyIsNotPrimitive(receiver, &done);

  compiler::JSHeapBroker* broker = masm->compilation_info()->broker();
  if (mode_ != ConvertReceiverMode::kNotNullOrUndefined) {
    Label convert_global_proxy;
    __ JumpIfRoot(receiver, RootIndex::kUndefinedValue, &convert_global_proxy,
                  Label::Distance::kNear);
    __ JumpIfNotRoot(
        receiver, RootIndex::kNullValue, &convert_to_object,
        v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);
    __ bind(&convert_global_proxy);
    // Patch receiver to global proxy.
    __ Move(ToRegister(result()),
            native_context_.global_proxy_object(broker).object());
    __ Jump(&done);
  }

  __ bind(&convert_to_object);
  __ CallBuiltin<Builtin::kToObject>(native_context_.object(),
                                     receiver_input());
  __ bind(&done);
}

int CheckDerivedConstructResult::MaxCallStackArgs() const { return 0; }
void CheckDerivedConstructResult::SetValueLocationConstraints() {
  UseRegister(construct_result_input());
  DefineSameAsFirst(this);
}
void CheckDerivedConstructResult::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  Register construct_result = ToRegister(construct_result_input());

  DCHECK_EQ(construct_result, ToRegister(result()));

  // If the result is an object (in the ECMA sense), we should get rid
  // of the receiver and use the result; see ECMA-262 section 13.2.2-7
  // on page 74.
  Label done, do_throw;

  __ CompareRoot(construct_result, RootIndex::kUndefinedValue);
  __ Assert(kNotEqual, AbortReason::kUnexpectedValue);

  // If the result is a smi, it is *not* an object in the ECMA sense.
  __ JumpIfSmi(construct_result, &do_throw, Label::Distance::kNear);

  // Check if the type of the result is not an object in the ECMA sense.
  __ JumpIfJSAnyIsNotPrimitive(construct_result, &done, Label::Distance::kNear);

  // Throw away the result of the constructor invocation and use the
  // implicit receiver as the result.
  __ bind(&do_throw);
  __ Jump(__ MakeDeferredCode(
      |](MaglevAssembler* masm, CheckDerivedConstructResult* node) {
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this));

  __ bind(&done);
}

int CheckConstructResult::MaxCallStackArgs() const { return 0; }
void CheckConstructResult::SetValueLocationConstraints() {
  UseRegister(construct_result_input());
  UseRegister(implicit_receiver_input());
  DefineSameAsFirst(this);
}
void CheckConstructResult::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  Register construct_result = ToRegister(construct_result_input());
  Register result_reg = ToRegister(result());

  DCHECK_EQ(construct_result, result_reg);

  // If the result is an object (in the ECMA sense), we should get rid
  // of the receiver and use the result; see ECMA-262 section 13.2.2-7
  // on page 74.
  Label done, use_receiver;

  // If the result is undefined, we'll use the implicit receiver.
  __ JumpIfRoot(construct_result, RootIndex::kUndefinedValue, &use_receiver,
                Label::Distance::kNear);

  // If the result is a smi, it is *not* an object in the ECMA sense.
  __ JumpIfSmi(construct_result, &use_receiver, Label::Distance::kNear);

  // Check if the type of the result is not an object in the ECMA sense.
  __ JumpIfJSAnyIsNotPrimitive(construct_result, &done, Label::Distance::kNear);

  // Throw away the result of the constructor invocation and use the
  // implicit receiver as the result.
  __ bind(&use_receiver);
  Register implicit_receiver = ToRegister(implicit_receiver_input());
  __ Move(result_reg, implicit_receiver);

  __ bind(&done);
}

int CreateObjectLiteral::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kCreateObjectLiteral)->nargs, 4);
  return 4;
}
void CreateObjectLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateObjectLiteral::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateObjectFromSlowBoilerplate>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      boilerplate_descriptor().object(),            // boilerplate descriptor
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CreateShallowArrayLiteral::MaxCallStackArgs() const {
  using D =
      CallInterfaceDescriptorFor<Builtin::kCreateShallowArrayLiteral>::type;
  return D::GetStackParameterCount();
}
void CreateShallowArrayLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateShallowArrayLiteral::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateShallowArrayLiteral>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      constant_elements().object(),                 // constant elements
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CreateArrayLiteral::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kCreateArrayLiteral)->nargs, 4);
  return 4;
}
void CreateArrayLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateArrayLiteral::GenerateCode(MaglevAssembler* masm,
                                      const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateArrayFromSlowBoilerplate>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      constant_elements().object(),                 // boilerplate descriptor
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CreateShallowObjectLiteral::MaxCallStackArgs() const {
  using D =
      CallInterfaceDescriptorFor<Builtin::kCreateShallowObjectLiteral>::type;
  return D::GetStackParameterCount();
}
void CreateShallowObjectLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateShallowObjectLiteral::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateShallowObjectLiteral>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      boilerplate_descriptor().object(),            // desc
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void AllocationBlock::SetValueLocationConstraints() { DefineAsRegister(this); }

void AllocationBlock::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ Allocate(register_snapshot(), ToRegister(result()), size(),
              allocation_type());
}

int CreateClosure::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(pretenured() ? Runtime::kNewClosure_Tenured
                                                : Runtime::kNewClosure)
                ->nargs,
            2);
  return 2;
}
void CreateClosure::SetValueLocationConstraints() {
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}
void CreateClosure::GenerateCode(MaglevAssembler* masm,
                                 const ProcessingState& state) {
  Runtime::FunctionId function_id =
      pretenured() ? Runtime::kNewClosure_Tenured : Runtime::kNewClosure;
  __ Push(shared_function_info().object(), feedback_cell().object());
  __ CallRuntime(function_id);
}

int FastCreateClosure::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kFastNewClosure>::type;
  return D::GetStackParameterCount();
}
void FastCreateClosure::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kFastNewClosure>::type;
  static_assert(D::HasContextParameter());
  UseFixed(context(), D::ContextRegister());
  DefineAsFixed(this, kReturnRegister0);
}
void FastCreateClosure::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  __ CallBuiltin<Builtin::kFastNewClosure>(
      context(),                        // context
      shared_function_info().object(),  // shared function info
      feedback_cell().object()          // feedback cell
  );
  masm->DefineLazyDeoptPoint(lazy_deopt_info());
}

int CreateFunctionContext::MaxCallStackArgs() const {
  if (scope_type() == FUNCTION_SCOPE) {
    using D = CallInterfaceDescriptorFor<
        Builtin::kFastNewFunctionContextFunction>::type;
    return D::GetStackParameterCount();
  } else {
    using D =
        CallInterfaceDescriptorFor<Builtin::kFastNewFunctionContextEval>::type;
    return D::GetStackParameterCount();
  }
}
void CreateFunctionContext::SetValueLocationConstraints() {
  DCHECK_LE(slot_count(),
            static_cast<uint32_t>(
                ConstructorBuiltins::MaximumFunctionContextSlots()));
  if (scope_type() == FUNCTION_SCOPE) {
    using D = CallInterfaceDescriptorFor<
        Builtin::kFastNewFunctionContextFunction>::type;
    static_assert(D::HasContextParameter());
    UseFixed(context(), D::ContextRegister());
  } else {
    DCHECK_EQ(scope_type(), ScopeType::EVAL_SCOPE);
    using D =
        CallInterfaceDescriptorFor<Builtin::kFastNewFunctionContextEval>::type;
    static_assert(D::HasContextParameter());
    UseFixed(context(), D::ContextRegister());
  }
  DefineAsFixed(this, kReturnRegister0);
}
void CreateFunctionContext::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  if (scope_type() == FUNCTION_SCOPE) {
    __ CallBuiltin<Builtin::kFastNewFunctionContextFunction>(
        context(),              // context
        scope_info().object(),  // scope info
        slot_count()            // slots
    );
  } else {
    __ CallBuiltin<Builtin::kFastNewFunctionContextEval>(
        context(),              // context
        scope_info().object(),  // scope info
        slot_count()            // slots
    );
  }
  masm->DefineLazyDeoptPoint(lazy_deopt_info());
}

int CreateRegExpLiteral::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kCreateRegExpLiteral>::type;
  return D::GetStackParameterCount();
}
void CreateRegExpLiteral::SetValueLocationConstraints() {
  DefineAsFixed(this, kReturnRegister0);
}
void CreateRegExpLiteral::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  __ CallBuiltin<Builtin::kCreateRegExpLiteral>(
      masm->native_context().object(),              // context
      feedback().vector,                            // feedback vector
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      pattern().object(),                           // pattern
      Smi::FromInt(flags())                         // flags
  );
  masm->DefineLazyDeoptPoint(lazy_deopt_info());
}

int GetTemplateObject::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kGetTemplateObject>::type;
  return D::GetStackParameterCount();
}
void GetTemplateObject::SetValueLocationConstraints() {
  using D = GetTemplateObjectDescriptor;
  UseFixed(description(), D::GetRegisterParameter(D::kDescription));
  DefineAsFixed(this, kReturnRegister0);
}
void GetTemplateObject::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  __ CallBuiltin<Builtin::kGetTemplateObject>(
      masm->native_context().object(),  // context
      shared_function_info_.object(),   // shared function info
      description(),                    // description
      feedback().index(),               // feedback slot
      feedback().vector                 // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int HasInPrototypeChain::MaxCallStackArgs() const {
  DCHECK_EQ(2, Runtime::FunctionForId(Runtime::kHasInPrototypeChain)->nargs);
  return 2;
}
void HasInPrototypeChain::SetValueLocationConstraints() {
  UseRegister(object());
  DefineAsRegister(this);
  set_temporaries_needed(2);
}
void HasInPrototypeChain::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register object_reg = ToRegister(object());
  Register result_reg = ToRegister(result());

  Label return_false, return_true;
  ZoneLabelRef done(masm);

  __ JumpIfSmi(object_reg, &return_false,
               v8_flags.debug_code ? Label::kFar : Label::kNear);

  // Loop through the prototype chain looking for the {prototype}.
  Register map = temps.Acquire();
  __ LoadMap(map, object_reg);
  Label loop;
  {
    __ bind(&loop);
    Register scratch = temps.Acquire();
    // Check if we can determine the prototype directly from the {object_map}.
    ZoneLabelRef if_objectisdirect(masm);
    Register instance_type = scratch;
    Condition jump_cond = __ CompareInstanceTypeRange(
        map, instance_type, FIRST_TYPE, LAST_SPECIAL_RECEIVER_TYPE);
    __ JumpToDeferredIf(
        jump_cond,
        |](MaglevAssembler* masm, RegisterSnapshot snapshot,
           Register object_reg, Register map, Register instance_type,
           Register result_reg, HasInPrototypeChain* node,
           ZoneLabelRef if_objectisdirect, ZoneLabelRef done) {
          Label return_runtime;
          // The {object_map} is a special receiver map or a primitive map,
          // check if we need to use the if_objectisspecial path in the runtime.
          __ JumpIfEqual(instance_type, JS_PROXY_TYPE, &return_runtime);

          int mask = Map::Bits1::HasNamedInterceptorBit::kMask |
                     Map::Bits1::IsAccessCheckNeededBit::kMask;
          __ TestUint8AndJumpIfAllClear(
              FieldMemOperand(map, Map::kBitFieldOffset), mask,
              *if_objectisdirect);

          __ bind(&return_runtime);
          {
            snapshot.live_registers.clear(result_reg);
            SaveRegisterStateForCall save_register_state(masm, snapshot);
            __ Push(object_reg, node->prototype().object());
            __ Move(kContextRegister, masm->native_context().object());
            __ CallRuntime(Runtime::kHasInPrototypeChain, 2);
            masm->DefineExceptionHandlerPoint(node);
            save_register_state.DefineSafepointWithLazyDeopt(
                node->lazy_deopt_info());
            __ Move(result_reg, kReturnRegister0);
          }
          __ Jump(*done);
        },
        register_snapshot(), object_reg, map, instance_type, result_reg, this,
        if_objectisdirect, done);
    instance_type = Register::no_reg();

    __ bind(*if_objectisdirect);
    // Check the current {object} prototype.
    Register object_prototype = scratch;
    __ LoadTaggedField(object_prototype, map, Map::kPrototypeOffset);
    __ JumpIfRoot(object_prototype, RootIndex::kNullValue, &return_false,
                  v8_flags.debug_code ? Label::kFar : Label::kNear);
    __ CompareTaggedAndJumpIf(object_prototype, prototype().object(), kEqual,
                              &return_true, Label::kNear);

    // Continue with the prototype.
    __ AssertNotSmi(object_prototype);
    __ LoadMap(map, object_prototype);
    __ Jump(&loop);
  }

  __ bind(&return_true);
  __ LoadRoot(result_reg, RootIndex::kTrueValue);
  __ Jump(*done, Label::kNear);

  __ bind(&return_false);
  __ LoadRoot(result_reg, RootIndex::kFalseValue);
  __ bind(*done);
}

void DebugBreak::SetValueLocationConstraints() {}
void DebugBreak::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  __ DebugBreak();
}

int Abort::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kAbort)->nargs, 1);
  return 1;
}
void Abort::SetValueLocationConstraints() {}
void Abort::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
  __ Push(Smi::FromInt(static_cast<int>(reason())));
  __ CallRuntime(Runtime::kAbort, 1);
  __ Trap();
}

void LogicalNot::SetValueLocationConstraints() {
  UseAny(value());
  DefineAsRegister(this);
}
void LogicalNot::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  if (v8_flags.debug_code) {
    // LogicalNot expects either TrueValue or FalseValue.
    Label next;
    __ JumpIf(__ IsRootConstant(value(), RootIndex::kFalseValue), &next);
    __ JumpIf(__ IsRootConstant(value(), RootIndex::kTrueValue), &next);
    __ Abort(AbortReason::kUnexpectedValue);
    __ bind(&next);
  }

  Label return_false, done;
  __ JumpIf(__ IsRootConstant(value(), RootIndex::kTrueValue), &return_false);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done);

  __ bind(&return_false);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);

  __ bind(&done);
}

int LoadNamedGeneric::MaxCallStackArgs() const {
  return LoadWithVectorDescriptor::GetStackParameterCount();
}
void LoadNamedGeneric::SetValueLocationConstraints() {
  using D = LoadWithVectorDescriptor;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  DefineAsFixed(this, kReturnRegister0);
}
void LoadNamedGeneric::GenerateCode(MaglevAssembler* masm,
                                    const ProcessingState& state) {
  __ CallBuiltin<Builtin::kLoadIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name().object(),                              // name
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int LoadNamedFromSuperGeneric::MaxCallStackArgs() const {
  return LoadWithReceiverAndVectorDescriptor::GetStackParameterCount();
}
void LoadNamedFromSuperGeneric::SetValueLocationConstraints() {
  using D = LoadWithReceiverAndVectorDescriptor;
  UseFixed(context(), kContextRegister);
  UseFixed(receiver(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(lookup_start_object(),
           D::GetRegisterParameter(D::kLookupStartObject));
  DefineAsFixed(this, kReturnRegister0);
}
void LoadNamedFromSuperGeneric::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ CallBuiltin<Builtin::kLoadSuperIC>(
      context(),                                    // context
      receiver(),                                   // receiver
      lookup_start_object(),                        // lookup start object
      name().object(),                              // name
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int SetNamedGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreIC>::type;
  return D::GetStackParameterCount();
}
void SetNamedGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void SetNamedGeneric::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStoreIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name().object(),                              // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int DefineNamedOwnGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineNamedOwnIC>::type;
  return D::GetStackParameterCount();
}
void DefineNamedOwnGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineNamedOwnIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void DefineNamedOwnGeneric::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  __ CallBuiltin<Builtin::kDefineNamedOwnIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name().object(),                              // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void UpdateJSArrayLength::SetValueLocationConstraints() {
  UseRegister(length_input());
  UseRegister(object_input());
  UseAndClobberRegister(index_input());
  DefineSameAsFirst(this);
}

void UpdateJSArrayLength::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register length = ToRegister(length_input());
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  DCHECK_EQ(length, ToRegister(result()));

  Label done, tag_length;
  if (v8_flags.debug_code) {
    __ AssertObjectType(object, JS_ARRAY_TYPE, AbortReason::kUnexpectedValue);
    static_assert(Internals::IsValidSmi(FixedArray::kMaxLength),
                  "MaxLength not a Smi");
    __ CompareInt32AndAssert(index, FixedArray::kMaxLength, kUnsignedLessThan,
                             AbortReason::kUnexpectedValue);
  }
  __ CompareInt32AndJumpIf(index, length, kUnsignedLessThan, &tag_length,
                           Label::kNear);
  __ IncrementInt32(index);  // This cannot overflow.
  __ SmiTag(length, index);
  __ StoreTaggedSignedField(object, JSArray::kLengthOffset, length);
  __ Jump(&done, Label::kNear);
  __ bind(&tag_length);
  __ SmiTag(length);
  __ bind(&done);
}

void EnsureWritableFastElements::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(object_input());
  set_temporaries_needed(1);
  DefineSameAsFirst(this);
}
void EnsureWritableFastElements::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register elements = ToRegister(elements_input());
  DCHECK_EQ(elements, ToRegister(result()));
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ EnsureWritableFastElements(register_snapshot(), elements, object, scratch);
}

void MaybeGrowFastElements::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(object_input());
  UseRegister(index_input());
  UseRegister(elements_length_input());
  if (IsSmiOrObjectElementsKind(elements_kind())) {
    set_temporaries_needed(1);
  }
  DefineSameAsFirst(this);
}
void MaybeGrowFastElements::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  Register elements = ToRegister(elements_input());
  Register object = ToRegister(object_input());
  Register index = ToRegister(index_input());
  Register elements_length = ToRegister(elements_length_input());
  DCHECK_EQ(elements, ToRegister(result()));

  ZoneLabelRef done(masm);

  __ CompareInt32AndJumpIf(
      index, elements_length, kUnsignedGreaterThanEqual,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, ZoneLabelRef done, Register object,
             Register index, Register result_reg, MaybeGrowFastElements* node) {
            {
              RegisterSnapshot snapshot = node->register_snapshot();
              snapshot.live_registers.clear(result_reg);
              snapshot.live_tagged_registers.clear(result_reg);
              SaveRegisterStateForCall save_register_state(masm, snapshot);
              using D = GrowArrayElementsDescriptor;
              if (index == D::GetRegisterParameter(D::kObject)) {
                // That implies that the first parameter move will clobber the
                // index value. So we use the result register as temporary.
                // TODO(leszeks): Use parallel moves to resolve cases like this.
                __ SmiTag(result_reg, index);
                index = result_reg;
              } else {
                __ SmiTag(index);
              }
              if (IsDoubleElementsKind(node->elements_kind())) {
                __ CallBuiltin<Builtin::kGrowFastDoubleElements>(object, index);
              } else {
                __ CallBuiltin<Builtin::kGrowFastSmiOrObjectElements>(object,
                                                                      index);
              }
              save_register_state.DefineSafepoint();
              __ Move(result_reg, kReturnRegister0);
            }
            __ EmitEagerDeoptIfSmi(node, result_reg,
                                   DeoptimizeReason::kCouldNotGrowElements);
            __ Jump(*done);
          },
          done, object, index, elements, this));

  __ bind(*done);
}

void ExtendPropertiesBackingStore::SetValueLocationConstraints() {
  UseRegister(property_array_input());
  UseRegister(object_input());
  DefineAsRegister(this);
  set_temporaries_needed(2);
}

void ExtendPropertiesBackingStore::GenerateCode(MaglevAssembler* masm,
                                                const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register old_property_array = ToRegister(property_array_input());
  Register result_reg = ToRegister(result());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register new_property_array =
      result_reg == object || result_reg == old_property_array ? temps.Acquire()
                                                               : result_reg;
  Register scratch = temps.Acquire();
  DCHECK(!AreAliased(object, old_property_array, new_property_array, scratch));

  int new_length = old_length_ + JSObject::kFieldsAdded;

  // Allocate new PropertyArray.
  {
    RegisterSnapshot snapshot = register_snapshot();
    // old_property_array needs to be live, since we'll read data from it.
    // Object needs to be live, since we write the new property array into it.
    snapshot.live_registers.set(object);
    snapshot.live_registers.set(old_property_array);
    snapshot.live_tagged_registers.set(object);
    snapshot.live_tagged_registers.set(old_property_array);

    Register size_in_bytes = scratch;
    __ Move(size_in_bytes, PropertyArray::SizeFor(new_length));
    __ Allocate(snapshot, new_property_array, size_in_bytes,
                AllocationType::kYoung);
    __ SetMapAsRoot(new_property_array, RootIndex::kPropertyArrayMap);
  }

  // Copy existing properties over.
  {
    RegisterSnapshot snapshot = register_snapshot();
    snapshot.live_registers.set(object);
    snapshot.live_registers.set(old_property_array);
    snapshot.live_registers.set(new_property_array);
    snapshot.live_tagged_registers.set(object);
    snapshot.live_tagged_registers.set(old_property_array);
    snapshot.live_tagged_registers.set(new_property_array);

    for (int i = 0; i < old_length_; ++i) {
      __ LoadTaggedFieldWithoutDecompressing(
          scratch, old_property_array, PropertyArray::OffsetOfElementAt(i));

      __ StoreTaggedFieldWithWriteBarrier(
          new_property_array, PropertyArray::OffsetOfElementAt(i), scratch,
          snapshot, MaglevAssembler::kValueIsCompressed,
          MaglevAssembler::kValueCanBeSmi);
    }
  }

  // Initialize new properties to undefined.
  __ LoadRoot(scratch, RootIndex::kUndefinedValue);
  for (int i = 0; i < JSObject::kFieldsAdded; ++i) {
    __ StoreTaggedFieldNoWriteBarrier(
        new_property_array, PropertyArray::OffsetOfElementAt(old_length_ + i),
        scratch);
  }

  // Read the hash.
  if (old_length_ == 0) {
    // The object might still have a hash, stored in properties_or_hash. If
    // properties_or_hash is a SMI, then it's the hash. It can also be an empty
    // PropertyArray.
    __ LoadTaggedField(scratch, object, JSObject::kPropertiesOrHashOffset);

    Label done;
    __ JumpIfSmi(scratch, &done);

    __ Move(scratch, PropertyArray::kNoHashSentinel);

    __ bind(&done);
    __ SmiUntag(scratch);
    __ ShiftLeft(scratch, PropertyArray::HashField::kShift);
  } else {
    __ LoadTaggedField(scratch, old_property_array,
                       PropertyArray::kLengthAndHashOffset);
    __ SmiUntag(scratch);
    __ AndInt32(scratch, PropertyArray::HashField::kMask);
  }

  // Add the new length and write the length-and-hash field.
  static_assert(PropertyArray::LengthField::kShift == 0);
  __ OrInt32(scratch, new_length);

  __ UncheckedSmiTagInt32(scratch, scratch);
  __ StoreTaggedFieldNoWriteBarrier(
      new_property_array, PropertyArray::kLengthAndHashOffset, scratch);

  {
    RegisterSnapshot snapshot = register_snapshot();
    // new_property_array needs to be live since we'll return it.
    snapshot.live_registers.set(new_property_array);
    snapshot.live_tagged_registers.set(new_property_array);

    __ StoreTaggedFieldWithWriteBarrier(
        object, JSObject::kPropertiesOrHashOffset, new_property_array, snapshot,
        MaglevAssembler::kValueIsDecompressed,
        MaglevAssembler::kValueCannotBeSmi);
  }
  if (result_reg != new_property_array) {
    __ Move(result_reg, new_property_array);
  }
}

int SetKeyedGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedStoreIC>::type;
  return D::GetStackParameterCount();
}
void SetKeyedGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedStoreIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(key_input(), D::GetRegisterParameter(D::kName));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void SetKeyedGeneric::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ CallBuiltin<Builtin::kKeyedStoreIC>(
      context(),                                    // context
      object_input(),                               // receiver
      key_input(),                                  // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int DefineKeyedOwnGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineKeyedOwnIC>::type;
  return D::GetStackParameterCount();
}
void DefineKeyedOwnGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kDefineKeyedOwnIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(key_input(), D::GetRegisterParameter(D::kName));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  UseFixed(flags_input(), D::GetRegisterParameter(D::kFlags));
  DefineAsFixed(this, kReturnRegister0);
}
void DefineKeyedOwnGeneric::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  __ CallBuiltin<Builtin::kDefineKeyedOwnIC>(
      context(),                                    // context
      object_input(),                               // receiver
      key_input(),                                  // name
      value_input(),                                // value
      flags_input(),                                // flags
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int StoreInArrayLiteralGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreInArrayLiteralIC>::type;
  return D::GetStackParameterCount();
}
void StoreInArrayLiteralGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kStoreInArrayLiteralIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(name_input(), D::GetRegisterParameter(D::kName));
  UseFixed(value_input(), D::GetRegisterParameter(D::kValue));
  DefineAsFixed(this, kReturnRegister0);
}
void StoreInArrayLiteralGeneric::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStoreInArrayLiteralIC>(
      context(),                                    // context
      object_input(),                               // receiver
      name_input(),                                 // name
      value_input(),                                // value
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void GeneratorRestoreRegister::SetValueLocationConstraints() {
  UseRegister(array_input());
  UseRegister(stale_input());
  DefineAsRegister(this);
  set_temporaries_needed(1);
}
void GeneratorRestoreRegister::GenerateCode(MaglevAssembler* masm,
                                            const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register temp = temps.Acquire();
  Register array = ToRegister(array_input());
  Register stale = ToRegister(stale_input());
  Register result_reg = ToRegister(result());

  // The input and the output can alias, if that happens we use a temporary
  // register and a move at the end.
  Register value = (array == result_reg ? temp : result_reg);

  // Loads the current value in the generator register file.
  __ LoadTaggedField(value, array, FixedArray::OffsetOfElementAt(index()));

  // And trashs it with StaleRegisterConstant.
  DCHECK(stale_input().node()->Is<RootConstant>());
  __ StoreTaggedFieldNoWriteBarrier(
      array, FixedArray::OffsetOfElementAt(index()), stale);

  if (value != result_reg) {
    __ Move(result_reg, value);
  }
}

int GeneratorStore::MaxCallStackArgs() const {
  return WriteBarrierDescriptor::GetStackParameterCount();
}
void GeneratorStore::SetValueLocationConstraints() {
  UseAny(context_input());
  UseRegister(generator_input());
  for (int i = 0; i < num_parameters_and_registers(); i++) {
    UseAny(parameters_and_registers(i));
  }
  RequireSpecificTemporary(WriteBarrierDescriptor::ObjectRegister());
  RequireSpecificTemporary(WriteBarrierDescriptor::SlotAddressRegister());
}
void GeneratorStore::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  Register generator = ToRegister(generator_input());
  Register array = WriteBarrierDescriptor::ObjectRegister();
  __ LoadTaggedField(array, generator,
                     JSGeneratorObject::kParametersAndRegistersOffset);

  RegisterSnapshot register_snapshot_during_store = register_snapshot();
  // Include the array and generator registers in the register snapshot while
  // storing parameters and registers, to avoid the write barrier clobbering
  // them.
  register_snapshot_during_store.live_registers.set(array);
  register_snapshot_during_store.live_tagged_registers.set(array);
  register_snapshot_during_store.live_registers.set(generator);
  register_snapshot_during_store.live_tagged_registers.set(generator);
  for (int i = 0; i < num_parameters_and_registers(); i++) {
    // Use WriteBarrierDescriptor::SlotAddressRegister() as the temporary for
    // the value -- it'll be clobbered by StoreTaggedFieldWithWriteBarrier since
    // it's not in the register snapshot, but that's ok, and a clobberable value
    // register lets the write barrier emit slightly better code.
    Input value_input = parameters_and_registers(i);
    Register value = __ FromAnyToRegister(
        value_input, WriteBarrierDescriptor::SlotAddressRegister());
    // Include the value register in the live set, in case it is used by future
    // inputs.
    register_snapshot_during_store.live_registers.set(value);
    register_snapshot_during_store.live_tagged_registers.set(value);
    __ StoreTaggedFieldWithWriteBarrier(
        array, FixedArray::OffsetOfElementAt(i), value,
        register_snapshot_during_store,
        value_input.node()->decompresses_tagged_result()
            ? MaglevAssembler::kValueIsDecompressed
            : MaglevAssembler::kValueIsCompressed,
        MaglevAssembler::kValueCanBeSmi);
  }

  __ StoreTaggedSignedField(generator, JSGeneratorObject::kContinuationOffset,
                            Smi::FromInt(suspend_id()));
  __ StoreTaggedSignedField(generator,
                            JSGeneratorObject::kInputOrDebugPosOffset,
                            Smi::FromInt(bytecode_offset()));

  // Use WriteBarrierDescriptor::SlotAddressRegister() as the scratch
  // register, see comment above. At this point we no longer need to preserve
  // the array or generator registers, so use the original register snapshot.
  Register context = __ FromAnyToRegister(
      context_input(), WriteBarrierDescriptor::SlotAddressRegister());
  __ StoreTaggedFieldWithWriteBarrier(
      generator, JSGeneratorObject::kContextOffset, context,
      register_snapshot(),
      context_input().node()->decompresses_tagged_result()
          ? MaglevAssembler::kValueIsDecompressed
          : MaglevAssembler::kValueIsCompressed,
      MaglevAssembler::kValueCannotBeSmi);
}

int GetKeyedGeneric::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedLoadIC>::type;
  return D::GetStackParameterCount();
}
void GetKeyedGeneric::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kKeyedLoadIC>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object_input(), D::GetRegisterParameter(D::kReceiver));
  UseFixed(key_input(), D::GetRegisterParameter(D::kName));
  DefineAsFixed(this, kReturnRegister0);
}
void GetKeyedGeneric::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  __ CallBuiltin<Builtin::kKeyedLoadIC>(
      context(),                                    // context
      object_input(),                               // receiver
      key_input(),                                  // name
      TaggedIndex::FromIntptr(feedback().index()),  // feedback slot
      feedback().vector                             // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void Int32ToNumber::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Int32ToNumber::GenerateCode(MaglevAssembler* masm,
                                 const ProcessingState& state) {
  Register object = ToRegister(result());
  Register value = ToRegister(input());
  ZoneLabelRef done(masm);
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  // Object is not allowed to alias value, because SmiTagInt32AndJumpIfFail will
  // clobber `object` even if the tagging fails, and we don't want it to clobber
  // `value`.
  bool input_output_alias = (object == value);
  Register res = object;
  if (input_output_alias) {
    res = temps.AcquireScratch();
  }
  __ SmiTagInt32AndJumpIfFail(
      res, value,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register object, Register value,
             Register scratch, ZoneLabelRef done, Int32ToNumber* node) {
            MaglevAssembler::TemporaryRegisterScope temps(masm);
            // AllocateHeapNumber needs a scratch register, and the res scratch
            // register isn't needed anymore, so return it to the pool.
            if (scratch.is_valid()) {
              temps.IncludeScratch(scratch);
            }
            DoubleRegister double_value = temps.AcquireScratchDouble();
            __ Int32ToDouble(double_value, value);
            __ AllocateHeapNumber(node->register_snapshot(), object,
                                  double_value);
            __ Jump(*done);
          },
          object, value, input_output_alias ? res : Register::no_reg(), done,
          this));
  if (input_output_alias) {
    __ Move(object, res);
  }
  __ bind(*done);
}

void Uint32ToNumber::SetValueLocationConstraints() {
  UseRegister(input());
#ifdef V8_TARGET_ARCH_X64
  // We emit slightly more efficient code if result is the same as input.
  DefineSameAsFirst(this);
#else
  DefineAsRegister(this);
#endif
}
void Uint32ToNumber::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  ZoneLabelRef done(masm);
  Register value = ToRegister(input());
  Register object = ToRegister(result());
  // Unlike Int32ToNumber, object is allowed to alias value here (indeed, the
  // code is better if it does). The difference is that Uint32 smi tagging first
  // does a range check, and doesn't clobber `object` on failure.
  __ SmiTagUint32AndJumpIfFail(
      object, value,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register object, Register value,
             ZoneLabelRef done, Uint32ToNumber* node) {
            MaglevAssembler::TemporaryRegisterScope temps(masm);
            DoubleRegister double_value = temps.AcquireScratchDouble();
            __ Uint32ToDouble(double_value, value);
            __ AllocateHeapNumber(node->register_snapshot(), object,
                                  double_value);
            __ Jump(*done);
          },
          object, value, done, this));
  __ bind(*done);
}

void Float64ToTagged::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Float64ToTagged::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());
  Label box, done;
  if (canonicalize_smi()) {
    __ TryTruncateDoubleToInt32(object, value, &box);
    __ SmiTagInt32AndJumpIfFail(object, &box);
    __ Jump(&done, Label::kNear);
    __ bind(&box);
  }
  __ AllocateHeapNumber(register_snapshot(), object, value);
  if (canonicalize_smi()) {
    __ bind(&done);
  }
}

void Float64ToHeapNumberForField::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Float64ToHeapNumberForField::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());
  __ AllocateHeapNumber(register_snapshot(), object, value);
}

void HoleyFloat64ToTagged::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void HoleyFloat64ToTagged::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  ZoneLabelRef done(masm);
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());
  Label box;
  if (canonicalize_smi()) {
    __ TryTruncateDoubleToInt32(object, value, &box);
    __ SmiTagInt32AndJumpIfFail(object, &box);
    __ Jump(*done, Label::kNear);
    __ bind(&box);
  }
  // Using return as scratch register.
  __ JumpIfHoleNan(
      value, ToRegister(result()),
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, Register object, ZoneLabelRef done) {
            // TODO(leszeks): Evaluate whether this is worth deferring.
            __ LoadRoot(object, RootIndex::kUndefinedValue);
            __ Jump(*done);
          },
          object, done));
  __ AllocateHeapNumber(register_snapshot(), object, value);
  __ bind(*done);
}

void Float64Round::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
  if (kind_ == Kind::kNearest) {
    set_double_temporaries_needed(1);
  }
}

void Int32AbsWithOverflow::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}

void Float64Abs::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}

void CheckedSmiTagFloat64::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void CheckedSmiTagFloat64::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register object = ToRegister(result());

  __ TryTruncateDoubleToInt32(
      object, value, __ GetDeoptLabel(this, DeoptimizeReason::kNotASmi));
  __ SmiTagInt32AndJumpIfFail(
      object, __ GetDeoptLabel(this, DeoptimizeReason::kNotASmi));
}

void StoreFloat64::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(value_input());
}
void StoreFloat64::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  Register object = ToRegister(object_input());
  DoubleRegister value = ToDoubleRegister(value_input());

  __ AssertNotSmi(object);
  __ StoreFloat64(FieldMemOperand(object, offset()), value);
}

void StoreInt32::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(value_input());
}
void StoreInt32::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register value = ToRegister(value_input());

  __ AssertNotSmi(object);
  __ StoreInt32(FieldMemOperand(object, offset()), value);
}

void StoreTaggedFieldNoWriteBarrier::SetValueLocationConstraints() {
  UseRegister(object_input());
  UseRegister(value_input());
}
void StoreTaggedFieldNoWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  Register object = ToRegister(object_input());
  Register value = ToRegister(value_input());

  __ AssertNotSmi(object);
  __ StoreTaggedFieldNoWriteBarrier(object, offset(), value);
}

int StringAt::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kStringCharCodeAt)->nargs, 2);
  return std::max(2, AllocateDescriptor::GetStackParameterCount());
}
void StringAt::SetValueLocationConstraints() {
  UseAndClobberRegister(string_input());
  UseAndClobberRegister(index_input());
  DefineAsRegister(this);
  set_temporaries_needed(1);
}
void StringAt::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  Register result_string = ToRegister(result());
  Register string = ToRegister(string_input());
  Register index = ToRegister(index_input());
  Register char_code = string;

  ZoneLabelRef done(masm);
  Label cached_one_byte_string;

  RegisterSnapshot save_registers = register_snapshot();
  __ StringCharCodeOrCodePointAt(
      BuiltinStringPrototypeCharCodeOrCodePointAt::kCharCodeAt, save_registers,
      char_code, string, index, scratch, Register::no_reg(),
      &cached_one_byte_string);
  __ StringFromCharCode(save_registers, &cached_one_byte_string, result_string,
                        char_code, scratch,
                        MaglevAssembler::CharCodeMaskMode::kValueIsInRange);
}

int BuiltinStringPrototypeCharCodeOrCodePointAt::MaxCallStackArgs() const {
  DCHECK_EQ(Runtime::FunctionForId(Runtime::kStringCharCodeAt)->nargs, 2);
  return 2;
}
void BuiltinStringPrototypeCharCodeOrCodePointAt::
    SetValueLocationConstraints() {
  UseAndClobberRegister(string_input());
  UseAndClobberRegister(index_input());
  DefineAsRegister(this);
  // TODO(victorgomes): Add a mode to the register allocator where we ensure
  // input cannot alias with output. We can then remove the second scratch.
  set_temporaries_needed(
      mode_ == BuiltinStringPrototypeCharCodeOrCodePointAt::kCodePointAt ? 2
                                                                         : 1);
}
void BuiltinStringPrototypeCharCodeOrCodePointAt::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch1 = temps.Acquire();
  Register scratch2 = Register::no_reg();
  if (mode_ == BuiltinStringPrototypeCharCodeOrCodePointAt::kCodePointAt) {
    scratch2 = temps.Acquire();
  }
  Register string = ToRegister(string_input());
  Register index = ToRegister(index_input());
  ZoneLabelRef done(masm);
  RegisterSnapshot save_registers = register_snapshot();
  __ StringCharCodeOrCodePointAt(mode_, save_registers, ToRegister(result()),
                                 string, index, scratch1, scratch2, *done);
  __ bind(*done);
}

void StringLength::SetValueLocationConstraints() {
  UseRegister(object_input());
  DefineAsRegister(this);
}
void StringLength::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  __ StringLength(ToRegister(result()), ToRegister(object_input()));
}

void StringConcat::SetValueLocationConstraints() {
  using D = StringAdd_CheckNoneDescriptor;
  UseFixed(lhs(), D::GetRegisterParameter(D::kLeft));
  UseFixed(rhs(), D::GetRegisterParameter(D::kRight));
  DefineAsFixed(this, kReturnRegister0);
}
void StringConcat::GenerateCode(MaglevAssembler* masm,
                                const ProcessingState& state) {
  __ CallBuiltin<Builtin::kStringAdd_CheckNone>(
      masm->native_context().object(),  // context
      lhs(),                            // left
      rhs()                             // right
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  DCHECK_EQ(kReturnRegister0, ToRegister(result()));
}

void StringWrapperConcat::SetValueLocationConstraints() {
  using D = StringAdd_CheckNoneDescriptor;
  UseFixed(lhs(), D::GetRegisterParameter(D::kLeft));
  UseFixed(rhs(), D::GetRegisterParameter(D::kRight));
  DefineAsFixed(this, kReturnRegister0);
}

void StringWrapperConcat::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);

  Register left = ToRegister(lhs());
  Label left_done;
  __ JumpIfString(left, &left_done);
  __ LoadTaggedField(left, left, JSPrimitiveWrapper::kValueOffset);
  __ Jump(&left_done);

  __ bind(&left_done);

  Register right = ToRegister(rhs());
  Label right_done;
  __ JumpIfString(right, &right_done);
  __ LoadTaggedField(right, right, JSPrimitiveWrapper::kValueOffset);
  __ Jump(&right_done);

  __ bind(&right_done);

  __ CallBuiltin<Builtin::kStringAdd_CheckNone>(
      masm->native_context().object(),  // context
      left,                             // left
      right                             // right
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  DCHECK_EQ(kReturnRegister0, ToRegister(result()));
}

void StringEqual::SetValueLocationConstraints() {
  using D = StringEqualDescriptor;
  UseFixed(lhs(), D::GetRegisterParameter(D::kLeft));
  UseFixed(rhs(), D::GetRegisterParameter(D::kRight));
  set_temporaries_needed(1);
  RequireSpecificTemporary(D::GetRegisterParameter(D::kLength));
  DefineAsFixed(this, kReturnRegister0);
}
void StringEqual::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  using D = StringEqualDescriptor;
  Label done, if_equal, if_not_equal;
  Register left = ToRegister(lhs());
  Register right = ToRegister(rhs());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register left_length = temps.Acquire();
  Register right_length = D::GetRegisterParameter(D::kLength);

  __ CmpTagged(left, right);
  __ JumpIf(kEqual, &if_equal,
            // Debug checks in StringLength can make this jump too long for a
            // near jump.
            v8_flags.debug_code ? Label::kFar : Label::kNear);

  __ StringLength(left_length, left);
  __ StringLength(right_length, right);
  __ CompareInt32AndJumpIf(left_length, right_length, kNotEqual, &if_not_equal,
                           Label::Distance::kNear);

  // The inputs are already in the right registers. The |left| and |right|
  // inputs were required to come in in the left/right inputs of the builtin,
  // and the |length| input of the builtin is where we loaded the length of the
  // right string (which matches the length of the left string when we get
  // here).
  DCHECK_EQ(right_length, D::GetRegisterParameter(D::kLength));
  __ CallBuiltin<Builtin::kStringEqual>(lhs(), rhs(),
                                        D::GetRegisterParameter(D::kLength));
  masm->DefineLazyDeoptPoint(this->lazy_deopt_info());
  __ Jump(&done, Label::Distance::kNear);

  __ bind(&if_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done, Label::Distance::kNear);

  __ bind(&if_not_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);

  __ bind(&done);
}

void TaggedEqual::SetValueLocationConstraints() {
  UseRegister(lhs());
  UseRegister(rhs());
  DefineAsRegister(this);
}
void TaggedEqual::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Label done, if_equal;
  __ CmpTagged(ToRegister(lhs()), ToRegister(rhs()));
  __ JumpIf(kEqual, &if_equal, Label::Distance::kNear);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);
  __ Jump(&done);
  __ bind(&if_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ bind(&done);
}

void TaggedNotEqual::SetValueLocationConstraints() {
  UseRegister(lhs());
  UseRegister(rhs());
  DefineAsRegister(this);
}
void TaggedNotEqual::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  Label done, if_equal;
  __ CmpTagged(ToRegister(lhs()), ToRegister(rhs()));
  __ JumpIf(kEqual, &if_equal, Label::Distance::kNear);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done);
  __ bind(&if_equal);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);
  __ bind(&done);
}

int TestInstanceOf::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kInstanceOf_WithFeedback>::type;
  return D::GetStackParameterCount();
}
void TestInstanceOf::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kInstanceOf_WithFeedback>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(object(), D::GetRegisterParameter(D::kLeft));
  UseFixed(callable(), D::GetRegisterParameter(D::kRight));
  DefineAsFixed(this, kReturnRegister0);
}
void TestInstanceOf::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  __ CallBuiltin<Builtin::kInstanceOf_WithFeedback>(
      context(),           // context
      object(),            // left
      callable(),          // right
      feedback().index(),  // feedback slot
      feedback().vector    // feedback vector
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

void TestTypeOf::SetValueLocationConstraints() {
  UseRegister(value());
  DefineAsRegister(this);
#ifdef V8_TARGET_ARCH_ARM
  set_temporaries_needed(1);
#endif
}
void TestTypeOf::GenerateCode(MaglevAssembler* masm,
                              const ProcessingState& state) {
#ifdef V8_TARGET_ARCH_ARM
  // Arm32 needs one extra scratch register for TestTypeOf, so take a maglev
  // temporary and allow it to be used as a macro assembler scratch register.
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  temps.IncludeScratch(temps.Acquire());
#endif
  Register object = ToRegister(value());
  Label is_true, is_false, done;
  __ TestTypeOf(object, literal_, &is_true, Label::Distance::kNear, true,
                &is_false, Label::Distance::kNear, false);
  // Fallthrough into true.
  __ bind(&is_true);
  __ LoadRoot(ToRegister(result()), RootIndex::kTrueValue);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&is_false);
  __ LoadRoot(ToRegister(result()), RootIndex::kFalseValue);
  __ bind(&done);
}

void ToBoolean::SetValueLocationConstraints() {
  UseRegister(value());
  DefineAsRegister(this);
}
void ToBoolean::GenerateCode(MaglevAssembler* masm,
                             const ProcessingState& state) {
  Register object = ToRegister(value());
  Register return_value = ToRegister(result());
  Label done;
  ZoneLabelRef object_is_true(masm), object_is_false(masm);
  // TODO(leszeks): We're likely to be calling this on an existing boolean --
  // maybe that's a case we should fast-path here and re-use that boolean value?
  __ ToBoolean(object, check_type(), object_is_true, object_is_false, true);
  __ bind(*object_is_true);
  __ LoadRoot(return_value, RootIndex::kTrueValue);
  __ Jump(&done);
  __ bind(*object_is_false);
  __ LoadRoot(return_value, RootIndex::kFalseValue);
  __ bind(&done);
}

void ToBooleanLogicalNot::SetValueLocationConstraints() {
  UseRegister(value());
  DefineAsRegister(this);
}
void ToBooleanLogicalNot::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register object = ToRegister(value());
  Register return_value = ToRegister(result());
  Label done;
  ZoneLabelRef object_is_true(masm), object_is_false(masm);
  __ ToBoolean(object, check_type(), object_is_true, object_is_false, true);
  __ bind(*object_is_true);
  __ LoadRoot(return_value, RootIndex::kFalseValue);
  __ Jump(&done);
  __ bind(*object_is_false);
  __ LoadRoot(return_value, RootIndex::kTrueValue);
  __ bind(&done);
}

int ToName::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToName>::type;
  return D::GetStackParameterCount();
}
void ToName::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToName>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void ToName::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
  __ CallBuiltin<Builtin::kToName>(context(),     // context
                                   value_input()  // input
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int ToNumberOrNumeric::MaxCallStackArgs() const {
  return TypeConversionDescriptor::GetStackParameterCount();
}
void ToNumberOrNumeric::SetValueLocationConstraints() {
  UseRegister(value_input());
  set_temporaries_needed(1);
  DefineAsRegister(this);
}
void ToNumberOrNumeric::GenerateCode(MaglevAssembler* masm,
                                     const ProcessingState& state) {
  ZoneLabelRef done(masm);
  Label move_and_return;
  Register object = ToRegister(value_input());
  Register result_reg = ToRegister(result());

  __ JumpIfSmi(object, &move_and_return, Label::kNear);
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ CompareMapWithRoot(object, RootIndex::kHeapNumberMap, scratch);
  __ JumpToDeferredIf(
      kNotEqual,
      |](MaglevAssembler* masm, Object::Conversion mode, Register object,
         Register result_reg, ToNumberOrNumeric* node, ZoneLabelRef done) {
        {
          RegisterSnapshot snapshot = node->register_snapshot();
          snapshot.live_registers.clear(result_reg);
          SaveRegisterStateForCall save_register_state(masm, snapshot);
          switch (mode) {
            case Object::Conversion::kToNumber:
              __ CallBuiltin<Builtin::kToNumber>(
                  masm->native_context().object(), object);
              break;
            case Object::Conversion::kToNumeric:
              __ CallBuiltin<Builtin::kToNumeric>(
                  masm->native_context().object(), object);
              break;
          }
          masm->DefineExceptionHandlerPoint(node);
          save_register_state.DefineSafepointWithLazyDeopt(
              node->lazy_deopt_info());
          __ Move(result_reg, kReturnRegister0);
        }
        __ Jump(*done);
      },
      mode(), object, result_reg, this, done);
  __ bind(&move_and_return);
  __ Move(result_reg, object);

  __ bind(*done);
}

int ToObject::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  return D::GetStackParameterCount();
}
void ToObject::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void ToObject::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  Register value = ToRegister(value_input());
  Label call_builtin, done;
  // Avoid the builtin call if {value} is a JSReceiver.
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(value);
  } else {
    __ JumpIfSmi(value, &call_builtin, Label::Distance::kNear);
  }
  __ JumpIfJSAnyIsNotPrimitive(value, &done, Label::Distance::kNear);
  __ bind(&call_builtin);
  __ CallBuiltin<Builtin::kToObject>(context(),     // context
                                     value_input()  // input
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  __ bind(&done);
}

int ToString::MaxCallStackArgs() const {
  using D = CallInterfaceDescriptorFor<Builtin::kToString>::type;
  return D::GetStackParameterCount();
}
void ToString::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kToString>::type;
  UseFixed(context(), kContextRegister);
  UseFixed(value_input(), D::GetRegisterParameter(D::kO));
  DefineAsFixed(this, kReturnRegister0);
}
void ToString::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  Register value = ToRegister(value_input());
  Label call_builtin, done;
  // Avoid the builtin call if {value} is a string.
  __ JumpIfSmi(value, &call_builtin, Label::Distance::kNear);
  __ JumpIfString(value, &done, Label::Distance::kNear);
  if (mode() == kConvertSymbol) {
    __ JumpIfNotObjectType(value, SYMBOL_TYPE, &call_builtin,
                           Label::Distance::kNear);
    __ Push(value);
    __ CallRuntime(Runtime::kSymbolDescriptiveString, 1);
    __ Jump(&done, Label::kNear);
  }
  __ bind(&call_builtin);
  __ CallBuiltin<Builtin::kToString>(context(),     // context
                                     value_input()  // input
  );
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
  __ bind(&done);
}

void NumberToString::SetValueLocationConstraints() {
  using D = CallInterfaceDescriptorFor<Builtin::kNumberToString>::type;
  UseFixed(value_input(), D::GetRegisterParameter(D::kInput));
  DefineAsFixed(this, kReturnRegister0);
}
void NumberToString::GenerateCode(MaglevAssembler* masm,
                                  const ProcessingState& state) {
  __ CallBuiltin<Builtin::kNumberToString>(value_input());
  masm->DefineLazyDeoptPoint(this->lazy_deopt_info());
}

int ThrowReferenceErrorIfHole::MaxCallStackArgs() const { return 1; }
void ThrowReferenceErrorIfHole::SetValueLocationConstraints() {
  UseAny(value());
}
void ThrowReferenceErrorIfHole::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ JumpToDeferredIf(
      __ IsRootConstant(value(), RootIndex::kTheHoleValue),
      |](MaglevAssembler* masm, ThrowReferenceErrorIfHole* node) {
        __ Push(node->name().object());
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowAccessedUninitializedVariable, 1);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);
}

int ThrowSuperNotCalledIfHole::MaxCallStackArgs() const { return 0; }
void ThrowSuperNotCalledIfHole::SetValueLocationConstraints() {
  UseAny(value());
}
void ThrowSuperNotCalledIfHole::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  __ JumpToDeferredIf(
      __ IsRootConstant(value(), RootIndex::kTheHoleValue),
      |](MaglevAssembler* masm, ThrowSuperNotCalledIfHole* node) {
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowSuperNotCalled, 0);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);
}

int ThrowSuperAlreadyCalledIfNotHole::MaxCallStackArgs() const { return 0; }
void ThrowSuperAlreadyCalledIfNotHole::SetValueLocationConstraints() {
  UseAny(value());
}
void ThrowSuperAlreadyCalledIfNotHole::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  __ JumpToDeferredIf(
      NegateCondition(__ IsRootConstant(value(), RootIndex::kTheHoleValue)),
      |](MaglevAssembler* masm, ThrowSuperAlreadyCalledIfNotHole* node) {
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowSuperAlreadyCalledError, 0);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);
}

int ThrowIfNotCallable::MaxCallStackArgs() const { return 1; }
void ThrowIfNotCallable::SetValueLocationConstraints() {
  UseRegister(value());
  set_temporaries_needed(1);
}
void ThrowIfNotCallable::GenerateCode(MaglevAssembler* masm,
                                      const ProcessingState& state) {
  Label* if_not_callable = __ MakeDeferredCode(
      |](MaglevAssembler* masm, ThrowIfNotCallable* node) {
        __ Push(node->value());
        __ Move(kContextRegister, masm->native_context().object());
        __ CallRuntime(Runtime::kThrowCalledNonCallable, 1);
        masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
        __ Abort(AbortReason::kUnexpectedReturnFromThrow);
      },
      this);

  Register value_reg = ToRegister(value());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ JumpIfNotCallable(value_reg, scratch, CheckType::kCheckHeapObject,
                       if_not_callable);
}

int ThrowIfNotSuperConstructor::MaxCallStackArgs() const { return 2; }
void ThrowIfNotSuperConstructor::SetValueLocationConstraints() {
  UseRegister(constructor());
  UseRegister(function());
  set_temporaries_needed(1);
}
void ThrowIfNotSuperConstructor::GenerateCode(MaglevAssembler* masm,
                                              const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  __ LoadMap(scratch, ToRegister(constructor()));
  static_assert(Map::kBitFieldOffsetEnd + 1 - Map::kBitFieldOffset == 1);
  __ TestUint8AndJumpIfAllClear(
      FieldMemOperand(scratch, Map::kBitFieldOffset),
      Map::Bits1::IsConstructorBit::kMask,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, ThrowIfNotSuperConstructor* node) {
            __ Push(ToRegister(node->constructor()),
                    ToRegister(node->function()));
            __ Move(kContextRegister, masm->native_context().object());
            __ CallRuntime(Runtime::kThrowNotSuperConstructor, 2);
            masm->DefineExceptionHandlerAndLazyDeoptPoint(node);
            __ Abort(AbortReason::kUnexpectedReturnFromThrow);
          },
          this));
}

void TruncateUint32ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void TruncateUint32ToInt32::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  // No code emitted -- as far as the machine is concerned, int32 is uint32.
  DCHECK_EQ(ToRegister(input()), ToRegister(result()));
}

void TruncateFloat64ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void TruncateFloat64ToInt32::GenerateCode(MaglevAssembler* masm,
                                          const ProcessingState& state) {
  __ TruncateDoubleToInt32(ToRegister(result()), ToDoubleRegister(input()));
}

void CheckedTruncateFloat64ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void CheckedTruncateFloat64ToInt32::GenerateCode(MaglevAssembler* masm,
                                                 const ProcessingState& state) {
  __ TryTruncateDoubleToInt32(
      ToRegister(result()), ToDoubleRegister(input()),
      __ GetDeoptLabel(this, DeoptimizeReason::kNotInt32));
}

void CheckedTruncateFloat64ToUint32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void CheckedTruncateFloat64ToUint32::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  __ TryTruncateDoubleToUint32(
      ToRegister(result()), ToDoubleRegister(input()),
      __ GetDeoptLabel(this, DeoptimizeReason::kNotUint32));
}

void UnsafeTruncateFloat64ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void UnsafeTruncateFloat64ToInt32::GenerateCode(MaglevAssembler* masm,
                                                const ProcessingState& state) {
#ifdef DEBUG
  Label fail, start;
  __ Jump(&start);
  __ bind(&fail);
  __ Abort(AbortReason::kFloat64IsNotAInt32);

  __ bind(&start);
  __ TryTruncateDoubleToInt32(ToRegister(result()), ToDoubleRegister(input()),
                              &fail);
#else
  // TODO(dmercadier): TruncateDoubleToInt32 does additional work when the
  // double doesn't fit in a 32-bit integer. This is not necessary for
  // UnsafeTruncateFloat64ToInt32 (since we statically know that it the double
  // fits in a 32-bit int) and could be instead just a Cvttsd2si (x64) or Fcvtzs
  // (arm64).
  __ TruncateDoubleToInt32(ToRegister(result()), ToDoubleRegister(input()));
#endif
}

void CheckedUint32ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void CheckedUint32ToInt32::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  Register input_reg = ToRegister(input());
  Label* fail = __ GetDeoptLabel(this, DeoptimizeReason::kNotInt32);
  __ CompareInt32AndJumpIf(input_reg, 0, kLessThan, fail);
}

void UnsafeTruncateUint32ToInt32::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void UnsafeTruncateUint32ToInt32::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
#ifdef DEBUG
  Register input_reg = ToRegister(input());
  __ CompareInt32AndAssert(input_reg, 0, kGreaterThanEqual,
                           AbortReason::kUint32IsNotAInt32);
#endif
  // No code emitted -- as far as the machine is concerned, int32 is uint32.
  DCHECK_EQ(ToRegister(input()), ToRegister(result()));
}

void Int32ToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void Int32ToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  Register value = ToRegister(input());
  Register result_reg = ToRegister(result());
  DCHECK_EQ(value, result_reg);
  Label min, done;
  __ CompareInt32AndJumpIf(value, 0, kLessThanEqual, &min);
  __ CompareInt32AndJumpIf(value, 255, kLessThanEqual, &done);
  __ Move(result_reg, 255);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&min);
  __ Move(result_reg, 0);
  __ bind(&done);
}

void Uint32ToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
}
void Uint32ToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  Register value = ToRegister(input());
  DCHECK_EQ(value, ToRegister(result()));
  Label done;
  __ CompareInt32AndJumpIf(value, 255, kUnsignedLessThanEqual, &done,
                           Label::Distance::kNear);
  __ Move(value, 255);
  __ bind(&done);
}

void Float64ToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineAsRegister(this);
}
void Float64ToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                         const ProcessingState& state) {
  DoubleRegister value = ToDoubleRegister(input());
  Register result_reg = ToRegister(result());
  Label min, max, done;
  __ ToUint8Clamped(result_reg, value, &min, &max, &done);
  __ bind(&min);
  __ Move(result_reg, 0);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&max);
  __ Move(result_reg, 255);
  __ bind(&done);
}

void CheckNumber::SetValueLocationConstraints() {
  UseRegister(receiver_input());
}
void CheckNumber::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
  Label done;
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.AcquireScratch();
  Register value = ToRegister(receiver_input());
  // If {value} is a Smi or a HeapNumber, we're done.
  __ JumpIfSmi(
      value, &done,
      v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);
  if (mode() == Object::Conversion::kToNumeric) {
    __ LoadMapForCompare(scratch, value);
    __ CompareTaggedRoot(scratch, RootIndex::kHeapNumberMap);
    // Jump to done if it is a HeapNumber.
    __ JumpIf(
        kEqual, &done,
        v8_flags.debug_code ? Label::Distance::kFar : Label::Distance::kNear);
    // Check if it is a BigInt.
    __ CompareTaggedRootAndEmitEagerDeoptIf(
        scratch, RootIndex::kBigIntMap, kNotEqual,
        DeoptimizeReason::kNotANumber, this);
  } else {
    __ CompareMapWithRootAndEmitEagerDeoptIf(
        value, RootIndex::kHeapNumberMap, scratch, kNotEqual,
        DeoptimizeReason::kNotANumber, this);
  }
  __ bind(&done);
}

void CheckedInternalizedString::SetValueLocationConstraints() {
  UseRegister(object_input());
  DefineSameAsFirst(this);
}
void CheckedInternalizedString::GenerateCode(MaglevAssembler* masm,
                                             const ProcessingState& state) {
  Register object = ToRegister(object_input());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register instance_type = temps.AcquireScratch();
  if (check_type() == CheckType::kOmitHeapObjectCheck) {
    __ AssertNotSmi(object);
  } else {
    __ EmitEagerDeoptIfSmi(this, object, DeoptimizeReason::kWrongMap);
  }
  __ LoadInstanceType(instance_type, object);
  __ RecordComment("Test IsInternalizedString");
  // Go to the slow path if this is a non-string, or a non-internalised string.
  static_assert((kStringTag | kInternalizedTag) == 0);
  ZoneLabelRef done(masm);
  __ TestInt32AndJumpIfAnySet(
      instance_type, kIsNotStringMask | kIsNotInternalizedMask,
      __ MakeDeferredCode(
          |](MaglevAssembler* masm, ZoneLabelRef done,
             CheckedInternalizedString* node, Register object,
             Register instance_type) {
            __ RecordComment("Deferred Test IsThinString");
            // Deopt if this isn't a string.
            __ TestInt32AndJumpIfAnySet(
                instance_type, kIsNotStringMask,
                __ GetDeoptLabel(node, DeoptimizeReason::kWrongMap));
            // Deopt if this isn't a thin string.
            static_assert(base::bits::CountPopulation(kThinStringTagBit) == 1);
            __ TestInt32AndJumpIfAllClear(
                instance_type, kThinStringTagBit,
                __ GetDeoptLabel(node, DeoptimizeReason::kWrongMap));
            // Load internalized string from thin string.
            __ LoadTaggedField(object, object, offsetof(ThinString, actual_));
            if (v8_flags.debug_code) {
              __ RecordComment("DCHECK IsInternalizedString");
              Label checked;
              __ LoadInstanceType(instance_type, object);
              __ TestInt32AndJumpIfAllClear(
                  instance_type, kIsNotStringMask | kIsNotInternalizedMask,
                  &checked);
              __ Abort(AbortReason::kUnexpectedValue);
              __ bind(&checked);
            }
            __ Jump(*done);
          },
          done, this, object, instance_type));
  __ bind(*done);
}

void CheckedNumberToUint8Clamped::SetValueLocationConstraints() {
  UseRegister(input());
  DefineSameAsFirst(this);
  set_temporaries_needed(1);
  set_double_temporaries_needed(1);
}
void CheckedNumberToUint8Clamped::GenerateCode(MaglevAssembler* masm,
                                               const ProcessingState& state) {
  Register value = ToRegister(input());
  Register result_reg = ToRegister(result());
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  DoubleRegister double_value = temps.AcquireDouble();
  Label is_not_smi, min, max, done;
  // Check if Smi.
  __ JumpIfNotSmi(value, &is_not_smi);
  // If Smi, convert to Int32.
  __ SmiToInt32(value);
  // Clamp.
  __ CompareInt32AndJumpIf(value, 0, kLessThanEqual, &min);
  __ CompareInt32AndJumpIf(value, 255, kGreaterThanEqual, &max);
  __ Jump(&done);
  __ bind(&is_not_smi);
  // Check if HeapNumber, deopt otherwise.
  __ CompareMapWithRootAndEmitEagerDeoptIf(value, RootIndex::kHeapNumberMap,
                                           scratch, kNotEqual,
                                           DeoptimizeReason::kNotANumber, this);
  // If heap number, get double value.
  __ LoadHeapNumberValue(double_value, value);
  // Clamp.
  __ ToUint8Clamped(value, double_value, &min, &max, &done);
  __ bind(&min);
  __ Move(result_reg, 0);
  __ Jump(&done, Label::Distance::kNear);
  __ bind(&max);
  __ Move(result_reg, 255);
  __ bind(&done);
}

void StoreFixedArrayElementWithWriteBarrier::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(index_input());
  UseRegister(value_input());
  RequireSpecificTemporary(WriteBarrierDescriptor::ObjectRegister());
  RequireSpecificTemporary(WriteBarrierDescriptor::SlotAddressRegister());
}
void StoreFixedArrayElementWithWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  Register elements = ToRegister(elements_input());
  Register index = ToRegister(index_input());
  Register value = ToRegister(value_input());
  __ StoreFixedArrayElementWithWriteBarrier(elements, index, value,
                                            register_snapshot());
}

void StoreFixedArrayElementNoWriteBarrier::SetValueLocationConstraints() {
  UseRegister(elements_input());
  UseRegister(index_input());
  UseRegister(value_input());
}
void StoreFixedArrayElementNoWriteBarrier::GenerateCode(
    MaglevAssembler* masm, const ProcessingState& state) {
  Register elements = ToRegister(elements_input());
  Register index = ToRegister(index_input());
  Register value = ToRegister(value_input());
  __ StoreFixedArrayElementNoWriteBarrier(elements, index, value);
}

// ---
// Arch agnostic call nodes
// ---

int Call::MaxCallStackArgs() const { return num_args(); }
void Call::SetValueLocationConstraints() {
  using D = CallTrampolineDescriptor;
  UseFixed(function(), D::GetRegisterParameter(D::kFunction));
  UseAny(arg(0));
  for (int i = 1; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}

void Call::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
  __ PushReverse(args());

  uint32_t arg_count = num_args();
  if (target_type_ == TargetType::kAny) {
    switch (receiver_mode_) {
      case ConvertReceiverMode::kNullOrUndefined:
        __ CallBuiltin<Builtin::kCall_ReceiverIsNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kNotNullOrUndefined:
        __ CallBuiltin<Builtin::kCall_ReceiverIsNotNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kAny:
        __ CallBuiltin<Builtin::kCall_ReceiverIsAny>(context(), function(),
                                                     arg_count);
        break;
    }
  } else {
    DCHECK_EQ(TargetType::kJSFunction, target_type_);
    switch (receiver_mode_) {
      case ConvertReceiverMode::kNullOrUndefined:
        __ CallBuiltin<Builtin::kCallFunction_ReceiverIsNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kNotNullOrUndefined:
        __ CallBuiltin<Builtin::kCallFunction_ReceiverIsNotNullOrUndefined>(
            context(), function(), arg_count);
        break;
      case ConvertReceiverMode::kAny:
        __ CallBuiltin<Builtin::kCallFunction_ReceiverIsAny>(
            context(), function(), arg_count);
        break;
    }
  }

  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallForwardVarargs::MaxCallStackArgs() const { return num_args(); }
void CallForwardVarargs::SetValueLocationConstraints() {
  using D = CallTrampolineDescriptor;
  UseFixed(function(), D::GetRegisterParameter(D::kFunction));
  UseAny(arg(0));
  for (int i = 1; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
}

void CallForwardVarargs::GenerateCode(MaglevAssembler* masm,
                                      const ProcessingState& state) {
  __ PushReverse(args());
  switch (target_type_) {
    case Call::TargetType::kJSFunction:
      __ CallBuiltin<Builtin::kCallFunctionForwardVarargs>(
          context(), function(), num_args(), start_index_);
      break;
    case Call::TargetType::kAny:
      __ CallBuiltin<Builtin::kCallForwardVarargs>(context(), function(),
                                                   num_args(), start_index_);
      break;
  }
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallSelf::MaxCallStackArgs() const {
  int actual_parameter_count = num_args() + 1;
  return std::max(expected_parameter_count_, actual_parameter_count);
}
void CallSelf::SetValueLocationConstraints() {
  UseAny(receiver());
  for (int i = 0; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(closure(), kJavaScriptCallTargetRegister);
  UseFixed(new_target(), kJavaScriptCallNewTargetRegister);
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
  set_temporaries_needed(1);
}

void CallSelf::GenerateCode(MaglevAssembler* masm,
                            const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  int actual_parameter_count = num_args() + 1;
  if (actual_parameter_count < expected_parameter_count_) {
    int number_of_undefineds =
        expected_parameter_count_ - actual_parameter_count;
    __ LoadRoot(scratch, RootIndex::kUndefinedValue);
    __ PushReverse(receiver(), args(),
                   RepeatValue(scratch, number_of_undefineds));
  } else {
    __ PushReverse(receiver(), args());
  }
  DCHECK_EQ(kContextRegister, ToRegister(context()));
  DCHECK_EQ(kJavaScriptCallTargetRegister, ToRegister(closure()));
  __ Move(kJavaScriptCallArgCountRegister, actual_parameter_count);
  __ CallSelf();
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallKnownJSFunction::MaxCallStackArgs() const {
  int actual_parameter_count = num_args() + 1;
  return std::max(expected_parameter_count_, actual_parameter_count);
}
void CallKnownJSFunction::SetValueLocationConstraints() {
  UseAny(receiver());
  for (int i = 0; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(closure(), kJavaScriptCallTargetRegister);
  UseFixed(new_target(), kJavaScriptCallNewTargetRegister);
  UseFixed(context(), kContextRegister);
  DefineAsFixed(this, kReturnRegister0);
  set_temporaries_needed(1);
}

void CallKnownJSFunction::GenerateCode(MaglevAssembler* masm,
                                       const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  Register scratch = temps.Acquire();
  int actual_parameter_count = num_args() + 1;
  if (actual_parameter_count < expected_parameter_count_) {
    int number_of_undefineds =
        expected_parameter_count_ - actual_parameter_count;
    __ LoadRoot(scratch, RootIndex::kUndefinedValue);
    __ PushReverse(receiver(), args(),
                   RepeatValue(scratch, number_of_undefineds));
  } else {
    __ PushReverse(receiver(), args());
  }
  // From here on, we're going to do a call, so all registers are valid temps,
  // except for the ones we're going to write. This is needed in case one of the
  // helper methods below wants to use a temp and one of these is in the temp
  // list (in particular, this can happen on arm64 where cp is a temp register
  // by default).
  temps.SetAvailable(MaglevAssembler::GetAllocatableRegisters() -
                     RegList{kContextRegister, kJavaScriptCallCodeStartRegister,
                             kJavaScriptCallTargetRegister,
                             kJavaScriptCallNewTargetRegister,
                             kJavaScriptCallArgCountRegister});
  DCHECK_EQ(kContextRegister, ToRegister(context()));
  DCHECK_EQ(kJavaScriptCallTargetRegister, ToRegister(closure()));
  __ Move(kJavaScriptCallArgCountRegister, actual_parameter_count);
  if (shared_function_info().HasBuiltinId()) {
    Builtin builtin = shared_function_info().builtin_id();

    // This SBXCHECK is a defense-in-depth measure to ensure that we always
    // generate valid calls here (with matching signatures).
    SBXCHECK_EQ(expected_parameter_count_,
                Builtins::GetFormalParameterCount(builtin));

    __ CallBuiltin(builtin);
  } else {
#if V8_ENABLE_LEAPTIERING
    __ CallJSDispatchEntry(dispatch_handle_, expected_parameter_count_);
#else
    __ CallJSFunction(kJavaScriptCallTargetRegister, expected_parameter_count_);
#endif
  }
  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
}

int CallKnownApiFunction::MaxCallStackArgs() const {
  int actual_parameter_count = num_args() + 1;
  return actual_parameter_count;
}

void CallKnownApiFunction::SetValueLocationConstraints() {
  if (api_holder_.has_value()) {
    UseAny(receiver());
  } else {
    // This is an "Api holder is receiver" case, ask register allocator to put
    // receiver value into the right register.
    UseFixed(receiver(), CallApiCallbackOptimizedDescriptor::HolderRegister());
  }
  for (int i = 0; i < num_args(); i++) {
    UseAny(arg(i));
  }
  UseFixed(context(), kContextRegister);

  DefineAsFixed(this, kReturnRegister0);

  if (inline_builtin()) {
    set_temporaries_needed(2);
  }
}

void CallKnownApiFunction::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
  MaglevAssembler::TemporaryRegisterScope temps(masm);
  __ PushReverse(receiver(), args());

  // From here on, we're going to do a call, so all registers are valid temps,
  // except for the ones we're going to write. This is needed in case one of the
  // helper methods below wants to use a temp and one of these is in the temp
  // list (in particular, this can happen on arm64 where cp is a temp register
  // by default).
  temps.SetAvailable(
      kAllocatableGeneralRegisters -
      RegList{
          kContextRegister,
          CallApiCallbackOptimizedDescriptor::HolderRegister(),
          CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister(),
          CallApiCallbackOptimizedDescriptor::FunctionTemplateInfoRegister(),
          CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister()});
  DCHECK_EQ(kContextRegister, ToRegister(context()));

  if (inline_builtin()) {
[Step 13: Duration 35.42 seconds| Input tokens: 375,357 | Output tokens: 8,966]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 14 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_file' with arguments: {'file_path': 'maglev/maglev-graph-builder.cc', 'section': 2}                                                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && wc -l '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Command: cd /Users/tarun/v8/v8/src && sed -n '3001,6000p' '/Users/tarun/v8/v8/src/maglev/maglev-graph-builder.cc'
Observations: Showing section 2/5 (lines 3001-6000) of 'maglev/maglev-graph-builder.cc':
      ValueNode* right = GetAccumulator();
      BuildCheckJSReceiver(left);
      BuildCheckJSReceiver(right);
      SetAccumulator(BuildTaggedEqual(left, right));
      return;
    }
  }

  BuildGenericBinaryOperationNode<kOperation>();
}

void MaglevGraphBuilder::VisitLdar() {
  MoveNodeBetweenRegisters(iterator_.GetRegisterOperand(0),
                           interpreter::Register::virtual_accumulator());
}

void MaglevGraphBuilder::VisitLdaZero() { SetAccumulator(GetSmiConstant(0)); }
void MaglevGraphBuilder::VisitLdaSmi() {
  int constant = iterator_.GetImmediateOperand(0);
  SetAccumulator(GetSmiConstant(constant));
}
void MaglevGraphBuilder::VisitLdaUndefined() {
  SetAccumulator(GetRootConstant(RootIndex::kUndefinedValue));
}
void MaglevGraphBuilder::VisitLdaNull() {
  SetAccumulator(GetRootConstant(RootIndex::kNullValue));
}
void MaglevGraphBuilder::VisitLdaTheHole() {
  SetAccumulator(GetRootConstant(RootIndex::kTheHoleValue));
}
void MaglevGraphBuilder::VisitLdaTrue() {
  SetAccumulator(GetRootConstant(RootIndex::kTrueValue));
}
void MaglevGraphBuilder::VisitLdaFalse() {
  SetAccumulator(GetRootConstant(RootIndex::kFalseValue));
}
void MaglevGraphBuilder::VisitLdaConstant() {
  SetAccumulator(GetConstant(GetRefOperand<HeapObject>(0)));
}

bool MaglevGraphBuilder::TrySpecializeLoadContextSlotToFunctionContext(
    ValueNode* context, int slot_index, ContextSlotMutability slot_mutability) {
  DCHECK(compilation_unit_->info()->specialize_to_function_context());

  if (slot_mutability == kMutable) return false;

  auto constant = TryGetConstant(context);
  if (!constant) return false;

  compiler::ContextRef context_ref = constant.value().AsContext();

  compiler::OptionalObjectRef maybe_slot_value =
      context_ref.get(broker(), slot_index);
  if (!maybe_slot_value.has_value()) return false;

  compiler::ObjectRef slot_value = maybe_slot_value.value();
  if (slot_value.IsHeapObject()) {
    // Even though the context slot is immutable, the context might have escaped
    // before the function to which it belongs has initialized the slot.  We
    // must be conservative and check if the value in the slot is currently the
    // hole or undefined. Only if it is neither of these, can we be sure that it
    // won't change anymore.
    //
    // See also: JSContextSpecialization::ReduceJSLoadContext.
    compiler::OddballType oddball_type =
        slot_value.AsHeapObject().map(broker()).oddball_type(broker());
    if (oddball_type == compiler::OddballType::kUndefined ||
        slot_value.IsTheHole()) {
      return false;
    }
  }

  // Fold the load of the immutable slot.

  SetAccumulator(GetConstant(slot_value));
  return true;
}

ValueNode* MaglevGraphBuilder::TrySpecializeLoadScriptContextSlot(
    ValueNode* context_node, int index) {
  if (!context_node->Is<Constant>()) return {};
  compiler::ContextRef context =
      context_node->Cast<Constant>()->ref().AsContext();
  DCHECK(context.object()->IsScriptContext());
  auto maybe_property = context.object()->GetScriptContextSideProperty(index);
  auto property =
      maybe_property ? maybe_property.value() : ContextSidePropertyCell::kOther;
  int offset = Context::OffsetOfElementAt(index);
  switch (property) {
    case ContextSidePropertyCell::kConst: {
      compiler::OptionalObjectRef constant = context.get(broker(), index);
      if (!constant.has_value()) {
        return BuildLoadTaggedField<LoadTaggedFieldForContextSlot>(context_node,
                                                                   offset);
      }
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context, index, property, broker());
      return GetConstant(*constant);
    }
    case ContextSidePropertyCell::kSmi: {
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context, index, property, broker());
      ValueNode* value = BuildLoadTaggedField<LoadTaggedFieldForContextSlot>(
          context_node, offset);
      EnsureType(value, NodeType::kSmi);
      return value;
    }
    case ContextSidePropertyCell::kMutableInt32:
      if (auto mutable_heap_number = context.get(broker(), index)) {
        if (!mutable_heap_number->IsHeapNumber()) {
          // TODO(victorgomes): In case the tag is out of date by now we could
          // retry this reduction.
          break;
        }
        broker()->dependencies()->DependOnScriptContextSlotProperty(
            context, index, property, broker());
        return AddNewNode<LoadInt32>(
            {GetConstant(*mutable_heap_number)},
            static_cast<int>(offsetof(HeapNumber, value_)));
      }
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context, index, property, broker());
      return AddNewNode<LoadHeapInt32>({context_node}, offset);
    case ContextSidePropertyCell::kMutableHeapNumber:
      if (auto mutable_heap_number = context.get(broker(), index)) {
        if (!mutable_heap_number->IsHeapNumber()) {
          // TODO(victorgomes): In case the tag is out of date by now we could
          // retry this reduction.
          break;
        }
        broker()->dependencies()->DependOnScriptContextSlotProperty(
            context, index, property, broker());
        return AddNewNode<LoadFloat64>(
            {GetConstant(*mutable_heap_number)},
            static_cast<int>(offsetof(HeapNumber, value_)));
      }
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context, index, property, broker());
      return AddNewNode<LoadDoubleField>({context_node}, offset);
    case ContextSidePropertyCell::kOther:
      break;
    default:
      UNREACHABLE();
  }
  return BuildLoadTaggedField<LoadTaggedFieldForContextSlot>(context_node,
                                                             offset);
}

ValueNode* MaglevGraphBuilder::LoadAndCacheContextSlot(
    ValueNode* context, int index, ContextSlotMutability slot_mutability,
    ContextKind context_kind) {
  int offset = Context::OffsetOfElementAt(index);
  ValueNode*& cached_value =
      slot_mutability == kMutable
          ? known_node_aspects().loaded_context_slots|{context, offset}]
          : known_node_aspects().loaded_context_constants|{context, offset}];
  if (cached_value) {
    if (v8_flags.trace_maglev_graph_building) {
      std::cout << "  * Reusing cached context slot "
                << PrintNodeLabel(graph_labeller(), context) << "|" << offset
                << "]: " << PrintNode(graph_labeller(), cached_value)
                << std::endl;
    }
    return cached_value;
  }
  known_node_aspects().UpdateMayHaveAliasingContexts(context);
  if (context_kind == ContextKind::kScriptContext) {
    cached_value = TrySpecializeLoadScriptContextSlot(context, index);
    if (cached_value) return cached_value;
    return cached_value =
               BuildLoadTaggedField<LoadTaggedFieldForScriptContextSlot>(
                   context, index);
  }
  return cached_value = BuildLoadTaggedField<LoadTaggedFieldForContextSlot>(
             context, offset);
}

bool MaglevGraphBuilder::ContextMayAlias(
    ValueNode* context, compiler::OptionalScopeInfoRef scope_info) {
  // Distinguishing contexts by their scope info only works if scope infos are
  // guaranteed to be unique.
  if (!scope_info.has_value()) {
    return true;
  }
  auto other = graph()->TryGetScopeInfo(context, broker());
  if (!other.has_value()) {
    return true;
  }
  return scope_info->equals(*other);
}

ReduceResult MaglevGraphBuilder::TrySpecializeStoreScriptContextSlot(
    ValueNode* context, int index, ValueNode* value, Node** store) {
  DCHECK_NOT_NULL(store);
  DCHECK(v8_flags.script_context_mutable_heap_number ||
         v8_flags.const_tracking_let);
  if (!context->Is<Constant>()) {
    *store = AddNewNode<StoreScriptContextSlotWithWriteBarrier>(
        {context, value}, index);
    return ReduceResult::Done();
  }

  compiler::ContextRef context_ref =
      context->Cast<Constant>()->ref().AsContext();
  DCHECK(context_ref.object()->IsScriptContext());
  auto maybe_property =
      context_ref.object()->GetScriptContextSideProperty(index);
  if (!maybe_property) {
    *store = AddNewNode<StoreScriptContextSlotWithWriteBarrier>(
        {context, value}, index);
    return ReduceResult::Done();
  }
  auto property = maybe_property.value();
  int offset = Context::OffsetOfElementAt(index);
  if (property == ContextSidePropertyCell::kConst) {
    compiler::OptionalObjectRef constant = context_ref.get(broker(), index);
    if (!constant.has_value() ||
        (constant->IsString() && !constant->IsInternalizedString())) {
      *store = AddNewNode<StoreScriptContextSlotWithWriteBarrier>(
          {context, value}, index);
      return ReduceResult::Done();
    }
    broker()->dependencies()->DependOnScriptContextSlotProperty(
        context_ref, index, property, broker());
    return BuildCheckValue(value, *constant);
  }

  if (!v8_flags.script_context_mutable_heap_number) {
    *store = BuildStoreTaggedField(context, value, offset,
                                   StoreTaggedMode::kDefault);
    return ReduceResult::Done();
  }

  switch (property) {
    case ContextSidePropertyCell::kConst:
      UNREACHABLE();
    case ContextSidePropertyCell::kSmi:
      RETURN_IF_ABORT(BuildCheckSmi(value));
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context_ref, index, property, broker());
      *store = BuildStoreTaggedField(context, value, offset,
                                     StoreTaggedMode::kDefault);
      break;
    case ContextSidePropertyCell::kMutableInt32:
      EnsureInt32(value, true);
      if (auto mutable_heap_number = context_ref.get(broker(), index)) {
        if (!mutable_heap_number->IsHeapNumber()) {
          // TODO(victorgomes): In case the tag is out of date by now we could
          // retry this reduction.
          return ReduceResult::Fail();
        }
        *store = AddNewNode<StoreInt32>(
            {GetConstant(*mutable_heap_number), value},
            static_cast<int>(offsetof(HeapNumber, value_)));
      } else {
        *store = AddNewNode<StoreHeapInt32>({context, value}, offset);
      }
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context_ref, index, property, broker());
      break;
    case ContextSidePropertyCell::kMutableHeapNumber:
      BuildCheckNumber(value);
      if (auto mutable_heap_number = context_ref.get(broker(), index)) {
        if (!mutable_heap_number->IsHeapNumber()) {
          // TODO(victorgomes): In case the tag is out of date by now we could
          // retry this reduction.
          return ReduceResult::Fail();
        }
        *store = AddNewNode<StoreFloat64>(
            {GetConstant(*mutable_heap_number), value},
            static_cast<int>(offsetof(HeapNumber, value_)));
      } else {
        *store = AddNewNode<StoreDoubleField>({context, value}, offset);
      }
      broker()->dependencies()->DependOnScriptContextSlotProperty(
          context_ref, index, property, broker());
      break;
    case ContextSidePropertyCell::kOther:
      *store = BuildStoreTaggedField(context, value, offset,
                                     StoreTaggedMode::kDefault);
      break;
    default:
      UNREACHABLE();
  }
  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::StoreAndCacheContextSlot(
    ValueNode* context, int index, ValueNode* value, ContextKind context_kind) {
  int offset = Context::OffsetOfElementAt(index);
  DCHECK_EQ(
      known_node_aspects().loaded_context_constants.count({context, offset}),
      0);

  Node* store = nullptr;
  if ((v8_flags.script_context_mutable_heap_number ||
       v8_flags.const_tracking_let) &&
      context_kind == ContextKind::kScriptContext) {
    ReduceResult result =
        TrySpecializeStoreScriptContextSlot(context, index, value, &store);
    RETURN_IF_ABORT(result);
    if (!store) {
      // If we didn't need to emit any store, there is nothing to cache.
      DCHECK(result.IsDone());
      return result;
    }
  } else {
    store = BuildStoreTaggedField(context, value, offset,
                                  StoreTaggedMode::kDefault);
  }

  if (v8_flags.trace_maglev_graph_building) {
    std::cout << "  * Recording context slot store "
              << PrintNodeLabel(graph_labeller(), context) << "|" << offset
              << "]: " << PrintNode(graph_labeller(), value) << std::endl;
  }
  known_node_aspects().UpdateMayHaveAliasingContexts(context);
  KnownNodeAspects::LoadedContextSlots& loaded_context_slots =
      known_node_aspects().loaded_context_slots;
  if (known_node_aspects().may_have_aliasing_contexts() ==
      KnownNodeAspects::ContextSlotLoadsAlias::Yes) {
    compiler::OptionalScopeInfoRef scope_info =
        graph()->TryGetScopeInfo(context, broker());
    for (auto& cache : loaded_context_slots) {
      if (std::get<int>(cache.first) == offset &&
          std::get<ValueNode*>(cache.first) != context) {
        if (ContextMayAlias(std::get<ValueNode*>(cache.first), scope_info) &&
            cache.second != value) {
          if (v8_flags.trace_maglev_graph_building) {
            std::cout << "  * Clearing probably aliasing value "
                      << PrintNodeLabel(graph_labeller(),
                                        std::get<ValueNode*>(cache.first))
                      << "|" << offset
                      << "]: " << PrintNode(graph_labeller(), value)
                      << std::endl;
          }
          cache.second = nullptr;
          if (is_loop_effect_tracking()) {
            loop_effects_->context_slot_written.insert(cache.first);
            loop_effects_->may_have_aliasing_contexts = true;
          }
        }
      }
    }
  }
  KnownNodeAspects::LoadedContextSlotsKey key{context, offset};
  auto updated = loaded_context_slots.emplace(key, value);
  if (updated.second) {
    if (is_loop_effect_tracking()) {
      loop_effects_->context_slot_written.insert(key);
    }
    unobserved_context_slot_stores_|key] = store;
  } else {
    if (updated.first->second != value) {
      updated.first->second = value;
      if (is_loop_effect_tracking()) {
        loop_effects_->context_slot_written.insert(key);
      }
    }
    if (known_node_aspects().may_have_aliasing_contexts() !=
        KnownNodeAspects::ContextSlotLoadsAlias::Yes) {
      auto last_store = unobserved_context_slot_stores_.find(key);
      if (last_store != unobserved_context_slot_stores_.end()) {
        MarkNodeDead(last_store->second);
        last_store->second = store;
      } else {
        unobserved_context_slot_stores_|key] = store;
      }
    }
  }
  return ReduceResult::Done();
}

void MaglevGraphBuilder::BuildLoadContextSlot(
    ValueNode* context, size_t depth, int slot_index,
    ContextSlotMutability slot_mutability, ContextKind context_kind) {
  context = GetContextAtDepth(context, depth);
  if (compilation_unit_->info()->specialize_to_function_context() &&
      TrySpecializeLoadContextSlotToFunctionContext(context, slot_index,
                                                    slot_mutability)) {
    return;  // Our work here is done.
  }

  // Always load the slot here as if it were mutable. Immutable slots have a
  // narrow range of mutability if the context escapes before the slot is
  // initialized, so we can't safely assume that the load can be cached in case
  // it's a load before initialization (e.g. var a = a + 42).
  current_interpreter_frame_.set_accumulator(
      LoadAndCacheContextSlot(context, slot_index, kMutable, context_kind));
}

ReduceResult MaglevGraphBuilder::BuildStoreContextSlot(
    ValueNode* context, size_t depth, int slot_index, ValueNode* value,
    ContextKind context_kind) {
  context = GetContextAtDepth(context, depth);
  return StoreAndCacheContextSlot(context, slot_index, value, context_kind);
}

void MaglevGraphBuilder::VisitLdaContextSlot() {
  ValueNode* context = LoadRegister(0);
  int slot_index = iterator_.GetIndexOperand(1);
  size_t depth = iterator_.GetUnsignedImmediateOperand(2);
  BuildLoadContextSlot(context, depth, slot_index, kMutable,
                       ContextKind::kDefault);
}
void MaglevGraphBuilder::VisitLdaScriptContextSlot() {
  ValueNode* context = LoadRegister(0);
  int slot_index = iterator_.GetIndexOperand(1);
  size_t depth = iterator_.GetUnsignedImmediateOperand(2);
  BuildLoadContextSlot(context, depth, slot_index, kMutable,
                       ContextKind::kScriptContext);
}
void MaglevGraphBuilder::VisitLdaImmutableContextSlot() {
  ValueNode* context = LoadRegister(0);
  int slot_index = iterator_.GetIndexOperand(1);
  size_t depth = iterator_.GetUnsignedImmediateOperand(2);
  BuildLoadContextSlot(context, depth, slot_index, kImmutable,
                       ContextKind::kDefault);
}
void MaglevGraphBuilder::VisitLdaCurrentContextSlot() {
  ValueNode* context = GetContext();
  int slot_index = iterator_.GetIndexOperand(0);
  BuildLoadContextSlot(context, 0, slot_index, kMutable, ContextKind::kDefault);
}
void MaglevGraphBuilder::VisitLdaCurrentScriptContextSlot() {
  ValueNode* context = GetContext();
  int slot_index = iterator_.GetIndexOperand(0);
  BuildLoadContextSlot(context, 0, slot_index, kMutable,
                       ContextKind::kScriptContext);
}
void MaglevGraphBuilder::VisitLdaImmutableCurrentContextSlot() {
  ValueNode* context = GetContext();
  int slot_index = iterator_.GetIndexOperand(0);
  BuildLoadContextSlot(context, 0, slot_index, kImmutable,
                       ContextKind::kDefault);
}

void MaglevGraphBuilder::VisitStaContextSlot() {
  ValueNode* context = LoadRegister(0);
  int slot_index = iterator_.GetIndexOperand(1);
  size_t depth = iterator_.GetUnsignedImmediateOperand(2);
  RETURN_VOID_IF_DONE(BuildStoreContextSlot(
      context, depth, slot_index, GetAccumulator(), ContextKind::kDefault));
}
void MaglevGraphBuilder::VisitStaCurrentContextSlot() {
  ValueNode* context = GetContext();
  int slot_index = iterator_.GetIndexOperand(0);
  RETURN_VOID_IF_DONE(BuildStoreContextSlot(
      context, 0, slot_index, GetAccumulator(), ContextKind::kDefault));
}

void MaglevGraphBuilder::VisitStaScriptContextSlot() {
  ValueNode* context = LoadRegister(0);
  int slot_index = iterator_.GetIndexOperand(1);
  size_t depth = iterator_.GetUnsignedImmediateOperand(2);
  RETURN_VOID_IF_DONE(BuildStoreContextSlot(context, depth, slot_index,
                                            GetAccumulator(),
                                            ContextKind::kScriptContext));
}

void MaglevGraphBuilder::VisitStaCurrentScriptContextSlot() {
  ValueNode* context = GetContext();
  int slot_index = iterator_.GetIndexOperand(0);
  RETURN_VOID_IF_DONE(BuildStoreContextSlot(
      context, 0, slot_index, GetAccumulator(), ContextKind::kScriptContext));
}

void MaglevGraphBuilder::VisitStar() {
  MoveNodeBetweenRegisters(interpreter::Register::virtual_accumulator(),
                           iterator_.GetRegisterOperand(0));
}
#define SHORT_STAR_VISITOR(Name, ...)                                          \
  void MaglevGraphBuilder::Visit##Name() {                                     \
    MoveNodeBetweenRegisters(                                                  \
        interpreter::Register::virtual_accumulator(),                          \
        interpreter::Register::FromShortStar(interpreter::Bytecode::k##Name)); \
  }
SHORT_STAR_BYTECODE_LIST(SHORT_STAR_VISITOR)
#undef SHORT_STAR_VISITOR

void MaglevGraphBuilder::VisitMov() {
  MoveNodeBetweenRegisters(iterator_.GetRegisterOperand(0),
                           iterator_.GetRegisterOperand(1));
}

void MaglevGraphBuilder::VisitPushContext() {
  MoveNodeBetweenRegisters(interpreter::Register::current_context(),
                           iterator_.GetRegisterOperand(0));
  SetContext(GetAccumulator());
}

void MaglevGraphBuilder::VisitPopContext() { SetContext(LoadRegister(0)); }

ValueNode* MaglevGraphBuilder::BuildTaggedEqual(ValueNode* lhs,
                                                ValueNode* rhs) {
  ValueNode* tagged_lhs = GetTaggedValue(lhs);
  ValueNode* tagged_rhs = GetTaggedValue(rhs);
  if (tagged_lhs == tagged_rhs) {
    return GetBooleanConstant(true);
  }
  if (HaveDifferentTypes(tagged_lhs, tagged_rhs)) {
    return GetBooleanConstant(false);
  }
  // TODO(victorgomes): We could retrieve the HeapObjectRef in Constant and
  // compare them.
  if (IsConstantNode(tagged_lhs->opcode()) && !tagged_lhs->Is<Constant>() &&
      tagged_lhs->opcode() == tagged_rhs->opcode()) {
    // Constants nodes are canonicalized, except for the node holding
    // HeapObjectRef, so equal constants should have been handled above.
    return GetBooleanConstant(false);
  }
  return AddNewNode<TaggedEqual>({tagged_lhs, tagged_rhs});
}

ValueNode* MaglevGraphBuilder::BuildTaggedEqual(ValueNode* lhs,
                                                RootIndex rhs_index) {
  return BuildTaggedEqual(lhs, GetRootConstant(rhs_index));
}

void MaglevGraphBuilder::VisitTestReferenceEqual() {
  ValueNode* lhs = LoadRegister(0);
  ValueNode* rhs = GetAccumulator();
  SetAccumulator(BuildTaggedEqual(lhs, rhs));
}

ValueNode* MaglevGraphBuilder::BuildTestUndetectable(ValueNode* value) {
  if (value->properties().value_representation() ==
      ValueRepresentation::kHoleyFloat64) {
    return AddNewNode<HoleyFloat64IsHole>({value});
  } else if (value->properties().value_representation() !=
             ValueRepresentation::kTagged) {
    return GetBooleanConstant(false);
  }

  if (auto maybe_constant = TryGetConstant(value)) {
    auto map = maybe_constant.value().map(broker());
    return GetBooleanConstant(map.is_undetectable());
  }

  NodeType node_type;
  if (CheckType(value, NodeType::kSmi, &node_type)) {
    return GetBooleanConstant(false);
  }

  auto it = known_node_aspects().FindInfo(value);
  if (known_node_aspects().IsValid(it)) {
    NodeInfo& info = it->second;
    if (info.possible_maps_are_known()) {
      // We check if all the possible maps have the same undetectable bit value.
      DCHECK_GT(info.possible_maps().size(), 0);
      bool first_is_undetectable = info.possible_maps()|0].is_undetectable();
      bool all_the_same_value =
          std::all_of(info.possible_maps().begin(), info.possible_maps().end(),
                      |first_is_undetectable](compiler::MapRef map) {
                        bool is_undetectable = map.is_undetectable();
                        return (first_is_undetectable && is_undetectable) ||
                               (!first_is_undetectable && !is_undetectable);
                      });
      if (all_the_same_value) {
        return GetBooleanConstant(first_is_undetectable);
      }
    }
  }

  enum CheckType type = GetCheckType(node_type);
  return AddNewNode<TestUndetectable>({value}, type);
}

MaglevGraphBuilder::BranchResult MaglevGraphBuilder::BuildBranchIfUndetectable(
    BranchBuilder& builder, ValueNode* value) {
  ValueNode* result = BuildTestUndetectable(value);
  switch (result->opcode()) {
    case Opcode::kRootConstant:
      switch (result->Cast<RootConstant>()->index()) {
        case RootIndex::kTrueValue:
        case RootIndex::kUndefinedValue:
        case RootIndex::kNullValue:
          return builder.AlwaysTrue();
        default:
          return builder.AlwaysFalse();
      }
    case Opcode::kHoleyFloat64IsHole:
      return BuildBranchIfFloat64IsHole(
          builder, result->Cast<HoleyFloat64IsHole>()->input().node());
    case Opcode::kTestUndetectable:
      return builder.Build<BranchIfUndetectable>(
          {result->Cast<TestUndetectable>()->value().node()},
          result->Cast<TestUndetectable>()->check_type());
    default:
      UNREACHABLE();
  }
}

void MaglevGraphBuilder::VisitTestUndetectable() {
  SetAccumulator(BuildTestUndetectable(GetAccumulator()));
}

void MaglevGraphBuilder::VisitTestNull() {
  ValueNode* value = GetAccumulator();
  SetAccumulator(BuildTaggedEqual(value, RootIndex::kNullValue));
}

void MaglevGraphBuilder::VisitTestUndefined() {
  ValueNode* value = GetAccumulator();
  SetAccumulator(BuildTaggedEqual(value, RootIndex::kUndefinedValue));
}

template <typename Function>
ReduceResult MaglevGraphBuilder::TryReduceTypeOf(ValueNode* value,
                                                 const Function& GetResult) {
  // Similar to TF, we assume that all undetectable receiver objects are also
  // callables. In practice, there is only one: document.all.
  switch (CheckTypes(
      value, {NodeType::kBoolean, NodeType::kNumber, NodeType::kString,
              NodeType::kSymbol, NodeType::kCallable, NodeType::kJSArray})) {
    case NodeType::kBoolean:
      return GetResult(TypeOfLiteralFlag::kBoolean, RootIndex::kboolean_string);
    case NodeType::kNumber:
      return GetResult(TypeOfLiteralFlag::kNumber, RootIndex::knumber_string);
    case NodeType::kString:
      return GetResult(TypeOfLiteralFlag::kString, RootIndex::kstring_string);
    case NodeType::kSymbol:
      return GetResult(TypeOfLiteralFlag::kSymbol, RootIndex::ksymbol_string);
    case NodeType::kCallable:
      return Select(
          |&](auto& builder) {
            return BuildBranchIfUndetectable(builder, value);
          },
          |&] {
            return GetResult(TypeOfLiteralFlag::kUndefined,
                             RootIndex::kundefined_string);
          },
          |&] {
            return GetResult(TypeOfLiteralFlag::kFunction,
                             RootIndex::kfunction_string);
          });
    case NodeType::kJSArray:
      // TODO(victorgomes): Track JSReceiver, non-callable types in Maglev.
      return GetResult(TypeOfLiteralFlag::kObject, RootIndex::kobject_string);
    default:
      break;
  }

  if (IsNullValue(value)) {
    return GetResult(TypeOfLiteralFlag::kObject, RootIndex::kobject_string);
  }
  if (IsUndefinedValue(value)) {
    return GetResult(TypeOfLiteralFlag::kUndefined,
                     RootIndex::kundefined_string);
  }

  return ReduceResult::Fail();
}

ReduceResult MaglevGraphBuilder::TryReduceTypeOf(ValueNode* value) {
  return TryReduceTypeOf(value,
                         |&](TypeOfLiteralFlag _, RootIndex idx) -> ValueNode* {
                           return GetRootConstant(idx);
                         });
}

void MaglevGraphBuilder::VisitTestTypeOf() {
  // TODO(v8:7700): Add a branch version of TestTypeOf that does not need to
  // materialise the boolean value.
  TypeOfLiteralFlag literal =
      interpreter::TestTypeOfFlags::Decode(GetFlag8Operand(0));
  if (literal == TypeOfLiteralFlag::kOther) {
    SetAccumulator(GetRootConstant(RootIndex::kFalseValue));
    return;
  }
  ValueNode* value = GetAccumulator();
  auto GetResult = |&](TypeOfLiteralFlag expected, RootIndex _) {
    return GetRootConstant(literal == expected ? RootIndex::kTrueValue
                                               : RootIndex::kFalseValue);
  };
  PROCESS_AND_RETURN_IF_DONE(TryReduceTypeOf(value, GetResult), SetAccumulator);

  SetAccumulator(AddNewNode<TestTypeOf>({value}, literal));
}

ReduceResult MaglevGraphBuilder::TryBuildScriptContextStore(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  DCHECK(global_access_feedback.IsScriptContextSlot());
  if (global_access_feedback.immutable()) {
    return ReduceResult::Fail();
  }
  auto script_context = GetConstant(global_access_feedback.script_context());
  return StoreAndCacheContextSlot(
      script_context, global_access_feedback.slot_index(), GetAccumulator(),
      ContextKind::kScriptContext);
}

ReduceResult MaglevGraphBuilder::TryBuildPropertyCellStore(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  DCHECK(global_access_feedback.IsPropertyCell());

  compiler::PropertyCellRef property_cell =
      global_access_feedback.property_cell();
  if (!property_cell.Cache(broker())) return ReduceResult::Fail();

  compiler::ObjectRef property_cell_value = property_cell.value(broker());
  if (property_cell_value.IsPropertyCellHole()) {
    // The property cell is no longer valid.
    return EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess);
  }

  PropertyDetails property_details = property_cell.property_details();
  DCHECK_EQ(PropertyKind::kData, property_details.kind());

  if (property_details.IsReadOnly()) {
    // Don't even bother trying to lower stores to read-only data
    // properties.
    // TODO(neis): We could generate code that checks if the new value
    // equals the old one and then does nothing or deopts, respectively.
    return ReduceResult::Fail();
  }

  switch (property_details.cell_type()) {
    case PropertyCellType::kUndefined:
      return ReduceResult::Fail();
    case PropertyCellType::kConstant: {
      // TODO(victorgomes): Support non-internalized string.
      if (property_cell_value.IsString() &&
          !property_cell_value.IsInternalizedString()) {
        return ReduceResult::Fail();
      }
      // Record a code dependency on the cell, and just deoptimize if the new
      // value doesn't match the previous value stored inside the cell.
      broker()->dependencies()->DependOnGlobalProperty(property_cell);
      ValueNode* value = GetAccumulator();
      return BuildCheckValue(value, property_cell_value);
    }
    case PropertyCellType::kConstantType: {
      // We rely on stability further below.
      if (property_cell_value.IsHeapObject() &&
          !property_cell_value.AsHeapObject().map(broker()).is_stable()) {
        return ReduceResult::Fail();
      }
      // Record a code dependency on the cell, and just deoptimize if the new
      // value's type doesn't match the type of the previous value in the cell.
      broker()->dependencies()->DependOnGlobalProperty(property_cell);
      ValueNode* value = GetAccumulator();
      if (property_cell_value.IsHeapObject()) {
        compiler::MapRef property_cell_value_map =
            property_cell_value.AsHeapObject().map(broker());
        broker()->dependencies()->DependOnStableMap(property_cell_value_map);
        BuildCheckHeapObject(value);
        RETURN_IF_ABORT(
            BuildCheckMaps(value, base::VectorOf({property_cell_value_map})));
      } else {
        RETURN_IF_ABORT(GetSmiValue(value));
      }
      ValueNode* property_cell_node = GetConstant(property_cell.AsHeapObject());
      BuildStoreTaggedField(property_cell_node, value,
                            PropertyCell::kValueOffset,
                            StoreTaggedMode::kDefault);
      break;
    }
    case PropertyCellType::kMutable: {
      // Record a code dependency on the cell, and just deoptimize if the
      // property ever becomes read-only.
      broker()->dependencies()->DependOnGlobalProperty(property_cell);
      ValueNode* property_cell_node = GetConstant(property_cell.AsHeapObject());
      BuildStoreTaggedField(property_cell_node, GetAccumulator(),
                            PropertyCell::kValueOffset,
                            StoreTaggedMode::kDefault);
      break;
    }
    case PropertyCellType::kInTransition:
      UNREACHABLE();
  }
  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::TryBuildScriptContextConstantLoad(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  DCHECK(global_access_feedback.IsScriptContextSlot());
  if (!global_access_feedback.immutable()) return ReduceResult::Fail();
  compiler::OptionalObjectRef maybe_slot_value =
      global_access_feedback.script_context().get(
          broker(), global_access_feedback.slot_index());
  if (!maybe_slot_value) return ReduceResult::Fail();
  return GetConstant(maybe_slot_value.value());
}

ReduceResult MaglevGraphBuilder::TryBuildScriptContextLoad(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  DCHECK(global_access_feedback.IsScriptContextSlot());
  RETURN_IF_DONE(TryBuildScriptContextConstantLoad(global_access_feedback));
  auto script_context = GetConstant(global_access_feedback.script_context());
  ContextSlotMutability mutability =
      global_access_feedback.immutable() ? kImmutable : kMutable;
  return LoadAndCacheContextSlot(script_context,
                                 global_access_feedback.slot_index(),
                                 mutability, ContextKind::kScriptContext);
}

ReduceResult MaglevGraphBuilder::TryBuildPropertyCellLoad(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  // TODO(leszeks): A bunch of this is copied from
  // js-native-context-specialization.cc -- I wonder if we can unify it
  // somehow.
  DCHECK(global_access_feedback.IsPropertyCell());

  compiler::PropertyCellRef property_cell =
      global_access_feedback.property_cell();
  if (!property_cell.Cache(broker())) return ReduceResult::Fail();

  compiler::ObjectRef property_cell_value = property_cell.value(broker());
  if (property_cell_value.IsPropertyCellHole()) {
    // The property cell is no longer valid.
    return EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForGenericNamedAccess);
  }

  PropertyDetails property_details = property_cell.property_details();
  PropertyCellType property_cell_type = property_details.cell_type();
  DCHECK_EQ(PropertyKind::kData, property_details.kind());

  if (!property_details.IsConfigurable() && property_details.IsReadOnly()) {
    return GetConstant(property_cell_value);
  }

  // Record a code dependency on the cell if we can benefit from the
  // additional feedback, or the global property is configurable (i.e.
  // can be deleted or reconfigured to an accessor property).
  if (property_cell_type != PropertyCellType::kMutable ||
      property_details.IsConfigurable()) {
    broker()->dependencies()->DependOnGlobalProperty(property_cell);
  }

  // Load from constant/undefined global property can be constant-folded.
  if (property_cell_type == PropertyCellType::kConstant ||
      property_cell_type == PropertyCellType::kUndefined) {
    return GetConstant(property_cell_value);
  }

  ValueNode* property_cell_node = GetConstant(property_cell.AsHeapObject());
  return BuildLoadTaggedField(property_cell_node, PropertyCell::kValueOffset);
}

ReduceResult MaglevGraphBuilder::TryBuildGlobalStore(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  if (global_access_feedback.IsScriptContextSlot()) {
    return TryBuildScriptContextStore(global_access_feedback);
  } else if (global_access_feedback.IsPropertyCell()) {
    return TryBuildPropertyCellStore(global_access_feedback);
  } else {
    DCHECK(global_access_feedback.IsMegamorphic());
    return ReduceResult::Fail();
  }
}

ReduceResult MaglevGraphBuilder::TryBuildGlobalLoad(
    const compiler::GlobalAccessFeedback& global_access_feedback) {
  if (global_access_feedback.IsScriptContextSlot()) {
    return TryBuildScriptContextLoad(global_access_feedback);
  } else if (global_access_feedback.IsPropertyCell()) {
    return TryBuildPropertyCellLoad(global_access_feedback);
  } else {
    DCHECK(global_access_feedback.IsMegamorphic());
    return ReduceResult::Fail();
  }
}

void MaglevGraphBuilder::VisitLdaGlobal() {
  // LdaGlobal <name_index> <slot>

  static const int kNameOperandIndex = 0;
  static const int kSlotOperandIndex = 1;

  compiler::NameRef name = GetRefOperand<Name>(kNameOperandIndex);
  FeedbackSlot slot = GetSlotOperand(kSlotOperandIndex);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  BuildLoadGlobal(name, feedback_source, TypeofMode::kNotInside);
}

void MaglevGraphBuilder::VisitLdaGlobalInsideTypeof() {
  // LdaGlobalInsideTypeof <name_index> <slot>

  static const int kNameOperandIndex = 0;
  static const int kSlotOperandIndex = 1;

  compiler::NameRef name = GetRefOperand<Name>(kNameOperandIndex);
  FeedbackSlot slot = GetSlotOperand(kSlotOperandIndex);
  compiler::FeedbackSource feedback_source{feedback(), slot};
  BuildLoadGlobal(name, feedback_source, TypeofMode::kInside);
}

void MaglevGraphBuilder::VisitStaGlobal() {
  // StaGlobal <name_index> <slot>
  FeedbackSlot slot = GetSlotOperand(1);
  compiler::FeedbackSource feedback_source{feedback(), slot};

  const compiler::ProcessedFeedback& access_feedback =
      broker()->GetFeedbackForGlobalAccess(feedback_source);

  if (access_feedback.IsInsufficient()) {
    RETURN_VOID_ON_ABORT(EmitUnconditionalDeopt(
        DeoptimizeReason::kInsufficientTypeFeedbackForGenericGlobalAccess));
  }

  const compiler::GlobalAccessFeedback& global_access_feedback =
      access_feedback.AsGlobalAccess();
  RETURN_VOID_IF_DONE(TryBuildGlobalStore(global_access_feedback));

  ValueNode* value = GetAccumulator();
  compiler::NameRef name = GetRefOperand<Name>(0);
  ValueNode* context = GetContext();
  AddNewNode<StoreGlobal>({context, value}, name, feedback_source);
}

void MaglevGraphBuilder::VisitLdaLookupSlot() {
  // LdaLookupSlot <name_index>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  SetAccumulator(BuildCallRuntime(Runtime::kLoadLookupSlot, {name}).value());
}

void MaglevGraphBuilder::VisitLdaLookupContextSlot() {
  // LdaLookupContextSlot <name_index> <feedback_slot> <depth>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  ValueNode* slot = GetTaggedIndexConstant(iterator_.GetIndexOperand(1));
  ValueNode* depth =
      GetTaggedIndexConstant(iterator_.GetUnsignedImmediateOperand(2));
  SetAccumulator(
      BuildCallBuiltin<Builtin::kLookupContextTrampoline>({name, depth, slot}));
}

void MaglevGraphBuilder::VisitLdaLookupScriptContextSlot() {
  // LdaLookupContextSlot <name_index> <feedback_slot> <depth>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  ValueNode* slot = GetTaggedIndexConstant(iterator_.GetIndexOperand(1));
  ValueNode* depth =
      GetTaggedIndexConstant(iterator_.GetUnsignedImmediateOperand(2));
  SetAccumulator(BuildCallBuiltin<Builtin::kLookupScriptContextTrampoline>(
      {name, depth, slot}));
}

bool MaglevGraphBuilder::CheckContextExtensions(size_t depth) {
  compiler::OptionalScopeInfoRef maybe_scope_info =
      graph()->TryGetScopeInfo(GetContext(), broker());
  if (!maybe_scope_info.has_value()) return false;
  compiler::ScopeInfoRef scope_info = maybe_scope_info.value();
  for (uint32_t d = 0; d < depth; d++) {
    CHECK_NE(scope_info.scope_type(), ScopeType::SCRIPT_SCOPE);
    CHECK_NE(scope_info.scope_type(), ScopeType::REPL_MODE_SCOPE);
    if (scope_info.HasContextExtensionSlot() &&
        !broker()->dependencies()->DependOnEmptyContextExtension(scope_info)) {
      // Using EmptyContextExtension dependency is not possible for this
      // scope_info, so generate dynamic checks.
      ValueNode* context = GetContextAtDepth(GetContext(), d);
      // Only support known contexts so that we can check that there's no
      // extension at compile time. Otherwise we could end up in a deopt loop
      // once we do get an extension.
      compiler::OptionalHeapObjectRef maybe_ref = TryGetConstant(context);
      if (!maybe_ref) return false;
      compiler::ContextRef context_ref = maybe_ref.value().AsContext();
      compiler::OptionalObjectRef extension_ref =
          context_ref.get(broker(), Context::EXTENSION_INDEX);
      // The extension may be concurrently installed while we're checking the
      // context, in which case it may still be uninitialized. This still
      // means an extension is about to appear, so we should block this
      // optimization.
      if (!extension_ref) return false;
      if (!extension_ref->IsUndefined()) return false;
      ValueNode* extension = LoadAndCacheContextSlot(
          context, Context::EXTENSION_INDEX, kMutable, ContextKind::kDefault);
      AddNewNode<CheckValue>({extension}, broker()->undefined_value());
    }
    CHECK_IMPLIES(!scope_info.HasOuterScopeInfo(), d + 1 == depth);
    if (scope_info.HasOuterScopeInfo()) {
      scope_info = scope_info.OuterScopeInfo(broker());
    }
  }
  return true;
}

void MaglevGraphBuilder::VisitLdaLookupGlobalSlot() {
  // LdaLookupGlobalSlot <name_index> <feedback_slot> <depth>
  compiler::NameRef name = GetRefOperand<Name>(0);
  if (CheckContextExtensions(iterator_.GetUnsignedImmediateOperand(2))) {
    FeedbackSlot slot = GetSlotOperand(1);
    compiler::FeedbackSource feedback_source{feedback(), slot};
    BuildLoadGlobal(name, feedback_source, TypeofMode::kNotInside);
  } else {
    ValueNode* name = GetConstant(GetRefOperand<Name>(0));
    ValueNode* slot = GetTaggedIndexConstant(iterator_.GetIndexOperand(1));
    ValueNode* depth =
        GetTaggedIndexConstant(iterator_.GetUnsignedImmediateOperand(2));
    ValueNode* result;
    if (parent_) {
      ValueNode* vector = GetConstant(feedback());
      result = BuildCallBuiltin<Builtin::kLookupGlobalIC>(
          {name, depth, slot, vector});
    } else {
      result = BuildCallBuiltin<Builtin::kLookupGlobalICTrampoline>(
          {name, depth, slot});
    }
    SetAccumulator(result);
  }
}

void MaglevGraphBuilder::VisitLdaLookupSlotInsideTypeof() {
  // LdaLookupSlotInsideTypeof <name_index>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  SetAccumulator(
      BuildCallRuntime(Runtime::kLoadLookupSlotInsideTypeof, {name}).value());
}

void MaglevGraphBuilder::VisitLdaLookupContextSlotInsideTypeof() {
  // LdaLookupContextSlotInsideTypeof <name_index> <context_slot> <depth>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  ValueNode* slot = GetTaggedIndexConstant(iterator_.GetIndexOperand(1));
  ValueNode* depth =
      GetTaggedIndexConstant(iterator_.GetUnsignedImmediateOperand(2));
  SetAccumulator(
      BuildCallBuiltin<Builtin::kLookupContextInsideTypeofTrampoline>(
          {name, depth, slot}));
}

void MaglevGraphBuilder::VisitLdaLookupScriptContextSlotInsideTypeof() {
  // LdaLookupContextSlotInsideTypeof <name_index> <context_slot> <depth>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  ValueNode* slot = GetTaggedIndexConstant(iterator_.GetIndexOperand(1));
  ValueNode* depth =
      GetTaggedIndexConstant(iterator_.GetUnsignedImmediateOperand(2));
  SetAccumulator(
      BuildCallBuiltin<Builtin::kLookupScriptContextInsideTypeofTrampoline>(
          {name, depth, slot}));
}

void MaglevGraphBuilder::VisitLdaLookupGlobalSlotInsideTypeof() {
  // LdaLookupGlobalSlotInsideTypeof <name_index> <context_slot> <depth>
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  ValueNode* slot = GetTaggedIndexConstant(iterator_.GetIndexOperand(1));
  ValueNode* depth =
      GetTaggedIndexConstant(iterator_.GetUnsignedImmediateOperand(2));
  ValueNode* result;
  if (parent_) {
    ValueNode* vector = GetConstant(feedback());
    result = BuildCallBuiltin<Builtin::kLookupGlobalICInsideTypeof>(
        {name, depth, slot, vector});
  } else {
    result = BuildCallBuiltin<Builtin::kLookupGlobalICInsideTypeofTrampoline>(
        {name, depth, slot});
  }
  SetAccumulator(result);
}

namespace {
Runtime::FunctionId StaLookupSlotFunction(uint8_t sta_lookup_slot_flags) {
  using Flags = interpreter::StoreLookupSlotFlags;
  switch (Flags::GetLanguageMode(sta_lookup_slot_flags)) {
    case LanguageMode::kStrict:
      return Runtime::kStoreLookupSlot_Strict;
    case LanguageMode::kSloppy:
      if (Flags::IsLookupHoistingMode(sta_lookup_slot_flags)) {
        return Runtime::kStoreLookupSlot_SloppyHoisting;
      } else {
        return Runtime::kStoreLookupSlot_Sloppy;
      }
  }
}
}  // namespace

void MaglevGraphBuilder::VisitStaLookupSlot() {
  // StaLookupSlot <name_index> <flags>
  ValueNode* value = GetAccumulator();
  ValueNode* name = GetConstant(GetRefOperand<Name>(0));
  uint32_t flags = GetFlag8Operand(1);
  EscapeContext();
  SetAccumulator(
      BuildCallRuntime(StaLookupSlotFunction(flags), {name, value}).value());
}

NodeType StaticTypeForNode(compiler::JSHeapBroker* broker,
                           LocalIsolate* isolate, ValueNode* node) {
  switch (node->properties().value_representation()) {
    case ValueRepresentation::kInt32:
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kFloat64:
      return NodeType::kNumber;
    case ValueRepresentation::kHoleyFloat64:
      return NodeType::kNumberOrOddball;
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kTagged:
      break;
  }
  switch (node->opcode()) {
    case Opcode::kPhi:
      return node->Cast<Phi>()->type();
    case Opcode::kCheckedSmiTagInt32:
    case Opcode::kCheckedSmiTagUint32:
    case Opcode::kCheckedSmiTagFloat64:
    case Opcode::kUnsafeSmiTagInt32:
    case Opcode::kUnsafeSmiTagUint32:
    case Opcode::kSmiConstant:
      return NodeType::kSmi;
    case Opcode::kInt32ToNumber:
    case Opcode::kUint32ToNumber:
    case Opcode::kFloat64ToTagged:
      return NodeType::kNumber;
    case Opcode::kHoleyFloat64ToTagged:
      return NodeType::kNumberOrOddball;
    case Opcode::kAllocationBlock:
    case Opcode::kInlinedAllocation:
      return StaticTypeForMap(node->Cast<InlinedAllocation>()->object()->map(),
                              broker);
    case Opcode::kRootConstant: {
      RootConstant* constant = node->Cast<RootConstant>();
      switch (constant->index()) {
        case RootIndex::kTrueValue:
        case RootIndex::kFalseValue:
          return NodeType::kBoolean;
        case RootIndex::kUndefinedValue:
        case RootIndex::kNullValue:
          return NodeType::kOddball;
        default:
          break;
      }
      ||fallthrough]];
    }
    case Opcode::kConstant: {
      compiler::HeapObjectRef ref =
          MaglevGraphBuilder::TryGetConstant(broker, isolate, node).value();
      return StaticTypeForConstant(broker, ref);
    }
    case Opcode::kToNumberOrNumeric:
      if (node->Cast<ToNumberOrNumeric>()->mode() ==
          Object::Conversion::kToNumber) {
        return NodeType::kNumber;
      }
      // TODO(verwaest): Check what we need here.
      return NodeType::kUnknown;
    case Opcode::kToString:
    case Opcode::kNumberToString:
    case Opcode::kStringConcat:
    case Opcode::kStringWrapperConcat:
      return NodeType::kString;
    case Opcode::kCheckedInternalizedString:
      return NodeType::kInternalizedString;
    case Opcode::kToObject:
    case Opcode::kCreateObjectLiteral:
    case Opcode::kCreateShallowObjectLiteral:
      return NodeType::kJSReceiver;
    case Opcode::kCreateArrayLiteral:
    case Opcode::kCreateShallowArrayLiteral:
      return NodeType::kJSArray;
    case Opcode::kToName:
      return NodeType::kName;
    case Opcode::kFastCreateClosure:
    case Opcode::kCreateClosure:
      return NodeType::kCallable;
    case Opcode::kInt32Compare:
    case Opcode::kFloat64Compare:
    case Opcode::kGenericEqual:
    case Opcode::kGenericStrictEqual:
    case Opcode::kGenericLessThan:
    case Opcode::kGenericLessThanOrEqual:
    case Opcode::kGenericGreaterThan:
    case Opcode::kGenericGreaterThanOrEqual:
    case Opcode::kLogicalNot:
    case Opcode::kStringEqual:
    case Opcode::kTaggedEqual:
    case Opcode::kTaggedNotEqual:
    case Opcode::kTestInstanceOf:
    case Opcode::kTestTypeOf:
    case Opcode::kTestUndetectable:
    case Opcode::kToBoolean:
    case Opcode::kToBooleanLogicalNot:
      return NodeType::kBoolean;
      // Not value nodes:
#define GENERATE_CASE(Name) case Opcode::k##Name:
      CONTROL_NODE_LIST(GENERATE_CASE)
      NON_VALUE_NODE_LIST(GENERATE_CASE)
#undef GENERATE_CASE
      UNREACHABLE();
    case Opcode::kTransitionElementsKind:
    // Unsorted value nodes. TODO(maglev): See which of these should return
    // something else than kUnknown.
    case Opcode::kIdentity:
    case Opcode::kArgumentsElements:
    case Opcode::kArgumentsLength:
    case Opcode::kRestLength:
    case Opcode::kCall:
    case Opcode::kCallBuiltin:
    case Opcode::kCallCPPBuiltin:
    case Opcode::kCallForwardVarargs:
    case Opcode::kCallRuntime:
    case Opcode::kCallWithArrayLike:
    case Opcode::kCallWithSpread:
    case Opcode::kCallKnownApiFunction:
    case Opcode::kCallKnownJSFunction:
    case Opcode::kCallSelf:
    case Opcode::kConstruct:
    case Opcode::kCheckConstructResult:
    case Opcode::kCheckDerivedConstructResult:
    case Opcode::kConstructWithSpread:
    case Opcode::kConvertReceiver:
    case Opcode::kConvertHoleToUndefined:
    case Opcode::kCreateFunctionContext:
    case Opcode::kCreateRegExpLiteral:
    case Opcode::kDeleteProperty:
    case Opcode::kEnsureWritableFastElements:
    case Opcode::kExtendPropertiesBackingStore:
    case Opcode::kForInPrepare:
    case Opcode::kForInNext:
    case Opcode::kGeneratorRestoreRegister:
    case Opcode::kGetIterator:
    case Opcode::kGetSecondReturnedValue:
    case Opcode::kGetTemplateObject:
    case Opcode::kHasInPrototypeChain:
    case Opcode::kInitialValue:
    case Opcode::kLoadTaggedField:
    case Opcode::kLoadTaggedFieldForProperty:
    case Opcode::kLoadTaggedFieldForContextSlot:
    case Opcode::kLoadTaggedFieldForScriptContextSlot:
    case Opcode::kLoadDoubleField:
    case Opcode::kLoadFloat64:
    case Opcode::kLoadInt32:
    case Opcode::kLoadHeapInt32:
    case Opcode::kLoadTaggedFieldByFieldIndex:
    case Opcode::kLoadFixedArrayElement:
    case Opcode::kLoadFixedDoubleArrayElement:
    case Opcode::kLoadHoleyFixedDoubleArrayElement:
    case Opcode::kLoadHoleyFixedDoubleArrayElementCheckedNotHole:
    case Opcode::kLoadSignedIntDataViewElement:
    case Opcode::kLoadDoubleDataViewElement:
    case Opcode::kLoadTypedArrayLength:
    case Opcode::kLoadSignedIntTypedArrayElement:
    case Opcode::kLoadUnsignedIntTypedArrayElement:
    case Opcode::kLoadDoubleTypedArrayElement:
    case Opcode::kLoadEnumCacheLength:
    case Opcode::kLoadGlobal:
    case Opcode::kLoadNamedGeneric:
    case Opcode::kLoadNamedFromSuperGeneric:
    case Opcode::kMaybeGrowFastElements:
    case Opcode::kMigrateMapIfNeeded:
    case Opcode::kSetNamedGeneric:
    case Opcode::kDefineNamedOwnGeneric:
    case Opcode::kStoreInArrayLiteralGeneric:
    case Opcode::kStoreGlobal:
    case Opcode::kGetKeyedGeneric:
    case Opcode::kSetKeyedGeneric:
    case Opcode::kDefineKeyedOwnGeneric:
    case Opcode::kRegisterInput:
    case Opcode::kCheckedSmiSizedInt32:
    case Opcode::kCheckedSmiUntag:
    case Opcode::kUnsafeSmiUntag:
    case Opcode::kCheckedObjectToIndex:
    case Opcode::kCheckedTruncateNumberOrOddballToInt32:
    case Opcode::kCheckedInt32ToUint32:
    case Opcode::kUnsafeInt32ToUint32:
    case Opcode::kCheckedUint32ToInt32:
    case Opcode::kChangeInt32ToFloat64:
    case Opcode::kChangeUint32ToFloat64:
    case Opcode::kCheckedTruncateFloat64ToInt32:
    case Opcode::kCheckedTruncateFloat64ToUint32:
    case Opcode::kTruncateNumberOrOddballToInt32:
    case Opcode::kCheckedNumberToInt32:
    case Opcode::kTruncateUint32ToInt32:
    case Opcode::kTruncateFloat64ToInt32:
    case Opcode::kUnsafeTruncateUint32ToInt32:
    case Opcode::kUnsafeTruncateFloat64ToInt32:
    case Opcode::kInt32ToUint8Clamped:
    case Opcode::kUint32ToUint8Clamped:
    case Opcode::kFloat64ToUint8Clamped:
    case Opcode::kCheckedNumberToUint8Clamped:
    case Opcode::kFloat64ToHeapNumberForField:
    case Opcode::kCheckedNumberOrOddballToFloat64:
    case Opcode::kUncheckedNumberOrOddballToFloat64:
    case Opcode::kCheckedNumberOrOddballToHoleyFloat64:
    case Opcode::kCheckedHoleyFloat64ToFloat64:
    case Opcode::kHoleyFloat64ToMaybeNanFloat64:
    case Opcode::kHoleyFloat64IsHole:
    case Opcode::kSetPendingMessage:
    case Opcode::kStringAt:
    case Opcode::kStringLength:
    case Opcode::kAllocateElementsArray:
    case Opcode::kUpdateJSArrayLength:
    case Opcode::kVirtualObject:
    case Opcode::kGetContinuationPreservedEmbedderData:
    case Opcode::kExternalConstant:
    case Opcode::kFloat64Constant:
    case Opcode::kInt32Constant:
    case Opcode::kUint32Constant:
    case Opcode::kTaggedIndexConstant:
    case Opcode::kTrustedConstant:
    case Opcode::kInt32AbsWithOverflow:
    case Opcode::kInt32AddWithOverflow:
    case Opcode::kInt32SubtractWithOverflow:
    case Opcode::kInt32MultiplyWithOverflow:
    case Opcode::kInt32DivideWithOverflow:
    case Opcode::kInt32ModulusWithOverflow:
    case Opcode::kInt32BitwiseAnd:
    case Opcode::kInt32BitwiseOr:
    case Opcode::kInt32BitwiseXor:
    case Opcode::kInt32ShiftLeft:
    case Opcode::kInt32ShiftRight:
    case Opcode::kInt32ShiftRightLogical:
    case Opcode::kInt32BitwiseNot:
    case Opcode::kInt32NegateWithOverflow:
    case Opcode::kInt32IncrementWithOverflow:
    case Opcode::kInt32DecrementWithOverflow:
    case Opcode::kInt32ToBoolean:
    case Opcode::kFloat64Abs:
    case Opcode::kFloat64Add:
    case Opcode::kFloat64Subtract:
    case Opcode::kFloat64Multiply:
    case Opcode::kFloat64Divide:
    case Opcode::kFloat64Exponentiate:
    case Opcode::kFloat64Modulus:
    case Opcode::kFloat64Negate:
    case Opcode::kFloat64Round:
    case Opcode::kFloat64ToBoolean:
    case Opcode::kFloat64Ieee754Unary:
    case Opcode::kCheckedSmiIncrement:
    case Opcode::kCheckedSmiDecrement:
    case Opcode::kGenericAdd:
    case Opcode::kGenericSubtract:
    case Opcode::kGenericMultiply:
    case Opcode::kGenericDivide:
    case Opcode::kGenericModulus:
    case Opcode::kGenericExponentiate:
    case Opcode::kGenericBitwiseAnd:
    case Opcode::kGenericBitwiseOr:
    case Opcode::kGenericBitwiseXor:
    case Opcode::kGenericShiftLeft:
    case Opcode::kGenericShiftRight:
    case Opcode::kGenericShiftRightLogical:
    case Opcode::kGenericBitwiseNot:
    case Opcode::kGenericNegate:
    case Opcode::kGenericIncrement:
    case Opcode::kGenericDecrement:
    case Opcode::kBuiltinStringFromCharCode:
    case Opcode::kBuiltinStringPrototypeCharCodeOrCodePointAt:
      return NodeType::kUnknown;
  }
}

bool MaglevGraphBuilder::CheckStaticType(ValueNode* node, NodeType type,
                                         NodeType* current_type) {
  NodeType static_type = StaticTypeForNode(broker(), local_isolate(), node);
  if (current_type) *current_type = static_type;
  return NodeTypeIs(static_type, type);
}

bool MaglevGraphBuilder::EnsureType(ValueNode* node, NodeType type,
                                    NodeType* old_type) {
  if (CheckStaticType(node, type, old_type)) return true;
  NodeInfo* known_info = GetOrCreateInfoFor(node);
  if (old_type) *old_type = known_info->type();
  if (NodeTypeIs(known_info->type(), type)) return true;
  known_info->CombineType(type);
  if (auto phi = node->TryCast<Phi>()) {
    known_info->CombineType(phi->type());
  }
  return false;
}

template <typename Function>
bool MaglevGraphBuilder::EnsureType(ValueNode* node, NodeType type,
                                    Function ensure_new_type) {
  if (CheckStaticType(node, type)) return true;
  NodeInfo* known_info = GetOrCreateInfoFor(node);
  if (NodeTypeIs(known_info->type(), type)) return true;
  ensure_new_type(known_info->type());
  known_info->CombineType(type);
  return false;
}

void MaglevGraphBuilder::SetKnownValue(ValueNode* node, compiler::ObjectRef ref,
                                       NodeType new_node_type) {
  DCHECK(!node->Is<Constant>());
  DCHECK(!node->Is<RootConstant>());
  NodeInfo* known_info = GetOrCreateInfoFor(node);
  // ref type should be compatible with type.
  DCHECK(NodeTypeIs(StaticTypeForConstant(broker(), ref), new_node_type));
  if (ref.IsHeapObject()) {
    DCHECK(IsInstanceOfNodeType(ref.AsHeapObject().map(broker()),
                                known_info->type(), broker()));
  } else {
    DCHECK(!NodeTypeIs(known_info->type(), NodeType::kAnyHeapObject));
  }
  known_info->CombineType(new_node_type);
  known_info->alternative().set_checked_value(GetConstant(ref));
}

NodeType MaglevGraphBuilder::CheckTypes(ValueNode* node,
                                        std::initializer_list<NodeType> types) {
  auto it = known_node_aspects().FindInfo(node);
  bool has_kna = known_node_aspects().IsValid(it);
  for (NodeType type : types) {
    if (CheckStaticType(node, type)) return type;
    if (has_kna) {
      if (NodeTypeIs(it->second.type(), type)) return type;
    }
  }
  return NodeType::kUnknown;
}

bool MaglevGraphBuilder::CheckType(ValueNode* node, NodeType type,
                                   NodeType* current_type) {
  if (CheckStaticType(node, type, current_type)) return true;
  auto it = known_node_aspects().FindInfo(node);
  if (!known_node_aspects().IsValid(it)) return false;
  if (current_type) *current_type = it->second.type();
  return NodeTypeIs(it->second.type(), type);
}

NodeType MaglevGraphBuilder::GetType(ValueNode* node) {
  auto it = known_node_aspects().FindInfo(node);
  if (!known_node_aspects().IsValid(it)) {
    return StaticTypeForNode(broker(), local_isolate(), node);
  }
  NodeType actual_type = it->second.type();
  if (auto phi = node->TryCast<Phi>()) {
    actual_type = CombineType(actual_type, phi->type());
  }
#ifdef DEBUG
  NodeType static_type = StaticTypeForNode(broker(), local_isolate(), node);
  if (!NodeTypeIs(actual_type, static_type)) {
    // In case we needed a numerical alternative of a smi value, the type
    // must generalize. In all other cases the node info type should reflect the
    // actual type.
    DCHECK(static_type == NodeType::kSmi && actual_type == NodeType::kNumber &&
           !known_node_aspects().TryGetInfoFor(node)->alternative().has_none());
  }
#endif  // DEBUG
  return actual_type;
}

bool MaglevGraphBuilder::HaveDifferentTypes(ValueNode* lhs, ValueNode* rhs) {
  return HasDifferentType(lhs, GetType(rhs));
}

// Note: this is conservative, ie, returns true if {lhs} cannot be {rhs}.
// It might return false even if {lhs} is not {rhs}.
bool MaglevGraphBuilder::HasDifferentType(ValueNode* lhs, NodeType rhs_type) {
  NodeType lhs_type = GetType(lhs);
  if (lhs_type == NodeType::kUnknown || rhs_type == NodeType::kUnknown) {
    return false;
  }
  return IntersectType(lhs_type, rhs_type) == NodeType::kUnknown;
}

bool MaglevGraphBuilder::MayBeNullOrUndefined(ValueNode* node) {
  NodeType static_type = StaticTypeForNode(broker(), local_isolate(), node);
  if (!NodeTypeMayBeNullOrUndefined(static_type)) return false;
  auto it = known_node_aspects().FindInfo(node);
  if (!known_node_aspects().IsValid(it)) return true;
  return NodeTypeMayBeNullOrUndefined(it->second.type());
}

ValueNode* MaglevGraphBuilder::BuildSmiUntag(ValueNode* node) {
  if (EnsureType(node, NodeType::kSmi)) {
    if (SmiValuesAre31Bits()) {
      if (auto phi = node->TryCast<Phi>()) {
        phi->SetUseRequires31BitValue();
      }
    }
    return AddNewNode<UnsafeSmiUntag>({node});
  } else {
    return AddNewNode<CheckedSmiUntag>({node});
  }
}

ValueNode* MaglevGraphBuilder::BuildNumberOrOddballToFloat64(
    ValueNode* node, NodeType allowed_input_type,
    TaggedToFloat64ConversionType conversion_type) {
  NodeType old_type;
  if (EnsureType(node, allowed_input_type, &old_type)) {
    if (old_type == NodeType::kSmi) {
      ValueNode* untagged_smi = BuildSmiUntag(node);
      return AddNewNode<ChangeInt32ToFloat64>({untagged_smi});
    }
    return AddNewNode<UncheckedNumberOrOddballToFloat64>({node},
                                                         conversion_type);
  } else {
    return AddNewNode<CheckedNumberOrOddballToFloat64>({node}, conversion_type);
  }
}

ReduceResult MaglevGraphBuilder::BuildCheckSmi(ValueNode* object,
                                               bool elidable) {
  if (CheckStaticType(object, NodeType::kSmi)) return object;
  if (CheckType(object, NodeType::kAnyHeapObject)) {
    return EmitUnconditionalDeopt(DeoptimizeReason::kNotASmi);
  }
  if (EnsureType(object, NodeType::kSmi) && elidable) return object;
  switch (object->value_representation()) {
    case ValueRepresentation::kInt32:
      if (!SmiValuesAre32Bits()) {
        AddNewNode<CheckInt32IsSmi>({object});
      }
      break;
    case ValueRepresentation::kUint32:
      AddNewNode<CheckUint32IsSmi>({object});
      break;
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      AddNewNode<CheckHoleyFloat64IsSmi>({object});
      break;
    case ValueRepresentation::kTagged:
      AddNewNode<CheckSmi>({object});
      break;
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
  }
  return object;
}

void MaglevGraphBuilder::BuildCheckHeapObject(ValueNode* object) {
  if (EnsureType(object, NodeType::kAnyHeapObject)) return;
  AddNewNode<CheckHeapObject>({object});
}

void MaglevGraphBuilder::BuildCheckString(ValueNode* object) {
  NodeType known_type;
  if (EnsureType(object, NodeType::kString, &known_type)) return;
  AddNewNode<CheckString>({object}, GetCheckType(known_type));
}

void MaglevGraphBuilder::BuildCheckStringOrStringWrapper(ValueNode* object) {
  NodeType known_type;
  if (EnsureType(object, NodeType::kStringOrStringWrapper, &known_type)) return;
  AddNewNode<CheckStringOrStringWrapper>({object}, GetCheckType(known_type));
}

void MaglevGraphBuilder::BuildCheckNumber(ValueNode* object) {
  if (EnsureType(object, NodeType::kNumber)) return;
  AddNewNode<CheckNumber>({object}, Object::Conversion::kToNumber);
}

void MaglevGraphBuilder::BuildCheckSymbol(ValueNode* object) {
  NodeType known_type;
  if (EnsureType(object, NodeType::kSymbol, &known_type)) return;
  AddNewNode<CheckSymbol>({object}, GetCheckType(known_type));
}

void MaglevGraphBuilder::BuildCheckJSReceiver(ValueNode* object) {
  NodeType known_type;
  if (EnsureType(object, NodeType::kJSReceiver, &known_type)) return;
  AddNewNode<CheckInstanceType>({object}, GetCheckType(known_type),
                                FIRST_JS_RECEIVER_TYPE, LAST_JS_RECEIVER_TYPE);
}

namespace {

class KnownMapsMerger {
 public:
  explicit KnownMapsMerger(compiler::JSHeapBroker* broker, Zone* zone,
                           base::Vector<const compiler::MapRef> requested_maps)
      : broker_(broker), zone_(zone), requested_maps_(requested_maps) {}

  void IntersectWithKnownNodeAspects(
      ValueNode* object, const KnownNodeAspects& known_node_aspects) {
    auto node_info_it = known_node_aspects.FindInfo(object);
    bool has_node_info = known_node_aspects.IsValid(node_info_it);
    NodeType type =
        has_node_info ? node_info_it->second.type() : NodeType::kUnknown;
    if (has_node_info && node_info_it->second.possible_maps_are_known()) {
      // TODO(v8:7700): Make intersection non-quadratic.
      for (compiler::MapRef possible_map :
           node_info_it->second.possible_maps()) {
        if (std::find(requested_maps_.begin(), requested_maps_.end(),
                      possible_map) != requested_maps_.end()) {
          // No need to add dependencies, we already have them for all known
          // possible maps.
          // Filter maps which are impossible given this objects type. Since we
          // want to prove that an object with map `map` is not an instance of
          // `type`, we cannot use `StaticTypeForMap`, as it only provides an
          // approximation. This filtering is done to avoid creating
          // non-sensical types later (e.g. if we think only a non-string map
          // is possible, after a string check).
          if (IsInstanceOfNodeType(possible_map, type, broker_)) {
            InsertMap(possible_map);
          }
        } else {
          known_maps_are_subset_of_requested_maps_ = false;
        }
      }
      if (intersect_set_.is_empty()) {
        node_type_ = NodeType::kUnknown;
      }
    } else {
      // A missing entry here means the universal set, i.e., we don't know
      // anything about the possible maps of the object. Intersect with the
      // universal set, which means just insert all requested maps.
      known_maps_are_subset_of_requested_maps_ = false;
      existing_known_maps_found_ = false;
      for (compiler::MapRef map : requested_maps_) {
        InsertMap(map);
      }
    }
  }

  void UpdateKnownNodeAspects(ValueNode* object,
                              KnownNodeAspects& known_node_aspects) {
    // Update known maps.
    auto node_info = known_node_aspects.GetOrCreateInfoFor(
        object, broker_, broker_->local_isolate());
    node_info->SetPossibleMaps(intersect_set_, any_map_is_unstable_, node_type_,
                               broker_);
    // Make sure known_node_aspects.any_map_for_any_node_is_unstable is updated
    // in case any_map_is_unstable changed to true for this object -- this can
    // happen if this was an intersection with the universal set which added new
    // possible unstable maps.
    if (any_map_is_unstable_) {
      known_node_aspects.any_map_for_any_node_is_unstable = true;
    }
    // At this point, known_node_aspects.any_map_for_any_node_is_unstable may be
    // true despite there no longer being any unstable maps for any nodes (if
    // this was the only node with unstable maps and this intersection removed
    // those). This is ok, because that's at worst just an overestimate -- we
    // could track whether this node's any_map_is_unstable flipped from true to
    // false, but this is likely overkill.
    // Insert stable map dependencies which weren't inserted yet. This is only
    // needed if our set of known maps was empty and we created it anew based on
    // maps we checked.
    if (!existing_known_maps_found_) {
      for (compiler::MapRef map : intersect_set_) {
        if (map.is_stable()) {
          broker_->dependencies()->DependOnStableMap(map);
        }
      }
    } else {
      // TODO(victorgomes): Add a DCHECK_SLOW that checks if the maps already
      // exist in the CompilationDependencySet.
    }
  }

  bool known_maps_are_subset_of_requested_maps() const {
    return known_maps_are_subset_of_requested_maps_;
  }
  bool emit_check_with_migration() const { return emit_check_with_migration_; }

  const compiler::ZoneRefSet<Map>& intersect_set() const {
    return intersect_set_;
  }

  NodeType node_type() const { return node_type_; }

 private:
  compiler::JSHeapBroker* broker_;
  Zone* zone_;
  base::Vector<const compiler::MapRef> requested_maps_;
  compiler::ZoneRefSet<Map> intersect_set_;
  bool known_maps_are_subset_of_requested_maps_ = true;
  bool existing_known_maps_found_ = true;
  bool emit_check_with_migration_ = false;
  bool any_map_is_unstable_ = false;
  NodeType node_type_ = static_cast<NodeType>(-1);

  Zone* zone() const { return zone_; }

  void InsertMap(compiler::MapRef map) {
    if (map.is_migration_target()) {
      emit_check_with_migration_ = true;
    }
    NodeType new_type = StaticTypeForMap(map, broker_);
    if (new_type == NodeType::kHeapNumber) {
      new_type = IntersectType(new_type, NodeType::kSmi);
    }
    node_type_ = IntersectType(node_type_, new_type);
    if (!map.is_stable()) {
      any_map_is_unstable_ = true;
    }
    intersect_set_.insert(map, zone());
  }
};

}  // namespace

ReduceResult MaglevGraphBuilder::BuildCheckMaps(
    ValueNode* object, base::Vector<const compiler::MapRef> maps,
    std::optional<ValueNode*> map,
    bool has_deprecated_map_without_migration_target) {
  // TODO(verwaest): Support other objects with possible known stable maps as
  // well.
  if (compiler::OptionalHeapObjectRef constant = TryGetConstant(object)) {
    // For constants with stable maps that match one of the desired maps, we
    // don't need to emit a map check, and can use the dependency -- we
    // can't do this for unstable maps because the constant could migrate
    // during compilation.
    compiler::MapRef constant_map = constant.value().map(broker());
    if (std::find(maps.begin(), maps.end(), constant_map) != maps.end()) {
      if (constant_map.is_stable()) {
        broker()->dependencies()->DependOnStableMap(constant_map);
        return ReduceResult::Done();
      }
      // TODO(verwaest): Reduce maps to the constant map.
    } else {
      // TODO(leszeks): Insert an unconditional deopt if the constant map
      // doesn't match the required map.
    }
  }

  NodeInfo* known_info = GetOrCreateInfoFor(object);

  // Calculates if known maps are a subset of maps, their map intersection and
  // whether we should emit check with migration.
  KnownMapsMerger merger(broker(), zone(), maps);
  merger.IntersectWithKnownNodeAspects(object, known_node_aspects());

  // If the known maps are the subset of the maps to check, we are done.
  if (merger.known_maps_are_subset_of_requested_maps()) {
    // The node type of known_info can get out of sync with the possible maps.
    // For instance after merging with an effectively dead branch (i.e., check
    // contradicting all possible maps).
    // TODO(olivf) Try to combine node_info and possible maps and ensure that
    // narrowing the type also clears impossible possible_maps.
    if (!NodeTypeIs(known_info->type(), merger.node_type())) {
      known_info->IntersectType(merger.node_type());
    }
#ifdef DEBUG
    // Double check that, for every possible map, it's one of the maps we'd
    // want to check.
    for (compiler::MapRef map :
         known_node_aspects().TryGetInfoFor(object)->possible_maps()) {
      DCHECK_NE(std::find(maps.begin(), maps.end(), map), maps.end());
    }
#endif
    return ReduceResult::Done();
  }

  if (merger.intersect_set().is_empty()) {
    return EmitUnconditionalDeopt(DeoptimizeReason::kWrongMap);
  }

  // TODO(v8:7700): Check if the {maps} - {known_maps} size is smaller than
  // {maps} \intersect {known_maps}, we can emit CheckNotMaps instead.

  // Emit checks.
  if (merger.emit_check_with_migration()) {
    AddNewNode<CheckMapsWithMigration>({object}, merger.intersect_set(),
                                       GetCheckType(known_info->type()));
  } else if (has_deprecated_map_without_migration_target) {
    AddNewNode<CheckMapsWithMigrationAndDeopt>(
        {object}, merger.intersect_set(), GetCheckType(known_info->type()));
  } else if (map) {
    AddNewNode<CheckMapsWithAlreadyLoadedMap>({object, *map},
                                              merger.intersect_set());
  } else {
    AddNewNode<CheckMaps>({object}, merger.intersect_set(),
                          GetCheckType(known_info->type()));
  }

  merger.UpdateKnownNodeAspects(object, known_node_aspects());
  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::BuildTransitionElementsKindOrCheckMap(
    ValueNode* heap_object, ValueNode* object_map,
    const ZoneVector<compiler::MapRef>& transition_sources,
    compiler::MapRef transition_target) {
  // TODO(marja): Optimizations based on what we know about the intersection of
  // known maps and transition sources or transition target.

  // TransitionElementsKind doesn't happen in cases where we'd need to do
  // CheckMapsWithMigration instead of CheckMaps.
  CHECK(!transition_target.is_migration_target());
  for (const compiler::MapRef transition_source : transition_sources) {
    CHECK(!transition_source.is_migration_target());
  }

  NodeInfo* known_info = GetOrCreateInfoFor(heap_object);

  AddNewNode<TransitionElementsKindOrCheckMap>(
      {heap_object, object_map}, transition_sources, transition_target);
  // After this operation, heap_object's map is transition_target (or we
  // deopted).
  known_info->SetPossibleMaps(
      PossibleMaps{transition_target}, !transition_target.is_stable(),
      StaticTypeForMap(transition_target, broker()), broker());
  DCHECK(transition_target.IsJSReceiverMap());
  if (!transition_target.is_stable()) {
    known_node_aspects().any_map_for_any_node_is_unstable = true;
  } else {
    broker()->dependencies()->DependOnStableMap(transition_target);
  }
  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::BuildCompareMaps(
    ValueNode* heap_object, ValueNode* object_map,
    base::Vector<const compiler::MapRef> maps, MaglevSubGraphBuilder* sub_graph,
    std::optional<MaglevSubGraphBuilder::Label>& if_not_matched) {
  GetOrCreateInfoFor(heap_object);
  KnownMapsMerger merger(broker(), zone(), maps);
  merger.IntersectWithKnownNodeAspects(heap_object, known_node_aspects());

  if (merger.intersect_set().is_empty()) {
    return ReduceResult::DoneWithAbort();
  }

  // TODO(pthier): Support map packing.
  DCHECK(!V8_MAP_PACKING_BOOL);

  // TODO(pthier): Handle map migrations.
  std::optional<MaglevSubGraphBuilder::Label> map_matched;
  const compiler::ZoneRefSet<Map>& relevant_maps = merger.intersect_set();
  if (relevant_maps.size() > 1) {
    map_matched.emplace(sub_graph, static_cast<int>(relevant_maps.size()));
    for (size_t map_index = 1; map_index < relevant_maps.size(); map_index++) {
      sub_graph->GotoIfTrue<BranchIfReferenceEqual>(
          &*map_matched,
          {object_map, GetConstant(relevant_maps.at(map_index))});
    }
  }
  if_not_matched.emplace(sub_graph, 1);
  sub_graph->GotoIfFalse<BranchIfReferenceEqual>(
      &*if_not_matched, {object_map, GetConstant(relevant_maps.at(0))});
  if (map_matched.has_value()) {
    sub_graph->Goto(&*map_matched);
    sub_graph->Bind(&*map_matched);
  }
  merger.UpdateKnownNodeAspects(heap_object, known_node_aspects());
  return ReduceResult::Done();
}

ReduceResult MaglevGraphBuilder::BuildTransitionElementsKindAndCompareMaps(
    ValueNode* heap_object, ValueNode* object_map,
    const ZoneVector<compiler::MapRef>& transition_sources,
    compiler::MapRef transition_target, MaglevSubGraphBuilder* sub_graph,
    std::optional<MaglevSubGraphBuilder::Label>& if_not_matched) {
  DCHECK(!transition_target.is_migration_target());

  NodeInfo* known_info = GetOrCreateInfoFor(heap_object);

  // TODO(pthier): Calculate and use the intersection of known maps with
  // (transition_sources union transition_target).

  ValueNode* new_map = AddNewNode<TransitionElementsKind>(
      {heap_object, object_map}, transition_sources, transition_target);

  // TODO(pthier): Support map packing.
  DCHECK(!V8_MAP_PACKING_BOOL);
  if_not_matched.emplace(sub_graph, 1);
  sub_graph->GotoIfFalse<BranchIfReferenceEqual>(
      &*if_not_matched, {new_map, GetConstant(transition_target)});
  // After the branch, object's map is transition_target.
  DCHECK(transition_target.IsJSReceiverMap());
  known_info->SetPossibleMaps(
      PossibleMaps{transition_target}, !transition_target.is_stable(),
      StaticTypeForMap(transition_target, broker()), broker());
  if (!transition_target.is_stable()) {
    known_node_aspects().any_map_for_any_node_is_unstable = true;
  } else {
    broker()->dependencies()->DependOnStableMap(transition_target);
  }
  return ReduceResult::Done();
}

namespace {
AllocationBlock* GetAllocation(ValueNode* object) {
  if (object->Is<InlinedAllocation>()) {
    object = object->Cast<InlinedAllocation>()->input(0).node();
  }
  if (object->Is<AllocationBlock>()) {
    return object->Cast<AllocationBlock>();
  }
  return nullptr;
}
}  // namespace

bool MaglevGraphBuilder::CanElideWriteBarrier(ValueNode* object,
                                              ValueNode* value) {
  if (value->Is<RootConstant>()) return true;
  if (CheckType(value, NodeType::kSmi)) {
    RecordUseReprHintIfPhi(value, UseRepresentation::kTagged);
    return true;
  }

  // No need for a write barrier if both object and value are part of the same
  // folded young allocation.
  AllocationBlock* allocation = GetAllocation(object);
  if (allocation != nullptr &&
      allocation->allocation_type() == AllocationType::kYoung &&
      allocation == GetAllocation(value)) {
    return true;
  }

  // If tagged and not Smi, we cannot elide write barrier.
  if (value->is_tagged()) return false;

  // If its alternative conversion node is Smi, {value} will be converted to
  // a Smi when tagged.
  NodeInfo* node_info = GetOrCreateInfoFor(value);
  if (ValueNode* tagged_alt = node_info->alternative().tagged()) {
    DCHECK(tagged_alt->properties().is_conversion());
    return CheckType(tagged_alt, NodeType::kSmi);
  }
  return false;
}

void MaglevGraphBuilder::BuildInitializeStore(InlinedAllocation* object,
                                              ValueNode* value, int offset) {
  const bool value_is_trusted = value->Is<TrustedConstant>();
  DCHECK(value->is_tagged());
  if (InlinedAllocation* inlined_value = value->TryCast<InlinedAllocation>()) {
    // Add to the escape set.
    auto escape_deps = graph()->allocations_escape_map().find(object);
    CHECK(escape_deps != graph()->allocations_escape_map().end());
    escape_deps->second.push_back(inlined_value);
    // Add to the elided set.
    auto& elided_map = graph()->allocations_elide_map();
    auto elided_deps = elided_map.try_emplace(inlined_value, zone()).first;
    elided_deps->second.push_back(object);
    inlined_value->AddNonEscapingUses();
  }
  if (value_is_trusted) {
    BuildStoreTrustedPointerField(object, value, offset,
                                  value->Cast<TrustedConstant>()->tag(),
                                  StoreTaggedMode::kInitializing);
  } else {
    BuildStoreTaggedField(object, value, offset,
                          StoreTaggedMode::kInitializing);
  }
}

namespace {
bool IsEscaping(Graph* graph, InlinedAllocation* alloc) {
  if (alloc->IsEscaping()) return true;
  auto it = graph->allocations_elide_map().find(alloc);
  if (it == graph->allocations_elide_map().end()) return false;
  for (InlinedAllocation* inner_alloc : it->second) {
    if (IsEscaping(graph, inner_alloc)) {
      return true;
    }
  }
  return false;
}

bool VerifyIsNotEscaping(VirtualObject::List vos, InlinedAllocation* alloc) {
  for (VirtualObject* vo : vos) {
    if (vo->type() != VirtualObject::kDefault) continue;
    if (vo->allocation() == alloc) continue;
    for (uint32_t i = 0; i < vo->slot_count(); i++) {
      ValueNode* nested_value = vo->get_by_index(i);
      if (!nested_value->Is<InlinedAllocation>()) continue;
      ValueNode* nested_alloc = nested_value->Cast<InlinedAllocation>();
      if (nested_alloc == alloc) {
        if (vo->allocation()->IsEscaping()) return false;
        if (!VerifyIsNotEscaping(vos, vo->allocation())) return false;
      }
    }
  }
  return true;
}
}  // namespace

bool MaglevGraphBuilder::CanTrackObjectChanges(ValueNode* receiver,
                                               TrackObjectMode mode) {
  DCHECK(!receiver->Is<VirtualObject>());
  if (!v8_flags.maglev_object_tracking) return false;
  if (!receiver->Is<InlinedAllocation>()) return false;
  InlinedAllocation* alloc = receiver->Cast<InlinedAllocation>();
  if (mode == TrackObjectMode::kStore) {
    // If we have two objects A and B, such that A points to B (it contains B in
    // one of its field), we cannot change B without also changing A, even if
    // both can be elided. For now, we escape both objects instead.
    if (graph_->allocations_elide_map().find(alloc) !=
        graph_->allocations_elide_map().end()) {
      return false;
    }
    if (alloc->IsEscaping()) return false;
    // Ensure object is escaped if we are within a try-catch block. This is
    // crucial because a deoptimization point inside the catch handler could
    // re-materialize objects differently, depending on whether the throw
    // occurred before or after this store. We could potentially relax this
    // requirement by verifying that no throwable nodes have been emitted since
    // the try-block started,  but for now, err on the side of caution and
    // always escape.
    if (IsInsideTryBlock()) return false;
  } else {
    DCHECK_EQ(mode, TrackObjectMode::kLoad);
    if (IsEscaping(graph_, alloc)) return false;
  }
  // We don't support loop phis inside VirtualObjects, so any access inside a
  // loop should escape the object, except for objects that were created since
  // the last loop header.
  if (IsInsideLoop()) {
    if (!is_loop_effect_tracking() ||
        !loop_effects_->allocations.contains(alloc)) {
      return false;
    }
  }
  // Iterate all live objects to be sure that the allocation is not escaping.
  SLOW_DCHECK(
      VerifyIsNotEscaping(current_interpreter_frame_.virtual_objects(), alloc));
  return true;
}

VirtualObject* MaglevGraphBuilder::GetObjectFromAllocation(
    InlinedAllocation* allocation) {
  VirtualObject* vobject = allocation->object();
  // If it hasn't be snapshotted yet, it is the latest created version of this
  // object, we don't need to search for it.
  if (vobject->IsSnapshot()) {
    vobject = current_interpreter_frame_.virtual_objects().FindAllocatedWith(
        allocation);
  }
  return vobject;
}

VirtualObject* MaglevGraphBuilder::GetModifiableObjectFromAllocation(
    InlinedAllocation* allocation) {
  VirtualObject* vobject = allocation->object();
  // If it hasn't be snapshotted yet, it is the latest created version of this
  // object and we can still modify it, we don't need to copy it.
  if (vobject->IsSnapshot()) {
    return DeepCopyVirtualObject(
        current_interpreter_frame_.virtual_objects().FindAllocatedWith(
            allocation));
  }
  return vobject;
}

void MaglevGraphBuilder::TryBuildStoreTaggedFieldToAllocation(ValueNode* object,
                                                              ValueNode* value,
                                                              int offset) {
  if (offset == HeapObject::kMapOffset) return;
  if (!CanTrackObjectChanges(object, TrackObjectMode::kStore)) return;
  // This avoids loop in the object graph.
  if (value->Is<InlinedAllocation>()) return;
  InlinedAllocation* allocation = object->Cast<InlinedAllocation>();
  VirtualObject* vobject = GetModifiableObjectFromAllocation(allocation);
  CHECK_EQ(vobject->type(), VirtualObject::kDefault);
  CHECK_NOT_NULL(vobject);
  vobject->set(offset, value);
  AddNonEscapingUses(allocation, 1);
  if (v8_flags.trace_maglev_object_tracking) {
    std::cout << "  * Setting value in virtual object "
              << PrintNodeLabel(graph_labeller(), vobject) << "|" << offset
              << "]: " << PrintNode(graph_labeller(), value) << std::endl;
  }
}

Node* MaglevGraphBuilder::BuildStoreTaggedField(ValueNode* object,
                                                ValueNode* value, int offset,
                                                StoreTaggedMode store_mode) {
  // The value may be used to initialize a VO, which can leak to IFS.
  // It should NOT be a conversion node, UNLESS it's an initializing value.
  // Initializing values are tagged before allocation, since conversion nodes
  // may allocate, and are not used to set a VO.
  DCHECK_IMPLIES(store_mode != StoreTaggedMode::kInitializing,
                 !value->properties().is_conversion());
  if (store_mode != StoreTaggedMode::kInitializing) {
    TryBuildStoreTaggedFieldToAllocation(object, value, offset);
  }
  if (CanElideWriteBarrier(object, value)) {
    return AddNewNode<StoreTaggedFieldNoWriteBarrier>({object, value}, offset,
                                                      store_mode);
  } else {
    return AddNewNode<StoreTaggedFieldWithWriteBarrier>({object, value}, offset,
                                                        store_mode);
  }
}

void MaglevGraphBuilder::BuildStoreTaggedFieldNoWriteBarrier(
    ValueNode* object, ValueNode* value, int offset,
    StoreTaggedMode store_mode) {
  // The value may be used to initialize a VO, which can leak to IFS.
  // It should NOT be a conversion node, UNLESS it's an initializing value.
  // Initializing values are tagged before allocation, since conversion nodes
  // may allocate, and are not used to set a VO.
  DCHECK_IMPLIES(store_mode != StoreTaggedMode::kInitializing,
                 !value->properties().is_conversion());
  DCHECK(CanElideWriteBarrier(object, value));
  if (store_mode != StoreTaggedMode::kInitializing) {
    TryBuildStoreTaggedFieldToAllocation(object, value, offset);
  }
  AddNewNode<StoreTaggedFieldNoWriteBarrier>({object, value}, offset,
                                             store_mode);
}

void MaglevGraphBuilder::BuildStoreTrustedPointerField(
    ValueNode* object, ValueNode* value, int offset, IndirectPointerTag tag,
    StoreTaggedMode store_mode) {
#ifdef V8_ENABLE_SANDBOX
  AddNewNode<StoreTrustedPointerFieldWithWriteBarrier>({object, value}, offset,
                                                       tag, store_mode);
#else
  BuildStoreTaggedField(object, value, offset, store_mode);
#endif  // V8_ENABLE_SANDBOX
}

ValueNode* MaglevGraphBuilder::BuildLoadFixedArrayElement(ValueNode* elements,
                                                          int index) {
  compiler::OptionalHeapObjectRef maybe_constant;
  if ((maybe_constant = TryGetConstant(elements)) &&
      maybe_constant.value().IsFixedArray()) {
    compiler::FixedArrayRef fixed_array_ref =
        maybe_constant.value().AsFixedArray();
    if (index >= 0 && static_cast<uint32_t>(index) < fixed_array_ref.length()) {
      compiler::OptionalObjectRef maybe_value =
          fixed_array_ref.TryGet(broker(), index);
      if (maybe_value) return GetConstant(*maybe_value);
    } else {
      return GetRootConstant(RootIndex::kTheHoleValue);
    }
  }
  if (CanTrackObjectChanges(elements, TrackObjectMode::kLoad)) {
    VirtualObject* vobject =
        GetObjectFromAllocation(elements->Cast<InlinedAllocation>());
    CHECK_EQ(vobject->type(), VirtualObject::kDefault);
    DCHECK(vobject->map().IsFixedArrayMap());
    ValueNode* length_node = vobject->get(offsetof(FixedArray, length_));
    if (auto length = TryGetInt32Constant(length_node)) {
      if (index >= 0 && index < length.value()) {
        return vobject->get(FixedArray::OffsetOfElementAt(index));
      } else {
        return GetRootConstant(RootIndex::kTheHoleValue);
      }
    }
  }
  if (index < 0 || index >= FixedArray::kMaxLength) {
    return GetRootConstant(RootIndex::kTheHoleValue);
  }
  return AddNewNode<LoadTaggedField>({elements},
                                     FixedArray::OffsetOfElementAt(index));
}

ValueNode* MaglevGraphBuilder::BuildLoadFixedArrayElement(ValueNode* elements,
                                                          ValueNode* index) {
  if (auto constant = TryGetInt32Constant(index)) {
    return BuildLoadFixedArrayElement(elements, constant.value());
  }
  return AddNewNode<LoadFixedArrayElement>({elements, index});
}

void MaglevGraphBuilder::BuildStoreFixedArrayElement(ValueNode* elements,
                                                     ValueNode* index,
                                                     ValueNode* value) {
  // TODO(victorgomes): Support storing element to a virtual object. If we
  // modify the elements array, we need to modify the original object to point
  // to the new elements array.
  if (CanElideWriteBarrier(elements, value)) {
    AddNewNode<StoreFixedArrayElementNoWriteBarrier>({elements, index, value});
  } else {
    AddNewNode<StoreFixedArrayElementWithWriteBarrier>(
        {elements, index, value});
  }
}

ValueNode* MaglevGraphBuilder::BuildLoadFixedDoubleArrayElement(
    ValueNode* elements, int index) {
  if (CanTrackObjectChanges(elements, TrackObjectMode::kLoad)) {
    VirtualObject* vobject =
        GetObjectFromAllocation(elements->Cast<InlinedAllocation>());
    compiler::FixedDoubleArrayRef elements_array = vobject->double_elements();
    if (index >= 0 && static_cast<uint32_t>(index) < elements_array.length()) {
      Float64 value = elements_array.GetFromImmutableFixedDoubleArray(index);
      return GetFloat64Constant(value.get_scalar());
    } else {
      return GetRootConstant(RootIndex::kTheHoleValue);
    }
  }
  if (index < 0 || index >= FixedArray::kMaxLength) {
    return GetRootConstant(RootIndex::kTheHoleValue);
  }
  return AddNewNode<LoadFixedDoubleArrayElement>(
      {elements, GetInt32Constant(index)});
}

ValueNode* MaglevGraphBuilder::BuildLoadFixedDoubleArrayElement(
    ValueNode* elements, ValueNode* index) {
  if (auto constant = TryGetInt32Constant(index)) {
    return BuildLoadFixedDoubleArrayElement(elements, constant.value());
  }
  return AddNewNode<LoadFixedDoubleArrayElement>({elements, index});
}

void MaglevGraphBuilder::BuildStoreFixedDoubleArrayElement(ValueNode* elements,
                                                           ValueNode* index,
                                                           ValueNode* value) {
  // TODO(victorgomes): Support storing double element to a virtual object.
  AddNewNode<StoreFixedDoubleArrayElement>({elements, index, value});
}

ValueNode* MaglevGraphBuilder::BuildLoadHoleyFixedDoubleArrayElement(
    ValueNode* elements, ValueNode* index, bool convert_hole) {
  if (convert_hole) {
    return AddNewNode<LoadHoleyFixedDoubleArrayElement>({elements, index});
  } else {
    return AddNewNode<LoadHoleyFixedDoubleArrayElementCheckedNotHole>(
        {elements, index});
  }
}

bool MaglevGraphBuilder::CanTreatHoleAsUndefined(
    base::Vector<const compiler::MapRef> const& receiver_maps) {
  // Check if all {receiver_maps} have one of the initial Array.prototype
  // or Object.prototype objects as their prototype (in any of the current
  // native contexts, as the global Array protector works isolate-wide).
  for (compiler::MapRef receiver_map : receiver_maps) {
    compiler::ObjectRef receiver_prototype = receiver_map.prototype(broker());
    if (!receiver_prototype.IsJSObject() ||
        !broker()->IsArrayOrObjectPrototype(receiver_prototype.AsJSObject())) {
      return false;
    }
  }

  // Check if the array prototype chain is intact.
  return broker()->dependencies()->DependOnNoElementsProtector();
}

compiler::OptionalObjectRef
MaglevGraphBuilder::TryFoldLoadDictPrototypeConstant(
    compiler::PropertyAccessInfo const& access_info) {
  DCHECK(V8_DICT_PROPERTY_CONST_TRACKING_BOOL);
  DCHECK(access_info.IsDictionaryProtoDataConstant());
  DCHECK(access_info.holder().has_value());

  compiler::OptionalObjectRef constant =
      access_info.holder()->GetOwnDictionaryProperty(
          broker(), access_info.dictionary_index(), broker()->dependencies());
  if (!constant.has_value()) return {};

  for (compiler::MapRef map : access_info.lookup_start_object_maps()) {
    DirectHandle<Map> map_handle = map.object();
    // Non-JSReceivers that passed AccessInfoFactory::ComputePropertyAccessInfo
    // must have different lookup start map.
    if (!IsJSReceiverMap(*map_handle)) {
      // Perform the implicit ToObject for primitives here.
      // Implemented according to ES6 section 7.3.2 GetV (V, P).
      Tagged<JSFunction> constructor =
          Map::GetConstructorFunction(
              *map_handle, *broker()->target_native_context().object())
              .value();
      // {constructor.initial_map()} is loaded/stored with acquire-release
      // semantics for constructors.
      map = MakeRefAssumeMemoryFence(broker(), constructor->initial_map());
      DCHECK(IsJSObjectMap(*map.object()));
    }
    broker()->dependencies()->DependOnConstantInDictionaryPrototypeChain(
        map, access_info.name(), constant.value(), PropertyKind::kData);
  }

  return constant;
}

compiler::OptionalJSObjectRef MaglevGraphBuilder::TryGetConstantDataFieldHolder(
    compiler::PropertyAccessInfo const& access_info,
    ValueNode* lookup_start_object) {
  if (!access_info.IsFastDataConstant()) return {};
  if (access_info.holder().has_value()) {
    return access_info.holder();
  }
  if (compiler::OptionalHeapObjectRef c = TryGetConstant(lookup_start_object)) {
    if (c.value().IsJSObject()) {
      return c.value().AsJSObject();
    }
  }
  return {};
}

compiler::OptionalObjectRef MaglevGraphBuilder::TryFoldLoadConstantDataField(
    compiler::JSObjectRef holder,
    compiler::PropertyAccessInfo const& access_info) {
  DCHECK(!access_info.field_representation().IsDouble());
  return holder.GetOwnFastConstantDataProperty(
      broker(), access_info.field_representation(), access_info.field_index(),
      broker()->dependencies());
}

std::optional<Float64> MaglevGraphBuilder::TryFoldLoadConstantDoubleField(
    compiler::JSObjectRef holder,
    compiler::PropertyAccessInfo const& access_info) {
  DCHECK(access_info.field_representation().IsDouble());
  return holder.GetOwnFastConstantDoubleProperty(
      broker(), access_info.field_index(), broker()->dependencies());
}

ReduceResult MaglevGraphBuilder::TryBuildPropertyGetterCall(
    compiler::PropertyAccessInfo const& access_info, ValueNode* receiver,
    ValueNode* lookup_start_object) {
  compiler::ObjectRef constant = access_info.constant().value();

  if (access_info.IsDictionaryProtoAccessorConstant()) {
    // For fast mode holders we recorded dependencies in BuildPropertyLoad.
    for (const compiler::MapRef map : access_info.lookup_start_object_maps()) {
      broker()->dependencies()->DependOnConstantInDictionaryPrototypeChain(
          map, access_info.name(), constant, PropertyKind::kAccessor);
    }
  }

  // Introduce the call to the getter function.
  if (constant.IsJSFunction()) {
    ConvertReceiverMode receiver_mode =
        receiver == lookup_start_object
            ? ConvertReceiverMode::kNotNullOrUndefined
            : ConvertReceiverMode::kAny;
    CallArguments args(receiver_mode, {receiver});
    return ReduceCallForConstant(constant.AsJSFunction(), args);
  } else {
    // Disable optimizations for super ICs using API getters, so that we get
    // the correct receiver checks.
    if (receiver != lookup_start_object) {
      return ReduceResult::Fail();
    }
    compiler::FunctionTemplateInfoRef templ = constant.AsFunctionTemplateInfo();
    CallArguments args(ConvertReceiverMode::kNotNullOrUndefined, {receiver});

    return ReduceCallForApiFunction(templ, {}, access_info.api_holder(), args);
  }
}

ReduceResult MaglevGraphBuilder::TryBuildPropertySetterCall(
    compiler::PropertyAccessInfo const& access_info, ValueNode* receiver,
    ValueNode* lookup_start_object, ValueNode* value) {
  // Setting super properties shouldn't end up here.
  DCHECK_EQ(receiver, lookup_start_object);
  compiler::ObjectRef constant = access_info.constant().value();
  if (constant.IsJSFunction()) {
    CallArguments args(ConvertReceiverMode::kNotNullOrUndefined,
                       {receiver, value});
    RETURN_IF_ABORT(ReduceCallForConstant(constant.AsJSFunction(), args));
  } else {
    compiler::FunctionTemplateInfoRef templ = constant.AsFunctionTemplateInfo();
    CallArguments args(ConvertReceiverMode::kNotNullOrUndefined,
                       {receiver, value});
    RETURN_IF_ABORT(
        ReduceCallForApiFunction(templ, {}, access_info.api_holder(), args));
  }
  // Ignore the return value of the setter call.
  return ReduceResult::Done();
}

ValueNode* MaglevGraphBuilder::BuildLoadField(
    compiler::PropertyAccessInfo const& access_info,
    ValueNode* lookup_start_object, compiler::NameRef name) {
  compiler::OptionalJSObjectRef constant_holder =
      TryGetConstantDataFieldHolder(access_info, lookup_start_object);
  if (constant_holder) {
    if (access_info.field_representation().IsDouble()) {
      std::optional<Float64> constant =
          TryFoldLoadConstantDoubleField(constant_holder.value(), access_info);
      if (constant.has_value()) {
        return GetFloat64Constant(constant.value());
      }
    } else {
      compiler::OptionalObjectRef constant =
          TryFoldLoadConstantDataField(constant_holder.value(), access_info);
      if (constant.has_value()) {
        return GetConstant(constant.value());
      }
    }
  }
  // Resolve property holder.
  ValueNode* load_source;
  if (access_info.holder().has_value()) {
    load_source = GetConstant(access_info.holder().value());
  } else {
    load_source = lookup_start_object;
  }

  FieldIndex field_index = access_info.field_index();
  if (!field_index.is_inobject()) {
    // The field is in the property array, first load it from there.
    load_source =
        BuildLoadTaggedField(load_source, JSReceiver::kPropertiesOrHashOffset);
  }

  // Do the load.
  if (field_index.is_double()) {
    return AddNewNode<LoadDoubleField>({load_source}, field_index.offset());
  }
  ValueNode* value = BuildLoadTaggedField<LoadTaggedFieldForProperty>(
      load_source, field_index.offset(), name);
  // Insert stable field information if present.
  if (access_info.field_representation().IsSmi()) {
    NodeInfo* known_info = GetOrCreateInfoFor(value);
    known_info->CombineType(NodeType::kSmi);
  } else if (access_info.field_representation().IsHeapObject()) {
    NodeInfo* known_info = GetOrCreateInfoFor(value);
    if (access_info.field_map().has_value() &&
        access_info.field_map().value().is_stable()) {
      DCHECK(access_info.field_map().value().IsJSReceiverMap());
      auto map = access_info.field_map().value();
      known_info->SetPossibleMaps(PossibleMaps{map}, false,
                                  StaticTypeForMap(map, broker()), broker());
      broker()->dependencies()->DependOnStableMap(map);
    } else {
      known_info->CombineType(NodeType::kAnyHeapObject);
    }
  }
  return value;
}

ValueNode* MaglevGraphBuilder::BuildLoadFixedArrayLength(
    ValueNode* fixed_array) {
  ValueNode* length =
      BuildLoadTaggedField(fixed_array, offsetof(FixedArray, length_));
  EnsureType(length, NodeType::kSmi);
  return length;
}

ValueNode* MaglevGraphBuilder::BuildLoadJSArrayLength(ValueNode* js_array,
                                                      NodeType length_type) {
  // TODO(leszeks): JSArray.length is known to be non-constant, don't bother
  // searching the constant values.
  ReduceResult known_length =
      TryReuseKnownPropertyLoad(js_array, broker()->length_string());
  if (known_length.IsDone()) {
    DCHECK(known_length.IsDoneWithValue());
    return known_length.value();
  }

  ValueNode* length = BuildLoadTaggedField<LoadTaggedFieldForProperty>(
      js_array, JSArray::kLengthOffset, broker()->length_string());
  GetOrCreateInfoFor(length)->CombineType(length_type);
  RecordKnownProperty(js_array, broker()->length_string(), length, false,
                      compiler::AccessMode::kLoad);
  return length;
}

void MaglevGraphBuilder::BuildStoreMap(ValueNode* object, compiler::MapRef map,
                                       StoreMap::Kind kind) {
  AddNewNode<StoreMap>({object}, map, kind);
  NodeType object_type = StaticTypeForMap(map, broker());
  NodeInfo* node_info = GetOrCreateInfoFor(object);
  if (map.is_stable()) {
    node_info->SetPossibleMaps(PossibleMaps{map}, false, object_type, broker());
    broker()->dependencies()->DependOnStableMap(map);
  } else {
    node_info->SetPossibleMaps(PossibleMaps{map}, true, object_type, broker());
    known_node_aspects().any_map_for_any_node_is_unstable = true;
  }
}

ValueNode* MaglevGraphBuilder::BuildExtendPropertiesBackingStore(
    compiler::MapRef map, ValueNode* receiver, ValueNode* property_array) {
  int length = map.NextFreePropertyIndex() - map.GetInObjectProperties();
  // Under normal circumstances, NextFreePropertyIndex() will always be larger
  // than GetInObjectProperties(). However, an attacker able to corrupt heap
  // memory can break this invariant, in which case we'll get confused here,
  // potentially causing a sandbox violation. This CHECK defends against that.
  SBXCHECK_GE(length, 0);
  return AddNewNode<ExtendPropertiesBackingStore>({property_array, receiver},
                                                  length);
}

ReduceResult MaglevGraphBuilder::TryBuildStoreField(
    compiler::PropertyAccessInfo const& access_info, ValueNode* receiver,
    compiler::AccessMode access_mode) {
  FieldIndex field_index = access_info.field_index();
  Representation field_representation = access_info.field_representation();

  compiler::OptionalMapRef original_map;
  if (access_info.HasTransitionMap()) {
    compiler::MapRef transition = access_info.transition_map().value();
    original_map = transition.GetBackPointer(broker()).AsMap();

    if (original_map->UnusedPropertyFields() == 0) {
      DCHECK(!field_index.is_inobject());
    }
    if (!field_index.is_inobject()) {
      // If slack tracking ends after this compilation started but before it's
      // finished, then {original_map} could be out-of-sync with {transition}.
      // In particular, its UnusedPropertyFields could be non-zero, which would
      // lead us to not extend the property backing store, while the underlying
      // Map has actually zero UnusedPropertyFields. Thus, we install a
      // dependency on {orininal_map} now, so that if such a situation happens,
      // we'll throw away the code.
      broker()->dependencies()->DependOnNoSlackTrackingChange(*original_map);
    }
  } else if (access_info.IsFastDataConstant() &&
             access_mode == compiler::AccessMode::kStore) {
    return EmitUnconditionalDeopt(DeoptimizeReason::kStoreToConstant);
  }

  ValueNode* store_target;
  if (field_index.is_inobject()) {
    store_target = receiver;
  } else {
    // The field is in the property array, first load it from there.
    store_target =
        BuildLoadTaggedField(receiver, JSReceiver::kPropertiesOrHashOffset);
    if (original_map && original_map->UnusedPropertyFields() == 0) {
      store_target = BuildExtendPropertiesBackingStore(*original_map, receiver,
                                                       store_target);
    }
  }

  if (field_representation.IsDouble()) {
    ValueNode* float64_value = GetAccumulator();
    if (access_info.HasTransitionMap()) {
      // Allocate the mutable double box owned by the field.
      ValueNode* value =
          AddNewNode<Float64ToHeapNumberForField>({float64_value});
      BuildStoreTaggedField(store_target, value, field_index.offset(),
                            StoreTaggedMode::kTransitioning);
      BuildStoreMap(receiver, access_info.transition_map().value(),
                    StoreMap::Kind::kTransitioning);
    } else {
      AddNewNode<StoreDoubleField>({store_target, float64_value},
                                   field_index.offset());
    }
    return ReduceResult::Done();
  }

  ValueNode* value = GetAccumulator();
  if (field_representation.IsSmi()) {
    RETURN_IF_ABORT(GetAccumulatorSmi());
  } else {
    if (field_representation.IsHeapObject()) {
      // Emit a map check for the field type, if needed, otherwise just a
      // HeapObject check.
      if (access_info.field_map().has_value()) {
        RETURN_IF_ABORT(BuildCheckMaps(
            value, base::VectorOf({access_info.field_map().value()})));
      } else {
        BuildCheckHeapObject(value);
      }
    }
  }

  StoreTaggedMode store_mode = access_info.HasTransitionMap()
                                   ? StoreTaggedMode::kTransitioning
                                   : StoreTaggedMode::kDefault;
  if (field_representation.IsSmi()) {
    BuildStoreTaggedFieldNoWriteBarrier(store_target, value,
                                        field_index.offset(), store_mode);
  } else {
    DCHECK(field_representation.IsHeapObject() ||
           field_representation.IsTagged());
    BuildStoreTaggedField(store_target, value, field_index.offset(),
                          store_mode);
  }
  if (access_info.HasTransitionMap()) {
    BuildStoreMap(receiver, access_info.transition_map().value(),
                  StoreMap::Kind::kTransitioning);
  }

  return ReduceResult::Done();
}

namespace {
bool AccessInfoGuaranteedConst(
    compiler::PropertyAccessInfo const& access_info) {
  if (!access_info.IsFastDataConstant() && !access_info.IsStringLength()) {
    return false;
  }

  // Even if we have a constant load, if the map is not stable, we cannot
  // guarantee that the load is preserved across side-effecting calls.
  // TODO(v8:7700): It might be possible to track it as const if we know
  // that we're still on the main transition tree; and if we add a
  // dependency on the stable end-maps of the entire tree.
  for (auto& map : access_info.lookup_start_object_maps()) {
    if (!map.is_stable()) {
      return false;
    }
  }
  return true;
}
}  // namespace

ReduceResult MaglevGraphBuilder::TryBuildPropertyLoad(
    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
    compiler::PropertyAccessInfo const& access_info) {
  if (access_info.holder().has_value() && !access_info.HasDictionaryHolder()) {
    broker()->dependencies()->DependOnStablePrototypeChains(
        access_info.lookup_start_object_maps(), kStartAtPrototype,
        access_info.holder().value());
  }

  switch (access_info.kind()) {
    case compiler::PropertyAccessInfo::kInvalid:
      UNREACHABLE();
    case compiler::PropertyAccessInfo::kNotFound:
      return GetRootConstant(RootIndex::kUndefinedValue);
    case compiler::PropertyAccessInfo::kDataField:
    case compiler::PropertyAccessInfo::kFastDataConstant: {
      ValueNode* result =
          BuildLoadField(access_info, lookup_start_object, name);
      RecordKnownProperty(lookup_start_object, name, result,
                          AccessInfoGuaranteedConst(access_info),
                          compiler::AccessMode::kLoad);
      return result;
    }
    case compiler::PropertyAccessInfo::kDictionaryProtoDataConstant: {
      compiler::OptionalObjectRef constant =
          TryFoldLoadDictPrototypeConstant(access_info);
      if (!constant.has_value()) return ReduceResult::Fail();
      return GetConstant(constant.value());
    }
    case compiler::PropertyAccessInfo::kFastAccessorConstant:
    case compiler::PropertyAccessInfo::kDictionaryProtoAccessorConstant:
      return TryBuildPropertyGetterCall(access_info, receiver,
                                        lookup_start_object);
    case compiler::PropertyAccessInfo::kModuleExport: {
      ValueNode* cell = GetConstant(access_info.constant().value().AsCell());
      return BuildLoadTaggedField<LoadTaggedFieldForProperty>(
          cell, Cell::kValueOffset, name);
    }
    case compiler::PropertyAccessInfo::kStringLength: {
      DCHECK_EQ(receiver, lookup_start_object);
      ValueNode* result = BuildLoadStringLength(receiver);
      RecordKnownProperty(lookup_start_object, name, result,
                          AccessInfoGuaranteedConst(access_info),
                          compiler::AccessMode::kLoad);
      return result;
    }
    case compiler::PropertyAccessInfo::kStringWrapperLength: {
      // TODO(dmercadier): update KnownNodeInfo.
      ValueNode* string = BuildLoadTaggedField(
          lookup_start_object, JSPrimitiveWrapper::kValueOffset);
      return AddNewNode<StringLength>({string});
    }
  }
}

ReduceResult MaglevGraphBuilder::TryBuildPropertyStore(
    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
    compiler::PropertyAccessInfo const& access_info,
    compiler::AccessMode access_mode) {
  if (access_info.holder().has_value()) {
    broker()->dependencies()->DependOnStablePrototypeChains(
        access_info.lookup_start_object_maps(), kStartAtPrototype,
        access_info.holder().value());
  }

  switch (access_info.kind()) {
    case compiler::PropertyAccessInfo::kFastAccessorConstant: {
      return TryBuildPropertySetterCall(access_info, receiver,
                                        lookup_start_object, GetAccumulator());
    }
    case compiler::PropertyAccessInfo::kDataField:
    case compiler::PropertyAccessInfo::kFastDataConstant: {
      ReduceResult res = TryBuildStoreField(access_info, receiver, access_mode);
      if (res.IsDone()) {
        RecordKnownProperty(
            receiver, name, current_interpreter_frame_.accumulator(),
            AccessInfoGuaranteedConst(access_info), access_mode);
        return res;
      }
      return ReduceResult::Fail();
    }
    case compiler::PropertyAccessInfo::kInvalid:
    case compiler::PropertyAccessInfo::kNotFound:
    case compiler::PropertyAccessInfo::kDictionaryProtoDataConstant:
    case compiler::PropertyAccessInfo::kDictionaryProtoAccessorConstant:
    case compiler::PropertyAccessInfo::kModuleExport:
    case compiler::PropertyAccessInfo::kStringLength:
    case compiler::PropertyAccessInfo::kStringWrapperLength:
      UNREACHABLE();
  }
}

ReduceResult MaglevGraphBuilder::TryBuildPropertyAccess(
    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
    compiler::PropertyAccessInfo const& access_info,
    compiler::AccessMode access_mode) {
  switch (access_mode) {
    case compiler::AccessMode::kLoad:
      return TryBuildPropertyLoad(receiver, lookup_start_object, name,
                                  access_info);
    case compiler::AccessMode::kStore:
    case compiler::AccessMode::kStoreInLiteral:
    case compiler::AccessMode::kDefine:
      DCHECK_EQ(receiver, lookup_start_object);
      return TryBuildPropertyStore(receiver, lookup_start_object, name,
                                   access_info, access_mode);
    case compiler::AccessMode::kHas:
      // TODO(victorgomes): BuildPropertyTest.
      return ReduceResult::Fail();
  }
}

template <typename GenericAccessFunc>
ReduceResult MaglevGraphBuilder::TryBuildNamedAccess(
    ValueNode* receiver, ValueNode* lookup_start_object,
    compiler::NamedAccessFeedback const& feedback,
    compiler::FeedbackSource const& feedback_source,
    compiler::AccessMode access_mode,
    GenericAccessFunc&& build_generic_access) {
  compiler::ZoneRefSet<Map> inferred_maps;

  bool has_deprecated_map_without_migration_target = false;
  if (compiler::OptionalHeapObjectRef c = TryGetConstant(lookup_start_object)) {
    compiler::MapRef constant_map = c.value().map(broker());
    if (c.value().IsJSFunction() &&
        feedback.name().equals(broker()->prototype_string())) {
      compiler::JSFunctionRef function = c.value().AsJSFunction();
      if (!constant_map.has_prototype_slot() ||
          !function.has_instance_prototype(broker()) ||
          function.PrototypeRequiresRuntimeLookup(broker()) ||
          access_mode != compiler::AccessMode::kLoad) {
        return ReduceResult::Fail();
      }
      compiler::HeapObjectRef prototype =
          broker()->dependencies()->DependOnPrototypeProperty(function);
      return GetConstant(prototype);
    }
    inferred_maps = compiler::ZoneRefSet<Map>(constant_map);
  } else if (feedback.maps().empty()) {
    // The IC is megamorphic.

    // We can't do megamorphic loads for lookups where the lookup start isn't
    // the receiver (e.g. load from super).
    if (receiver != lookup_start_object) return ReduceResult::Fail();

    // Use known possible maps if we have any.
    NodeInfo* object_info =
        known_node_aspects().TryGetInfoFor(lookup_start_object);
    if (object_info && object_info->possible_maps_are_known()) {
      inferred_maps = object_info->possible_maps();
    } else {
      // If we have no known maps, make the access megamorphic.
      switch (access_mode) {
        case compiler::AccessMode::kLoad:
          return BuildCallBuiltin<Builtin::kLoadIC_Megamorphic>(
              {GetTaggedValue(receiver), GetConstant(feedback.name())},
              feedback_source);
        case compiler::AccessMode::kStore:
          return BuildCallBuiltin<Builtin::kStoreIC_Megamorphic>(
              {GetTaggedValue(receiver), GetConstant(feedback.name()),
               GetTaggedValue(GetAccumulator())},
              feedback_source);
        case compiler::AccessMode::kDefine:
          return ReduceResult::Fail();
        case compiler::AccessMode::kHas:
        case compiler::AccessMode::kStoreInLiteral:
          UNREACHABLE();
      }
    }
  } else {
    // TODO(leszeks): This is doing duplicate work with BuildCheckMaps,
    // consider passing the merger into there.
    KnownMapsMerger merger(broker(), zone(), base::VectorOf(feedback.maps()));
    merger.IntersectWithKnownNodeAspects(lookup_start_object,
                                         known_node_aspects());
    inferred_maps = merger.intersect_set();
    has_deprecated_map_without_migration_target =
        feedback.has_deprecated_map_without_migration_target();
  }

  if (inferred_maps.is_empty()) {
    return EmitUnconditionalDeopt(DeoptimizeReason::kWrongMap);
  }

  ZoneVector<compiler::PropertyAccessInfo> access_infos(zone());
  ZoneVector<compiler::PropertyAccessInfo> access_infos_for_feedback(zone());

  for (compiler::MapRef map : inferred_maps) {
    if (map.is_deprecated()) continue;

    // TODO(v8:12547): Support writing to objects in shared space, which
    // need a write barrier that calls Object::Share to ensure the RHS is
    // shared.
    if (InstanceTypeChecker::IsAlwaysSharedSpaceJSObject(map.instance_type()) &&
        access_mode == compiler::AccessMode::kStore) {
      return ReduceResult::Fail();
    }

    compiler::PropertyAccessInfo access_info =
        broker()->GetPropertyAccessInfo(map, feedback.name(), access_mode);
    access_infos_for_feedback.push_back(access_info);
  }

  compiler::AccessInfoFactory access_info_factory(broker(), zone());
  if (!access_info_factory.FinalizePropertyAccessInfos(
          access_infos_for_feedback, access_mode, &access_infos)) {
    return ReduceResult::Fail();
  }

  // Check for monomorphic case.
  if (access_infos.size() == 1) {
    compiler::PropertyAccessInfo const& access_info = access_infos.front();
    base::Vector<const compiler::MapRef> maps =
        base::VectorOf(access_info.lookup_start_object_maps());
    if (HasOnlyStringMaps(maps)) {
      // Check for string maps before checking if we need to do an access
      // check. Primitive strings always get the prototype from the native
      // context they're operated on, so they don't need the access check.
      BuildCheckString(lookup_start_object);
    } else if (HasOnlyNumberMaps(maps)) {
      BuildCheckNumber(lookup_start_object);
    } else {
      RETURN_IF_ABORT(
          BuildCheckMaps(lookup_start_object, maps, {},
                         has_deprecated_map_without_migration_target));
    }

    // Generate the actual property
    return TryBuildPropertyAccess(receiver, lookup_start_object,
                                  feedback.name(), access_info, access_mode);
  } else {
    // TODO(victorgomes): Unify control flow logic with
    // TryBuildPolymorphicElementAccess.
    return TryBuildPolymorphicPropertyAccess(
        receiver, lookup_start_object, feedback, access_mode, access_infos,
        build_generic_access);
  }
}

ValueNode* MaglevGraphBuilder::GetInt32ElementIndex(ValueNode* object) {
  RecordUseReprHintIfPhi(object, UseRepresentation::kInt32);

  switch (object->properties().value_representation()) {
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kTagged:
      NodeType old_type;
      if (SmiConstant* constant = object->TryCast<SmiConstant>()) {
        return GetInt32Constant(constant->value().value());
      } else if (CheckType(object, NodeType::kSmi, &old_type)) {
        auto& alternative = GetOrCreateInfoFor(object)->alternative();
        return alternative.get_or_set_int32(
            |&]() { return BuildSmiUntag(object); });
      } else {
        // TODO(leszeks): Cache this knowledge/converted value somehow on
        // the node info.
        return AddNewNode<CheckedObjectToIndex>({object},
                                                GetCheckType(old_type));
      }
    case ValueRepresentation::kInt32:
      // Already good.
      return object;
    case ValueRepresentation::kUint32:
    case ValueRepresentation::kFloat64:
    case ValueRepresentation::kHoleyFloat64:
      return GetInt32(object);
  }
}

// TODO(victorgomes): Consider caching the values and adding an
// uint32_alternative in node_info.
ReduceResult MaglevGraphBuilder::GetUint32ElementIndex(ValueNode* object) {
  // Don't record a Uint32 Phi use here, since the tagged path goes via
  // GetInt32ElementIndex, making this an Int32 Phi use.

  switch (object->properties().value_representation()) {
    case ValueRepresentation::kIntPtr:
      UNREACHABLE();
    case ValueRepresentation::kTagged:
      // TODO(victorgomes): Consider creating a CheckedObjectToUnsignedIndex.
      if (SmiConstant* constant = object->TryCast<SmiConstant>()) {
        int32_t value = constant->value().value();
        if (value < 0) {
          return EmitUnconditionalDeopt(DeoptimizeReason::kNotUint32);
        }
        return GetUint32Constant(value);
      }
      return AddNewNode<CheckedInt32ToUint32>({GetInt32ElementIndex(object)});
    case ValueRepresentation::kInt32:
      if (Int32Constant* constant = object->TryCast<Int32Constant>()) {
        int32_t value = constant->value();
        if (value < 0) {
          return EmitUnconditionalDeopt(DeoptimizeReason::kNotUint32);
        }
        return GetUint32Constant(value);
      }
      return AddNewNode<CheckedInt32ToUint32>({object});
    case ValueRepresentation::kUint32:
      return object;
    case ValueRepresentation::kFloat64:
      if (Float64Constant* constant = object->TryCast<Float64Constant>()) {
        double value = constant->value().get_scalar();
        uint32_t uint32_value;
        if (!DoubleToUint32IfEqualToSelf(value, &uint32_value)) {
          return EmitUnconditionalDeopt(DeoptimizeReason::kNotUint32);
        }
        if (Smi::IsValid(uint32_value)) {
          return GetUint32Constant(uint32_value);
        }
      }
      ||fallthrough]];
    case ValueRepresentation::kHoleyFloat64: {
      // CheckedTruncateFloat64ToUint32 will gracefully deopt on holes.
      return AddNewNode<CheckedTruncateFloat64ToUint32>({object});
    }
  }
}

ReduceResult MaglevGraphBuilder::TryBuildElementAccessOnString(
    ValueNode* object, ValueNode* index_object,
    compiler::KeyedAccessMode const& keyed_mode) {
  // Strings are immutable and `in` cannot be used on strings
  if (keyed_mode.access_mode() != compiler::AccessMode::kLoad) {
    return ReduceResult::Fail();
  }

  // Ensure that {object} is actually a String.
  BuildCheckString(object);

  ValueNode* length = BuildLoadStringLength(object);
  ValueNode* index = GetInt32ElementIndex(index_object);
  auto emit_load = |&] { return AddNewNode<StringAt>({object, index}); };

  if (LoadModeHandlesOOB(keyed_mode.load_mode()) &&
      broker()->dependencies()->DependOnNoElementsProtector()) {
    ValueNode* positive_index;
    GET_VALUE_OR_ABORT(positive_index, GetUint32ElementIndex(index));
    ValueNode* uint32_length = AddNewNode<UnsafeInt32ToUint32>({length});
    return Select(
        |&](auto& builder) {
          return BuildBranchIfUint32Compare(builder, Operation::kLessThan,
                                            positive_index, uint32_length);
        },
        emit_load, |&] { return GetRootConstant(RootIndex::kUndefinedValue); });
  } else {
    RETURN_IF_ABORT(TryBuildCheckInt32Condition(
        index, length, AssertCondition::kUnsignedLessThan,
        DeoptimizeReason::kOutOfBounds));
    return emit_load();
  }
}

namespace {
ReduceResult TryFindLoadedProperty(
    const KnownNodeAspects::LoadedPropertyMap& loaded_properties,
    ValueNode* lookup_start_object,
    KnownNodeAspects::LoadedPropertyMapKey name) {
  auto props_for_name = loaded_properties.find(name);
  if (props_for_name == loaded_properties.end()) return ReduceResult::Fail();

  auto it = props_for_name->second.find(lookup_start_object);
  if (it == props_for_name->second.end()) return ReduceResult::Fail();

  return it->second;
}

bool CheckConditionIn32(int32_t lhs, int32_t rhs, AssertCondition condition) {
  switch (condition) {
    case AssertCondition::kEqual:
      return lhs == rhs;
    case AssertCondition::kNotEqual:
      return lhs != rhs;
    case AssertCondition::kLessThan:
      return lhs < rhs;
    case AssertCondition::kLessThanEqual:
      return lhs <= rhs;
    case AssertCondition::kGreaterThan:
      return lhs > rhs;
    case AssertCondition::kGreaterThanEqual:
      return lhs >= rhs;
    case AssertCondition::kUnsignedLessThan:
      return static_cast<uint32_t>(lhs) < static_cast<uint32_t>(rhs);
    case AssertCondition::kUnsignedLessThanEqual:
      return static_cast<uint32_t>(lhs) <= static_cast<uint32_t>(rhs);
    case AssertCondition::kUnsignedGreaterThan:
      return static_cast<uint32_t>(lhs) > static_cast<uint32_t>(rhs);
    case AssertCondition::kUnsignedGreaterThanEqual:
      return static_cast<uint32_t>(lhs) >= static_cast<uint32_t>(rhs);
  }
}

bool CompareInt32(int32_t lhs, int32_t rhs, Operation operation) {
  switch (operation) {
    case Operation::kEqual:
    case Operation::kStrictEqual:
      return lhs == rhs;
    case Operation::kLessThan:
      return lhs < rhs;
    case Operation::kLessThanOrEqual:
      return lhs <= rhs;
    case Operation::kGreaterThan:
      return lhs > rhs;
    case Operation::kGreaterThanOrEqual:
      return lhs >= rhs;
    default:
      UNREACHABLE();
  }
}

bool CompareUint32(uint32_t lhs, uint32_t rhs, Operation operation) {
  switch (operation) {
    case Operation::kEqual:
    case Operation::kStrictEqual:
      return lhs == rhs;
    case Operation::kLessThan:
      return lhs < rhs;
    case Operation::kLessThanOrEqual:
      return lhs <= rhs;
    case Operation::kGreaterThan:
      return lhs > rhs;
    case Operation::kGreaterThanOrEqual:
      return lhs >= rhs;
    default:
      UNREACHABLE();
  }
}

}  // namespace

ReduceResult MaglevGraphBuilder::TryBuildCheckInt32Condition(
    ValueNode* lhs, ValueNode* rhs, AssertCondition condition,
    DeoptimizeReason reason) {
  auto lhs_const = TryGetInt32Constant(lhs);
  if (lhs_const) {
    auto rhs_const = TryGetInt32Constant(rhs);
    if (rhs_const) {
      if (CheckConditionIn32(lhs_const.value(), rhs_const.value(), condition)) {
        return ReduceResult::Done();
[Step 14: Duration 24.34 seconds| Input tokens: 556,968 | Output tokens: 9,834]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 15 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'init_rag_db' with arguments: {'id': 'ID:v8_maglev_foreach_map'}                                                                                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: initialized RAG DB ID:v8_maglev_foreach_map at /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/ID:v8_maglev_foreach_map.json
[Step 15: Duration 63.71 seconds| Input tokens: 766,876 | Output tokens: 12,106]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 16 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_foreach_tryreduce', 'Body': '// src/maglev/maglev-graph-builder.cc (excerpt)\nReduceResult MaglevGraphBuilder::TryReduceArrayForEach(\n       │
│ compiler::JSFunctionRef target, CallArguments& args) {\n  if (!CanSpeculateCall()) return ReduceResult::Fail();\n  ValueNode* receiver = args.receiver();\n  if (!receiver) return ReduceResult::Fail();\n   │
│ if (args.count() < 1) { ... return ReduceResult::Fail(); }\n\n  auto node_info = known_node_aspects().TryGetInfoFor(receiver);\n  if (!node_info || !node_info->possible_maps_are_known()) { ... return      │
│ ReduceResult::Fail(); }\n\n  ElementsKind elements_kind;\n  if (!CanInlineArrayIteratingBuiltin(broker(), node_info->possible_maps(),\n                                      &elements_kind)) { ... return   │
│ ReduceResult::Fail(); }\n\n  if (!broker()->dependencies()->DependOnNoElementsProtector()) { ... return ReduceResult::Fail(); }\n\n  ValueNode* callback = args[0];\n  if (!callback->is_tagged()) return    │
│ ReduceResult::Fail();\n  ValueNode* this_arg = args.count() > 1 ? args[1] : GetRootConstant(RootIndex::kUndefinedValue);\n\n  ValueNode* original_length = BuildLoadJSArrayLength(receiver);\n\n  // Elide   │
│ callable check if already known callable; otherwise wrap in lazy deopt.\n  EnsureType(callback, NodeType::kCallable, [&](NodeType old_type) {\n    DeoptFrameScope lazy_deopt_scope(\n        this,          │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n        base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                                    GetSmiConstant(0), original_length}));\n    │
│ AddNewNode<ThrowIfNotCallable>({callback});\n  });\n\n  ValueNode* original_length_int32 = GetInt32(original_length);\n\n  bool receiver_maps_were_unstable = node_info->possible_maps_are_unstable();\n     │
│ PossibleMaps receiver_maps_before_loop(node_info->possible_maps());\n\n  // Build subgraph and loop variables (index, length)\n  MaglevSubGraphBuilder sub_builder(this, 2);\n                               │
│ MaglevSubGraphBuilder::Variable var_index(0);\n  MaglevSubGraphBuilder::Variable var_length(1);\n  MaglevSubGraphBuilder::Label loop_end(&sub_builder, 1);\n\n  sub_builder.set(var_index,                   │
│ GetSmiConstant(0));\n  sub_builder.set(var_length, original_length);\n  MaglevSubGraphBuilder::LoopLabel loop_header =\n      sub_builder.BeginLoop({&var_index, &var_length});\n\n  // Reset known receiver │
│ maps and cached length for first-iteration semantics\n  if (receiver_maps_were_unstable) {\n    node_info->SetPossibleMaps(receiver_maps_before_loop, receiver_maps_were_unstable,\n                         │
│ NodeType::kUnknown, broker());\n    known_node_aspects().any_map_for_any_node_is_unstable = true;\n  }\n  RecordKnownProperty(receiver, broker()->length_string(),\n                                         │
│ sub_builder.get(var_length), false,\n                      compiler::AccessMode::kLoad);\n\n  // Loop condition: index < original_length\n  Phi* index_tagged = sub_builder.get(var_index)->Cast<Phi>();\n   │
│ EnsureType(index_tagged, NodeType::kSmi);\n  ValueNode* index_int32 = GetInt32(index_tagged);\n  sub_builder.GotoIfFalse<BranchIfInt32Compare>(\n      &loop_end, {index_int32, original_length_int32},      │
│ Operation::kLessThan);\n\n  // Index increment with eager deopt continuation in case of overflow\n  ValueNode* next_index_int32 = nullptr;\n  {\n    DeoptFrameScope eager_deopt_scope(\n        this,       │
│ Builtin::kArrayForEachLoopEagerDeoptContinuation, target,\n        base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                                    index_int32, original_length}));\n         │
│ next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32});\n    EnsureType(next_index_int32, NodeType::kSmi);\n  }\n\n  // Load elements and array element at index, specialized by           │
│ elements_kind\n  ValueNode* elements = BuildLoadElements(receiver);\n  ValueNode* element;\n  if (IsDoubleElementsKind(elements_kind)) {\n    element = BuildLoadFixedDoubleArrayElement(elements,           │
│ index_int32);\n  } else {\n    element = BuildLoadFixedArrayElement(elements, index_int32);\n  }\n\n  // Optional hole check to skip call on HOLEY kinds\n  std::optional<MaglevSubGraphBuilder::Label>      │
│ skip_call;\n  if (IsHoleyElementsKind(elements_kind)) {\n    skip_call.emplace(&sub_builder, 2, {&var_length});\n    if (elements_kind == HOLEY_DOUBLE_ELEMENTS) {\n                                         │
│ sub_builder.GotoIfTrue<BranchIfFloat64IsHole>(&*skip_call, {element});\n    } else {\n      sub_builder.GotoIfTrue<BranchIfRootConstant>(&*skip_call, {element},\n                                           │
│ RootIndex::kTheHoleValue);\n    }\n  }\n\n  // Make the callback call under lazy deopt continuation to preserve stack\n  ReduceResult result;\n  {\n    DeoptFrameScope lazy_deopt_scope(\n        this,     │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n        base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                                    next_index_int32, original_length}));\n\n   │
│ CallArguments call_args =\n        args.count() < 2\n            ? CallArguments(ConvertReceiverMode::kNullOrUndefined,\n                            {element, index_tagged, receiver})\n            :       │
│ CallArguments(ConvertReceiverMode::kAny,\n                            {this_arg, element, index_tagged, receiver});\n    SaveCallSpeculationScope saved(this);\n    result = ReduceCall(callback, call_args, │
│ saved.value());\n  }\n\n  // After call, re-check maps if unstable and array length for deopt\n  if (!result.IsDoneWithAbort()) {\n    bool recheck_maps_after_call = receiver_maps_were_unstable;\n    if   │
│ (recheck_maps_after_call) {\n      if (auto receiver_info_after_call = known_node_aspects().TryGetInfoFor(receiver)) {\n        if (receiver_info_after_call->possible_maps_are_known()) {\n                 │
│ recheck_maps_after_call = !receiver_maps_before_loop.contains(\n              receiver_info_after_call->possible_maps());\n        }\n      }\n    }\n\n    DeoptFrameScope eager_deopt_scope(\n             │
│ this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,\n        base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                                    next_index_int32,                    │
│ original_length}));\n\n    if (recheck_maps_after_call) {\n      bool emit_check_with_migration = std::any_of(\n          receiver_maps_before_loop.begin(), receiver_maps_before_loop.end(),\n              │
│ [](compiler::MapRef map) { return map.is_migration_target(); });\n      if (emit_check_with_migration) {\n        AddNewNode<CheckMapsWithMigration>({receiver}, receiver_maps_before_loop,\n                │
│ CheckType::kOmitHeapObjectCheck);\n      } else {\n        AddNewNode<CheckMaps>({receiver}, receiver_maps_before_loop,\n                              CheckType::kOmitHeapObjectCheck);\n      }\n    }\n\n │
│ // Deopt if Array length changed\n    ValueNode* current_length = BuildLoadJSArrayLength(receiver);\n    sub_builder.set(var_length, current_length);\n    if (current_length != original_length) {\n        │
│ RETURN_IF_ABORT(TryBuildCheckInt32Condition(original_length_int32, current_length,\n                                      AssertCondition::kUnsignedLessThanEqual,\n                                         │
│ DeoptimizeReason::kArrayLengthChanged));\n    }\n  }\n\n  if (skip_call.has_value()) { sub_builder.GotoOrTrim(&*skip_call); sub_builder.Bind(&*skip_call); }\n\n  sub_builder.set(var_index,                 │
│ next_index_int32);\n  sub_builder.EndLoop(&loop_header);\n  sub_builder.Bind(&loop_end);\n\n  return GetRootConstant(RootIndex::kUndefinedValue);\n}\n\n// Helpers referenced in TryReduceArrayForEach       │
│ region\nnamespace {\nbool CanInlineArrayIteratingBuiltin(compiler::JSHeapBroker* broker,\n                                    const PossibleMaps& maps,\n                                    ElementsKind*   │
│ kind_return) { ... }\n}\n\nValueNode* MaglevGraphBuilder::BuildLoadJSArrayLength(ValueNode* js_array, NodeType length_type=NodeType::kSmi);\nValueNode* MaglevGraphBuilder::BuildLoadElements(ValueNode*     │
│ object);\n// MaglevSubGraphBuilder, BranchIfInt32Compare, Int32IncrementWithOverflow,\n// BranchIfFloat64IsHole, CheckMaps, CheckMapsWithMigration, etc are defined in maglev-ir.h/.cc and used here.',      │
│ 'Context': ['maglev_foreach_deopt', 'maglev_foreach_loop', 'maglev_throw_if_not_callable', 'maglev_elements_kind', 'maglev_length_changed'], 'Explanation': 'This code block shows                           │
│ MaglevGraphBuilder::TryReduceArrayForEach lowering Array.prototype.forEach into a Maglev loop. It captures: (1) eligibility checks (fast array maps, NoElementsProtector), (2) throwing if callback is not   │
│ callable wrapped in a lazy deopt continuation to preserve stack, (3) constructing a subgraph loop with index and length phis and a BranchIfInt32Compare for loop condition, (4) index increment via          │
│ Int32IncrementWithOverflow under an eager deopt continuation, (5) element load specialized for FixedArray vs FixedDoubleArray and hole checks for HOLEY kinds, (6) making the callback call under a lazy     │
│ deopt continuation, (7) after-call map rechecks and length check using CheckInt32Condition with UnsignedLessThanEqual and deopt reason ArrayLengthChanged, and (8) subgraph loop plumbing. It references     │
│ helpers such as BuildLoadJSArrayLength, BuildLoadElements, and ElementsKind specialization.', 'FileLine': 'maglev/maglev-graph-builder.cc:8061-8345'}                                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_foreach_tryreduce to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_foreach_tryreduce.json
[Step 16: Duration 29.84 seconds| Input tokens: 976,869 | Output tokens: 14,399]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 17 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_foreach_deopt', 'Body': '// src/maglev/maglev-graph-builder.cc (excerpts for deopt scopes and continuations)\n// Registration of deopt        │
│ continuations in graph builder switch\n// case Builtin::kArrayForEachLoopLazyDeoptContinuation: ...\n// Within TryReduceArrayForEach:\n// 1) Wrap ThrowIfNotCallable in a lazy deopt continuation so thrown  │
│ TypeError has correct stack\nDeoptFrameScope lazy_deopt_scope(\n    this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n    base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n          │
│ GetSmiConstant(0), original_length}));\nAddNewNode<ThrowIfNotCallable>({callback});\n\n// 2) Eager deopt around index increment overflow and after-call checks\nDeoptFrameScope eager_deopt_scope(\n         │
│ this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,\n    base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                                index_int32,                                 │
│ original_length}));\nnext_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32});\n\n// 3) Lazy deopt for the callback invocation to preserve stack then ToBoolean etc as                       │
│ needed\nDeoptFrameScope lazy_deopt_scope(\n    this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n    base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                               │
│ next_index_int32, original_length}));\nReduceResult result = ReduceCall(callback, call_args, saved.value());\n\n// 4) Post-call eager deopt continuation ensures deopt can resume after loop check and map   │
│ checks\nDeoptFrameScope eager_deopt_scope(\n    this, Builtin::kArrayForEachLoopEagerDeoptContinuation, target,\n    base::VectorOf<ValueNode*>({receiver, callback, this_arg,\n                             │
│ next_index_int32, original_length}));\n// Recheck maps, check length, possibly deopt with reason kArrayLengthChanged.\n\n// The DeoptFrameScope type and its IsLazyDeoptContinuationFrame logic:\nclass      │
│ V8_NODISCARD MaglevGraphBuilder::DeoptFrameScope {\n public:\n  DeoptFrameScope(MaglevGraphBuilder* builder, Builtin continuation, ...);\n  ...\n  bool IsLazyDeoptContinuationFrame() const {\n    if       │
│ (data_.tag() != DeoptFrame::FrameType::kBuiltinContinuationFrame) return false;\n    switch (data_.get<...>().builtin_id) {\n      case Builtin::kGetIteratorWithFeedbackLazyDeoptContinuation:\n      case  │
│ Builtin::kCallIteratorWithFeedbackLazyDeoptContinuation:\n      case Builtin::kArrayForEachLoopLazyDeoptContinuation:\n      case Builtin::kGenericLazyDeoptContinuation:\n      case                        │
│ Builtin::kToBooleanLazyDeoptContinuation:\n        return true;\n      default:\n        return false;\n    }\n  }\n};\n\n// Torque builtins defining continuations used by Maglev ArrayForEach:\n//         │
│ src/builtins/array-foreach.tq\ntransitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(...)\n  return ArrayForEachLoopContinuation(...);\ntransitioning javascript builtin                   │
│ ArrayForEachLoopLazyDeoptContinuation(...)\n  return ArrayForEachLoopContinuation(...);\ntransitioning builtin ArrayForEachLoopContinuation(... for k from initialK; k < len; k++)\n  ...\n', 'Context':     │
│ ['maglev_foreach_tryreduce', 'torque_array_foreach', 'maglev_deopt_scope_class', 'maglev_throw_if_not_callable'], 'Explanation': 'This item documents how Maglev wires deopt continuation frames for the     │
│ inlined Array.prototype.forEach loop. It shows DeoptFrameScope use sites: lazy continuations around ThrowIfNotCallable and callback call, and eager continuations around the index increment and post-call   │
│ checks. It also records DeoptFrameScope::IsLazyDeoptContinuationFrame and ties them to the Torque-defined ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation builtins in       │
│ builtins/array-foreach.tq.', 'FileLine': 'maglev/maglev-graph-builder.cc:8127-8345; maglev/maglev-graph-builder.cc:339-427; builtins/array-foreach.tq:6-40,94-140'}                                          │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_foreach_deopt to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_foreach_deopt.json
[Step 17: Duration 15.00 seconds| Input tokens: 1,189,208 | Output tokens: 15,388]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 18 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_throw_if_not_callable', 'Body': '// ThrowIfNotCallable node definition and codegen\n// src/maglev/maglev-ir.h\nclass ThrowIfNotCallable :     │
│ public FixedInputNodeT<1, ThrowIfNotCallable> {\n public:\n  static constexpr OpProperties kProperties = OpProperties::CanThrow() | OpProperties::DeferredCall();\n  Input& value() { return Node::input(0); │
│ }\n  int MaxCallStackArgs() const;\n  void SetValueLocationConstraints();\n  void GenerateCode(MaglevAssembler*, const ProcessingState&);\n};\n\n// src/maglev/maglev-ir.cc (codegen)\nint                   │
│ ThrowIfNotCallable::MaxCallStackArgs() const { return 1; }\nvoid ThrowIfNotCallable::SetValueLocationConstraints() {\n  UseRegister(value());\n  set_temporaries_needed(1);\n}\nvoid                         │
│ ThrowIfNotCallable::GenerateCode(MaglevAssembler* masm,\n                                      const ProcessingState& state) {\n  Label* if_not_callable = __ MakeDeferredCode(\n      [](MaglevAssembler*   │
│ masm, ThrowIfNotCallable* node) {\n        __ Push(node->value());\n        __ Move(kContextRegister, masm->native_context().object());\n        __ CallRuntime(Runtime::kThrowCalledNonCallable, 1);\n      │
│ masm->DefineExceptionHandlerAndLazyDeoptPoint(node);\n        __ Abort(AbortReason::kUnexpectedReturnFromThrow);\n      },\n      this);\n  Register value_reg = ToRegister(value());\n                      │
│ MaglevAssembler::TemporaryRegisterScope temps(masm);\n  Register scratch = temps.Acquire();\n  __ JumpIfNotCallable(value_reg, scratch, CheckType::kCheckHeapObject,\n                                       │
│ if_not_callable);\n}\n\n// Maglev graph builder uses ThrowIfNotCallable within a lazy deopt scope for Array.prototype.forEach\n// src/maglev/maglev-graph-builder.cc\n// EnsureType(callback,                │
│ NodeType::kCallable, [&](NodeType old_type) {\n//   DeoptFrameScope lazy_deopt_scope(\n//     this, Builtin::kArrayForEachLoopLazyDeoptContinuation, target,\n//     base::VectorOf<ValueNode*>({receiver,   │
│ callback, this_arg,\n//                                 GetSmiConstant(0), original_length}));\n//   AddNewNode<ThrowIfNotCallable>({callback});\n// });', 'Context': ['maglev_foreach_tryreduce',           │
│ 'maglev_foreach_deopt'], 'Explanation': 'Defines the ThrowIfNotCallable IR node with codegen that emits a runtime throw when the input is not callable. In MaglevGraphBuilder::TryReduceArrayForEach, the    │
│ callback argument is validated by creating a ThrowIfNotCallable under a lazy deopt continuation so that exceptions get the proper JavaScript stack. This integrates with Maglev’s exception/lazy deopt       │
│ handling for the forEach loop lowering.', 'FileLine': 'maglev/maglev-ir.h:9739-9754; maglev/maglev-ir.cc:5424-5440; maglev/maglev-graph-builder.cc:8127-8135'}                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_throw_if_not_callable to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_throw_if_not_callable.json
[Step 18: Duration 13.36 seconds| Input tokens: 1,402,589 | Output tokens: 16,088]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 19 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_elements_kind', 'Body': '// Specialization bits used by Array.prototype.forEach lowering for elements kinds, hole checks, and loads\n//       │
│ src/maglev/maglev-graph-builder.cc (within TryReduceArrayForEach)\nElementsKind elements_kind;\nif (!CanInlineArrayIteratingBuiltin(broker(), node_info->possible_maps(), &elements_kind)) {\n  return       │
│ ReduceResult::Fail();\n}\nif (!broker()->dependencies()->DependOnNoElementsProtector()) return ReduceResult::Fail();\n...\nValueNode* elements = BuildLoadElements(receiver);\nValueNode* element;\nif       │
│ (IsDoubleElementsKind(elements_kind)) {\n  element = BuildLoadFixedDoubleArrayElement(elements, index_int32);\n} else {\n  element = BuildLoadFixedArrayElement(elements,                                    │
│ index_int32);\n}\nstd::optional<MaglevSubGraphBuilder::Label> skip_call;\nif (IsHoleyElementsKind(elements_kind)) {\n  skip_call.emplace(&sub_builder, 2, {&var_length});\n  if (elements_kind ==            │
│ HOLEY_DOUBLE_ELEMENTS) {\n    sub_builder.GotoIfTrue<BranchIfFloat64IsHole>(&*skip_call, {element});\n  } else {\n    sub_builder.GotoIfTrue<BranchIfRootConstant>(&*skip_call, {element},                   │
│ RootIndex::kTheHoleValue);\n  }\n}\n\n// Under the hood, these helpers are called/available elsewhere:\n// BuildLoadElements(object) -> loads JSObject::kElementsOffset\n// BuildLoadFixedArrayElement /     │
│ BuildLoadFixedDoubleArrayElement\n// BuildLoadHoleyFixedDoubleArrayElement + ConvertHoleToUndefined / CheckNotHole used in other array accesses\n\n// NoElementsProtector dependency\n//                     │
│ src/execution/protectors.h is referenced via broker()->dependencies()->DependOnNoElementsProtector()\n\n// Map checks and transitions helpers used across arrays and element ICs\n// - BuildCheckMaps,       │
│ CheckMaps, CheckMapsWithMigration\n// - BuildTransitionElementsKindOrCheckMap\n', 'Context': ['maglev_foreach_tryreduce', 'maglev_foreach_deopt'], 'Explanation': 'Captures elements kind specialization for │
│ forEach lowering: it gates inlining on CanInlineArrayIteratingBuiltin and NoElementsProtector; loads elements backing store; uses FixedArray vs FixedDoubleArray loads; emits hole checks for HOLEY kinds    │
│ and skips callbacks on holes. It also references reusable helpers and protector dependency logic for correctness of fast-element assumptions.', 'FileLine': 'maglev/maglev-graph-builder.cc:8089-8219;       │
│ 6001-6200 (helper BuildLoadElements)'}                                                                                                                                                                       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_elements_kind to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_elements_kind.json
[Step 19: Duration 9.82 seconds| Input tokens: 1,616,723 | Output tokens: 16,665]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 20 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_length_changed', 'Body': '// Deopt reason and check site for array length change in forEach loop\n// src/maglev/maglev-graph-builder.cc       │
│ (within TryReduceArrayForEach)\n// After callback, re-load length and compare to original; deopt if changed.\nValueNode* current_length = BuildLoadJSArrayLength(receiver);\nsub_builder.set(var_length,     │
│ current_length);\nif (current_length != original_length) {\n  RETURN_IF_ABORT(\n      TryBuildCheckInt32Condition(original_length_int32, current_length,\n                                                   │
│ AssertCondition::kUnsignedLessThanEqual,\n                                  DeoptimizeReason::kArrayLengthChanged));\n}\n\n// CheckInt32Condition IR node emits a deopt on mismatch with given reason\n//    │
│ src/maglev/maglev-ir.cc\nvoid CheckInt32Condition::GenerateCode(MaglevAssembler* masm,\n                                       const ProcessingState& state) {\n  Label* fail = __ GetDeoptLabel(this,       │
│ reason());\n  __ CompareInt32AndJumpIf(ToRegister(left_input()), ToRegister(right_input()),\n                           NegateCondition(ToCondition(condition())), fail);\n}\n\n// ArrayLengthChanged reason │
│ is declared in src/deoptimizer/deoptimize-reason.h\n// and propagated into codegen deopt labels.\n', 'Context': ['maglev_foreach_tryreduce'], 'Explanation': 'Highlights the exact site in Maglev’s          │
│ Array.prototype.forEach lowering where it enforces that the array length hasn’t changed during iteration. It re-reads the JSArray length and emits a CheckInt32Condition with                                │
│ AssertCondition::kUnsignedLessThanEqual and DeoptimizeReason::kArrayLengthChanged, ensuring correctness if the callback mutates the length.', 'FileLine': 'maglev/maglev-graph-builder.cc:8306-8320;         │
│ maglev/maglev-ir.cc:... CheckInt32Condition::GenerateCode'}                                                                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_length_changed to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_length_changed.json
[Step 20: Duration 6.81 seconds| Input tokens: 1,831,483 | Output tokens: 17,083]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 21 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_foreach_loop', 'Body': '// MaglevSubGraphBuilder loop construction, conditions, and arithmetic used by forEach lowering\n//                   │
│ src/maglev/maglev-graph-builder.cc (selected helpers and snippets)\n// MaglevSubGraphBuilder::BeginLoop, EndLoop, labels, and branching helpers:\nclass MaglevGraphBuilder::MaglevSubGraphBuilder {\n        │
│ LoopLabel BeginLoop(std::initializer_list<Variable*> loop_vars);\n  void EndLoop(LoopLabel* loop_label);\n  void GotoIfFalse<BranchIfInt32Compare>(Label* false_target, {lhs, rhs}, Operation op);\n  void   │
│ GotoIfTrue<BranchIfFloat64IsHole>(Label* true_target, {value});\n  void GotoIfTrue<BranchIfRootConstant>(Label* true_target, {value}, RootIndex);\n  // Label, Variable plumbing; Select/Branch combinators, │
│ etc.\n};\n\n// Integer comparison branch used for the loop guard (index < length)\nclass BranchIfInt32Compare : public BranchControlNodeT<2, BranchIfInt32Compare> {\n  Operation operation() const; // e.g. │
│ kLessThan\n  void GenerateCode(...); // emits CompareInt32 and branches\n};\n\n// Index arithmetic node with overflow trapping (eager deopt continuation guards)\nclass Int32IncrementWithOverflow : public  │
│ ValueNodeT<Int32IncrementWithOverflow> { ... };\n\n// Example usage in forEach lowering (simplified):\nsub_builder.set(var_index, GetSmiConstant(0));\nsub_builder.set(var_length,                           │
│ original_length);\nMaglevSubGraphBuilder::LoopLabel loop_header =\n    sub_builder.BeginLoop({&var_index, &var_length});\nPhi* index_tagged =                                                                │
│ sub_builder.get(var_index)->Cast<Phi>();\nEnsureType(index_tagged, NodeType::kSmi);\nValueNode* index_int32 = GetInt32(index_tagged);\nsub_builder.GotoIfFalse<BranchIfInt32Compare>(&loop_end,\n            │
│ {index_int32, original_length_int32}, Operation::kLessThan);\nValueNode* next_index_int32 = AddNewNode<Int32IncrementWithOverflow>({index_int32});\nsub_builder.set(var_index,                               │
│ next_index_int32);\nsub_builder.EndLoop(&loop_header);\n\n// UnsafeSmiUntag helper used by other paths (array push/pop) and by generic untag\nValueNode* MaglevGraphBuilder::BuildSmiUntag(ValueNode* node)  │
│ {\n  if (EnsureType(node, NodeType::kSmi)) return AddNewNode<UnsafeSmiUntag>({node});\n  else return AddNewNode<CheckedSmiUntag>({node});\n}\n\n// CheckInt32Condition node used for deopt checks, such as   │
│ ArrayLengthChanged\nclass CheckInt32Condition : public FixedInputNodeT<2, CheckInt32Condition> {\n  AssertCondition condition() const; // e.g., kUnsignedLessThanEqual\n  DeoptimizeReason reason() const;   │
│ // e.g., kArrayLengthChanged\n  void GenerateCode(...);            // emits CompareInt32 and deopt on fail\n};\n', 'Context': ['maglev_foreach_tryreduce', 'maglev_length_changed'], 'Explanation': 'This    │
│ RAG item collects the loop construction mechanics used by Maglev for lowering Array.prototype.forEach: sub-graph loop API, label/phi handling, the index guard using BranchIfInt32Compare, arithmetic with   │
│ Int32IncrementWithOverflow, and the CheckInt32Condition node used for post-call deopt validations. It references BuildSmiUntag as part of integer handling and ties to the previously stored forEach         │
│ lowering and ArrayLengthChanged entries.', 'FileLine': 'maglev/maglev-graph-builder.cc: (BeginLoop) ~ lines 3150-3270; TryReduceArrayForEach: ~8061-8345; maglev/maglev-ir.h: Branch/Int32/Check classes'}   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_foreach_loop to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_foreach_loop.json
[Step 21: Duration 15.00 seconds| Input tokens: 2,046,710 | Output tokens: 17,960]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 22 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'read_rag_db_id' with arguments: {'id': 'ID:v8_maglev_foreach_map'}                                                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: {}
[Step 22: Duration 4.56 seconds| Input tokens: 2,262,865 | Output tokens: 17,987]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 23 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'torque_array_foreach', 'Body': '// src/builtins/array-foreach.tq (key parts)\nnamespace array {\ntransitioning javascript builtin                    │
│ ArrayForEachLoopEagerDeoptContinuation(\n    js-implicit context: NativeContext, receiver: JSAny)(callback: JSAny,\n    thisArg: JSAny, initialK: JSAny, length: JSAny): JSAny {\n  const jsreceiver =       │
│ Cast<JSReceiver>(receiver) otherwise unreachable;\n  const callbackfn = Cast<Callable>(callback) otherwise unreachable;\n  const numberK = Cast<Number>(initialK) otherwise unreachable;\n  const            │
│ numberLength = Cast<Number>(length) otherwise unreachable;\n  return ArrayForEachLoopContinuation(jsreceiver, callbackfn, thisArg,\n      Undefined, jsreceiver, numberK, numberLength,                      │
│ Undefined);\n}\n\ntransitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(\n    js-implicit context: NativeContext, receiver: JSAny)(callback: JSAny,\n    thisArg: JSAny, initialK: JSAny,   │
│ length: JSAny, _result: JSAny): JSAny {\n  const jsreceiver = Cast<JSReceiver>(receiver) otherwise unreachable;\n  const callbackfn = Cast<Callable>(callback) otherwise unreachable;\n  const numberK =     │
│ Cast<Number>(initialK) otherwise unreachable;\n  const numberLength = Cast<Number>(length) otherwise unreachable;\n  return ArrayForEachLoopContinuation(jsreceiver, callbackfn, thisArg,\n      Undefined,  │
│ jsreceiver, numberK, numberLength, Undefined);\n}\n\ntransitioning builtin ArrayForEachLoopContinuation(\n    implicit context: Context)(_receiver: JSReceiver, callbackfn: Callable,\n    thisArg: JSAny,   │
│ _array: JSAny, o: JSReceiver, initialK: Number, len: Number,\n    _to: JSAny): JSAny {\n  for (let k: Number = initialK; k < len; k = k + 1) {\n    const kPresent: Boolean = HasProperty_Inline(o, k);\n    │
│ if (kPresent == True) {\n      const kValue: JSAny = GetProperty(o, k);\n      Call(context, callbackfn, thisArg, kValue, k, o);\n    }\n  }\n  return Undefined;\n}\n\ntransitioning macro                  │
│ FastArrayForEach(implicit context: Context)(o: JSReceiver,\n    len: Number, callbackfn: Callable, thisArg: JSAny): JSAny\n    labels Bailout(Smi) {\n  let k: Smi = 0;\n  const smiLen = Cast<Smi>(len)     │
│ otherwise goto Bailout(k);\n  const fastO = Cast<FastJSArray>(o) otherwise goto Bailout(k);\n  let fastOW = NewFastJSArrayWitness(fastO);\n  for (; k < smiLen; k++) {\n    fastOW.Recheck() otherwise goto  │
│ Bailout(k);\n    if (k >= fastOW.Get().length) goto Bailout(k);\n    const value: JSAny = fastOW.LoadElementNoHole(k) otherwise continue;\n    Call(context, callbackfn, thisArg, value, k, fastOW.Get());\n │
│ }\n  return Undefined;\n}\n\ntransitioning javascript builtin ArrayForEach(...)\n  try { ... try FastArrayForEach(o, len, callbackfn, thisArg) otherwise Bailout; ...\n         return                       │
│ ArrayForEachLoopContinuation(o, callbackfn, thisArg, Undefined, o, k, len, Undefined);\n  } label TypeError deferred { ThrowCalledNonCallable(arguments[0]); }\n', 'Context': ['maglev_foreach_deopt',       │
│ 'maglev_foreach_tryreduce'], 'Explanation': 'Torque builtins define the deopt continuation entrypoints used by Maglev for Array.prototype.forEach. Eager and Lazy deopt continuations both tail into         │
│ ArrayForEachLoopContinuation, which runs a canonical indexed loop over [0, len), calling the callback for present indices. Maglev DeoptFrameScopes reference                                                 │
│ Builtin::kArrayForEachLoop(Eager|Lazy)DeoptContinuation to reconstruct state and resume in Torque in case of deopt or exception.', 'FileLine': 'builtins/array-foreach.tq:6-40,94-167'}                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote torque_array_foreach to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/torque_array_foreach.json
[Step 23: Duration 37.74 seconds| Input tokens: 2,479,062 | Output tokens: 20,021]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 24 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'write_rag_db_id' with arguments: {'id': 'maglev_trycatch_handlers', 'Body': '// Try/catch wiring in Maglev: CreateCatchContext and SetPendingMessage\n//                                      │
│ src/maglev/maglev-graph-builder.cc\nvoid MaglevGraphBuilder::VisitCreateCatchContext() {\n  // CreateCatchContext <exception> <scope_info_idx>\n  ValueNode* exception = LoadRegister(0);\n                  │
│ compiler::ScopeInfoRef scope_info = GetRefOperand<ScopeInfo>(1);\n  auto context = CreateContext(\n      broker()->target_native_context().catch_context_map(broker()),\n                                    │
│ Context::MIN_CONTEXT_EXTENDED_SLOTS, scope_info, GetContext(), exception);\n  SetAccumulator(BuildInlinedAllocation(context, AllocationType::kYoung));\n  graph()->record_scope_info(GetAccumulator(),       │
│ scope_info);\n  ClearCurrentAllocationBlock();\n}\n...\nvoid MaglevGraphBuilder::VisitSetPendingMessage() {\n  ValueNode* message = GetAccumulator();\n                                                      │
│ SetAccumulator(AddNewNode<SetPendingMessage>({message}));\n}\n\n// Node for SetPendingMessage\n// src/maglev/maglev-ir.h\nclass SetPendingMessage : public FixedInputValueNodeT<1, SetPendingMessage> {\n    │
│ public:\n  using Base = FixedInputValueNodeT<1, SetPendingMessage>;\n  explicit SetPendingMessage(uint64_t bitfield) : Base(bitfield) {}\n  // kProperties, InputTypes omitted\n  void                       │
│ SetValueLocationConstraints();\n  void GenerateCode(MaglevAssembler* masm, const ProcessingState& state);\n};\n\n// src/maglev/maglev-ir.cc\nvoid SetPendingMessage::SetValueLocationConstraints() { /*      │
│ UseRegister, DefineAsReg */ }\nvoid SetPendingMessage::GenerateCode(MaglevAssembler* masm,\n                                     const ProcessingState& state) {\n  // Implementation updates the VM pending │
│ message slot; codegen sites often call\n  // masm->DefineExceptionHandlerAndLazyDeoptPoint for nodes that can throw or need handlers.\n}\n\n// Related bytecode producers in the interpreter (for            │
│ context)\n// interpreter/bytecode-array-builder.cc: BytecodeArrayBuilder::SetPendingMessage()\n// interpreter/interpreter-generator.cc: IGNITION_HANDLER(SetPendingMessage, ...)\n', 'Context':              │
│ ['maglev_throw_if_not_callable', 'maglev_foreach_deopt'], 'Explanation': 'Shows how Maglev builds catch contexts and wires exception message state. The CreateCatchContext bytecode allocates a catch        │
│ context object inlined in Maglev and records scope info. The SetPendingMessage bytecode is lowered to a SetPendingMessage IR node whose codegen updates the isolate’s pending message slot, integrating with │
│ exception handling; various Maglev node codegens define exception handler and lazy deopt points to integrate with handlers.', 'FileLine': 'maglev/maglev-graph-builder.cc:12733-12744;13761-13765;           │
│ maglev/maglev-ir.h:4243-4249; maglev/maglev-ir.cc:6466-6474'}                                                                                                                                                │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: OK: wrote maglev_trycatch_handlers to /Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rag_db/maglev_trycatch_handlers.json
[Step 24: Duration 47.99 seconds| Input tokens: 2,696,249 | Output tokens: 21,269]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 25 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'final_answer' with arguments: {'answer': '### 1. Task outcome (short version):\nMapped the full Maglev path that recognizes and lowers Array.prototype.forEach into a fast index loop,        │
│ including elements-kinds specialization, map/protector guards, and hole handling; identified the deopt continuation builtins and how Maglev wires lazy/eager deopt frames; pinpointed ThrowIfNotCallable and │
│ exception plumbing (CreateCatchContext, SetPendingMessage); and collected the exact IR nodes for representation/untagging and loop arithmetic, plus the deopt reason for ArrayLengthChanged. All canonical   │
│ file paths/symbols are listed, and RAG IDs are provided to fetch the concrete code blocks.\n\n### 2. Task outcome (extremely detailed version):\n\nA) Recognition and loop lowering in Maglev                │
│ (Array.prototype.forEach):\n- Core recognition and lowering:\n  - File: src/maglev/maglev-graph-builder.cc\n  - Symbol: v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach\n  - What it does:\n │
│ - Checks CanSpeculateCall, validates receiver has known maps, and uses CanInlineArrayIteratingBuiltin to confirm fast-array iteration across the map set; records a dependency on the NoElementsProtector.\n │
│ - Validates the callback using ThrowIfNotCallable wrapped under a lazy deopt continuation.\n    - Builds an index loop with MaglevSubGraphBuilder over [0, original_length), using:\n      -                 │
│ BranchIfInt32Compare for the loop guard index < original_length.\n      - Int32IncrementWithOverflow (under eager deopt continuation) for i++.\n      - BuildLoadElements(receiver) then element load        │
│ specialized by elements kind:\n        - FixedArray vs FixedDoubleArray (BuildLoadFixedArrayElement / BuildLoadFixedDoubleArrayElement).\n      - For HOLEY kinds, hole checks:\n        - HOLEY_DOUBLE:     │
│ BranchIfFloat64IsHole; HOLEY (tagged): BranchIfRootConstant(TheHoleValue) and skip the callback.\n      - Invokes the callback under a lazy deopt continuation so exceptions/stack are correct.\n      -     │
│ After the call, re-check maps if receiver maps were unstable and ensure array length didn’t change (kArrayLengthChanged) using CheckInt32Condition with UnsignedLessThanEqual.\n    - Returns undefined (per │
│ spec) when done.\n  - Helper calls used by that path:\n    - BuildLoadJSArrayLength(receiver)\n    - BuildLoadElements(receiver)\n    - MaglevSubGraphBuilder::BeginLoop/EndLoop and the branch helpers\n  - │
│ RAG: maglev_foreach_tryreduce, maglev_foreach_loop, maglev_elements_kind, maglev_length_changed\n\n- Helpers and utilities (same file):\n  - CanInlineArrayIteratingBuiltin (static helper in an unnamed     │
│ namespace): tests maps support for fast array iteration and computes a unified ElementsKind across the observed maps.\n  - BuildLoadJSArrayLength(receiver): loads JSArray::length and records known         │
│ property cache entry.\n  - BuildLoadElements(receiver): loads JSObject::elements.\n  - MaglevSubGraphBuilder: local control-flow/phi framework to build structured loops/branches used in lowering.\n\nB)    │
│ Deopt continuation builtins and DeoptFrameScope wiring:\n- Maglev deopt frame mechanism:\n  - File: src/maglev/maglev-graph-builder.cc\n  - Symbol: class                                                    │
│ v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope\n    - Provides builtins continuation frames and marks certain ones as “lazy deopt frames”.\n    - IsLazyDeoptContinuationFrame() recognizes       │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation (among other lazy continuations).\n  - In TryReduceArrayForEach, DeoptFrameScope is created around:\n    - ThrowIfNotCallable: lazy continuation             │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation.\n    - Index increment overflow check: eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.\n    - Callback call: lazy continuation         │
│ Builtin::kArrayForEachLoopLazyDeoptContinuation.\n    - Post-call recheck (maps/length): eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.\n  - RAG: maglev_foreach_deopt\n\n- Torque     │
│ builtins for continuations (ArrayForEachLoopEager/LazyDeoptContinuation):\n  - File: src/builtins/array-foreach.tq\n  - Symbols:\n    - transitioning javascript builtin                                     │
│ ArrayForEachLoopEagerDeoptContinuation(...)\n    - transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(...)\n    - transitioning builtin ArrayForEachLoopContinuation(...)\n  - Both      │
│ eager and lazy continuations tail-call ArrayForEachLoopContinuation, which performs the canonical JS forEach index loop.\n  - RAG: torque_array_foreach\n\nC) ThrowIfNotCallable and exception wiring        │
│ (try/catch/plumbing):\n- ThrowIfNotCallable node:\n  - Files: src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc\n  - Symbols: v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints,          │
│ GenerateCode}\n  - GenerateCode emits a deferred path to Runtime::kThrowCalledNonCallable and marks an exception handler + lazy deopt point.\n  - Used in TryReduceArrayForEach under a lazy DeoptFrameScope │
│ to maintain correct JS stack upon exception.\n  - RAG: maglev_throw_if_not_callable, maglev_foreach_deopt\n\n- Exception edge/handler locations and catch contexts:\n  - File:                               │
│ src/maglev/maglev-graph-builder.cc\n  - Create catch contexts:\n    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext constructs the catch context object inline and records scope info.\n │
│ - Pending message plumbing:\n    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage creates a SetPendingMessage node, reflected by:\n      - Node: v8::internal::maglev::SetPendingMessage   │
│ in maglev-ir.h/maglev-ir.cc.\n  - Many Maglev nodes that call builtins/runtime mark exception handlers and lazy deopt points (e.g., ThrowIfNotCallable, calls, some builtins), ensuring proper exception     │
│ routing to handlers.\n  - RAG: maglev_trycatch_handlers\n\nD) Representation/untagging and loop arithmetic nodes; ArrayLengthChanged deopt reason:\n- Untagging and representation helpers:\n  - File:       │
│ src/maglev/maglev-graph-builder.cc\n  - Symbol: v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag\n    - Emits UnsafeSmiUntag (fast path) or CheckedSmiUntag.\n- Loop arithmetic and comparisons:\n  - │
│ Nodes in src/maglev/maglev-ir.h/.cc:\n    - v8::internal::maglev::Int32IncrementWithOverflow (used for i++)\n    - v8::internal::maglev::BranchIfInt32Compare (loop guard)\n    -                            │
│ v8::internal::maglev::CheckInt32Condition (used for post-call length check)\n- Array length deopt reason:\n  - In TryReduceArrayForEach: after callback, reload current_length and emit                      │
│ TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, DeoptimizeReason::kArrayLengthChanged).\n  - CheckInt32Condition::GenerateCode deopts on         │
│ mismatch using the provided reason.\n  - RAG: maglev_length_changed, maglev_foreach_loop\n\nE) Elements kinds specialization, map checks, protectors, hole checks, and concrete element loads:\n- Elements   │
│ kinds and protector dependency (NoElementsProtector):\n  - TryReduceArrayForEach uses CanInlineArrayIteratingBuiltin to ensure maps support fast iteration and unifies kind; it then depends on              │
│ NoElementsProtector (broker()->dependencies()->DependOnNoElementsProtector()) to assume standard prototype semantics.\n- Element loads:\n  - For PACKED kinds: BuildLoadFixedArrayElement\n  - For           │
│ PACKED_DOUBLE kinds: BuildLoadFixedDoubleArrayElement\n  - For HOLEY kinds: hole checks skip callback\n    - HOLEY_DOUBLE: BranchIfFloat64IsHole\n    - HOLEY (tagged):                                      │
│ BranchIfRootConstant(TheHoleValue)\n- Map checks and transitions used elsewhere in array element paths:\n  - CheckMaps / CheckMapsWithMigration and BuildTransitionElementsKindOrCheckMap support            │
│ correctness across transitions.\n- RAG: maglev_elements_kind, maglev_foreach_tryreduce\n\nF) Canonical file paths and symbol names (organized per topic):\n- Lowering/loop building:\n  -                    │
│ src/maglev/maglev-graph-builder.cc\n    - v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach\n    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadJSArrayLength\n    -                     │
│ v8::internal::maglev::MaglevGraphBuilder::BuildLoadElements\n    - v8::internal::maglev::MaglevGraphBuilder::MaglevSubGraphBuilder::{BeginLoop, EndLoop, Branch/Goto helpers}\n    -                         │
│ v8::internal::maglev::MaglevGraphBuilder::TryBuildCheckInt32Condition (used for deopt checks)\n- Deopt scopes/continuations:\n  - src/maglev/maglev-graph-builder.cc\n    -                                  │
│ v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope\n  - src/builtins/array-foreach.tq\n    - ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation,                           │
│ ArrayForEachLoopContinuation\n- ThrowIfNotCallable and exception plumbing:\n  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc\n    -                                                                       │
│ v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}\n  - src/maglev/maglev-graph-builder.cc\n    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext\n     │
│ - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage\n  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc\n    - v8::internal::maglev::SetPendingMessage::{SetValueLocationConstraints,        │
│ GenerateCode}\n- Representation and loop arithmetic:\n  - src/maglev/maglev-graph-builder.cc\n    - v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag (calls UnsafeSmiUntag / CheckedSmiUntag)\n  -    │
│ src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc\n    - v8::internal::maglev::Int32IncrementWithOverflow\n    - v8::internal::maglev::BranchIfInt32Compare\n    - v8::internal::maglev::CheckInt32Condition   │
│ (with DeoptimizeReason::kArrayLengthChanged)\n- Elements specialization and loads:\n  - src/maglev/maglev-graph-builder.cc\n    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedArrayElement\n     │
│ - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedDoubleArrayElement\n    - Conditionals using BranchIfFloat64IsHole and BranchIfRootConstant for hole checks\n  - protector dependency accessed via │
│ broker->dependencies()->DependOnNoElementsProtector (protector is defined in execution/protectors, but the callsites are in Maglev builder)\n\nG) Why this fast path is safe:\n- Map set is validated        │
│ (monomorphic/polymorphic checks) and (if unstable) revalidated after callback, so in-loop assumptions remain correct.\n- NoElementsProtector keeps prototype chain assumptions valid (e.g., no custom        │
│ getters for holes during iteration for HOLEY behavior).\n- Post-call array length check ensures we deopt if user code mutates length in a way that breaks the index traversal contract.\n- Hole checks       │
│ guarantee skip semantics match spec for missing indices.\n\n### 3. Additional context (if relevant):\n- There is also a TF/JSCallReducer path referencing the same continuations                             │
│ (compiler/js-call-reducer.cc), but for this task the Maglev-specific builder is primary.\n- TryReduceArrayIteratorPrototypeNext implements a similar style of fast path for array iterators, demonstrating   │
│ the same elements-kinds specialization/hole behavior patterns.\n\nReturned RAG DB IDs (with short explanations):\n- maglev_foreach_tryreduce — Full TryReduceArrayForEach lowering: loop build,              │
│ elements-kind specialization, lazy/eager deopt scopes, hole checks, callback call.\n- maglev_foreach_deopt — Where DeoptFrameScope is constructed and how ArrayForEachLoop(Eager|Lazy)DeoptContinuation are  │
│ referenced; includes DeoptFrameScope::IsLazyDeoptContinuationFrame and usage sites in forEach.\n- torque_array_foreach — Torque definitions of ArrayForEachLoopEagerDeoptContinuation,                       │
│ ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation (the interpreter-continuation loop).\n- maglev_throw_if_not_callable — IR node definition/codegen for ThrowIfNotCallable and its use │
│ under lazy deopt scope in TryReduceArrayForEach.\n- maglev_elements_kind — Elements-kinds specialization, protector dependency, element loads, and hole checks used by forEach lowering.\n-                  │
│ maglev_length_changed — Post-callback length recheck and CheckInt32Condition with DeoptimizeReason::kArrayLengthChanged.\n- maglev_foreach_loop — MaglevSubGraphBuilder loop construction,                   │
│ BranchIfInt32Compare, Int32IncrementWithOverflow, CheckInt32Condition (selection of nodes used by forEach loop body).\n- maglev_trycatch_handlers — CreateCatchContext and SetPendingMessage in Maglev, plus │
│ SetPendingMessage node definition/codegen.\n'}                                                                                                                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: ### 1. Task outcome (short version):
Mapped the full Maglev path that recognizes and lowers Array.prototype.forEach into a fast index loop, including elements-kinds specialization, map/protector guards, and hole handling; identified the deopt 
continuation builtins and how Maglev wires lazy/eager deopt frames; pinpointed ThrowIfNotCallable and exception plumbing (CreateCatchContext, SetPendingMessage); and collected the exact IR nodes for 
representation/untagging and loop arithmetic, plus the deopt reason for ArrayLengthChanged. All canonical file paths/symbols are listed, and RAG IDs are provided to fetch the concrete code blocks.

### 2. Task outcome (extremely detailed version):

A) Recognition and loop lowering in Maglev (Array.prototype.forEach):
- Core recognition and lowering:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach
  - What it does:
    - Checks CanSpeculateCall, validates receiver has known maps, and uses CanInlineArrayIteratingBuiltin to confirm fast-array iteration across the map set; records a dependency on the NoElementsProtector.
    - Validates the callback using ThrowIfNotCallable wrapped under a lazy deopt continuation.
    - Builds an index loop with MaglevSubGraphBuilder over |0, original_length), using:
      - BranchIfInt32Compare for the loop guard index < original_length.
      - Int32IncrementWithOverflow (under eager deopt continuation) for i++.
      - BuildLoadElements(receiver) then element load specialized by elements kind:
        - FixedArray vs FixedDoubleArray (BuildLoadFixedArrayElement / BuildLoadFixedDoubleArrayElement).
      - For HOLEY kinds, hole checks:
        - HOLEY_DOUBLE: BranchIfFloat64IsHole; HOLEY (tagged): BranchIfRootConstant(TheHoleValue) and skip the callback.
      - Invokes the callback under a lazy deopt continuation so exceptions/stack are correct.
      - After the call, re-check maps if receiver maps were unstable and ensure array length didn’t change (kArrayLengthChanged) using CheckInt32Condition with UnsignedLessThanEqual.
    - Returns undefined (per spec) when done.
  - Helper calls used by that path:
    - BuildLoadJSArrayLength(receiver)
    - BuildLoadElements(receiver)
    - MaglevSubGraphBuilder::BeginLoop/EndLoop and the branch helpers
  - RAG: maglev_foreach_tryreduce, maglev_foreach_loop, maglev_elements_kind, maglev_length_changed

- Helpers and utilities (same file):
  - CanInlineArrayIteratingBuiltin (static helper in an unnamed namespace): tests maps support for fast array iteration and computes a unified ElementsKind across the observed maps.
  - BuildLoadJSArrayLength(receiver): loads JSArray::length and records known property cache entry.
  - BuildLoadElements(receiver): loads JSObject::elements.
  - MaglevSubGraphBuilder: local control-flow/phi framework to build structured loops/branches used in lowering.

B) Deopt continuation builtins and DeoptFrameScope wiring:
- Maglev deopt frame mechanism:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: class v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope
    - Provides builtins continuation frames and marks certain ones as “lazy deopt frames”.
    - IsLazyDeoptContinuationFrame() recognizes Builtin::kArrayForEachLoopLazyDeoptContinuation (among other lazy continuations).
  - In TryReduceArrayForEach, DeoptFrameScope is created around:
    - ThrowIfNotCallable: lazy continuation Builtin::kArrayForEachLoopLazyDeoptContinuation.
    - Index increment overflow check: eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.
    - Callback call: lazy continuation Builtin::kArrayForEachLoopLazyDeoptContinuation.
    - Post-call recheck (maps/length): eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.
  - RAG: maglev_foreach_deopt

- Torque builtins for continuations (ArrayForEachLoopEager/LazyDeoptContinuation):
  - File: src/builtins/array-foreach.tq
  - Symbols:
    - transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(...)
    - transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(...)
    - transitioning builtin ArrayForEachLoopContinuation(...)
  - Both eager and lazy continuations tail-call ArrayForEachLoopContinuation, which performs the canonical JS forEach index loop.
  - RAG: torque_array_foreach

C) ThrowIfNotCallable and exception wiring (try/catch/plumbing):
- ThrowIfNotCallable node:
  - Files: src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
  - Symbols: v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}
  - GenerateCode emits a deferred path to Runtime::kThrowCalledNonCallable and marks an exception handler + lazy deopt point.
  - Used in TryReduceArrayForEach under a lazy DeoptFrameScope to maintain correct JS stack upon exception.
  - RAG: maglev_throw_if_not_callable, maglev_foreach_deopt

- Exception edge/handler locations and catch contexts:
  - File: src/maglev/maglev-graph-builder.cc
  - Create catch contexts:
    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext constructs the catch context object inline and records scope info.
  - Pending message plumbing:
    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage creates a SetPendingMessage node, reflected by:
      - Node: v8::internal::maglev::SetPendingMessage in maglev-ir.h/maglev-ir.cc.
  - Many Maglev nodes that call builtins/runtime mark exception handlers and lazy deopt points (e.g., ThrowIfNotCallable, calls, some builtins), ensuring proper exception routing to handlers.
  - RAG: maglev_trycatch_handlers

D) Representation/untagging and loop arithmetic nodes; ArrayLengthChanged deopt reason:
- Untagging and representation helpers:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag
    - Emits UnsafeSmiUntag (fast path) or CheckedSmiUntag.
- Loop arithmetic and comparisons:
  - Nodes in src/maglev/maglev-ir.h/.cc:
    - v8::internal::maglev::Int32IncrementWithOverflow (used for i++)
    - v8::internal::maglev::BranchIfInt32Compare (loop guard)
    - v8::internal::maglev::CheckInt32Condition (used for post-call length check)
- Array length deopt reason:
  - In TryReduceArrayForEach: after callback, reload current_length and emit TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, 
DeoptimizeReason::kArrayLengthChanged).
  - CheckInt32Condition::GenerateCode deopts on mismatch using the provided reason.
  - RAG: maglev_length_changed, maglev_foreach_loop

E) Elements kinds specialization, map checks, protectors, hole checks, and concrete element loads:
- Elements kinds and protector dependency (NoElementsProtector):
  - TryReduceArrayForEach uses CanInlineArrayIteratingBuiltin to ensure maps support fast iteration and unifies kind; it then depends on NoElementsProtector 
(broker()->dependencies()->DependOnNoElementsProtector()) to assume standard prototype semantics.
- Element loads:
  - For PACKED kinds: BuildLoadFixedArrayElement
  - For PACKED_DOUBLE kinds: BuildLoadFixedDoubleArrayElement
  - For HOLEY kinds: hole checks skip callback
    - HOLEY_DOUBLE: BranchIfFloat64IsHole
    - HOLEY (tagged): BranchIfRootConstant(TheHoleValue)
- Map checks and transitions used elsewhere in array element paths:
  - CheckMaps / CheckMapsWithMigration and BuildTransitionElementsKindOrCheckMap support correctness across transitions.
- RAG: maglev_elements_kind, maglev_foreach_tryreduce

F) Canonical file paths and symbol names (organized per topic):
- Lowering/loop building:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadJSArrayLength
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadElements
    - v8::internal::maglev::MaglevGraphBuilder::MaglevSubGraphBuilder::{BeginLoop, EndLoop, Branch/Goto helpers}
    - v8::internal::maglev::MaglevGraphBuilder::TryBuildCheckInt32Condition (used for deopt checks)
- Deopt scopes/continuations:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope
  - src/builtins/array-foreach.tq
    - ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopContinuation
- ThrowIfNotCallable and exception plumbing:
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext
    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::SetPendingMessage::{SetValueLocationConstraints, GenerateCode}
- Representation and loop arithmetic:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag (calls UnsafeSmiUntag / CheckedSmiUntag)
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::Int32IncrementWithOverflow
    - v8::internal::maglev::BranchIfInt32Compare
    - v8::internal::maglev::CheckInt32Condition (with DeoptimizeReason::kArrayLengthChanged)
- Elements specialization and loads:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedArrayElement
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedDoubleArrayElement
    - Conditionals using BranchIfFloat64IsHole and BranchIfRootConstant for hole checks
  - protector dependency accessed via broker->dependencies()->DependOnNoElementsProtector (protector is defined in execution/protectors, but the callsites are in Maglev builder)

G) Why this fast path is safe:
- Map set is validated (monomorphic/polymorphic checks) and (if unstable) revalidated after callback, so in-loop assumptions remain correct.
- NoElementsProtector keeps prototype chain assumptions valid (e.g., no custom getters for holes during iteration for HOLEY behavior).
- Post-call array length check ensures we deopt if user code mutates length in a way that breaks the index traversal contract.
- Hole checks guarantee skip semantics match spec for missing indices.

### 3. Additional context (if relevant):
- There is also a TF/JSCallReducer path referencing the same continuations (compiler/js-call-reducer.cc), but for this task the Maglev-specific builder is primary.
- TryReduceArrayIteratorPrototypeNext implements a similar style of fast path for array iterators, demonstrating the same elements-kinds specialization/hole behavior patterns.

Returned RAG DB IDs (with short explanations):
- maglev_foreach_tryreduce — Full TryReduceArrayForEach lowering: loop build, elements-kind specialization, lazy/eager deopt scopes, hole checks, callback call.
- maglev_foreach_deopt — Where DeoptFrameScope is constructed and how ArrayForEachLoop(Eager|Lazy)DeoptContinuation are referenced; includes DeoptFrameScope::IsLazyDeoptContinuationFrame and usage sites in 
forEach.
- torque_array_foreach — Torque definitions of ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation (the interpreter-continuation loop).
- maglev_throw_if_not_callable — IR node definition/codegen for ThrowIfNotCallable and its use under lazy deopt scope in TryReduceArrayForEach.
- maglev_elements_kind — Elements-kinds specialization, protector dependency, element loads, and hole checks used by forEach lowering.
- maglev_length_changed — Post-callback length recheck and CheckInt32Condition with DeoptimizeReason::kArrayLengthChanged.
- maglev_foreach_loop — MaglevSubGraphBuilder loop construction, BranchIfInt32Compare, Int32IncrementWithOverflow, CheckInt32Condition (selection of nodes used by forEach loop body).
- maglev_trycatch_handlers — CreateCatchContext and SetPendingMessage in Maglev, plus SetPendingMessage node definition/codegen.
Final answer: ### 1. Task outcome (short version):
Mapped the full Maglev path that recognizes and lowers Array.prototype.forEach into a fast index loop, including elements-kinds specialization, map/protector guards, and hole handling; identified the deopt 
continuation builtins and how Maglev wires lazy/eager deopt frames; pinpointed ThrowIfNotCallable and exception plumbing (CreateCatchContext, SetPendingMessage); and collected the exact IR nodes for 
representation/untagging and loop arithmetic, plus the deopt reason for ArrayLengthChanged. All canonical file paths/symbols are listed, and RAG IDs are provided to fetch the concrete code blocks.

### 2. Task outcome (extremely detailed version):

A) Recognition and loop lowering in Maglev (Array.prototype.forEach):
- Core recognition and lowering:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach
  - What it does:
    - Checks CanSpeculateCall, validates receiver has known maps, and uses CanInlineArrayIteratingBuiltin to confirm fast-array iteration across the map set; records a dependency on the NoElementsProtector.
    - Validates the callback using ThrowIfNotCallable wrapped under a lazy deopt continuation.
    - Builds an index loop with MaglevSubGraphBuilder over [0, original_length), using:
      - BranchIfInt32Compare for the loop guard index < original_length.
      - Int32IncrementWithOverflow (under eager deopt continuation) for i++.
      - BuildLoadElements(receiver) then element load specialized by elements kind:
        - FixedArray vs FixedDoubleArray (BuildLoadFixedArrayElement / BuildLoadFixedDoubleArrayElement).
      - For HOLEY kinds, hole checks:
        - HOLEY_DOUBLE: BranchIfFloat64IsHole; HOLEY (tagged): BranchIfRootConstant(TheHoleValue) and skip the callback.
      - Invokes the callback under a lazy deopt continuation so exceptions/stack are correct.
      - After the call, re-check maps if receiver maps were unstable and ensure array length didn’t change (kArrayLengthChanged) using CheckInt32Condition with UnsignedLessThanEqual.
    - Returns undefined (per spec) when done.
  - Helper calls used by that path:
    - BuildLoadJSArrayLength(receiver)
    - BuildLoadElements(receiver)
    - MaglevSubGraphBuilder::BeginLoop/EndLoop and the branch helpers
  - RAG: maglev_foreach_tryreduce, maglev_foreach_loop, maglev_elements_kind, maglev_length_changed

- Helpers and utilities (same file):
  - CanInlineArrayIteratingBuiltin (static helper in an unnamed namespace): tests maps support for fast array iteration and computes a unified ElementsKind across the observed maps.
  - BuildLoadJSArrayLength(receiver): loads JSArray::length and records known property cache entry.
  - BuildLoadElements(receiver): loads JSObject::elements.
  - MaglevSubGraphBuilder: local control-flow/phi framework to build structured loops/branches used in lowering.

B) Deopt continuation builtins and DeoptFrameScope wiring:
- Maglev deopt frame mechanism:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: class v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope
    - Provides builtins continuation frames and marks certain ones as “lazy deopt frames”.
    - IsLazyDeoptContinuationFrame() recognizes Builtin::kArrayForEachLoopLazyDeoptContinuation (among other lazy continuations).
  - In TryReduceArrayForEach, DeoptFrameScope is created around:
    - ThrowIfNotCallable: lazy continuation Builtin::kArrayForEachLoopLazyDeoptContinuation.
    - Index increment overflow check: eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.
    - Callback call: lazy continuation Builtin::kArrayForEachLoopLazyDeoptContinuation.
    - Post-call recheck (maps/length): eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.
  - RAG: maglev_foreach_deopt

- Torque builtins for continuations (ArrayForEachLoopEager/LazyDeoptContinuation):
  - File: src/builtins/array-foreach.tq
  - Symbols:
    - transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(...)
    - transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(...)
    - transitioning builtin ArrayForEachLoopContinuation(...)
  - Both eager and lazy continuations tail-call ArrayForEachLoopContinuation, which performs the canonical JS forEach index loop.
  - RAG: torque_array_foreach

C) ThrowIfNotCallable and exception wiring (try/catch/plumbing):
- ThrowIfNotCallable node:
  - Files: src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
  - Symbols: v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}
  - GenerateCode emits a deferred path to Runtime::kThrowCalledNonCallable and marks an exception handler + lazy deopt point.
  - Used in TryReduceArrayForEach under a lazy DeoptFrameScope to maintain correct JS stack upon exception.
  - RAG: maglev_throw_if_not_callable, maglev_foreach_deopt

- Exception edge/handler locations and catch contexts:
  - File: src/maglev/maglev-graph-builder.cc
  - Create catch contexts:
    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext constructs the catch context object inline and records scope info.
  - Pending message plumbing:
    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage creates a SetPendingMessage node, reflected by:
      - Node: v8::internal::maglev::SetPendingMessage in maglev-ir.h/maglev-ir.cc.
  - Many Maglev nodes that call builtins/runtime mark exception handlers and lazy deopt points (e.g., ThrowIfNotCallable, calls, some builtins), ensuring proper exception routing to handlers.
  - RAG: maglev_trycatch_handlers

D) Representation/untagging and loop arithmetic nodes; ArrayLengthChanged deopt reason:
- Untagging and representation helpers:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag
    - Emits UnsafeSmiUntag (fast path) or CheckedSmiUntag.
- Loop arithmetic and comparisons:
  - Nodes in src/maglev/maglev-ir.h/.cc:
    - v8::internal::maglev::Int32IncrementWithOverflow (used for i++)
    - v8::internal::maglev::BranchIfInt32Compare (loop guard)
    - v8::internal::maglev::CheckInt32Condition (used for post-call length check)
- Array length deopt reason:
  - In TryReduceArrayForEach: after callback, reload current_length and emit TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, 
DeoptimizeReason::kArrayLengthChanged).
  - CheckInt32Condition::GenerateCode deopts on mismatch using the provided reason.
  - RAG: maglev_length_changed, maglev_foreach_loop

E) Elements kinds specialization, map checks, protectors, hole checks, and concrete element loads:
- Elements kinds and protector dependency (NoElementsProtector):
  - TryReduceArrayForEach uses CanInlineArrayIteratingBuiltin to ensure maps support fast iteration and unifies kind; it then depends on NoElementsProtector 
(broker()->dependencies()->DependOnNoElementsProtector()) to assume standard prototype semantics.
- Element loads:
  - For PACKED kinds: BuildLoadFixedArrayElement
  - For PACKED_DOUBLE kinds: BuildLoadFixedDoubleArrayElement
  - For HOLEY kinds: hole checks skip callback
    - HOLEY_DOUBLE: BranchIfFloat64IsHole
    - HOLEY (tagged): BranchIfRootConstant(TheHoleValue)
- Map checks and transitions used elsewhere in array element paths:
  - CheckMaps / CheckMapsWithMigration and BuildTransitionElementsKindOrCheckMap support correctness across transitions.
- RAG: maglev_elements_kind, maglev_foreach_tryreduce

F) Canonical file paths and symbol names (organized per topic):
- Lowering/loop building:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadJSArrayLength
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadElements
    - v8::internal::maglev::MaglevGraphBuilder::MaglevSubGraphBuilder::{BeginLoop, EndLoop, Branch/Goto helpers}
    - v8::internal::maglev::MaglevGraphBuilder::TryBuildCheckInt32Condition (used for deopt checks)
- Deopt scopes/continuations:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope
  - src/builtins/array-foreach.tq
    - ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopContinuation
- ThrowIfNotCallable and exception plumbing:
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext
    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::SetPendingMessage::{SetValueLocationConstraints, GenerateCode}
- Representation and loop arithmetic:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag (calls UnsafeSmiUntag / CheckedSmiUntag)
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::Int32IncrementWithOverflow
    - v8::internal::maglev::BranchIfInt32Compare
    - v8::internal::maglev::CheckInt32Condition (with DeoptimizeReason::kArrayLengthChanged)
- Elements specialization and loads:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedArrayElement
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedDoubleArrayElement
    - Conditionals using BranchIfFloat64IsHole and BranchIfRootConstant for hole checks
  - protector dependency accessed via broker->dependencies()->DependOnNoElementsProtector (protector is defined in execution/protectors, but the callsites are in Maglev builder)

G) Why this fast path is safe:
- Map set is validated (monomorphic/polymorphic checks) and (if unstable) revalidated after callback, so in-loop assumptions remain correct.
- NoElementsProtector keeps prototype chain assumptions valid (e.g., no custom getters for holes during iteration for HOLEY behavior).
- Post-call array length check ensures we deopt if user code mutates length in a way that breaks the index traversal contract.
- Hole checks guarantee skip semantics match spec for missing indices.

### 3. Additional context (if relevant):
- There is also a TF/JSCallReducer path referencing the same continuations (compiler/js-call-reducer.cc), but for this task the Maglev-specific builder is primary.
- TryReduceArrayIteratorPrototypeNext implements a similar style of fast path for array iterators, demonstrating the same elements-kinds specialization/hole behavior patterns.

Returned RAG DB IDs (with short explanations):
- maglev_foreach_tryreduce — Full TryReduceArrayForEach lowering: loop build, elements-kind specialization, lazy/eager deopt scopes, hole checks, callback call.
- maglev_foreach_deopt — Where DeoptFrameScope is constructed and how ArrayForEachLoop(Eager|Lazy)DeoptContinuation are referenced; includes DeoptFrameScope::IsLazyDeoptContinuationFrame and usage sites in 
forEach.
- torque_array_foreach — Torque definitions of ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation (the interpreter-continuation loop).
- maglev_throw_if_not_callable — IR node definition/codegen for ThrowIfNotCallable and its use under lazy deopt scope in TryReduceArrayForEach.
- maglev_elements_kind — Elements-kinds specialization, protector dependency, element loads, and hole checks used by forEach lowering.
- maglev_length_changed — Post-callback length recheck and CheckInt32Condition with DeoptimizeReason::kArrayLengthChanged.
- maglev_foreach_loop — MaglevSubGraphBuilder loop construction, BranchIfInt32Compare, Int32IncrementWithOverflow, CheckInt32Condition (selection of nodes used by forEach loop body).
- maglev_trycatch_handlers — CreateCatchContext and SetPendingMessage in Maglev, plus SetPendingMessage node definition/codegen.

[Step 25: Duration 90.80 seconds| Input tokens: 2,914,153 | Output tokens: 26,294]
Observations: Here is the final answer from your managed agent 'V8Search':
### 1. Task outcome (short version):
Mapped the full Maglev path that recognizes and lowers Array.prototype.forEach into a fast index loop, including elements-kinds specialization, map/protector guards, and hole handling; identified the deopt 
continuation builtins and how Maglev wires lazy/eager deopt frames; pinpointed ThrowIfNotCallable and exception plumbing (CreateCatchContext, SetPendingMessage); and collected the exact IR nodes for 
representation/untagging and loop arithmetic, plus the deopt reason for ArrayLengthChanged. All canonical file paths/symbols are listed, and RAG IDs are provided to fetch the concrete code blocks.

### 2. Task outcome (extremely detailed version):

A) Recognition and loop lowering in Maglev (Array.prototype.forEach):
- Core recognition and lowering:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach
  - What it does:
    - Checks CanSpeculateCall, validates receiver has known maps, and uses CanInlineArrayIteratingBuiltin to confirm fast-array iteration across the map set; records a dependency on the NoElementsProtector.
    - Validates the callback using ThrowIfNotCallable wrapped under a lazy deopt continuation.
    - Builds an index loop with MaglevSubGraphBuilder over |0, original_length), using:
      - BranchIfInt32Compare for the loop guard index < original_length.
      - Int32IncrementWithOverflow (under eager deopt continuation) for i++.
      - BuildLoadElements(receiver) then element load specialized by elements kind:
        - FixedArray vs FixedDoubleArray (BuildLoadFixedArrayElement / BuildLoadFixedDoubleArrayElement).
      - For HOLEY kinds, hole checks:
        - HOLEY_DOUBLE: BranchIfFloat64IsHole; HOLEY (tagged): BranchIfRootConstant(TheHoleValue) and skip the callback.
      - Invokes the callback under a lazy deopt continuation so exceptions/stack are correct.
      - After the call, re-check maps if receiver maps were unstable and ensure array length didn’t change (kArrayLengthChanged) using CheckInt32Condition with UnsignedLessThanEqual.
    - Returns undefined (per spec) when done.
  - Helper calls used by that path:
    - BuildLoadJSArrayLength(receiver)
    - BuildLoadElements(receiver)
    - MaglevSubGraphBuilder::BeginLoop/EndLoop and the branch helpers
  - RAG: maglev_foreach_tryreduce, maglev_foreach_loop, maglev_elements_kind, maglev_length_changed

- Helpers and utilities (same file):
  - CanInlineArrayIteratingBuiltin (static helper in an unnamed namespace): tests maps support for fast array iteration and computes a unified ElementsKind across the observed maps.
  - BuildLoadJSArrayLength(receiver): loads JSArray::length and records known property cache entry.
  - BuildLoadElements(receiver): loads JSObject::elements.
  - MaglevSubGraphBuilder: local control-flow/phi framework to build structured loops/branches used in lowering.

B) Deopt continuation builtins and DeoptFrameScope wiring:
- Maglev deopt frame mechanism:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: class v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope
    - Provides builtins continuation frames and marks certain ones as “lazy deopt frames”.
    - IsLazyDeoptContinuationFrame() recognizes Builtin::kArrayForEachLoopLazyDeoptContinuation (among other lazy continuations).
  - In TryReduceArrayForEach, DeoptFrameScope is created around:
    - ThrowIfNotCallable: lazy continuation Builtin::kArrayForEachLoopLazyDeoptContinuation.
    - Index increment overflow check: eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.
    - Callback call: lazy continuation Builtin::kArrayForEachLoopLazyDeoptContinuation.
    - Post-call recheck (maps/length): eager continuation Builtin::kArrayForEachLoopEagerDeoptContinuation.
  - RAG: maglev_foreach_deopt

- Torque builtins for continuations (ArrayForEachLoopEager/LazyDeoptContinuation):
  - File: src/builtins/array-foreach.tq
  - Symbols:
    - transitioning javascript builtin ArrayForEachLoopEagerDeoptContinuation(...)
    - transitioning javascript builtin ArrayForEachLoopLazyDeoptContinuation(...)
    - transitioning builtin ArrayForEachLoopContinuation(...)
  - Both eager and lazy continuations tail-call ArrayForEachLoopContinuation, which performs the canonical JS forEach index loop.
  - RAG: torque_array_foreach

C) ThrowIfNotCallable and exception wiring (try/catch/plumbing):
- ThrowIfNotCallable node:
  - Files: src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
  - Symbols: v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}
  - GenerateCode emits a deferred path to Runtime::kThrowCalledNonCallable and marks an exception handler + lazy deopt point.
  - Used in TryReduceArrayForEach under a lazy DeoptFrameScope to maintain correct JS stack upon exception.
  - RAG: maglev_throw_if_not_callable, maglev_foreach_deopt

- Exception edge/handler locations and catch contexts:
  - File: src/maglev/maglev-graph-builder.cc
  - Create catch contexts:
    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext constructs the catch context object inline and records scope info.
  - Pending message plumbing:
    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage creates a SetPendingMessage node, reflected by:
      - Node: v8::internal::maglev::SetPendingMessage in maglev-ir.h/maglev-ir.cc.
  - Many Maglev nodes that call builtins/runtime mark exception handlers and lazy deopt points (e.g., ThrowIfNotCallable, calls, some builtins), ensuring proper exception routing to handlers.
  - RAG: maglev_trycatch_handlers

D) Representation/untagging and loop arithmetic nodes; ArrayLengthChanged deopt reason:
- Untagging and representation helpers:
  - File: src/maglev/maglev-graph-builder.cc
  - Symbol: v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag
    - Emits UnsafeSmiUntag (fast path) or CheckedSmiUntag.
- Loop arithmetic and comparisons:
  - Nodes in src/maglev/maglev-ir.h/.cc:
    - v8::internal::maglev::Int32IncrementWithOverflow (used for i++)
    - v8::internal::maglev::BranchIfInt32Compare (loop guard)
    - v8::internal::maglev::CheckInt32Condition (used for post-call length check)
- Array length deopt reason:
  - In TryReduceArrayForEach: after callback, reload current_length and emit TryBuildCheckInt32Condition(original_length_int32, current_length, AssertCondition::kUnsignedLessThanEqual, 
DeoptimizeReason::kArrayLengthChanged).
  - CheckInt32Condition::GenerateCode deopts on mismatch using the provided reason.
  - RAG: maglev_length_changed, maglev_foreach_loop

E) Elements kinds specialization, map checks, protectors, hole checks, and concrete element loads:
- Elements kinds and protector dependency (NoElementsProtector):
  - TryReduceArrayForEach uses CanInlineArrayIteratingBuiltin to ensure maps support fast iteration and unifies kind; it then depends on NoElementsProtector 
(broker()->dependencies()->DependOnNoElementsProtector()) to assume standard prototype semantics.
- Element loads:
  - For PACKED kinds: BuildLoadFixedArrayElement
  - For PACKED_DOUBLE kinds: BuildLoadFixedDoubleArrayElement
  - For HOLEY kinds: hole checks skip callback
    - HOLEY_DOUBLE: BranchIfFloat64IsHole
    - HOLEY (tagged): BranchIfRootConstant(TheHoleValue)
- Map checks and transitions used elsewhere in array element paths:
  - CheckMaps / CheckMapsWithMigration and BuildTransitionElementsKindOrCheckMap support correctness across transitions.
- RAG: maglev_elements_kind, maglev_foreach_tryreduce

F) Canonical file paths and symbol names (organized per topic):
- Lowering/loop building:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::TryReduceArrayForEach
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadJSArrayLength
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadElements
    - v8::internal::maglev::MaglevGraphBuilder::MaglevSubGraphBuilder::{BeginLoop, EndLoop, Branch/Goto helpers}
    - v8::internal::maglev::MaglevGraphBuilder::TryBuildCheckInt32Condition (used for deopt checks)
- Deopt scopes/continuations:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::DeoptFrameScope
  - src/builtins/array-foreach.tq
    - ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation, ArrayForEachLoopContinuation
- ThrowIfNotCallable and exception plumbing:
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::ThrowIfNotCallable::{SetValueLocationConstraints, GenerateCode}
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::VisitCreateCatchContext
    - v8::internal::maglev::MaglevGraphBuilder::VisitSetPendingMessage
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::SetPendingMessage::{SetValueLocationConstraints, GenerateCode}
- Representation and loop arithmetic:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::BuildSmiUntag (calls UnsafeSmiUntag / CheckedSmiUntag)
  - src/maglev/maglev-ir.h, src/maglev/maglev-ir.cc
    - v8::internal::maglev::Int32IncrementWithOverflow
    - v8::internal::maglev::BranchIfInt32Compare
    - v8::internal::maglev::CheckInt32Condition (with DeoptimizeReason::kArrayLengthChanged)
- Elements specialization and loads:
  - src/maglev/maglev-graph-builder.cc
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedArrayElement
    - v8::internal::maglev::MaglevGraphBuilder::BuildLoadFixedDoubleArrayElement
    - Conditionals using BranchIfFloat64IsHole and BranchIfRootConstant for hole checks
  - protector dependency accessed via broker->dependencies()->DependOnNoElementsProtector (protector is defined in execution/protectors, but the callsites are in Maglev builder)

G) Why this fast path is safe:
- Map set is validated (monomorphic/polymorphic checks) and (if unstable) revalidated after callback, so in-loop assumptions remain correct.
- NoElementsProtector keeps prototype chain assumptions valid (e.g., no custom getters for holes during iteration for HOLEY behavior).
- Post-call array length check ensures we deopt if user code mutates length in a way that breaks the index traversal contract.
- Hole checks guarantee skip semantics match spec for missing indices.

### 3. Additional context (if relevant):
- There is also a TF/JSCallReducer path referencing the same continuations (compiler/js-call-reducer.cc), but for this task the Maglev-specific builder is primary.
- TryReduceArrayIteratorPrototypeNext implements a similar style of fast path for array iterators, demonstrating the same elements-kinds specialization/hole behavior patterns.

Returned RAG DB IDs (with short explanations):
- maglev_foreach_tryreduce — Full TryReduceArrayForEach lowering: loop build, elements-kind specialization, lazy/eager deopt scopes, hole checks, callback call.
- maglev_foreach_deopt — Where DeoptFrameScope is constructed and how ArrayForEachLoop(Eager|Lazy)DeoptContinuation are referenced; includes DeoptFrameScope::IsLazyDeoptContinuationFrame and usage sites in 
forEach.
- torque_array_foreach — Torque definitions of ArrayForEachLoopEagerDeoptContinuation, ArrayForEachLoopLazyDeoptContinuation, and ArrayForEachLoopContinuation (the interpreter-continuation loop).
- maglev_throw_if_not_callable — IR node definition/codegen for ThrowIfNotCallable and its use under lazy deopt scope in TryReduceArrayForEach.
- maglev_elements_kind — Elements-kinds specialization, protector dependency, element loads, and hole checks used by forEach lowering.
- maglev_length_changed — Post-callback length recheck and CheckInt32Condition with DeoptimizeReason::kArrayLengthChanged.
- maglev_foreach_loop — MaglevSubGraphBuilder loop construction, BranchIfInt32Compare, Int32IncrementWithOverflow, CheckInt32Condition (selection of nodes used by forEach loop body).
- maglev_trycatch_handlers — CreateCatchContext and SetPendingMessage in Maglev, plus SetPendingMessage node definition/codegen.
[Step 2: Duration 603.26 seconds| Input tokens: 9,353 | Output tokens: 2,528]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'V8Search' with arguments: {'task': '{\n  "TASK": "Locate and extract the exact V8 Maglev code paths that recognize and lower Array.prototype.forEach into a specialized index loop.           │
│ Specifically target: (a) MaglevGraphBuilder::TryReduceArrayForEach and any helpers in src/maglev/maglev-graph-builder.cc that build the loop (MaglevSubGraphBuilder, BuildLoadJSArrayLength,                 │
│ BuildLoadElements, element-load and hole-check helpers, map/protector guards); (b) deopt continuation definitions/usages for ArrayForEachLoopEagerDeoptContinuation and                                      │
│ ArrayForEachLoopLazyDeoptContinuation (Torque builtins in src/builtins/array-foreach.tq) and where Maglev wires DeoptFrameScope to those continuations; (c) ThrowIfNotCallable emission for the callback and │
│ exception-edge wiring (ThrowIfNotCallable node in maglev-ir.*, and the points where exception handlers and lazy deopt points are defined); (d) representation/untagging and loop arithmetic                  │
│ (UnsafeSmiUntag/CheckedSmiUntag, Int32IncrementWithOverflow, BranchIfInt32Compare, CheckInt32Condition with UnsignedLessThanEqual) including the deopt reason DeoptimizeReason::kArrayLengthChanged; (e)     │
│ specialization by elements kinds (PACKED_SMI/DOUBLE/HOLEY), hole handling, NoElementsProtector dependency, and map checks (CheckMaps/CheckMapsWithMigration); (f) try/catch wiring points including          │
│ CreateCatchContext and SetPendingMessage materialization.",\n  "REASON": "We need an authoritative map of the concrete files, symbol names, and node implementations to (1) understand exactly how Maglev    │
│ recognizes and lowers Array.prototype.forEach, (2) verify where eager/lazy deopt continuations are attached and what metadata is captured, (3) pinpoint ThrowIfNotCallable and exception-path plumbing into  │
│ catch contexts, (4) trace representation selection and length-change guards, (5) enumerate elements-kind specialization and feedback/protector gating, and (6) collect the fragile points to design a        │
│ targeted fuzzing ProgramTemplate."\n}'}                                                                                                                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────── New run - V8Search ─────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                              │
│ You're a helpful agent named 'V8Search'.                                                                                                                                                                     │
│ You have been submitted this task by your manager.                                                                                                                                                           │
│ ---                                                                                                                                                                                                          │
│ Task:                                                                                                                                                                                                        │
│ {                                                                                                                                                                                                            │
│   "TASK": "Locate and extract the exact V8 Maglev code paths that recognize and lower Array.prototype.forEach into a specialized index loop. Specifically target: (a)                                        │
│ MaglevGraphBuilder::TryReduceArrayForEach and any helpers in src/maglev/maglev-graph-builder.cc that build the loop (MaglevSubGraphBuilder, BuildLoadJSArrayLength, BuildLoadElements, element-load and      │
│ hole-check helpers, map/protector guards); (b) deopt continuation definitions/usages for ArrayForEachLoopEagerDeoptContinuation and ArrayForEachLoopLazyDeoptContinuation (Torque builtins in                │
│ src/builtins/array-foreach.tq) and where Maglev wires DeoptFrameScope to those continuations; (c) ThrowIfNotCallable emission for the callback and exception-edge wiring (ThrowIfNotCallable node in         │
│ maglev-ir.*, and the points where exception handlers and lazy deopt points are defined); (d) representation/untagging and loop arithmetic (UnsafeSmiUntag/CheckedSmiUntag, Int32IncrementWithOverflow,       │
│ BranchIfInt32Compare, CheckInt32Condition with UnsignedLessThanEqual) including the deopt reason DeoptimizeReason::kArrayLengthChanged; (e) specialization by elements kinds (PACKED_SMI/DOUBLE/HOLEY), hole │
│ handling, NoElementsProtector dependency, and map checks (CheckMaps/CheckMapsWithMigration); (f) try/catch wiring points including CreateCatchContext and SetPendingMessage materialization.",               │
│   "REASON": "We need an authoritative map of the concrete files, symbol names, and node implementations to (1) understand exactly how Maglev recognizes and lowers Array.prototype.forEach, (2) verify where │
│ eager/lazy deopt continuations are attached and what metadata is captured, (3) pinpoint ThrowIfNotCallable and exception-path plumbing into catch contexts, (4) trace representation selection and           │
│ length-change guards, (5) enumerate elements-kind specialization and feedback/protector gating, and (6) collect the fragile points to design a targeted fuzzing ProgramTemplate."                            │
│ }                                                                                                                                                                                                            │
│ ---                                                                                                                                                                                                          │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.                    │
│                                                                                                                                                                                                              │
│ Your final_answer WILL HAVE to contain these parts:                                                                                                                                                          │
│ ### 1. Task outcome (short version):                                                                                                                                                                         │
│ ### 2. Task outcome (extremely detailed version):                                                                                                                                                            │
│ ### 3. Additional context (if relevant):                                                                                                                                                                     │
│                                                                                                                                                                                                              │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.                                                                                        │
│ And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.                                                              │
│                                                                                                                                                                                                              │
╰─ LiteLLMModel - gpt-5 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.cc', 'options': "--filter='maglev-graph-builder.cc'"}                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='maglev-graph-builder.cc' 'maglev-graph-builder.cc' | head -n 1000
Observations: unknown option: maglev-graph-builder.cc
[Step 1: Duration 9.48 seconds| Input tokens: 2,511 | Output tokens: 171]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.cc', 'options': "--filter='maglev-graph-builder.cc'"}                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.h', 'options': "--filter='maglev-ir.h'"}                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.cc', 'options': "--filter='maglev-ir.cc'"}                                                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'array-foreach.tq', 'options': "--filter='array-foreach.tq'"}                                                                                       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='maglev-graph-builder.cc' 'maglev-graph-builder.cc' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ArrayForEachLoopEagerDeoptContinuation', 'options': "--filter='ArrayForEachLoopEagerDeoptContinuation'"}                                           │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='maglev-ir.h' 'maglev-ir.h' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='maglev-ir.cc' 'maglev-ir.cc' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='array-foreach.tq' 'array-foreach.tq' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'NoElementsProtector', 'options': "--filter='NoElementsProtector'"}                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='ArrayForEachLoopEagerDeoptContinuation' 'ArrayForEachLoopEagerDeoptContinuation' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': "--filter='ThrowIfNotCallable'"}                                                                                   │

Observations: unknown option: maglev-graph-builder.cc
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='NoElementsProtector' 'NoElementsProtector' | head -n 1000
Observations: unknown option: maglev-ir.h
Observations: unknown option: maglev-ir.cc
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'options': '-L 2 maglev'}                                                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'builtins/array-foreach.tq', 'options': "--filter='builtins/array-foreach.tq'"}                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: unknown option: array-foreach.tq
Command: cd /Users/tarun/v8/v8/src && tree -L 2 maglev | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='ThrowIfNotCallable' 'ThrowIfNotCallable' | head -n 1000
Observations: unknown option: ArrayForEachLoopEagerDeoptContinuation
Observations: unknown option: NoElementsProtector
Observations: maglev
├── arm
│   ├── maglev-assembler-arm-inl.h
│   ├── maglev-assembler-arm.cc
│   └── maglev-ir-arm.cc
├── arm64
│   ├── maglev-assembler-arm64-inl.h
│   ├── maglev-assembler-arm64.cc
│   └── maglev-ir-arm64.cc
├── DEPS
├── DIR_METADATA
├── maglev-assembler-inl.h
├── maglev-assembler.cc
├── maglev-assembler.h
├── maglev-basic-block.h
├── maglev-code-gen-state.h
├── maglev-code-generator.cc
├── maglev-code-generator.h
├── maglev-compilation-info.cc
├── maglev-compilation-info.h
├── maglev-compilation-unit.cc
├── maglev-compilation-unit.h
├── maglev-compiler.cc
├── maglev-compiler.h
├── maglev-concurrent-dispatcher.cc
├── maglev-concurrent-dispatcher.h
├── maglev-graph-builder.cc
├── maglev-graph-builder.h
├── maglev-graph-labeller.h
├── maglev-graph-printer.cc
├── maglev-graph-printer.h
├── maglev-graph-processor.h
├── maglev-graph-verifier.h
├── maglev-graph.h
├── maglev-interpreter-frame-state.cc
├── maglev-interpreter-frame-state.h
├── maglev-ir-inl.h
├── maglev-ir.cc
├── maglev-ir.h
├── maglev-phi-representation-selector.cc
├── maglev-phi-representation-selector.h
├── maglev-pipeline-statistics.cc
├── maglev-pipeline-statistics.h
├── maglev-post-hoc-optimizations-processors.h
├── maglev-pre-regalloc-codegen-processors.h
├── maglev-regalloc-data.h
├── maglev-regalloc.cc
├── maglev-regalloc.h
├── maglev-register-frame-array.h
├── maglev.cc
├── maglev.h
├── OWNERS
├── riscv
│   ├── maglev-assembler-riscv-inl.h
│   ├── maglev-assembler-riscv.cc
│   └── maglev-ir-riscv.cc
├── s390
│   ├── maglev-assembler-s390-inl.h
│   ├── maglev-assembler-s390.cc
│   └── maglev-ir-s390.cc
└── x64
    ├── maglev-assembler-x64-inl.h
    ├── maglev-assembler-x64.cc
    └── maglev-ir-x64.cc

6 directories, 58 files
Observations: unknown option: ThrowIfNotCallable
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter='builtins/array-foreach.tq' 'builtins/array-foreach.tq' | head -n 1000
Observations: unknown option: builtins/array-foreach.tq
[Step 2: Duration 18.53 seconds| Input tokens: 5,081 | Output tokens: 1,118]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.cc', 'options': '--filter maglev-graph-builder.cc'}                                                                           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'array-foreach.tq', 'options': '--filter array-foreach.tq'}                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.h', 'options': '--filter maglev-ir.h'}                                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.cc', 'options': '--filter maglev-ir.cc'}                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ArrayForEachLoopEagerDeoptContinuation', 'options': '--filter ArrayForEachLoopEagerDeoptContinuation'}                                             │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter array-foreach.tq 'array-foreach.tq' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-graph-builder.cc 'maglev-graph-builder.cc' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'DeoptFrameScope', 'options': '--filter DeoptFrameScope'}                                                                                           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-ir.h 'maglev-ir.h' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-ir.h 'maglev-ir.h' | head -n 1000Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-ir.cc 'maglev-ir.cc' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Observations: unknown option: maglev-graph-builder.cc
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter ArrayForEachLoopEagerDeoptContinuation 'ArrayForEachLoopEagerDeoptContinuation' | head -n 1000
│ Calling tool: 'tree' with arguments: {'options': '-L 2 builtins'}                                                                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Observations: unknown option: array-foreach.tq
Observations: unknown option: maglev-ir.cc
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter DeoptFrameScope 'DeoptFrameScope' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ThrowIfNotCallable', 'options': '--filter ThrowIfNotCallable'}                                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Observations: unknown option: maglev-ir.h
Observations: unknown option: ArrayForEachLoopEagerDeoptContinuation
Command: cd /Users/tarun/v8/v8/src && tree -L 2 builtins | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'NoElementsProtector', 'options': '--filter NoElementsProtector'}                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Observations: unknown option: DeoptFrameScope
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter ThrowIfNotCallable 'ThrowIfNotCallable' | head -n 1000
├── accessors.cc
├── accessors.h
├── aggregate-error.tq
├── arm
│   └── builtins-arm.cc
├── arm64
│   └── builtins-arm64.cc
├── array-at.tq
├── array-concat.tq
├── array-copywithin.tq
├── array-every.tq
├── array-filter.tq
├── array-find.tq
├── array-findindex.tq
├── array-findlast.tq
├── array-findlastindex.tq
├── array-flat.tq
├── array-foreach.tq
├── array-from-async.tq
├── array-from.tq
├── array-isarray.tq
├── array-join.tq
├── array-lastindexof.tq
├── array-map.tq
├── array-of.tq
├── array-reduce-right.tq
├── array-reduce.tq
├── array-reverse.tq
├── array-shift.tq
├── array-slice.tq
├── array-some.tq
├── array-splice.tq
├── array-to-reversed.tq
├── array-to-sorted.tq
├── array-to-spliced.tq
├── array-unshift.tq
├── array-with.tq
├── array.tq
├── arraybuffer.tq
├── base.tq
├── boolean.tq
├── builtins-abstract-module-source.cc
├── builtins-api.cc
├── builtins-array-gen.cc
├── builtins-array-gen.h
├── builtins-array.cc
├── builtins-arraybuffer.cc
├── builtins-async-disposable-stack.cc
├── builtins-async-function-gen.cc
├── builtins-async-gen.cc
├── builtins-async-gen.h
├── builtins-async-generator-gen.cc
├── builtins-async-iterator-gen.cc
├── builtins-async-module.cc
├── builtins-atomics-synchronization.cc
├── builtins-bigint-gen.cc
├── builtins-bigint-gen.h
├── builtins-bigint.cc
├── builtins-bigint.tq
├── builtins-call-gen.cc
├── builtins-call-gen.h
├── builtins-callsite.cc
├── builtins-collections-gen.cc
├── builtins-collections-gen.h
├── builtins-collections.cc
├── builtins-console.cc
├── builtins-constructor-gen.cc
├── builtins-constructor-gen.h
├── builtins-constructor.h
├── builtins-conversion-gen.cc
├── builtins-data-view-gen.h
├── builtins-dataview.cc
├── builtins-date-gen.cc
├── builtins-date.cc
├── builtins-definitions.h
├── builtins-descriptors.h
├── builtins-disposable-stack.cc
├── builtins-error.cc
├── builtins-function.cc
├── builtins-generator-gen.cc
├── builtins-global-gen.cc
├── builtins-global.cc
├── builtins-handler-gen.cc
├── builtins-ic-gen.cc
├── builtins-inl.h
├── builtins-internal-gen.cc
├── builtins-internal.cc
├── builtins-interpreter-gen.cc
├── builtins-intl-gen.cc
├── builtins-intl.cc
├── builtins-iterator-gen.cc
├── builtins-iterator-gen.h
├── builtins-json.cc
├── builtins-lazy-gen.cc
├── builtins-lazy-gen.h
├── builtins-microtask-queue-gen.cc
├── builtins-number-gen.cc
├── builtins-number-tsa.cc
├── builtins-number.cc
├── builtins-object-gen.cc
├── builtins-object-gen.h
├── builtins-object.cc
├── builtins-promise-gen.cc
├── builtins-promise-gen.h
├── builtins-promise.h
├── builtins-proxy-gen.cc
Observations: builtins
├── builtins-reflect.cc
├── builtins-regexp-gen.cc
├── builtins-regexp-gen.h
├── builtins-regexp.cc
├── builtins-shadow-realm-gen.cc
├── builtins-shadow-realm.cc
├── builtins-shared-array.cc
├── builtins-sharedarraybuffer-gen.cc
├── builtins-sharedarraybuffer.cc
├── builtins-string-gen.cc
├── builtins-string-gen.h
├── builtins-string-tsa.cc
├── builtins-string.cc
├── builtins-string.tq
├── builtins-struct.cc
├── builtins-symbol.cc
├── builtins-temporal-gen.cc
├── builtins-temporal.cc
├── builtins-trace.cc
├── builtins-typed-array-gen.cc
├── builtins-typed-array-gen.h
├── builtins-typed-array.cc
├── builtins-utils-gen.h
├── builtins-utils-inl.h
├── builtins-utils.h
├── builtins-wasm-gen.cc
├── builtins-wasm-gen.h
├── builtins-weak-refs.cc
├── builtins.cc
├── builtins.h
├── cast.tq
├── collections.tq
├── console.tq
├── constants-table-builder.cc
├── constants-table-builder.h
├── constructor.tq
├── conversion.tq
├── convert.tq
├── data-view-ops.h
├── data-view.tq
├── DEPS
├── DIR_METADATA
├── finalization-registry.tq
├── frame-arguments.tq
├── frames.tq
├── function.tq
├── generate-bytecodes-builtins-list.cc
├── growable-fixed-array-gen.cc
├── growable-fixed-array-gen.h
├── growable-fixed-array.tq
├── ia32
│   └── builtins-ia32.cc
├── ic-callable.tq
├── ic.tq
├── internal-coverage.tq
├── internal.tq
├── iterator-from.tq
├── iterator-helpers.tq
├── iterator.tq
├── js-to-js.tq
├── js-to-wasm.tq
├── loong64
│   └── builtins-loong64.cc
├── map-groupby.tq
├── math.tq
├── mips64
│   └── builtins-mips64.cc
├── number-builtins-reducer-inl.h
├── number.tq
├── object-fromentries.tq
├── object-groupby.tq
├── object.tq
├── OWNERS
├── ppc
│   └── builtins-ppc.cc
├── profile-data-reader.cc
├── profile-data-reader.h
├── promise-abstract-operations.tq
├── promise-all-element-closure.tq
├── promise-all.tq
├── promise-any.tq
├── promise-constructor.tq
├── promise-finally.tq
├── promise-jobs.tq
├── promise-misc.tq
├── promise-race.tq
├── promise-reaction-job.tq
├── promise-resolve.tq
├── promise-then.tq
├── promise-try.tq
├── promise-withresolvers.tq
├── proxy-constructor.tq
├── proxy-delete-property.tq
├── proxy-get-property.tq
├── proxy-get-prototype-of.tq
├── proxy-has-property.tq
├── proxy-is-extensible.tq
├── proxy-prevent-extensions.tq
├── proxy-revocable.tq
├── proxy-revoke.tq
├── proxy-set-property.tq
├── proxy-set-prototype-of.tq
├── proxy.tq
├── reflect.tq
├── regexp-exec.tq
├── regexp-match-all.tq
├── regexp-match.tq
├── regexp-replace.tq
├── regexp-search.tq
├── regexp-source.tq
├── regexp-split.tq
├── regexp-test.tq
├── regexp.tq
├── riscv
│   └── builtins-riscv.cc
├── s390
│   └── builtins-s390.cc
├── set-difference.tq
├── set-intersection.tq
├── set-is-disjoint-from.tq
├── set-is-subset-of.tq
├── set-is-superset-of.tq
├── set-symmetric-difference.tq
├── set-union.tq
├── setup-builtins-internal.cc
├── string-at.tq
├── string-endswith.tq
├── string-html.tq
├── string-includes.tq
├── string-indexof.tq
├── string-iswellformed.tq
├── string-iterator.tq
├── string-match-search.tq
├── string-pad.tq
├── string-repeat.tq
├── string-replaceall.tq
├── string-slice.tq
├── string-startswith.tq
├── string-substr.tq
├── string-substring.tq
├── string-towellformed.tq
├── string-trim.tq
├── suppressed-error.tq
├── symbol.tq
├── torque-csa-header-includes.h
├── torque-internal.tq
├── typed-array-at.tq
├── typed-array-createtypedarray.tq
├── typed-array-entries.tq
├── typed-array-every.tq
├── typed-array-filter.tq
├── typed-array-find.tq
├── typed-array-findindex.tq
├── typed-array-findlast.tq
├── typed-array-findlastindex.tq
├── typed-array-foreach.tq
├── typed-array-from.tq
├── typed-array-keys.tq
├── typed-array-of.tq
├── typed-array-reduce.tq
├── typed-array-reduceright.tq
├── typed-array-set.tq
├── typed-array-slice.tq
├── typed-array-some.tq
├── typed-array-sort.tq
├── typed-array-subarray.tq
├── typed-array-to-reversed.tq
├── typed-array-to-sorted.tq
├── typed-array-values.tq
├── typed-array-with.tq
├── typed-array.tq
├── wasm-strings.tq
├── wasm-to-js.tq
├── wasm.tq
├── weak-ref.tq
└── x64
    └── builtins-x64.cc

10 directories, 275 files
├── builtins-proxy-gen.h
Observations: unknown option: ThrowIfNotCallable
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter NoElementsProtector 'NoElementsProtector' | head -n 1000
Observations: unknown option: NoElementsProtector
[Step 3: Duration 23.19 seconds| Input tokens: 9,214 | Output tokens: 1,913]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.cc'}                                                                                                                          │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.h'}                                                                                                                           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-graph-builder.cc' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.cc'}                                                                                                                                     │
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.h'}                                                                                                                                      │

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-graph-builder.h' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'array-foreach.tq'}                                                                                                                                 │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-ir.h' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Observations: unknown option: maglev-graph-builder.cc
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-ir.cc' | head -n 1000
Observations: unknown option: maglev-graph-builder.h
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ThrowIfNotCallable'}                                                                                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'DeoptFrameScope'}                                                                                                                                  │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'array-foreach.tq' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'NoElementsProtector'}                                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'ThrowIfNotCallable' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'DeoptFrameScope' | head -n 1000
│ Calling tool: 'tree' with arguments: {'options': '-L 1 .'}                                                                                                                                                   │

Observations: unknown option: maglev-ir.h
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'NoElementsProtector' | head -n 1000
Observations: unknown option: maglev-ir.cc
Observations: unknown option: array-foreach.tq
Observations: unknown option: ThrowIfNotCallable
Observations: unknown option: DeoptFrameScope
Observations: unknown option: NoElementsProtector
Command: cd /Users/tarun/v8/v8/src && tree -L 1 . | head -n 1000
Observations: .
├── api
├── asmjs
├── ast
├── base
├── baseline
├── bigint
├── builtins
├── codegen
├── common
├── compiler
├── compiler-dispatcher
├── d8
├── date
├── debug
├── deoptimizer
├── DEPS
├── diagnostics
├── DIR_METADATA
├── execution
├── extensions
├── flags
├── fuzzilli
├── handles
├── heap
├── ic
├── init
├── inspector
├── interpreter
├── json
├── libplatform
├── libsampler
├── logging
├── maglev
├── numbers
├── objects
├── OWNERS
├── parsing
├── profiler
├── regexp
├── roots
├── runtime
├── sandbox
├── snapshot
├── strings
├── tasks
├── temporal
├── torque
├── tracing
├── trap-handler
├── utils
├── wasm
└── zone

50 directories, 3 files
[Step 4: Duration 17.56 seconds| Input tokens: 18,105 | Output tokens: 2,816]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev', 'options': '--filter maglev-graph-builder.cc'}                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev', 'options': '--filter maglev-ir.h'}                                                                                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'builtins', 'options': '--filter array-foreach.tq'}                                                                                                 │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-graph-builder.cc 'maglev' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev', 'options': '--filter DeoptFrameScope'}                                                                                                    │
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-ir.h 'maglev' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev', 'options': '--filter maglev-ir-inl.h'}                                                                                                    │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter array-foreach.tq 'builtins' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev', 'options': '--filter maglev-ir.cc'}                                                                                                       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter DeoptFrameScope 'maglev' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev', 'options': '--filter ThrowIfNotCallable'}                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'builtins', 'options': '--filter ArrayForEachLoopEagerDeoptContinuation'}                                                                           │

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-ir-inl.h 'maglev' | head -n 1000Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter maglev-ir.cc 'maglev' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'builtins', 'options': '--filter array-foreach.tq'}                                                                                                 │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter ThrowIfNotCallable 'maglev' | head -n 1000
Observations: unknown option: maglev
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter ArrayForEachLoopEagerDeoptContinuation 'builtins' | head -n 1000
Observations: unknown option: maglev
Observations: unknown option: builtins
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'options': '-L 2 maglev'}                                                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: unknown option: maglev
Observations: unknown option: maglev
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter array-foreach.tq 'builtins' | head -n 1000
Observations: unknown option: maglev
Observations: unknown option: maglev
Observations: unknown option: builtins
Observations: unknown option: builtins
Command: cd /Users/tarun/v8/v8/src && tree -L 2 maglev | head -n 1000
Observations: maglev
├── arm
│   ├── maglev-assembler-arm-inl.h
│   ├── maglev-assembler-arm.cc
│   └── maglev-ir-arm.cc
├── arm64
│   ├── maglev-assembler-arm64-inl.h
│   ├── maglev-assembler-arm64.cc
│   └── maglev-ir-arm64.cc
├── DEPS
├── DIR_METADATA
├── maglev-assembler-inl.h
├── maglev-assembler.cc
├── maglev-assembler.h
├── maglev-basic-block.h
├── maglev-code-gen-state.h
├── maglev-code-generator.cc
├── maglev-code-generator.h
├── maglev-compilation-info.cc
├── maglev-compilation-info.h
├── maglev-compilation-unit.cc
├── maglev-compilation-unit.h
├── maglev-compiler.cc
├── maglev-compiler.h
├── maglev-concurrent-dispatcher.cc
├── maglev-concurrent-dispatcher.h
├── maglev-graph-builder.cc
├── maglev-graph-builder.h
├── maglev-graph-labeller.h
├── maglev-graph-printer.cc
├── maglev-graph-printer.h
├── maglev-graph-processor.h
├── maglev-graph-verifier.h
├── maglev-graph.h
├── maglev-interpreter-frame-state.cc
├── maglev-interpreter-frame-state.h
├── maglev-ir-inl.h
├── maglev-ir.cc
├── maglev-ir.h
├── maglev-phi-representation-selector.cc
├── maglev-phi-representation-selector.h
├── maglev-pipeline-statistics.cc
├── maglev-pipeline-statistics.h
├── maglev-post-hoc-optimizations-processors.h
├── maglev-pre-regalloc-codegen-processors.h
├── maglev-regalloc-data.h
├── maglev-regalloc.cc
├── maglev-regalloc.h
├── maglev-register-frame-array.h
├── maglev.cc
├── maglev.h
├── OWNERS
├── riscv
│   ├── maglev-assembler-riscv-inl.h
│   ├── maglev-assembler-riscv.cc
│   └── maglev-ir-riscv.cc
├── s390
│   ├── maglev-assembler-s390-inl.h
│   ├── maglev-assembler-s390.cc
│   └── maglev-ir-s390.cc
└── x64
    ├── maglev-assembler-x64-inl.h
    ├── maglev-assembler-x64.cc
    └── maglev-ir-x64.cc

6 directories, 58 files
[Step 5: Duration 13.15 seconds| Input tokens: 27,879 | Output tokens: 3,552]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 6 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=maglev-graph-builder.cc'}                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=maglev-graph-builder.cc '' | head -n 1000
Observations: unknown option:
[Step 6: Duration 14.77 seconds| Input tokens: 39,176 | Output tokens: 4,227]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 7 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=maglev-graph-builder.cc'}                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=maglev-ir.h'}                                                                                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=maglev-ir.cc'}                                                                                                             │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=ThrowIfNotCallable'}                                                                                                       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=maglev-graph-builder.cc '' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=maglev-ir.h '' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=maglev-ir.cc '' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=NoElementsProtector'}                                                                                                      │
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=array-foreach.tq'}                                                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=ThrowIfNotCallable '' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=ArrayForEachLoopEagerDeoptContinuation'}                                                                                   │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=NoElementsProtector '' | head -n 1000
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=ArrayForEachLoopLazyDeoptContinuation'}                                                                                    │
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=array-foreach.tq '' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=ArrayForEachLoopEagerDeoptContinuation '' | head -n 1000
Observations: unknown option:
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': '', 'options': '--filter=DeoptFrameScope'}                                                                                                          │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'options': '-L 3 maglev'}                                                                                                                                              │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=ArrayForEachLoopLazyDeoptContinuation '' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Observations: unknown option:
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter=DeoptFrameScope '' | head -n 1000
Observations: unknown option:
Observations: unknown option:
Observations: unknown option:
Observations: unknown option:
Observations: unknown option:
Observations: unknown option:
Observations: unknown option:
Command: cd /Users/tarun/v8/v8/src && tree -L 3 maglev | head -n 1000
Observations: maglev
├── arm
│   ├── maglev-assembler-arm-inl.h
│   ├── maglev-assembler-arm.cc
│   └── maglev-ir-arm.cc
├── arm64
│   ├── maglev-assembler-arm64-inl.h
│   ├── maglev-assembler-arm64.cc
│   └── maglev-ir-arm64.cc
├── DEPS
├── DIR_METADATA
├── maglev-assembler-inl.h
├── maglev-assembler.cc
├── maglev-assembler.h
├── maglev-basic-block.h
├── maglev-code-gen-state.h
├── maglev-code-generator.cc
├── maglev-code-generator.h
├── maglev-compilation-info.cc
├── maglev-compilation-info.h
├── maglev-compilation-unit.cc
├── maglev-compilation-unit.h
├── maglev-compiler.cc
├── maglev-compiler.h
├── maglev-concurrent-dispatcher.cc
├── maglev-concurrent-dispatcher.h
├── maglev-graph-builder.cc
├── maglev-graph-builder.h
├── maglev-graph-labeller.h
├── maglev-graph-printer.cc
├── maglev-graph-printer.h
├── maglev-graph-processor.h
├── maglev-graph-verifier.h
├── maglev-graph.h
├── maglev-interpreter-frame-state.cc
├── maglev-interpreter-frame-state.h
├── maglev-ir-inl.h
├── maglev-ir.cc
├── maglev-ir.h
├── maglev-phi-representation-selector.cc
├── maglev-phi-representation-selector.h
├── maglev-pipeline-statistics.cc
├── maglev-pipeline-statistics.h
├── maglev-post-hoc-optimizations-processors.h
├── maglev-pre-regalloc-codegen-processors.h
├── maglev-regalloc-data.h
├── maglev-regalloc.cc
├── maglev-regalloc.h
├── maglev-register-frame-array.h
├── maglev.cc
├── maglev.h
├── OWNERS
├── riscv
│   ├── maglev-assembler-riscv-inl.h
│   ├── maglev-assembler-riscv.cc
│   └── maglev-ir-riscv.cc
├── s390
│   ├── maglev-assembler-s390-inl.h
│   ├── maglev-assembler-s390.cc
│   └── maglev-ir-s390.cc
└── x64
    ├── maglev-assembler-x64-inl.h
    ├── maglev-assembler-x64.cc
    └── maglev-ir-x64.cc

6 directories, 58 files
[Step 7: Duration 19.00 seconds| Input tokens: 50,518 | Output tokens: 5,009]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 8 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.cc', 'options': "--filter 'maglev-graph-builder.cc'"}                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.h', 'options': "--filter 'maglev-ir.h'"}                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.cc', 'options': "--filter 'maglev-ir.cc'"}                                                                                               │
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'maglev-graph-builder.cc' 'maglev-graph-builder.cc' | head -n 1000
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Observations: unknown option: maglev-graph-builder.cc
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'array-foreach.tq', 'options': "--filter 'array-foreach.tq'"}                                                                                       │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'maglev-ir.h' 'maglev-ir.h' | head -n 1000
Observations: unknown option: maglev-ir.h
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'maglev-ir.cc' 'maglev-ir.cc' | head -n 1000
Observations: unknown option: maglev-ir.cc
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf --filter 'array-foreach.tq' 'array-foreach.tq' | head -n 1000
Observations: unknown option: array-foreach.tq
[Step 8: Duration 12.78 seconds| Input tokens: 63,339 | Output tokens: 5,672]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 9 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-graph-builder.cc'}                                                                                                                          │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'array-foreach.tq'}                                                                                                                                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-graph-builder.cc' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.h'}                                                                                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir.cc'}                                                                                                                                     │
│ Calling tool: 'tree' with arguments: {'options': '-L 2 builtins'}                                                                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'array-foreach.tq' | head -n 1000
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-ir.cc' | head -n 1000
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'maglev-ir-inl.h'}                                                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: unknown option: maglev-graph-builder.cc
Observations: unknown option: array-foreach.tq
Observations: unknown option: maglev-ir.cc
Command: cd /Users/tarun/v8/v8/src && tree -L 2 builtins | head -n 1000
Observations: builtins
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-ir.h' | head -n 1000
├── accessors.h
├── aggregate-error.tq
├── arm
│   └── builtins-arm.cc
├── arm64
│   └── builtins-arm64.cc
├── array-at.tq
├── array-concat.tq
├── array-copywithin.tq
├── array-every.tq
├── array-filter.tq
├── array-find.tq
├── array-findindex.tq
├── array-findlast.tq
├── array-findlastindex.tq
├── array-flat.tq
├── array-foreach.tq
├── array-from-async.tq
├── array-from.tq
├── array-isarray.tq
├── array-join.tq
├── array-lastindexof.tq
├── array-map.tq
├── array-of.tq
├── array-reduce-right.tq
├── array-reduce.tq
├── array-reverse.tq
├── array-shift.tq
├── array-slice.tq
├── array-some.tq
├── array-splice.tq
├── array-to-reversed.tq
├── array-to-sorted.tq
├── array-to-spliced.tq
├── array-unshift.tq
├── array-with.tq
├── array.tq
├── arraybuffer.tq
├── base.tq
├── boolean.tq
├── builtins-abstract-module-source.cc
├── builtins-api.cc
├── builtins-array-gen.cc
├── builtins-array-gen.h
├── builtins-array.cc
├── builtins-arraybuffer.cc
├── builtins-async-disposable-stack.cc
├── builtins-async-function-gen.cc
├── builtins-async-gen.cc
├── builtins-async-gen.h
├── builtins-async-generator-gen.cc
├── builtins-async-iterator-gen.cc
├── builtins-async-module.cc
├── builtins-atomics-synchronization.cc
├── builtins-bigint-gen.cc
├── builtins-bigint-gen.h
├── builtins-bigint.cc
├── builtins-bigint.tq
├── builtins-call-gen.cc
├── builtins-call-gen.h
├── builtins-callsite.cc
├── builtins-collections-gen.cc
├── builtins-collections-gen.h
├── builtins-collections.cc
├── builtins-console.cc
├── builtins-constructor-gen.cc
├── builtins-constructor-gen.h
├── builtins-constructor.h
├── builtins-conversion-gen.cc
├── builtins-data-view-gen.h
├── builtins-dataview.cc
├── builtins-date-gen.cc
├── builtins-date.cc
├── builtins-definitions.h
├── builtins-descriptors.h
├── builtins-disposable-stack.cc
├── builtins-error.cc
├── builtins-function.cc
├── builtins-generator-gen.cc
├── builtins-global-gen.cc
├── builtins-global.cc
├── builtins-handler-gen.cc
├── builtins-ic-gen.cc
├── builtins-inl.h
├── builtins-internal-gen.cc
├── builtins-internal.cc
├── builtins-interpreter-gen.cc
├── builtins-intl-gen.cc
├── builtins-intl.cc
├── builtins-iterator-gen.cc
├── builtins-iterator-gen.h
├── builtins-json.cc
├── builtins-lazy-gen.cc
├── builtins-lazy-gen.h
├── builtins-microtask-queue-gen.cc
├── builtins-number-gen.cc
├── builtins-number-tsa.cc
├── builtins-number.cc
├── builtins-object-gen.cc
├── builtins-object-gen.h
├── builtins-object.cc
├── builtins-promise-gen.cc
├── builtins-promise-gen.h
├── builtins-promise.h
├── builtins-proxy-gen.cc
├── builtins-proxy-gen.h
├── builtins-reflect.cc
├── builtins-regexp-gen.cc
├── builtins-regexp-gen.h
├── builtins-regexp.cc
├── builtins-shadow-realm-gen.cc
├── builtins-shadow-realm.cc
├── builtins-shared-array.cc
├── builtins-sharedarraybuffer-gen.cc
├── builtins-sharedarraybuffer.cc
├── builtins-string-gen.cc
├── builtins-string-gen.h
├── builtins-string-tsa.cc
├── builtins-string.cc
├── builtins-string.tq
├── builtins-struct.cc
├── builtins-symbol.cc
├── builtins-temporal-gen.cc
├── builtins-temporal.cc
├── builtins-trace.cc
├── builtins-typed-array-gen.cc
├── builtins-typed-array-gen.h
├── builtins-typed-array.cc
├── builtins-utils-gen.h
├── builtins-utils-inl.h
├── builtins-utils.h
├── builtins-wasm-gen.cc
├── builtins-wasm-gen.h
├── builtins-weak-refs.cc
├── builtins.cc
├── builtins.h
├── cast.tq
├── collections.tq
├── console.tq
├── constants-table-builder.cc
├── constants-table-builder.h
├── constructor.tq
├── conversion.tq
├── convert.tq
├── data-view-ops.h
├── data-view.tq
├── accessors.cc
├── DIR_METADATA
├── finalization-registry.tq
├── frame-arguments.tq
├── frames.tq
├── function.tq
├── generate-bytecodes-builtins-list.cc
├── growable-fixed-array-gen.cc
├── growable-fixed-array-gen.h
├── growable-fixed-array.tq
├── ia32
│   └── builtins-ia32.cc
├── ic-callable.tq
├── ic.tq
├── internal-coverage.tq
├── internal.tq
├── iterator-from.tq
├── iterator-helpers.tq
├── iterator.tq
├── js-to-js.tq
├── js-to-wasm.tq
├── loong64
│   └── builtins-loong64.cc
├── map-groupby.tq
├── math.tq
├── mips64
│   └── builtins-mips64.cc
├── number-builtins-reducer-inl.h
├── number.tq
├── object-fromentries.tq
├── object-groupby.tq
├── object.tq
├── OWNERS
├── ppc
│   └── builtins-ppc.cc
├── profile-data-reader.cc
├── profile-data-reader.h
├── promise-abstract-operations.tq
├── promise-all-element-closure.tq
├── promise-all.tq
├── promise-any.tq
├── promise-constructor.tq
├── promise-finally.tq
├── promise-jobs.tq
├── promise-misc.tq
├── promise-race.tq
├── promise-reaction-job.tq
├── promise-resolve.tq
├── promise-then.tq
├── promise-try.tq
├── promise-withresolvers.tq
├── proxy-constructor.tq
├── proxy-delete-property.tq
├── proxy-get-property.tq
├── proxy-get-prototype-of.tq
├── proxy-has-property.tq
├── proxy-is-extensible.tq
├── proxy-prevent-extensions.tq
├── proxy-revocable.tq
├── proxy-revoke.tq
├── proxy-set-property.tq
├── proxy-set-prototype-of.tq
├── proxy.tq
├── reflect.tq
├── regexp-exec.tq
├── regexp-match-all.tq
├── DEPS
├── regexp-replace.tq
├── regexp-search.tq
├── regexp-source.tq
├── regexp-split.tq
├── regexp-test.tq
├── regexp.tq
├── riscv
│   └── builtins-riscv.cc
├── s390
│   └── builtins-s390.cc
├── set-difference.tq
├── set-intersection.tq
├── set-is-disjoint-from.tq
├── set-is-subset-of.tq
├── set-is-superset-of.tq
├── set-symmetric-difference.tq
├── set-union.tq
├── setup-builtins-internal.cc
├── string-at.tq
├── string-endswith.tq
├── string-html.tq
├── string-includes.tq
├── string-indexof.tq
├── string-iswellformed.tq
├── string-iterator.tq
├── string-match-search.tq
├── string-pad.tq
├── string-repeat.tq
├── string-replaceall.tq
├── string-slice.tq
├── string-startswith.tq
├── string-substr.tq
├── string-substring.tq
├── string-towellformed.tq
├── string-trim.tq
├── suppressed-error.tq
├── symbol.tq
├── torque-csa-header-includes.h
├── torque-internal.tq
├── typed-array-at.tq
├── typed-array-createtypedarray.tq
├── typed-array-entries.tq
├── typed-array-every.tq
├── typed-array-filter.tq
├── typed-array-find.tq
├── typed-array-findindex.tq
├── typed-array-findlast.tq
├── typed-array-findlastindex.tq
├── typed-array-foreach.tq
├── typed-array-from.tq
├── typed-array-keys.tq
├── typed-array-of.tq
├── typed-array-reduce.tq
├── typed-array-reduceright.tq
├── typed-array-set.tq
├── typed-array-slice.tq
├── typed-array-some.tq
├── typed-array-sort.tq
├── typed-array-subarray.tq
├── typed-array-to-reversed.tq
├── typed-array-to-sorted.tq
├── typed-array-values.tq
├── typed-array-with.tq
├── typed-array.tq
├── wasm-strings.tq
├── wasm-to-js.tq
├── wasm.tq
├── weak-ref.tq
└── x64
    └── builtins-x64.cc

10 directories, 275 files
├── regexp-match.tq
Observations: unknown option: maglev-ir.h
Command: cd /Users/tarun/v8/v8/src && rg --hidden --no-follow --no-ignore-vcs --files 2>/dev/null | fzf  'maglev-ir-inl.h' | head -n 1000
Observations: unknown option: maglev-ir-inl.h
[Step 9: Duration 9.20 seconds| Input tokens: 76,357 | Output tokens: 6,199]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 10 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[Step 10: Duration 2.12 seconds]
[Step 3: Duration 176.05 seconds| Input tokens: 19,209 | Output tokens: 4,525]
[Step 2: Duration 1340.05 seconds| Input tokens: 7,405 | Output tokens: 1,600]
Traceback (most recent call last):
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rises-the-fog.py", line 101, in <module>
    else:
    
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rises-the-fog.py", line 95, in main
    try:
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/FoG.py", line 251, in start_system
    result = self.run_task(
        task_description="Initialize Root Manager orchestration",
    ...<5 lines>...
        }
    )
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 323, in run_task
    agent_output = manager_agent.run(prompt)
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 292, in _wrapped_run
    result = orig_run(prompt, *args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1322, in _step_stream
    for output in self.process_tool_calls(chat_message, memory_step):
                  ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1407, in process_tool_calls
    tool_output = process_single_tool_call(tool_call)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1376, in process_single_tool_call
    tool_call_result = self.execute_tool_call(tool_name, tool_arguments)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1474, in execute_tool_call
    return tool(**arguments) if is_managed_agent else tool(**arguments, sanitize_inputs_outputs=True)
           ~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 877, in __call__
    result = self.run(full_task, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1322, in _step_stream
    for output in self.process_tool_calls(chat_message, memory_step):
                  ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1407, in process_tool_calls
    tool_output = process_single_tool_call(tool_call)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1376, in process_single_tool_call
    tool_call_result = self.execute_tool_call(tool_name, tool_arguments)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1474, in execute_tool_call
    return tool(**arguments) if is_managed_agent else tool(**arguments, sanitize_inputs_outputs=True)
           ~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 877, in __call__
    result = self.run(full_task, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1290, in _step_stream
    chat_message: ChatMessage = self.model.generate(
                                ~~~~~~~~~~~~~~~~~~~^
        input_messages,
        ^^^^^^^^^^^^^^^
        stop_sequences=["Observation:", "Calling tools:"],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        tools_to_call_from=self.tools_and_managed_agents,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/models.py", line 1243, in generate
    response = self.client.completion(**completion_kwargs)
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 164, in wrapped_completion
    result = original_completion(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/utils.py", line 1244, in wrapper
    result = original_function(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/main.py", line 2125, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/llms/openai/openai.py", line 673, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/litellm_core_utils/logging_utils.py", line 237, in sync_wrapper
    result = func(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/llms/openai/openai.py", line 471, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
        request,
        stream=stream or self._should_stream_response_body(request=request),
        **kwargs,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
        request,
    ...<2 lines>...
        history=[],
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
        request,
        follow_redirects=follow_redirects,
        history=history,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
        pool_request.request
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
        self.READ_NUM_BYTES, timeout=timeout
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_backends/sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py", line 1285, in recv
    return self.read(buflen)
           ~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py", line 1140, in read
    return self._sslobj.read(len)
           ~~~~~~~~~~~~~~~~~^^^^^
KeyboardInterrupt
