I must go in; the fog is rising
Initializing FatherOfGod
╭───────────────────────────────────── New run - FatherOfGeorge ──────────────────────────────────────╮
│                                                                                                     │
│ Task: Initialize FoG orchestration across PickSection, FatherOfGeorge, CodeAnalyzer, and            │
│ ProgramBuilder                                                                                      │
│                                                                                                     │
│ Context:                                                                                            │
│ - PickSection: Select a promising V8 subsystem or JIT-related area to explore                       │
│ - FatherOfGeorge: Coordinate between analysis and program generation                                │
│ - CodeAnalyzer: Analyze V8 code and knowledge bases to guide fuzz inputs                            │
│ - ProgramBuilder: Synthesize JavaScript programs and templates for fuzzing                          │
│                                                                                                     │
╰─ LiteLLMModel - gpt-5-mini ─────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[Step 1: Duration 0.95 seconds]
Traceback (most recent call last):
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rises-the-fog.py", line 101, in <module>
    sys.exit(main())
             ~~~~^^
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rises-the-fog.py", line 95, in main
    a.system.start_system()
    ~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/FoG.py", line 221, in start_system
    result = self.run_task(
        task_description="Initialize FoG orchestration across PickSection, FatherOfGeorge, CodeAnalyzer, and ProgramBuilder",
    ...<5 lines>...
        }
    )
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 323, in run_task
    agent_output = manager_agent.run(prompt)
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 292, in _wrapped_run
    result = orig_run(prompt, *args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1290, in _step_stream
    chat_message: ChatMessage = self.model.generate(
                                ~~~~~~~~~~~~~~~~~~~^
        input_messages,
        ^^^^^^^^^^^^^^^
        stop_sequences=["Observation:", "Calling tools:"],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        tools_to_call_from=self.tools_and_managed_agents,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/models.py", line 1243, in generate
    response = self.client.completion(**completion_kwargs)
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 164, in wrapped_completion
    result = original_completion(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/utils.py", line 1244, in wrapper
    result = original_function(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/main.py", line 2125, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/llms/openai/openai.py", line 673, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/litellm_core_utils/logging_utils.py", line 237, in sync_wrapper
    result = func(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/llms/openai/openai.py", line 471, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
        request,
        stream=stream or self._should_stream_response_body(request=request),
        **kwargs,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
        request,
    ...<2 lines>...
        history=[],
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
        request,
        follow_redirects=follow_redirects,
        history=history,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
        pool_request.request
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
        self.READ_NUM_BYTES, timeout=timeout
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_backends/sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py", line 1285, in recv
    return self.read(buflen)
           ~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py", line 1140, in read
    return self._sslobj.read(len)
           ~~~~~~~~~~~~~~~~~^^^^^
KeyboardInterrupt
I must go in; the fog is rising
Initializing FatherOfGod
╭─────────────────────────────────────────────────────────────────────────────────────────── New run - FatherOfGeorge ────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                                 │
│ Task: Initialize FoG orchestration across PickSection, FatherOfGeorge, CodeAnalyzer, and ProgramBuilder                                                                                                         │
│                                                                                                                                                                                                                 │
│ Context:                                                                                                                                                                                                        │
│ - PickSection: Select a promising V8 subsystem or JIT-related area to explore                                                                                                                                   │
│ - FatherOfGeorge: Coordinate between analysis and program generation                                                                                                                                            │
│ - CodeAnalyzer: Analyze V8 code and knowledge bases to guide fuzz inputs                                                                                                                                        │
│ - ProgramBuilder: Synthesize JavaScript programs and templates for fuzzing                                                                                                                                      │
│                                                                                                                                                                                                                 │
╰─ LiteLLMModel - gpt-5-mini ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'PickSection' with arguments: {'task': 'Select a promising V8 subsystem or JIT-related area to explore for directed fuzzing. Provide a concrete starting code region (specific file paths and     │
│ function/class names and line ranges if possible) that is relevant to TurboFan, Ignition, or the interface between the Ignition interpreter and TurboFan optimized code (including deoptimization and bailout   │
│ paths). Explain why the region is promising (complexity, speculative optimizations, frequent deopt paths, known tricky invariants, recent churn), list any immediate dependencies required to analyze/build     │
│ that region (other files, headers, runtime flags to enable), and suggest 3 entry points (exported functions, runtime functions, bytecode handlers, etc.) to target when generating JS programs/templates. Also  │
│ provide recommended artifacts to pass to the next stage (code snippets, AST/IR fragments, control-flow hotspots) and any constraints (e.g., requires specific V8 build flags like --turbo, --no-always-opt).    │
│ Keep the response actionable for the CodeAnalyzer and ProgramBuilder agents that will run next.'}                                                                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────────────────────────────────────────────────────────────────────── New run - PickSection ─────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                                                 │
│ You're a helpful agent named 'PickSection'.                                                                                                                                                                     │
│ You have been submitted this task by your manager.                                                                                                                                                              │
│ ---                                                                                                                                                                                                             │
│ Task:                                                                                                                                                                                                           │
│ Select a promising V8 subsystem or JIT-related area to explore for directed fuzzing. Provide a concrete starting code region (specific file paths and function/class names and line ranges if possible) that is │
│ relevant to TurboFan, Ignition, or the interface between the Ignition interpreter and TurboFan optimized code (including deoptimization and bailout paths). Explain why the region is promising (complexity,    │
│ speculative optimizations, frequent deopt paths, known tricky invariants, recent churn), list any immediate dependencies required to analyze/build that region (other files, headers, runtime flags to enable), │
│ and suggest 3 entry points (exported functions, runtime functions, bytecode handlers, etc.) to target when generating JS programs/templates. Also provide recommended artifacts to pass to the next stage (code │
│ snippets, AST/IR fragments, control-flow hotspots) and any constraints (e.g., requires specific V8 build flags like --turbo, --no-always-opt). Keep the response actionable for the CodeAnalyzer and            │
│ ProgramBuilder agents that will run next.                                                                                                                                                                       │
│ ---                                                                                                                                                                                                             │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.                       │
│                                                                                                                                                                                                                 │
│ Your final_answer WILL HAVE to contain these parts:                                                                                                                                                             │
│ ### 1. Task outcome (short version):                                                                                                                                                                            │
│ ### 2. Task outcome (extremely detailed version):                                                                                                                                                               │
│ ### 3. Additional context (if relevant):                                                                                                                                                                        │
│                                                                                                                                                                                                                 │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.                                                                                           │
│ And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.                                                                 │
│                                                                                                                                                                                                                 │
╰─ LiteLLMModel - gpt-5-mini ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'get_random_entry_data' with arguments: {}                                                                                                                                                        │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: this is entry data for compiler/regress-841117
{"js": "// Copyright 2018 the V8 project authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\n// Flags: 
--allow-natives-syntax\n\nvar v = 1e9;\nfunction f() { return Math.floor(v / 10); }\n%PrepareFunctionForOptimization(f);\nassertEquals(1e8, f());\n%OptimizeFunctionOnNextCall(f);\nassertEquals(1e8, f());\n", 
"Fuzzilli": "v0 <- LoadInteger '1000000000'\nv1 <- CreateNamedVariable 'v', 'var', v0\nv2 <- BeginPlainFunction -> \n    v3 <- LoadInteger '10'\n    v4 <- BinaryOperation v1, '/', v3\n    v5 <- 
CreateNamedVariable 'Math', 'none'\n    v6 <- CallMethod v5, 'floor', |v4]\n    Return v6\nEndPlainFunction\nv7 <- Eval '%PrepareFunctionForOptimization(%@)', |v2]\nv8 <- LoadInteger '100000000'\nv9 <- 
CallFunction v2, |]\nv10 <- CreateNamedVariable 'assertEquals', 'none'\nv11 <- CallFunction v10, |v8, v9]\nv12 <- Eval '%OptimizeFunctionOnNextCall(%@)', |v2]\nv13 <- LoadInteger '100000000'\nv14 <- CallFunction
v2, |]\nv15 <- CallFunction v10, |v13, v14]\n\n\nvar v = 1000000000;\nfunction f() {\n    return Math.floor(v / 10);\n}\nconst v7 = %PrepareFunctionForOptimization(f);\nassertEquals(100000000, f());\nconst v12 =
%OptimizeFunctionOnNextCall(f);\nassertEquals(100000000, f());", "execution_data": "Concurrent maglev has been disabled for tracing.\nregressions/compiler/regress-841117.js:9: SyntaxError: Unexpected token 
'%'\n%PrepareFunctionForOptimization(f);\n^\nSyntaxError: Unexpected token '%'\n\n"}
[Step 1: Duration 5.22 seconds| Input tokens: 1,405 | Output tokens: 149]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'regress-841117'}                                                                                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: compiler/regress-841117
[Step 2: Duration 3.49 seconds| Input tokens: 3,321 | Output tokens: 308]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'get_js_entry_data_by_name' with arguments: {'file_name': 'compiler/regress-841117'}                                                                                                              │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: {"js": "// Copyright 2018 the V8 project authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.\n\n// Flags: 
--allow-natives-syntax\n\nvar v = 1e9;\nfunction f() { return Math.floor(v / 10); }\n%PrepareFunctionForOptimization(f);\nassertEquals(1e8, f());\n%OptimizeFunctionOnNextCall(f);\nassertEquals(1e8, f());\n", 
"Fuzzilli": "v0 <- LoadInteger '1000000000'\nv1 <- CreateNamedVariable 'v', 'var', v0\nv2 <- BeginPlainFunction -> \n    v3 <- LoadInteger '10'\n    v4 <- BinaryOperation v1, '/', v3\n    v5 <- 
CreateNamedVariable 'Math', 'none'\n    v6 <- CallMethod v5, 'floor', |v4]\n    Return v6\nEndPlainFunction\nv7 <- Eval '%PrepareFunctionForOptimization(%@)', |v2]\nv8 <- LoadInteger '100000000'\nv9 <- 
CallFunction v2, |]\nv10 <- CreateNamedVariable 'assertEquals', 'none'\nv11 <- CallFunction v10, |v8, v9]\nv12 <- Eval '%OptimizeFunctionOnNextCall(%@)', |v2]\nv13 <- LoadInteger '100000000'\nv14 <- CallFunction
v2, |]\nv15 <- CallFunction v10, |v13, v14]\n\n\nvar v = 1000000000;\nfunction f() {\n    return Math.floor(v / 10);\n}\nconst v7 = %PrepareFunctionForOptimization(f);\nassertEquals(100000000, f());\nconst v12 =
%OptimizeFunctionOnNextCall(f);\nassertEquals(100000000, f());", "execution_data": "Concurrent maglev has been disabled for tracing.\nregressions/compiler/regress-841117.js:9: SyntaxError: Unexpected token 
'%'\n%PrepareFunctionForOptimization(f);\n^\nSyntaxError: Unexpected token '%'\n\n"}
[Step 3: Duration 4.40 seconds| Input tokens: 5,283 | Output tokens: 533]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'compiler/regress-'}                                                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: compiler/regress-377213712-2
compiler/regress-945644
compiler/regress-419744895
compiler/regress-inline-callfunctionstub
compiler/regress-5538
compiler/regress-949435
compiler/regress-797596
compiler/regress-2
compiler/regress-gvn
compiler/regress-v8-5756
compiler/regress-1126771
compiler/regress-crbug-1486342
compiler/regress-3
compiler/regress-625966
compiler/regress-671574
compiler/regress-crbug-1399490
compiler/regress-crbug-1209558
compiler/regress-445859
compiler/regress-5129
compiler/regress-1196683
compiler/regress-crbug-369685641
compiler/regress-402646504
compiler/regress-4389-6
compiler/regress-or
compiler/regress-3786
compiler/regress-379519287
compiler/regress-952586
compiler/regress-628516
compiler/regress-995430
compiler/regress-674469
compiler/regress-funcaller
compiler/regress-1102053
compiler/regress-639210
compiler/regress-946889
compiler/regress-crbug-540593
compiler/regress-762057
compiler/regress-1125145
compiler/regress-dead-throw-inlining
compiler/regress-347724915
compiler/regress-1
compiler/regress-crbug-11564
compiler/regress-3185901
compiler/regress-385386138
compiler/regress-crbug-1483259
compiler/regress-4206
compiler/regress-758096
compiler/regress-1158049
compiler/regress-crbug-1487463
compiler/regress-1196185
compiler/regress-crbug-1488746
compiler/regress-377213712-1
compiler/regress-780658
compiler/regress-crbug-1520697
compiler/regress-1074736
compiler/regress-452427
compiler/regress-726554
compiler/regress-int32array-outofbounds-nan
compiler/regress-1195650
compiler/regress-1394
compiler/regress-957559
compiler/regress-crbug-1228233
compiler/regress-5
compiler/regress-1302572
compiler/regress-crbug-1241464
compiler/regress-crbug-1509576
compiler/regress-469089
compiler/regress-1082704
compiler/regress-3249650
compiler/regress-406999116
compiler/regress-1199345
compiler/regress-1068494
compiler/regress-1239601
compiler/regress-665680
compiler/regress-1217562
compiler/regress-924151
compiler/regress-crbug-1479323
compiler/regress-934175
compiler/regress-1127319
compiler/regress-444695
compiler/regress-1067544
compiler/regress-396463255
compiler/regress-9945-1
compiler/regress-9087
compiler/regress-shift-right
compiler/regress-977670
compiler/regress-796041
compiler/regress-675704
compiler/regress-376307767
compiler/regress-3136962
compiler/regress-394650781
compiler/regress-const
compiler/regress-394327270
compiler/regress-lbranch-double
compiler/regress-crbug-1221812
compiler/regress-loop-variable-unsigned
compiler/regress-crbug-1228407
compiler/regress-8380
compiler/regress-445267
compiler/regress-crbug-1201011
compiler/regress-673244
compiler/regress-loop-variable-if
compiler/regress-607493
compiler/regress-633497
compiler/regress-729369
compiler/regress-713367
compiler/regress-376818204
compiler/regress-650215
compiler/regress-v8-6077
compiler/regress-668760
compiler/regress-register-allocator3
compiler/regress-439743
compiler/regress-bit-number-constant
compiler/regress-bound-functions
compiler/regress-1072171
compiler/regress-1024936
compiler/regress-939316
compiler/regress-788539
compiler/regress-1224277
compiler/regress-v8-6631
compiler/regress-628403
compiler/regress-max
compiler/regress-1225607
compiler/regress-393411524
compiler/regress-1200490
compiler/regress-1126249
compiler/regress-961986
compiler/regress-1109174
compiler/regress-700883
compiler/regress-446778
compiler/regress-335310000
compiler/regress-902608
compiler/regress-400052777
compiler/regress-1146652
compiler/regress-number-is-hole-nan
compiler/regress-801097
compiler/regress-398431403
compiler/regress-3218915
compiler/regress-491578
compiler/regress-1423468
compiler/regress-1102683
compiler/regress-715651
compiler/regress-9945-2
compiler/regress-340663085
compiler/regress-815392
compiler/regress-crbug-1230260
compiler/regress-625558
compiler/regress-739902
compiler/regress-gap
compiler/regress-932392
compiler/regress-995562
compiler/regress-1053604
compiler/regress-758983
compiler/regress-958350
compiler/regress-9137-2
compiler/regress-572409
compiler/regress-890057
compiler/regress-817225
compiler/regress-crbug-1518396
compiler/regress-4389-3
compiler/regress-crbug-1226264
compiler/regress-serialized-slots
compiler/regress-725743
compiler/regress-808472
compiler/regress-math-sign-nan-type
compiler/regress-crbug-1457532
compiler/regress-715204
compiler/regress-crbug-1211215
compiler/regress-crbug-1474066
compiler/regress-1177368
compiler/regress-600593
compiler/regress-714483
compiler/regress-string-to-number-add
compiler/regress-crbug-1234770
compiler/regress-arguments
compiler/regress-crbug-1474379
compiler/regress-4207
compiler/regress-crbug-1517218
compiler/regress-crbug-1463334
compiler/regress-loadfield
compiler/regress-intoverflow
compiler/regress-1028862
compiler/regress-escape-analysis-indirect
compiler/regress-crbug-1227677
compiler/regress-463056
compiler/regress-992684
compiler/regress-446156
compiler/regress-shift-right-logical
compiler/regress-1065737
compiler/regress-1051017
compiler/regress-4389-1
compiler/regress-loop-deopt
compiler/regress-shared-deopt
compiler/regress-1028208
compiler/regress-626986
compiler/regress-compare-negate
compiler/regress-733181
compiler/regress-331909453
compiler/regress-1150649
compiler/regress-crbug-1399626
compiler/regress-to-number-binop-deopt
compiler/regress-895799
compiler/regress-694088
compiler/regress-crbug-1467471
compiler/regress-376100382
compiler/regress-416359
compiler/regress-1062916
compiler/regress-630611
compiler/regress-731495
compiler/regress-crbug-1247763
compiler/regress-1236716
compiler/regress-store-store-elim
compiler/regress-621147
compiler/regress-stacktrace
compiler/regress-register-allocator
compiler/regress-359729268
compiler/regress-4470-1
compiler/regress-truncate-number-or-undefined-to-float64
compiler/regress-crbug-1223107
compiler/regress-4389-4
compiler/regress-crbug-974474
compiler/regress-944062-1
compiler/regress-761892
compiler/regress-773954
compiler/regress-1094132
compiler/regress-1085
compiler/regress-crbug-1148758
compiler/regress-966560-2
compiler/regress-1071743
compiler/regress-468727
compiler/regress-nonextensiblearray-store-outofbounds
compiler/regress-crbug-1519399
compiler/regress-crbug-1426299
compiler/regress-392667952
compiler/regress-crbug-1234764
compiler/regress-443744
compiler/regress-shift-left
compiler/regress-1092011
compiler/regress-644048
compiler/regress-910838
compiler/regress-store-holey-double-array
compiler/regress-crbug-974476
compiler/regress-420689315
compiler/regress-v8-9113
compiler/regress-1202924
compiler/regress-913232
compiler/regress-919754
compiler/regress-f64-w32-change
compiler/regress-664490
compiler/regress-1250216
compiler/regress-1177369
compiler/regress-1161357
compiler/regress-variable-liveness-let
compiler/regress-392660676
compiler/regress-3812
compiler/regress-5074
compiler/regress-324782095
compiler/regress-v8-5573
compiler/regress-444508
compiler/regress-crbug-1484894
compiler/regress-645851
compiler/regress-lazy-deopt
compiler/regress-1063661
compiler/regress-1127405
compiler/regress-register-allocator2
compiler/regress-958021
compiler/regress-9137-1
compiler/regress-644633
compiler/regress-crbug-1484895
compiler/regress-1203116
compiler/regress-4389-5
compiler/regress-crbug-1468717
compiler/regress-378408466
compiler/regress-1198705
compiler/regress-crbug-1353360
compiler/regress-905555-2
compiler/regress-4389-2
compiler/regress-638132
compiler/regress-1084820
compiler/regress-411262
compiler/regress-106351
compiler/regress-1182647
compiler/regress-8
compiler/regress-944062-2
compiler/regress-1070892
compiler/regress-v8-9139
compiler/regress-crbug-1464516
compiler/regress-crbug-1355824
compiler/regress-1104514
compiler/regress-crbug-1323114
compiler/regress-crbug-11977
compiler/regress-786521
compiler/regress-1202625
compiler/regress-toint32
compiler/regress-628773
compiler/regress-791245
compiler/regress-664117
compiler/regress-330638807
compiler/regress-crbug-1520774
compiler/regress-890620
compiler/regress-crbug-1201057
compiler/regress-crbug-1201082
compiler/regress-rep-change
compiler/regress-356183775
compiler/regress-stacktrace-methods
compiler/regress-445858
compiler/regress-356196918
compiler/regress-793863
compiler/regress-772420
compiler/regress-339492379
compiler/regress-variable-liveness
compiler/regress-935092
compiler/regress-1092650
compiler/regress-1037771
compiler/regress-451012
compiler/regress-4413-1
compiler/regress-arrayliteral
compiler/regress-1245949
compiler/regress-1195777
compiler/regress-945187
compiler/regress-736567
compiler/regress-377213712-3
compiler/regress-997100
compiler/regress-669517
compiler/regress-966560-1
compiler/regress-ntl-effect
compiler/regress-1202312
compiler/regress-888923
compiler/regress-crbug-1479847
compiler/regress-621423
compiler/regress-closures-with-eval
compiler/regress-799263
compiler/regress-crbug-1465075
compiler/regress-803022
compiler/regress-7121
compiler/regress-crbug-1486048
compiler/regress-958420
compiler/regress-9041
compiler/regress-sealedarray-store-outofbounds
compiler/regress-funarguments
compiler/regress-394120836
compiler/regress-deopt-call-as-function
compiler/regress-445732
compiler/regress-3260426
compiler/regress-5100
compiler/regress-445907
compiler/regress-5278
compiler/regress-5158
compiler/regress-404356997
compiler/regress-772872
compiler/regress-5320
compiler/regress-0
compiler/regress-905555
compiler/regress-347905990
compiler/regress-crbug-965513
compiler/regress-884052
compiler/regress-crbug-1470495
compiler/regress-crbug-1344965
compiler/regress-1061678
compiler/regress-crbug-1408013
compiler/regress-crbug-1486238
compiler/regress-958716
compiler/regress-445876
compiler/regress-crbug-1507662
compiler/regress-4
compiler/regress-1061803
compiler/regress-9017
compiler/regress-1215514
compiler/regress-crbug-1399627
compiler/regress-1459796
compiler/regress-693425
compiler/regress-1227324
compiler/regress-447567
compiler/regress-841117
compiler/regress-crbug-1502042
compiler/regress-6
compiler/regress-468162
compiler/regress-7
compiler/regress-177883
[Step 4: Duration 4.83 seconds| Input tokens: 7,758 | Output tokens: 691]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 5 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'PrepareFunctionForOptimization'}                                                                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: No results found
[Step 5: Duration 7.15 seconds| Input tokens: 13,278 | Output tokens: 1,041]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 6 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'PrepareFunctionForOptimization OptimizeFunctionOnNextCall deopt ignition turbfan bailout deoptimization frame materialization', 'top_k': 5,    │
│ 'topic_filter': 'v8'}                                                                                                                                                                                           │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/turbofan.txt",
    "similarity": 0.423,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/turbofan.txt\n\nTurboFan \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research TurboFan TurboFan is one of 
V8\u2019s optimizing compilers leveraging a concept called \u201cSea of Nodes\u201d . One of V8\u2019s blog posts offers a high-level overview of TurboFan . More details can be found in the following resources. 
Articles and blog posts # A tale of TurboFan Ignition + TurboFan and ES2015 An introduction to speculative optimization in V8 Talks # CodeStubAssembler: Redux An overview of the TurboFan compiler TurboFan IR 
TurboFan\u2019s JIT Design Fast arithmetic for dynamic languages Deoptimization in V8 TurboFan: a new code generation architecture for V8 ( video ) An internship on laziness (+ blog post ) Design documents # 
These are design documents that are mostly concerned with TurboFan internals. Function context specialization Rest parameters and arguments exotic objects optimization plan TurboFan developer tools integration 
TurboFan inlining TurboFan inlining heuristics TurboFan redundant bounds and overflow check elimination Lazy deoptimization without code patching Register allocator Projection nodes in TurboFan Related design 
documents # These are design documents that also affect TurboFan in a significant way. Computed property names (re)design document ES2015 and beyond performance plan Iterator builtins design document Making 
ES2015 classes fast RegExp builtins (re)design document Spread call performance Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are 
licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/ignition-interpreter.txt",
    "similarity": 0.403,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/ignition-interpreter.txt\n\nFiring up the Ignition interpreter \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features 
Research Firing up the Ignition interpreter Published 23 August 2016 \u00b7 Tagged with internals V8 and other modern JavaScript engines get their speed via just-in-time (JIT) compilation of script to native 
machine code immediately prior to execution. Code is initially compiled by a baseline compiler, which can generate non-optimized machine code quickly. The compiled code is analyzed during runtime and optionally 
re-compiled dynamically with a more advanced optimizing compiler for peak performance. In V8, this script execution pipeline has a variety of special cases and conditions which require complex machinery to 
switch between the baseline compiler and two optimizing compilers, Crankshaft and TurboFan. One of the issues with this approach (in addition to architectural complexity) is that the JITed machine code can 
consume a significant amount of memory, even if the code is only executed once. In order to mitigate this overhead, the V8 team has built a new JavaScript interpreter, called Ignition, which can replace 
V8\u2019s baseline compiler, executing code with less memory overhead and paving the way for a simpler script execution pipeline. With Ignition, V8 compiles JavaScript functions to a concise bytecode, which is 
between 50% to 25% the size of the equivalent baseline machine code. This bytecode is then executed by a high-performance interpreter which yields execution speeds on real-world websites close to those of code 
generated by V8\u2019s existing baseline compiler. In Chrome 53, Ignition will be enabled for Android devices which have limited RAM (512 MB or less), where memory savings are most needed. Results from early 
experiments in the field show that Ignition reduces the memory of each Chrome tab by around 5%. V8\u2019s compilation pipeline with Ignition enabled Details # In building Ignition\u2019s bytecode interpreter, 
the team considered a number of potential implementation approaches. A traditional interpreter, written in C++ would not be able to interact efficiently with the rest of V8\u2019s generated code. An alternative 
would have been to hand-code the interpreter in assembly code, however given V8 supports nine architecture ports, this would have entailed substantial engineering overhead. Instead, we opted for an approach 
which leveraged the strength of TurboFan, our new optimizing compiler, which is already tuned for optimal interaction with the V8 runtime and other generated code. The Ignition interpreter uses TurboFan\u2019s 
low-level, architecture-independent macro-assembly instructions to generate bytecode handlers for each opcode. TurboFan compiles these instructions to the target architecture, performing low-level instruction 
selection and machine register allocation in the process. This results in highly optimized interpreter code which can execute the bytecode instructions and interact with the rest of the V8 virtual machine in a 
low-overhead manner, with a minimal amount of new machinery added to the codebase. Ignition is a register machine, with each bytecode specifying its inputs and outputs as explicit register operands, as opposed 
to a stack machine where each bytecode would consume inputs and push outputs on an implicit stack. A special accumulator register is an implicit input and output register for many bytecodes. This reduces the 
size of bytecodes by avoiding the need to specify specific register operands. Since many JavaScript expressions involve chains of operations which are evaluated from left to right, the temporary results of these
operations can often remain in the accumulator throughout the expression\u2019s evaluation, minimizing the need for operations which load and store to explicit registers. As the bytecode is generated, it passes 
through a series of inline-optimization stages. These stages perform simple analysis on the bytecode stream, replacing common patterns with faster sequences, remove some redundant operations, and minimize the 
number of unnecessary register loads and transfers. Together, the optimizations further reduce the size of the bytecode and improve performance. For further details on the implementation of Ignition, see our 
BlinkOn talk: Future # Our focus for Ignition up until now has been to reduce V8\u2019s memory overhead. However, adding Ignition to our script execution pipeline opens up a number of future possibilities. The 
Ignition pipeline has been designed to enable us to make smarter decisions about when to execute and optimize code to speed up loading web pages and reduce jank and to make the interchange between V8\u2019s 
various components more efficient. Stay tuned for future developments in Ignition and V8. Posted by Ross McIlroy, V8 Ignition Jump Starter. Branding Terms Privacy Twitter Edit this page on GitHub Except as 
otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see
our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.402,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/ignition.txt",
    "similarity": 0.392,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/ignition.txt\n\nIgnition \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Ignition V8 features an interpreter 
called Ignition. Ignition is a fast low-level register-based interpreter written using the backend of TurboFan . The V8 blog post presents a high-level overview of the Ignition interpreter. More details can be 
found in the following resources: Talks # V8: Hooking up the Ignition to the Turbofan Ignition: jump-starting an interpreter for V8 Ignition: an interpreter for V8 ( video ) Articles # Understanding V8\u2019s 
bytecode Design Docs # Ignition design document Register equivalence optimization Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are 
licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb.txt",
    "similarity": 0.389,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb.txt\n\nDebugging builtins with GDB \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Debugging builtins 
with GDB As of V8 v6.9, it\u2019s possible to create breakpoints in GDB (and possibly other debuggers) to debug CSA / ASM / Torque builtins. (gdb) tb i::Isolate::Init Temporary breakpoint 1 at 0x7ffff706742b: 
i::Isolate::Init. (2 locations) (gdb) r Thread 1 \"d8\" hit Temporary breakpoint 1, 0x00007ffff7c55bc0 in Isolate::Init (gdb) br Builtins_RegExpPrototypeExec Breakpoint 2 at 0x7ffff7ac8784 (gdb) c Thread 1 
\"d8\" hit Breakpoint 2, 0x00007ffff7ac8784 in Builtins_RegExpPrototypeExec () Note that it works well to use a temporary breakpoint (shortcut tb in GDB) instead of a regular breakpoint ( br ) for this, since 
you only need it at process start. Builtins are also visible in stack traces: (gdb) bt #0 0x00007ffff7ac8784 in Builtins_RegExpPrototypeExec () #1 0x00007ffff78f5066 in Builtins_ArgumentsAdaptorTrampoline () #2 
0x000039751d2825b1 in ?? () #3 0x000037ef23a0fa59 in ?? () #4 0x0000000000000000 in ?? () Caveats: Only works with embedded builtins. Breakpoints can only be set at the start of the builtin. The initial 
breakpoint in Isolate::Init is needed prior to setting the builtin breakpoint, since GDB modifies the binary and we verify a hash of the builtins section in the binary at startup. Otherwise, V8 complains about a
hash mismatch: # Fatal error in ../../src/isolate.cc, line 117 # Check failed: d.Hash() == d.CreateHash() (11095509419988753467 vs. 3539781814546519144). Branding Terms Privacy Twitter Edit this page on GitHub 
Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For 
details, see our site policies ."
  }
]
[Step 6: Duration 6.20 seconds| Input tokens: 18,840 | Output tokens: 1,291]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 7 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'lazy-deopt'}                                                                                                                        │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: string-constructor-inlining-lazy-deopt
wasm/wrapper-inlining-lazy-deopt
compiler/lazy-deopt-in-literal
compiler/regress-lazy-deopt
compiler/lazy-deopt-async-function-resolve
regress/regress-lazy-deopt-inlining2
regress/regress-lazy-deopt-reloc
regress/regress-lazy-deopt-inlining
es6/iterator-call-lazy-deopt
es6/iterator-get-lazy-deopt
maglev/lazy-deopt-with-onstack-activation
maglev/lazy-deopt-without-onstack-activation
turboshaft/maglev-frontend/js-call-lazy-deopt
turboshaft/maglev-frontend/lazy-deopt-multi-return
[Step 7: Duration 19.71 seconds| Input tokens: 27,900 | Output tokens: 2,600]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 8 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'Deoptimizer Deoptimize deoptimize.cc deoptimizer.cc Ignition deoptimization bailout deopt frame materialization TurboFan deopt bailout file    │
│ src/deoptimizer.cc src/compiler/deoptimizer', 'top_k': 5, 'topic_filter': 'v8'}                                                                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/turbofan.txt",
    "similarity": 0.466,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/turbofan.txt\n\nTurboFan \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research TurboFan TurboFan is one of 
V8\u2019s optimizing compilers leveraging a concept called \u201cSea of Nodes\u201d . One of V8\u2019s blog posts offers a high-level overview of TurboFan . More details can be found in the following resources. 
Articles and blog posts # A tale of TurboFan Ignition + TurboFan and ES2015 An introduction to speculative optimization in V8 Talks # CodeStubAssembler: Redux An overview of the TurboFan compiler TurboFan IR 
TurboFan\u2019s JIT Design Fast arithmetic for dynamic languages Deoptimization in V8 TurboFan: a new code generation architecture for V8 ( video ) An internship on laziness (+ blog post ) Design documents # 
These are design documents that are mostly concerned with TurboFan internals. Function context specialization Rest parameters and arguments exotic objects optimization plan TurboFan developer tools integration 
TurboFan inlining TurboFan inlining heuristics TurboFan redundant bounds and overflow check elimination Lazy deoptimization without code patching Register allocator Projection nodes in TurboFan Related design 
documents # These are design documents that also affect TurboFan in a significant way. Computed property names (re)design document ES2015 and beyond performance plan Iterator builtins design document Making 
ES2015 classes fast RegExp builtins (re)design document Spread call performance Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are 
licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/webassembly-opcode.txt",
    "similarity": 0.433,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/webassembly-opcode.txt\n\nWebAssembly - adding a new opcode \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
WebAssembly - adding a new opcode WebAssembly (Wasm) is a binary instruction format for a stack-based virtual machine. This tutorial walks the reader through implementing a new WebAssembly instruction in V8. 
WebAssembly is implemented in V8 in three parts: the interpreter the baseline compiler (Liftoff) the optimizing compiler (TurboFan) The rest of this document focuses on the TurboFan pipeline, walking through how
to add a new Wasm instruction and implement it in TurboFan. At a high level, Wasm instructions are compiled into a TurboFan graph, and we rely on the TurboFan pipeline to compile the graph into (ultimately) 
machine code. For more on TurboFan, check out the V8 docs . Opcodes/Instructions # Let\u2019s define a new instruction that adds 1 to an int32 (on the top of the stack). Note: A list of instructions supported by
all Wasm implementations can be found in the spec . All Wasm instructions are defined in src/wasm/wasm-opcodes.h . The instructions are grouped roughly by what they do, e.g. control, memory, SIMD, atomic, etc. 
Let\u2019s add our new instruction, I32Add1 , to the FOREACH_SIMPLE_OPCODE section: diff --git a/src/wasm/wasm-opcodes.h b/src/wasm/wasm-opcodes.h index 6970c667e7..867cbf451a 100644 --- 
a/src/wasm/wasm-opcodes.h +++ b/src/wasm/wasm-opcodes.h @@ -96,6 +96,7 @@ bool IsJSCompatibleSignature(const FunctionSig* sig, bool hasBigIntFeature); // Expressions with signatures. #define 
FOREACH_SIMPLE_OPCODE(V) \\ + V(I32Add1, 0xee, i_i) \\ V(I32Eqz, 0x45, i_i) \\ V(I32Eq, 0x46, i_ii) \\ V(I32Ne, 0x47, i_ii) \\ WebAssembly is a binary format, so 0xee specifies the encoding of this instruction. 
In this tutorial we chose 0xee as it is currently unused. Note: Actually adding an instruction to the spec involves work beyond what is described here. We can run a simple unit test for opcodes with: $ 
tools/dev/gm.py x64.debug unittests/WasmOpcodesTest* ... |==========] Running 1 test from 1 test suite. |----------] Global test environment set-up. |----------] 1 test from WasmOpcodesTest | RUN ] 
WasmOpcodesTest.EveryOpcodeHasAName ../../test/unittests/wasm/wasm-opcodes-unittest.cc:27: Failure Value of: false Actual: false Expected: true WasmOpcodes::OpcodeName(kExprI32Add1) == \"unknown\"; plazz halp in
src/wasm/wasm-opcodes.cc | FAILED ] WasmOpcodesTest.EveryOpcodeHasAName This error indicates that we don\u2019t have a name for our new instruction. Adding a name for the new opcode can be done in 
src/wasm/wasm-opcodes.cc : diff --git a/src/wasm/wasm-opcodes.cc b/src/wasm/wasm-opcodes.cc index 5ed664441d..2d4e9554fe 100644 --- a/src/wasm/wasm-opcodes.cc +++ b/src/wasm/wasm-opcodes.cc @@ -75,6 +75,7 @@ 
const char* WasmOpcodes::OpcodeName(WasmOpcode opcode) { // clang-format off // Standard opcodes + CASE_I32_OP(Add1, \"add1\") CASE_INT_OP(Eqz, \"eqz\") CASE_ALL_OP(Eq, \"eq\") CASE_I64x2_OP(Eq, \"eq\") By 
adding our new instruction in FOREACH_SIMPLE_OPCODE , we are skipping a fair amount of work that is done in src/wasm/function-body-decoder-impl.h , which decodes Wasm opcodes and calls into the TurboFan graph 
generator. Thus, depending on what your opcode does, you might have more work to do. We skip this in the interest of brevity. Writing a test for the new opcode # Wasm tests can be found in test/cctest/wasm/ . 
Let\u2019s take a look at test/cctest/wasm/test-run-wasm.cc , where many \u201csimple\u201d opcodes are tested. There are many examples in this file that we can follow. The general setup is: create a WasmRunner 
set up globals to hold result (optional) set up locals as parameters to instruction (optional) build the wasm module run it and compare with an expected output Here\u2019s a simple test for our new opcode: diff 
--git a/test/cctest/wasm/test-run-wasm.cc b/test/cctest/wasm/test-run-wasm.cc index 26df61ceb8..b1ee6edd71 100644 --- a/test/cctest/wasm/test-run-wasm.cc +++ b/test/cctest/wasm/test-run-wasm.cc @@ -28,6 +28,15 
@@ namespace test_run_wasm { #define RET(x) x, kExprReturn #define RET_I8(x) WASM_I32V_2(x), kExprReturn + #define WASM_I32_ADD1(x) x, kExprI32Add1 + + WASM_EXEC_TEST(Int32Add1) { + WasmRunner<int32_t> 
r(execution_tier); + // 10 + 1 + BUILD(r, WASM_I32_ADD1(WASM_I32V_1(10))); + CHECK_EQ(11, r.Call()); + } + WASM_EXEC_TEST(Int32Const) { WasmRunner<int32_t> r(execution_tier); const int32_t kExpectedValue = 
0x11223344; Run the test: $ tools/dev/gm.py x64.debug 'cctest/test-run-wasm-simd/RunWasmTurbofan_I32Add1' ... === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in 
../../src/compiler/wasm-compiler.cc, line 988 # Unsupported opcode 0xee:i32.add1 Tip: Finding the test name can be tricky, since the test definition is behind a macro. Use Code Search to click around to discover
the macro definitions. This error indicates that the compiler does not know of our new instruction. That will change in the next section. Compiling Wasm into TurboFan # In the introduction, we mentioned that 
Wasm instructions are compiled into a TurboFan graph. wasm-compiler.cc is where this happens. Let\u2019s take a look at an example opcode, I32Eqz : switch ( opcode ) { case wasm :: kExprI32Eqz : op = m -> 
Word32Equal ( ) ; return graph ( ) -> NewNode ( op , input , mcgraph ( ) -> Int32Constant ( 0 ) ) ; This switches on the Wasm opcode wasm::kExprI32Eqz , and builds a TurboFan graph consisting of the operation 
Word32Equal with the inputs input , which is the argument to the Wasm instruction, and a constant 0 . The Word32Equal operator is provided by the underlying V8 abstract machine, which is 
architecture-independent. Later in the pipeline, this abstract machine operator will be translated into architecture-dependent assembly. For our new opcode, I32Add1 , we need a graph that adds a constant 1 to 
the input, so we can resuse an existing machine operator, Int32Add , passing it the input, and a constant 1: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index 
f666bbb7c1..399293c03b 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 @@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; 
MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return graph()->NewNode(m->Int32Add(), input, mcgraph()->Int32Constant(1)); case wasm::kExprI32Eqz: op = 
m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); This is enough to get the test passing. However, not all instructions have an existing TurboFan machine operator. In that case 
we have to add this new operator to the machine. Let\u2019s try that. TurboFan machine operators # We want to add the knowledge of Int32Add1 to the TurboFan machine. So let\u2019s pretend that it exists and use 
it first: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index f666bbb7c1..1d93601584 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 
@@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return 
graph()->NewNode(m->Int32Add1(), input); case wasm::kExprI32Eqz: op = m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); Trying to run the same test leads to a compilation failure
that hints at where to make changes: ../../src/compiler/wasm-compiler.cc:717:34: error: no member named 'Int32Add1' in 'v8::internal::compiler::MachineOperatorBuilder'; did you mean 'Int32Add'? return 
graph()->NewNode(m->Int32Add1(), input); ^~~~~~~~~ Int32Add There are a couple of places that needs to be modified to add an operator: src/compiler/machine-operator.cc header src/compiler/machine-operator.h list
of opcodes that the machine understands src/compiler/opcodes.h verifier src/compiler/verifier.cc diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc index 16e838c2aa..fdd6d951f0 
100644 --- a/src/compiler/machine-operator.cc +++ b/src/compiler/machine-operator.cc @@ -136,6 +136,7 @@ MachineType AtomicOpType(Operator const* op) { #define MACHINE_PURE_OP_LIST(V) \\ 
PURE_BINARY_OP_LIST_32(V) \\ PURE_BINARY_OP_LIST_64(V) \\ + V(Int32Add1, Operator::kNoProperties, 1, 0, 1) \\ V(Word32Clz, Operator::kNoProperties, 1, 0, 1) \\ V(Word64Clz, Operator::kNoProperties, 1, 0, 1) \\ 
V(Word32ReverseBytes, Operator::kNoProperties, 1, 0, 1) \\ diff --git a/src/compiler/machine-operator.h b/src/compiler/machine-operator.h index a2b9fce0ee..f95e75a445 100644 --- a/src/compiler/machine-operator.h
+++ b/src/compiler/machine-operator.h @@ -265,6 +265,8 @@ class V8_EXPORT_PRIVATE MachineOperatorBuilder final const Operator* Word32PairShr(); const Operator* Word32PairSar(); + const Operator* Int32Add1(); + 
const Operator* Int32Add(); const Operator* Int32AddWithOverflow(); const Operator* Int32Sub(); diff --git a/src/compiler/opcodes.h b/src/compiler/opcodes.h index ce24a0bd3f..2c8c5ebaca 100644 --- 
a/src/compiler/opcodes.h +++ b/src/compiler/opcodes.h @@ -506,6 +506,7 @@ V(Float64LessThanOrEqual) #define MACHINE_UNOP_32_LIST(V) \\ + V(Int32Add1) \\ V(Word32Clz) \\ V(Word32Ctz) \\ V(Int32AbsWithOverflow) \\
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc index 461aef0023..95251934ce 100644 --- a/src/compiler/verifier.cc +++ b/src/compiler/verifier.cc @@ -1861,6 +1861,7 @@ void 
Verifier::Visitor::Check(Node* node, const AllNodes& all) { case IrOpcode::kSignExtendWord16ToInt64: case IrOpcode::kSignExtendWord32ToInt64: case IrOpcode::kStaticAssert: + case IrOpcode::kInt32Add1: #define 
SIMD_MACHINE_OP_CASE(Name) case IrOpcode::k##Name: MACHINE_SIMD_OP_LIST(SIMD_MACHINE_OP_CASE) Running the test again now gives us a different failure: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # 
Fatal error in ../../src/compiler/backend/instruction-selector.cc, line 2072 # Unexpected operator #289:Int32Add1 @ node #7 Instruction selection # So far we have been working at the TurboFan level, dealing with
(a sea of) nodes in the TurboFan graph. However, at the assembly level, we have instructions and operands. Instruction selection is the process of translating this graph to instructions and operands. The last 
test error indicated that we need something in src/compiler/backend/instruction-selector.cc . This is a big file with a giant switch statement over all the machine opcodes. It calls into architecture specific 
instruction selection, using the visitor pattern to emit instructions for each type of node. Since we added a new TurboFan machine opcode, we need to add it here as well: diff --git 
a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc index 3152b2d41e..7375085649 100644 --- a/src/compiler/backend/instruction-selector.cc +++ 
b/src/compiler/backend/instruction-selector.cc @@ -2067,6 +2067,8 @@ void InstructionSelector::VisitNode(Node* node) { return MarkAsWord32(node), VisitS1x16AnyTrue(node); case IrOpcode::kS1x16AllTrue: return 
MarkAsWord32(node), VisitS1x16AllTrue(node); + case IrOpcode::kInt32Add1: + return MarkAsWord32(node), VisitInt32Add1(node); default: FATAL(\"Unexpected operator #%d:%s @ node #%d\", node->opcode(), 
node->op()->mnemonic(), node->id()); Instruction selection is architecture dependent, so we have to add it to the architecture specific instruction selector files too. For this codelab we only focus on the x64 
architecture, so src/compiler/backend/x64/instruction-selector-x64.cc needs to be modified: diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc 
b/src/compiler/backend/x64/instruction-selector-x64.cc index 2324e119a6..4b55671243 100644 --- a/src/compiler/backend/x64/instruction-selector-x64.cc +++ b/src/compiler/backend/x64/instruction-selector-x64.cc @@
-841,6 +841,11 @@ void InstructionSelector::VisitWord32ReverseBytes(Node* node) { Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); } + void InstructionSelector::VisitInt32Add1(Node*
node) { + X64OperandGenerator g(this); + Emit(kX64Int32Add1, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); + } + And we also need to add this new x64-specific opcode, kX64Int32Add1 to 
src/compiler/backend/x64/instruction-codes-x64.h : diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h index 9b8be0e0b5..7f5faeb87b 100644 --- 
a/src/compiler/backend/x64/instruction-codes-x64.h +++ b/src/compiler/backend/x64/instruction-codes-x64.h @@ -12,6 +12,7 @@ namespace compiler { // X64-specific opcodes that specify which assembly sequence to 
emit. // Most opcodes specify a single instruction. #define TARGET_ARCH_OPCODE_LIST(V) \\ + V(X64Int32Add1) \\ V(X64Add) \\ V(X64Add32) \\ V(X64And) \\ Instruction scheduling and code generation # Running our 
test, we see new compilation errors: ../../src/compiler/backend/x64/instruction-scheduler-x64.cc:15:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(instr->arch_opcode()) { ^ 1 error generated. ... ../../src/compiler/backend/x64/code-generator-x64.cc:733:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(arch_opcode) { ^ 1 error generated. Instruction scheduling takes care of dependencies that instructions may have to allow for more optimization (e.g. instruction reordering). Our new opcode has no data 
dependency, so we can add it simply to: src/compiler/backend/x64/instruction-scheduler-x64.cc : diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc 
b/src/compiler/backend/x64/instruction-scheduler-x64.cc index 79eda7e78d..3667a84577 100644 --- a/src/compiler/backend/x64/instruction-scheduler-x64.cc +++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -13,6 +13,7 @@ bool InstructionScheduler::SchedulerSupported() { return true; } int InstructionScheduler::GetTargetInstructionFlags( const Instruction* instr) const { switch (instr->arch_opcode()) { + case 
kX64Int32Add1: case kX64Add: case kX64Add32: case kX64And: Code generation is where we translate our architecture specific opcodes into assembly. Let\u2019s add a clause to 
src/compiler/backend/x64/code-generator-x64.cc : diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc index 61c3a45a16..9c37ed7464 100644 --- 
a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -731,6 +731,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction( InstructionCode 
opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { 
Handle<Code> code = i.InputCode(0); For now we leave our code generation empty, and we can run the test to make sure everything compiles: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in
../../test/cctest/wasm/test-run-wasm.cc, line 37 # Check failed: 11 == r.Call() (11 vs. 10). This failure is expected, since our new instruction is not implemented yet \u2014 it is essentially a no-op, so our 
actual value was unchanged ( 10 ). To implement our opcode, we can use the add assembly instruction: diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc 
index 6c828d6bc4..260c8619f2 100644 --- a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -744,6 +744,11 @@ CodeGenerator::CodeGenResult 
CodeGenerator::AssembleArchInstruction( InstructionCode opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + 
DCHECK_EQ(i.OutputRegister(), i.InputRegister(0)); + __ addl(i.InputRegister(0), Immediate(1)); + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { Handle<Code> code = i.InputCode(0); And
this makes the test pass: Luckily for us addl is already implemented. If our new opcode required writing a new assembly instruction implementation, we would add it to src/compiler/backend/x64/assembler-x64.cc , 
where the assembly instruction is encoded into bytes and emitted. Tip: To inspect the generated code, we can pass --print-code to cctest . Other architectures # In this codelab we only implemented this new 
instruction for x64. The steps required for other architectures are similar: add TurboFan machine operators, use the platform-dependent files for instruction selection, scheduling, code generation, assembler. 
Tip: if we compile what we have done so far on another target, e.g. arm64, we are likely to get errors in linking. To resolve those errors, add UNIMPLEMENTED() stubs. Branding Terms Privacy Twitter Edit this 
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.43,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/lazy-unlinking.txt",
    "similarity": 0.426,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/lazy-unlinking.txt\n\nAn internship on laziness: lazy unlinking of deoptimized functions \u00b7 V8 V8 Show navigation Home Blog Docs 
Tools JS/Wasm features Research An internship on laziness: lazy unlinking of deoptimized functions Published 04 October 2017 \u00b7 Tagged with memory internals Roughly three months ago, I joined the V8 team 
(Google Munich) as an intern and since then I\u2019ve been working on the VM\u2019s Deoptimizer \u2014 something completely new to me which proved to be an interesting and challenging project. The first part of 
my internship focused on improving the VM security-wise . The second part focused on performance improvements. Namely, on the removal of a data-structure used for the unlinking of previously deoptimized 
functions, which was a performance bottleneck during garbage collection. This blog post describes this second part of my internship. I\u2019ll explain how V8 used to unlink deoptimized functions, how we changed 
this, and what performance improvements were obtained. Let\u2019s (very) briefly recap the V8 pipeline for a JavaScript function: V8\u2019s interpreter, Ignition, collects profiling information about that 
function while interpreting it. Once the function becomes hot, this information is passed to V8\u2019s compiler, TurboFan, which generates optimized machine code. When the profiling information is no longer 
valid \u2014 for example because one of the profiled objects gets a different type during runtime \u2014 the optimized machine code might become invalid. In that case, V8 needs to deoptimize it. An overview of 
V8, as seen in JavaScript Start-up Performance Upon optimization, TurboFan generates a code object, i.e. the optimized machine code, for the function under optimization. When this function is invoked the next 
time, V8 follows the link to optimized code for that function and executes it. Upon deoptimization of this function, we need to unlink the code object in order to make sure that it won\u2019t be executed again. 
How does that happen? For example, in the following code, the function f1 will be invoked many times (always passing an integer as argument). TurboFan then generates machine code for that specific case. function
g ( ) { return ( i ) => i ; } // Create a closure. const f1 = g ( ) ; // Optimize f1. for ( var i = 0 ; i < 1000 ; i ++ ) f1 ( 0 ) ; Each function also has a trampoline to the interpreter \u2014 more details in 
these slides \u2014 and will keep a pointer to this trampoline in its SharedFunctionInfo (SFI). This trampoline will be used whenever V8 needs to go back to unoptimized code. Thus, upon deoptimization, triggered
by passing an argument of a different type, for example, the Deoptimizer can simply set the code field of the JavaScript function to this trampoline. An overview of V8, as seen in JavaScript Start-up Performance
Although this seems simple, it forces V8 to keep weak lists of optimized JavaScript functions. This is because it is possible to have different functions pointing to the same optimized code object. We can extend
our example as follows, and the functions f1 and f2 both point to the same optimized code. const f2 = g ( ) ; f2 ( 0 ) ; If the function f1 is deoptimized (for example by invoking it with an object of different 
type {x: 0} ) we need to make sure that the invalidated code will not be executed again by invoking f2 . Thus, upon deoptimization, V8 used to iterate over all the optimized JavaScript functions, and would 
unlink those that pointed to the code object being deoptimized. This iteration in applications with many optimized JavaScript functions became a performance bottleneck. Moreover, other than slowing down 
deoptimization, V8 used to iterate over these lists upon stop-the-world cycles of garbage collection, making it even worse. In order to have an idea of the impact of such data-structure in the performance of V8,
we wrote a micro-benchmark that stresses its usage, by triggering many scavenge cycles after creating many JavaScript functions. function g ( ) { return ( i ) => i + 1 ; } // Create an initial closure and 
optimize. var f = g ( ) ; f ( 0 ) ; f ( 0 ) ; % OptimizeFunctionOnNextCall ( f ) ; f ( 0 ) ; // Create 2M closures; those will get the previously optimized code. var a = | ] ; for ( var i = 0 ; i < 2000000 ; i 
++ ) { var h = g ( ) ; h ( ) ; a . push ( h ) ; } // Now cause scavenges; all of them are slow. for ( var i = 0 ; i < 1000 ; i ++ ) { new Array ( 50000 ) ; } When running this benchmark, we could observe that V8
spent around 98% of its execution time on garbage collection. We then removed this data structure, and instead used an approach for lazy unlinking , and this was what we observed on x64: Although this is just a 
micro-benchmark that creates many JavaScript functions and triggers many garbage collection cycles, it gives us an idea of the overhead introduced by this data structure. Other more realistic applications where 
we saw some overhead, and which motivated this work, were the router benchmark implemented in Node.js and ARES-6 benchmark suite . Lazy unlinking # Rather than unlinking optimized code from JavaScript functions 
upon deoptimization, V8 postpones it for the next invocation of such functions. When such functions are invoked, V8 checks whether they have been deoptimized, unlinks them and then continues with their lazy 
compilation. If these functions are never invoked again, then they will never be unlinked and the deoptimized code objects will not be collected. However, given that during deoptimization, we invalidate all the 
embedded fields of the code object, we only keep that code object alive. The commit that removed this list of optimized JavaScript functions required changes in several parts of the VM, but the basic idea is as 
follows. When assembling the optimized code object, we check if this is the code of a JavaScript function. If so, in its prologue, we assemble machine code to bail out if the code object has been deoptimized. 
Upon deoptimization we don\u2019t modify the deoptimized code \u2014 code patching is gone. Thus, its bit marked_for_deoptimization is still set when invoking the function again. TurboFan generates code to check
it, and if it is set, then V8 jumps to a new builtin, CompileLazyDeoptimizedCode , that unlinks the deoptimized code from the JavaScript function and then continues with lazy compilation. In more detail, the 
first step is to generate instructions that load the address of the code being currently assembled. We can do that in x64, with the following code: Label current ; // Load effective address of current 
instruction into rcx. __ leaq ( rcx , Operand ( & current ) ) ; __ bind ( & current ) ; After that we need to obtain where in the code object the marked_for_deoptimization bit lives. int pc = __ pc_offset ( ) ; 
int offset = Code :: kKindSpecificFlags1Offset - ( Code :: kHeaderSize + pc ) ; We can then test the bit and if it is set, we jump to the CompileLazyDeoptimizedCode built in. // Test if the bit is set, that is, 
if the code is marked for deoptimization. __ testl ( Operand ( rcx , offset ) , Immediate ( 1 << Code :: kMarkedForDeoptimizationBit ) ) ; // Jump to builtin if it is. __ j ( not_zero , /* handle to builtin code
here */ , RelocInfo :: CODE_TARGET ) ; On the side of this CompileLazyDeoptimizedCode builtin, all that\u2019s left to do is to unlink the code field from the JavaScript function and set it to the trampoline to 
the Interpreter entry. So, considering that the address of the JavaScript function is in the register rdi , we can obtain the pointer to the SharedFunctionInfo with: // Field read to obtain the 
SharedFunctionInfo. __ movq ( rcx , FieldOperand ( rdi , JSFunction :: kSharedFunctionInfoOffset ) ) ; \u2026and similarly the trampoline with: // Field read to obtain the code object. __ movq ( rcx , 
FieldOperand ( rcx , SharedFunctionInfo :: kCodeOffset ) ) ; Then we can use it to update the function slot for the code pointer: // Update the code field of the function with the trampoline. __ movq ( 
FieldOperand ( rdi , JSFunction :: kCodeOffset ) , rcx ) ; // Write barrier to protect the field. __ RecordWriteField ( rdi , JSFunction :: kCodeOffset , rcx , r15 , kDontSaveFPRegs , OMIT_REMEMBERED_SET , 
OMIT_SMI_CHECK ) ; This produces the same result as before. However, rather than taking care of the unlinking in the Deoptimizer, we need to worry about it during code generation. Hence the handwritten assembly.
The above is how it works in the x64 architecture . We have implemented it for ia32 , arm , arm64 , mips , and mips64 as well. This new technique is already integrated in V8 and, as we\u2019ll discuss later, 
allows for performance improvements. However, it comes with a minor disadvantage: Before, V8 would consider unlinking only upon deoptimization. Now, it has to do so in the activation of all optimized functions. 
Moreover, the approach to check the marked_for_deoptimization bit is not as efficient as it could be, given that we need to do some work to obtain the address of the code object. Note that this happens when 
entering every optimized function. A possible solution for this issue is to keep in a code object a pointer to itself. Rather than doing work to find the address of the code object whenever the function is 
invoked, V8 would do it only once, after its construction. Results # We now look at the performance gains and regressions obtained with this project. General improvements on x64 # The following plot shows us 
some improvements and regressions, relative to the previous commit. Note that the higher, the better. The promises benchmarks are the ones where we see greater improvements, observing almost 33% gain for the 
bluebird-parallel benchmark, and 22.40% for wikipedia . We also observed a few regressions in some benchmarks. This is related to the issue explained above, on checking whether the code is marked for 
deoptimization. We also see improvements in the ARES-6 benchmark suite. Note that in this chart too, the higher the better. These programs used to spend considerable amount of time in GC-related activities. With
lazy unlinking we improve performance by 1.9% overall. The most notable case is the Air steadyState where we get an improvement of around 5.36%. AreWeFastYet results # The performance results for the Octane and 
ARES-6 benchmark suites also showed up on the AreWeFastYet tracker. We looked at these performance results on September 5th, 2017, using the provided default machine (macOS 10.10 64-bit, Mac Pro, shell). 
Cross-browser results on Octane as seen on AreWeFastYet Cross-browser results on ARES-6 as seen on AreWeFastYet Impact on Node.js # We can also see performance improvements in the router-benchmark . The 
following two plots show the number of operations per second of each tested router. Thus the higher the better. We have performed two kinds of experiments with this benchmark suite. Firstly, we ran each test in 
isolation, so that we could see the performance improvement, independently from the remaining tests. Secondly, we ran all tests at once, without switching of the VM, thus simulating an environment where each 
test is integrated with other functionalities. For the first experiment, we saw that the router and express tests perform about twice as many operations than before, in the same amount of time. For the second 
experiment, we saw even greater improvement. In some of the cases, such as routr , server-router and router , the benchmark performs approximately 3.80\u00d7, 3\u00d7 and 2\u00d7 more operations, respectively. 
This happens because V8 accumulates more optimized JavaScript functions, test after test. Thus, whenever executing a given test, if a garbage collection cycle is triggered, V8 has to visit the optimized 
functions from the current test and from the previous ones. Further optimization # Now that V8 does not keep the linked-list of JavaScript functions in the context, we can remove the field next from the 
JSFunction class. Although this is a simple modification, it allows us to save the size of a pointer per function, which represent significant savings in several web pages: Benchmark Kind Memory savings 
(absolute) Memory savings (relative) facebook.com Average effective size 170 KB 3.70% twitter.com Average size of allocated objects 284 KB 1.20% cnn.com Average size of allocated objects 788 KB 1.53% youtube.com
Average size of allocated objects 129 KB 0.79% Acknowledgments # Throughout my internship, I had lots of help from several people, who were always available to answer my many questions. Thus I would like to 
thank the following people: Benedikt Meurer, Jaroslav Sevcik, and Michael Starzinger for discussions on how the compiler and the deoptimizer work, Ulan Degenbaev for helping with the garbage collector whenever I
broke it, and Mathias Bynens, Peter Marshall, Camillo Bruni, and Maya Armyanova for proofreading this article. Finally, this article is my last contribution as a Google intern and I would like to take the 
opportunity to thank everyone in the V8 team, and especially my host, Benedikt Meurer, for hosting me and for giving me the opportunity to work on such an interesting project \u2014 I definitely learned a lot 
and enjoyed my time at Google! Posted by Juliana Franco ( @jupvfranco ), Laziness Expert. Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples
from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb.txt",
    "similarity": 0.422,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb.txt\n\nDebugging builtins with GDB \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Debugging builtins 
with GDB As of V8 v6.9, it\u2019s possible to create breakpoints in GDB (and possibly other debuggers) to debug CSA / ASM / Torque builtins. (gdb) tb i::Isolate::Init Temporary breakpoint 1 at 0x7ffff706742b: 
i::Isolate::Init. (2 locations) (gdb) r Thread 1 \"d8\" hit Temporary breakpoint 1, 0x00007ffff7c55bc0 in Isolate::Init (gdb) br Builtins_RegExpPrototypeExec Breakpoint 2 at 0x7ffff7ac8784 (gdb) c Thread 1 
\"d8\" hit Breakpoint 2, 0x00007ffff7ac8784 in Builtins_RegExpPrototypeExec () Note that it works well to use a temporary breakpoint (shortcut tb in GDB) instead of a regular breakpoint ( br ) for this, since 
you only need it at process start. Builtins are also visible in stack traces: (gdb) bt #0 0x00007ffff7ac8784 in Builtins_RegExpPrototypeExec () #1 0x00007ffff78f5066 in Builtins_ArgumentsAdaptorTrampoline () #2 
0x000039751d2825b1 in ?? () #3 0x000037ef23a0fa59 in ?? () #4 0x0000000000000000 in ?? () Caveats: Only works with embedded builtins. Breakpoints can only be set at the start of the builtin. The initial 
breakpoint in Isolate::Init is needed prior to setting the builtin breakpoint, since GDB modifies the binary and we verify a hash of the builtins section in the binary at startup. Otherwise, V8 complains about a
hash mismatch: # Fatal error in ../../src/isolate.cc, line 117 # Check failed: d.Hash() == d.CreateHash() (11095509419988753467 vs. 3539781814546519144). Branding Terms Privacy Twitter Edit this page on GitHub 
Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For 
details, see our site policies ."
  }
]
[Step 8: Duration 8.18 seconds| Input tokens: 37,158 | Output tokens: 3,055]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 9 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'regress-841117'}                                                                                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: compiler/regress-841117
[Step 9: Duration 9.12 seconds| Input tokens: 56,464 | Output tokens: 3,534]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 10 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'src/deoptimizer.cc Deoptimizer CompileLazyDeoptimizedCode CompileLazyDeoptimizedCode builtin Code::kMarkedForDeoptimizationBit deoptimizer.cc  │
│ deopt frame materialization', 'top_k': 5, 'topic_filter': 'v8'}                                                                                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.466,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/webassembly-opcode.txt",
    "similarity": 0.456,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/webassembly-opcode.txt\n\nWebAssembly - adding a new opcode \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
WebAssembly - adding a new opcode WebAssembly (Wasm) is a binary instruction format for a stack-based virtual machine. This tutorial walks the reader through implementing a new WebAssembly instruction in V8. 
WebAssembly is implemented in V8 in three parts: the interpreter the baseline compiler (Liftoff) the optimizing compiler (TurboFan) The rest of this document focuses on the TurboFan pipeline, walking through how
to add a new Wasm instruction and implement it in TurboFan. At a high level, Wasm instructions are compiled into a TurboFan graph, and we rely on the TurboFan pipeline to compile the graph into (ultimately) 
machine code. For more on TurboFan, check out the V8 docs . Opcodes/Instructions # Let\u2019s define a new instruction that adds 1 to an int32 (on the top of the stack). Note: A list of instructions supported by
all Wasm implementations can be found in the spec . All Wasm instructions are defined in src/wasm/wasm-opcodes.h . The instructions are grouped roughly by what they do, e.g. control, memory, SIMD, atomic, etc. 
Let\u2019s add our new instruction, I32Add1 , to the FOREACH_SIMPLE_OPCODE section: diff --git a/src/wasm/wasm-opcodes.h b/src/wasm/wasm-opcodes.h index 6970c667e7..867cbf451a 100644 --- 
a/src/wasm/wasm-opcodes.h +++ b/src/wasm/wasm-opcodes.h @@ -96,6 +96,7 @@ bool IsJSCompatibleSignature(const FunctionSig* sig, bool hasBigIntFeature); // Expressions with signatures. #define 
FOREACH_SIMPLE_OPCODE(V) \\ + V(I32Add1, 0xee, i_i) \\ V(I32Eqz, 0x45, i_i) \\ V(I32Eq, 0x46, i_ii) \\ V(I32Ne, 0x47, i_ii) \\ WebAssembly is a binary format, so 0xee specifies the encoding of this instruction. 
In this tutorial we chose 0xee as it is currently unused. Note: Actually adding an instruction to the spec involves work beyond what is described here. We can run a simple unit test for opcodes with: $ 
tools/dev/gm.py x64.debug unittests/WasmOpcodesTest* ... |==========] Running 1 test from 1 test suite. |----------] Global test environment set-up. |----------] 1 test from WasmOpcodesTest | RUN ] 
WasmOpcodesTest.EveryOpcodeHasAName ../../test/unittests/wasm/wasm-opcodes-unittest.cc:27: Failure Value of: false Actual: false Expected: true WasmOpcodes::OpcodeName(kExprI32Add1) == \"unknown\"; plazz halp in
src/wasm/wasm-opcodes.cc | FAILED ] WasmOpcodesTest.EveryOpcodeHasAName This error indicates that we don\u2019t have a name for our new instruction. Adding a name for the new opcode can be done in 
src/wasm/wasm-opcodes.cc : diff --git a/src/wasm/wasm-opcodes.cc b/src/wasm/wasm-opcodes.cc index 5ed664441d..2d4e9554fe 100644 --- a/src/wasm/wasm-opcodes.cc +++ b/src/wasm/wasm-opcodes.cc @@ -75,6 +75,7 @@ 
const char* WasmOpcodes::OpcodeName(WasmOpcode opcode) { // clang-format off // Standard opcodes + CASE_I32_OP(Add1, \"add1\") CASE_INT_OP(Eqz, \"eqz\") CASE_ALL_OP(Eq, \"eq\") CASE_I64x2_OP(Eq, \"eq\") By 
adding our new instruction in FOREACH_SIMPLE_OPCODE , we are skipping a fair amount of work that is done in src/wasm/function-body-decoder-impl.h , which decodes Wasm opcodes and calls into the TurboFan graph 
generator. Thus, depending on what your opcode does, you might have more work to do. We skip this in the interest of brevity. Writing a test for the new opcode # Wasm tests can be found in test/cctest/wasm/ . 
Let\u2019s take a look at test/cctest/wasm/test-run-wasm.cc , where many \u201csimple\u201d opcodes are tested. There are many examples in this file that we can follow. The general setup is: create a WasmRunner 
set up globals to hold result (optional) set up locals as parameters to instruction (optional) build the wasm module run it and compare with an expected output Here\u2019s a simple test for our new opcode: diff 
--git a/test/cctest/wasm/test-run-wasm.cc b/test/cctest/wasm/test-run-wasm.cc index 26df61ceb8..b1ee6edd71 100644 --- a/test/cctest/wasm/test-run-wasm.cc +++ b/test/cctest/wasm/test-run-wasm.cc @@ -28,6 +28,15 
@@ namespace test_run_wasm { #define RET(x) x, kExprReturn #define RET_I8(x) WASM_I32V_2(x), kExprReturn + #define WASM_I32_ADD1(x) x, kExprI32Add1 + + WASM_EXEC_TEST(Int32Add1) { + WasmRunner<int32_t> 
r(execution_tier); + // 10 + 1 + BUILD(r, WASM_I32_ADD1(WASM_I32V_1(10))); + CHECK_EQ(11, r.Call()); + } + WASM_EXEC_TEST(Int32Const) { WasmRunner<int32_t> r(execution_tier); const int32_t kExpectedValue = 
0x11223344; Run the test: $ tools/dev/gm.py x64.debug 'cctest/test-run-wasm-simd/RunWasmTurbofan_I32Add1' ... === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in 
../../src/compiler/wasm-compiler.cc, line 988 # Unsupported opcode 0xee:i32.add1 Tip: Finding the test name can be tricky, since the test definition is behind a macro. Use Code Search to click around to discover
the macro definitions. This error indicates that the compiler does not know of our new instruction. That will change in the next section. Compiling Wasm into TurboFan # In the introduction, we mentioned that 
Wasm instructions are compiled into a TurboFan graph. wasm-compiler.cc is where this happens. Let\u2019s take a look at an example opcode, I32Eqz : switch ( opcode ) { case wasm :: kExprI32Eqz : op = m -> 
Word32Equal ( ) ; return graph ( ) -> NewNode ( op , input , mcgraph ( ) -> Int32Constant ( 0 ) ) ; This switches on the Wasm opcode wasm::kExprI32Eqz , and builds a TurboFan graph consisting of the operation 
Word32Equal with the inputs input , which is the argument to the Wasm instruction, and a constant 0 . The Word32Equal operator is provided by the underlying V8 abstract machine, which is 
architecture-independent. Later in the pipeline, this abstract machine operator will be translated into architecture-dependent assembly. For our new opcode, I32Add1 , we need a graph that adds a constant 1 to 
the input, so we can resuse an existing machine operator, Int32Add , passing it the input, and a constant 1: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index 
f666bbb7c1..399293c03b 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 @@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; 
MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return graph()->NewNode(m->Int32Add(), input, mcgraph()->Int32Constant(1)); case wasm::kExprI32Eqz: op = 
m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); This is enough to get the test passing. However, not all instructions have an existing TurboFan machine operator. In that case 
we have to add this new operator to the machine. Let\u2019s try that. TurboFan machine operators # We want to add the knowledge of Int32Add1 to the TurboFan machine. So let\u2019s pretend that it exists and use 
it first: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index f666bbb7c1..1d93601584 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 
@@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return 
graph()->NewNode(m->Int32Add1(), input); case wasm::kExprI32Eqz: op = m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); Trying to run the same test leads to a compilation failure
that hints at where to make changes: ../../src/compiler/wasm-compiler.cc:717:34: error: no member named 'Int32Add1' in 'v8::internal::compiler::MachineOperatorBuilder'; did you mean 'Int32Add'? return 
graph()->NewNode(m->Int32Add1(), input); ^~~~~~~~~ Int32Add There are a couple of places that needs to be modified to add an operator: src/compiler/machine-operator.cc header src/compiler/machine-operator.h list
of opcodes that the machine understands src/compiler/opcodes.h verifier src/compiler/verifier.cc diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc index 16e838c2aa..fdd6d951f0 
100644 --- a/src/compiler/machine-operator.cc +++ b/src/compiler/machine-operator.cc @@ -136,6 +136,7 @@ MachineType AtomicOpType(Operator const* op) { #define MACHINE_PURE_OP_LIST(V) \\ 
PURE_BINARY_OP_LIST_32(V) \\ PURE_BINARY_OP_LIST_64(V) \\ + V(Int32Add1, Operator::kNoProperties, 1, 0, 1) \\ V(Word32Clz, Operator::kNoProperties, 1, 0, 1) \\ V(Word64Clz, Operator::kNoProperties, 1, 0, 1) \\ 
V(Word32ReverseBytes, Operator::kNoProperties, 1, 0, 1) \\ diff --git a/src/compiler/machine-operator.h b/src/compiler/machine-operator.h index a2b9fce0ee..f95e75a445 100644 --- a/src/compiler/machine-operator.h
+++ b/src/compiler/machine-operator.h @@ -265,6 +265,8 @@ class V8_EXPORT_PRIVATE MachineOperatorBuilder final const Operator* Word32PairShr(); const Operator* Word32PairSar(); + const Operator* Int32Add1(); + 
const Operator* Int32Add(); const Operator* Int32AddWithOverflow(); const Operator* Int32Sub(); diff --git a/src/compiler/opcodes.h b/src/compiler/opcodes.h index ce24a0bd3f..2c8c5ebaca 100644 --- 
a/src/compiler/opcodes.h +++ b/src/compiler/opcodes.h @@ -506,6 +506,7 @@ V(Float64LessThanOrEqual) #define MACHINE_UNOP_32_LIST(V) \\ + V(Int32Add1) \\ V(Word32Clz) \\ V(Word32Ctz) \\ V(Int32AbsWithOverflow) \\
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc index 461aef0023..95251934ce 100644 --- a/src/compiler/verifier.cc +++ b/src/compiler/verifier.cc @@ -1861,6 +1861,7 @@ void 
Verifier::Visitor::Check(Node* node, const AllNodes& all) { case IrOpcode::kSignExtendWord16ToInt64: case IrOpcode::kSignExtendWord32ToInt64: case IrOpcode::kStaticAssert: + case IrOpcode::kInt32Add1: #define 
SIMD_MACHINE_OP_CASE(Name) case IrOpcode::k##Name: MACHINE_SIMD_OP_LIST(SIMD_MACHINE_OP_CASE) Running the test again now gives us a different failure: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # 
Fatal error in ../../src/compiler/backend/instruction-selector.cc, line 2072 # Unexpected operator #289:Int32Add1 @ node #7 Instruction selection # So far we have been working at the TurboFan level, dealing with
(a sea of) nodes in the TurboFan graph. However, at the assembly level, we have instructions and operands. Instruction selection is the process of translating this graph to instructions and operands. The last 
test error indicated that we need something in src/compiler/backend/instruction-selector.cc . This is a big file with a giant switch statement over all the machine opcodes. It calls into architecture specific 
instruction selection, using the visitor pattern to emit instructions for each type of node. Since we added a new TurboFan machine opcode, we need to add it here as well: diff --git 
a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc index 3152b2d41e..7375085649 100644 --- a/src/compiler/backend/instruction-selector.cc +++ 
b/src/compiler/backend/instruction-selector.cc @@ -2067,6 +2067,8 @@ void InstructionSelector::VisitNode(Node* node) { return MarkAsWord32(node), VisitS1x16AnyTrue(node); case IrOpcode::kS1x16AllTrue: return 
MarkAsWord32(node), VisitS1x16AllTrue(node); + case IrOpcode::kInt32Add1: + return MarkAsWord32(node), VisitInt32Add1(node); default: FATAL(\"Unexpected operator #%d:%s @ node #%d\", node->opcode(), 
node->op()->mnemonic(), node->id()); Instruction selection is architecture dependent, so we have to add it to the architecture specific instruction selector files too. For this codelab we only focus on the x64 
architecture, so src/compiler/backend/x64/instruction-selector-x64.cc needs to be modified: diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc 
b/src/compiler/backend/x64/instruction-selector-x64.cc index 2324e119a6..4b55671243 100644 --- a/src/compiler/backend/x64/instruction-selector-x64.cc +++ b/src/compiler/backend/x64/instruction-selector-x64.cc @@
-841,6 +841,11 @@ void InstructionSelector::VisitWord32ReverseBytes(Node* node) { Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); } + void InstructionSelector::VisitInt32Add1(Node*
node) { + X64OperandGenerator g(this); + Emit(kX64Int32Add1, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); + } + And we also need to add this new x64-specific opcode, kX64Int32Add1 to 
src/compiler/backend/x64/instruction-codes-x64.h : diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h index 9b8be0e0b5..7f5faeb87b 100644 --- 
a/src/compiler/backend/x64/instruction-codes-x64.h +++ b/src/compiler/backend/x64/instruction-codes-x64.h @@ -12,6 +12,7 @@ namespace compiler { // X64-specific opcodes that specify which assembly sequence to 
emit. // Most opcodes specify a single instruction. #define TARGET_ARCH_OPCODE_LIST(V) \\ + V(X64Int32Add1) \\ V(X64Add) \\ V(X64Add32) \\ V(X64And) \\ Instruction scheduling and code generation # Running our 
test, we see new compilation errors: ../../src/compiler/backend/x64/instruction-scheduler-x64.cc:15:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(instr->arch_opcode()) { ^ 1 error generated. ... ../../src/compiler/backend/x64/code-generator-x64.cc:733:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(arch_opcode) { ^ 1 error generated. Instruction scheduling takes care of dependencies that instructions may have to allow for more optimization (e.g. instruction reordering). Our new opcode has no data 
dependency, so we can add it simply to: src/compiler/backend/x64/instruction-scheduler-x64.cc : diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc 
b/src/compiler/backend/x64/instruction-scheduler-x64.cc index 79eda7e78d..3667a84577 100644 --- a/src/compiler/backend/x64/instruction-scheduler-x64.cc +++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -13,6 +13,7 @@ bool InstructionScheduler::SchedulerSupported() { return true; } int InstructionScheduler::GetTargetInstructionFlags( const Instruction* instr) const { switch (instr->arch_opcode()) { + case 
kX64Int32Add1: case kX64Add: case kX64Add32: case kX64And: Code generation is where we translate our architecture specific opcodes into assembly. Let\u2019s add a clause to 
src/compiler/backend/x64/code-generator-x64.cc : diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc index 61c3a45a16..9c37ed7464 100644 --- 
a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -731,6 +731,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction( InstructionCode 
opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { 
Handle<Code> code = i.InputCode(0); For now we leave our code generation empty, and we can run the test to make sure everything compiles: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in
../../test/cctest/wasm/test-run-wasm.cc, line 37 # Check failed: 11 == r.Call() (11 vs. 10). This failure is expected, since our new instruction is not implemented yet \u2014 it is essentially a no-op, so our 
actual value was unchanged ( 10 ). To implement our opcode, we can use the add assembly instruction: diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc 
index 6c828d6bc4..260c8619f2 100644 --- a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -744,6 +744,11 @@ CodeGenerator::CodeGenResult 
CodeGenerator::AssembleArchInstruction( InstructionCode opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + 
DCHECK_EQ(i.OutputRegister(), i.InputRegister(0)); + __ addl(i.InputRegister(0), Immediate(1)); + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { Handle<Code> code = i.InputCode(0); And
this makes the test pass: Luckily for us addl is already implemented. If our new opcode required writing a new assembly instruction implementation, we would add it to src/compiler/backend/x64/assembler-x64.cc , 
where the assembly instruction is encoded into bytes and emitted. Tip: To inspect the generated code, we can pass --print-code to cctest . Other architectures # In this codelab we only implemented this new 
instruction for x64. The steps required for other architectures are similar: add TurboFan machine operators, use the platform-dependent files for instruction selection, scheduling, code generation, assembler. 
Tip: if we compile what we have done so far on another target, e.g. arm64, we are likely to get errors in linking. To resolve those errors, add UNIMPLEMENTED() stubs. Branding Terms Privacy Twitter Edit this 
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-speculative-optimizations.txt",
    "similarity": 0.449,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-speculative-optimizations.txt\n\nSpeculative Optimizations for WebAssembly using Deopts and Inlining \u00b7 V8 V8 Show navigation 
Home Blog Docs Tools JS/Wasm features Research Speculative Optimizations for WebAssembly using Deopts and Inlining Published 24 June 2025 \u00b7 Tagged with WebAssembly internals In this blog post, we explain 
two optimizations for WebAssembly that we recently implemented in V8 and that shipped with Google Chrome M137, namely speculative call_indirect inlining and deoptimization support for WebAssembly. In 
combination, they allow us to generate better machine code by making assumptions based on runtime feedback. This speeds up WebAssembly execution, in particular for WasmGC programs. On a set of Dart 
microbenchmarks for example, the speedup by the combination of both optimizations is more than 50% on average, and on larger, realistic applications and benchmarks shown below the speedup is between 1% and 8%. 
Deoptimizations are also an important building block for further optimizations in the future. Background # Fast execution of JavaScript relies heavily on speculative optimizations . That is, JIT-compilers make 
assumptions when generating machine code based on feedback that was collected during earlier executions. For example, given the expression a + b , the compiler can generate machine code for an integer addition 
if past feedback indicates that a and b are integers (and not strings, floating point numbers, or other objects). Without making such assumptions, the compiler would have to emit generic code that handles the 
full behavior of the + operator in JavaScript , which is complex and thus much slower. If the program later behaves differently and thus violates assumptions made when generating the optimized code, V8 performs 
a deoptimization (or deopt, for short). That means throwing away the optimized code and continuing execution in unoptimized code (and collecting more feedback to possibly tier-up again later). In contrast to 
JavaScript, fast execution of WebAssembly hasn\u2019t required speculative optimizations and deopts. One reason is that WebAssembly programs can already be optimized quite well because more information is 
statically available as e.g., functions, instructions, and variables are all statically typed. Another reason is that WebAssembly binaries are often compiled from C, C++, or Rust. These source languages are also
more amenable to static analysis than JavaScript, and thus toolchains such as Emscripten (based on LLVM) or Binaryen can already optimize the program ahead-of-time. This results in fairly well-optimized 
binaries, at least when targeting WebAssembly 1.0, which launched in 2017 . Motivation # Then why do we employ more speculative optimizations for WebAssembly in V8 now? One reason is that WebAssembly has evolved
with the introduction of WasmGC , the WebAssembly Garbage Collection proposal . It better supports compiling \u201cmanaged\u201d languages such as Java , Kotlin , or Dart to WebAssembly. The resulting WasmGC 
bytecode is more high-level than Wasm 1.0, e.g., it supports rich types, such as structs and arrays, subtyping, and operations on such types. The generated machine code for WasmGC thus benefits more from 
speculative optimizations. One particularly important optimization is inlining , that is, replacing a call instruction with the body of the callee function. Not only does this get rid of the administrative 
overhead associated with the call itself (which might be higher than the actual work for very small functions), but it also enables many other, subsequent optimizations to \u201csee through\u201d the function 
call, even if those optimizations are not inter-procedural. No wonder inlining was already recognized in 1971 as one of the most important optimizations in Frances Allen\u2019s seminal \u201cCatalogue of 
Optimizing Transformations\u201d . One complication for inlining are indirect calls , i.e., calls where the callee is only known at runtime and can be one of many potential targets. This is particularly true for
the languages that compile to WasmGC. Consider Java or Kotlin where methods are virtual by default, whereas in C++ one has to opt-in by marking them explicitly. If there is not a single statically known callee, 
inlining is obviously not as straightforward. Speculative Inlining # This is where speculative inlining comes into play. In theory, indirect calls can target many different functions, but in practice, they still
often go to a single target (called a monomorphic function call) or a few select cases (called polymorphic ). We record those targets when executing the unoptimized code, and then inline up to four target 
functions when generating the optimized code. Speculative inlining overview The figure above shows the high-level picture. We start at the top left, with the unoptimized code for function func_a , generated by 
Liftoff , our baseline compiler for WebAssembly. At each call site Liftoff also emits code to update the feedback vector . This metadata array exists once per function, and it contains one entry per call 
instruction in the given function. Each entry records the call targets and counts for this particular call site. The example feedback at the bottom of the figure shows a monomorphic entry for the call_indirect 
in func_a ; here the call target was 1337 times func_b . When a function is hot enough to tier-up to TurboFan , i.e., gets compiled with our optimizing compiler, we come to the second step. TurboFan reads the 
corresponding feedback vector and decides whether and which targets to inline at each call site. Whether it is worthwhile inlining one or multiple callees depends on a variety of heuristics . E.g., large 
functions are never inlined, tiny functions are almost always inlined, and generally there is a maximum inlining budget after which no more inlining into a function happens, as that also has costs in terms of 
compile time and generated machine code size. As in many places in compilers, particularly multi-tier JITs, these trade-offs are quite complex and get tuned over time. In this example, TurboFan decides to inline
func_b into func_a . On the upper right hand side of the figure, we see the result of speculative inlining in the generated optimized code. Instead of an indirect call, the code first checks if the target at 
runtime matches what we have assumed during compilation. If that\u2019s the case, we continue executing the inlined body of the corresponding function. Subsequent optimizations can also transform the inlined 
code further, given the now available surrounding context. E.g., constant propagation and constant folding could specialize the code to this particular call site or GVN could hoist out repeated computations. In 
the case of polymorphic feedback, TurboFan can also emit a series of target checks and inlined bodies, not just one as in this example. Technical Deep Dive # So much for the high-level picture. For readers 
interested in the implementation, we look at some more details and concrete code in this section. In the figure above, the feedback vector is only shown conceptually as an array of entries and only one kind of 
entry is shown. Below, we see that each entry can go through four stages over the course of the execution: Initially, all entries are uninitialized (all call counts are zero), potentially transitioning to 
monomorphic (a single call target was recorded), polymorphic (up to four call targets), and finally megamorphic (more than four targets, where we don\u2019t inline any more and thus don\u2019t need to record 
call counts either). Each entry is actually a pair of objects such that the most common monomorphic case can store both the call count and the target inline in the vector, i.e., without an additional allocation.
For the polymorphic case, feedback is stored in an out-of-line array, as shown below. The builtin that updates a feedback entry (corresponding to update_feedback() in the first figure) is written in Torque . 
(It\u2019s quite easy to read, give it a try!) It first checks for monomorphic or polymorphic \u201chits\u201d, where just the count has to be incremented. This is again because they are the most common cases 
and thus performance-sensitive. The feedback vector and its entries are JavaScript objects (e.g., the call counts are Smis ), so they live on the managed heap. As such, it is part of the V8 sandbox and is 
automatically cleaned up if the corresponding Wasm instance (see below) is no longer reachable. Details of the feedback vector Next, let\u2019s look at the effect of inlining on an actual WebAssembly program. 
The example function below does 200M indirect calls in a loop to a single target inlinee that contains an addition. Obviously, this is a somewhat simplified microbenchmark, but it demonstrates the benefits of 
speculative inlining well. ( func $example ( param $iterations i32 ) ( result i32 ) ( local $sum i32 ) block $ block loop $ loop local .get $iterations i32 . eqz br_if $ block ;; terminate loop local .get 
$iterations ;; update loop counter i32 . const 1 i32 . sub local .set $iterations i32 . const 7 ;; argument for the function call i32 . const 1 ;; table index, refering to $inlinee call_indirect ( param i32 ) ( 
result i32 ) local .get $sum i32 . add local .set $sum br $ loop ;; repeat end end local .get $sum ) ... ( func $inlinee ( param $x i32 ) ( result i32 ) local .get $x i32 . const 37 i32 . add ) For readers not 
familiar with the WebAssembly text format, here is a rough C equivalent of the above program: int inlinee ( int x ) { return x + 37 ; } int ( * func_ptr ) ( int ) = inlinee ; int example ( int iterations ) { int
sum = 0 ; while ( iterations != 0 ) { iterations -- ; sum += func_ptr ( 7 ) ; } return sum ; } The next figure shows excerpts of TurboFan\u2019s intermediate representation for the example function, visualized 
with Turbolizer : Speculative inlining and Wasm deopts are enabled on the right, and disabled on the left. In both versions, we have to check whether the table index argument to the call_indirect instruction is 
in-bounds, as per the WebAssembly semantics (first red box in both cases). Without inlining, we also have to check whether the function at this index has the correct signature before actually calling it (second 
red box on the left). Finally, the first green box on the left is the indirect call, and the second green box is the addition of the result of said call. In the green box on the right, we see that after inlining
and further optimizations, the call is completely gone and the addition in inlinee and the addition in example were constant-folded into a single addition with a constant. Altogether, on this particular 
microbenchmark, inlining, deopts, and subsequent optimizations speed up the program from around 675 ms to 90 ms execution time on an x64 workstation. In this case, the optimized machine code with inlining is 
even smaller than without (968 vs. 1572 bytes), although that certainly need not be. TurboFan IR without and with inlining Finally, we want to briefly explain the Wasm instance check and target check that the 
code with speculative inlining does on the right. Semantically, Wasm functions are closures over a Wasm instance (which \u201cholds\u201d the current state of globals, tables, imports from the host, etc.). 
Correctly inlining functions that belong to a different instance (e.g., which are called via an imported table) would hence require additional compiler machinery as well as solving a few obstacles in our general
handling of generated code. Luckily, most calls are within a single instance anyway, so for the time being we check that the call target\u2019s instance matches the current instance, which lets the compiler make
the simplifying assumption that both instances are the same. If not, we deoptimize in block 8 (due to wrong instance) or block 6 (due to wrong target). This additional Wasm instance check was specifically 
introduced for the new call_indirect inlining. WebAssembly also has another kind of indirect call, call_ref , for which we already added inlining support when launching our WasmGC implementation . The fast path 
for call_ref inlining doesn\u2019t require an explicit instance check, since the WasmFuncRef object that is the call_ref input already includes the instance the function closes over, so comparing the target for 
equality subsumes both checks. With the new call_indirect inlining, V8 now supports inlining Wasm-to-Wasm calls for all types of call instructions: direct call s, call_ref , call_indirect , and their respective 
tail-call variants return_call , return_call_ref , and return_call_indirect . Deoptimization # So far, we have focused on inlining and how that improves the optimized code. But what happens if we cannot stay on 
a fast path, i.e., if one of the assumptions made during optimization turns out to be false at runtime? This is where deopts come into play. The very first figure of this post already shows the high-level idea 
of a deopt: We cannot continue execution in the optimized code because it has made some assumptions that are now invalidated. So instead, we \u201cgo back\u201d to the unoptimized baseline code. Crucially, this 
transition to unoptimized code happens in the middle of executing the current function , i.e., when the optimized code has already performed operations with side-effects (say, called the underlying operating 
system), which we cannot undo, and while it is holding intermediate values in registers and on the stack. So a deopt cannot just jump to the beginning of the unoptimized function, but instead does something much
more interesting: First, we save the current program state . We do this by calling into the runtime from optimized code. The Deoptimizer then serializes the current program state into an internal data structure,
the FrameDescription . This entails reading out CPU registers and inspecting the stack frame of the function to be deoptimized. Next, we transform this state such that it fits the unoptimized code , that is, 
placing values in the correct registers and stack slots that the Liftoff-generated code expects at the deoptimization point. E.g., a value that TurboFan code has put in a register may now end up in a stack slot.
The stack layout and expectations for the baseline code (its \u201ccalling convention\u201d for the deopt point, if you will) are read out of metadata generated by Liftoff during compilation. And finally, we 
replace the optimized stack frame with the corresponding unoptimized stack frame(s) and jump to the middle of the baseline code , corresponding to the point in the optimized code where the deopt was triggered. 
Obviously, this is quite complex machinery, which begs the question: Why do we go through the hassle of it all and not just generate a slow path in optimized code? Let\u2019s compare the intermediate 
representation for the earlier example WebAssembly code, left without and right with deopts. The three red boxes (table bounds / Wasm instance / target check) are fundamentally the same. The difference is in the
code for the slow path. Without deopts, we don\u2019t have the option of returning to baseline code, so the optimized code must handle the full, generic behavior of an indirect call, as shown in the yellow box 
on the left. This unfortunately impedes further optimizations and thus results in slower execution. TurboFan IR without and with deopts There are two reasons why the non-deopt slow path impedes optimizations. 
First, it contains more operations, in particular a call, which we have to assume goes anywhere and has arbitrary side-effects. Second, note how the Deoptimize operation in block 8 on the right has no successor 
in the control-flow graph, whereas the yellow slow path on the left has a successor. In particular for loops, the deoptimization node/block will not produce data-flow facts (in the sense of data-flow analyses 
and optimizations, such as live variables , load elimination, or escape analysis ) that propagate to the next iteration of the loop. In essence, the deopt point \u201cjust\u201d terminates the execution of the 
function, without much effect on the surrounding code, which can be utilized nicely by subsequent optimizations. Finally, this also explains why the combination of speculative optimizations (e.g., inlining) and 
deoptimization is so useful: The first adds a fast path based on speculative assumptions, and deopts allow the compiler to not worry much about the cases where the assumptions turn out to be false. Concretely, 
for the earlier microbenchmark with 200M indirect calls, performing just speculative inlining without deopts speeds up the program \u201conly\u201d to about 180 ms, compared to 90 ms with both inlining and 
deopts (and 675 ms without either). Technical Deep Dive # For the interested reader, we now again look at a concrete example and technical details, this time for when we deopt. Let's assume we execute the 
optimized code from above but the function stored at table index 1 has changed in the meantime. The table bounds check and Wasm instance check will pass, but the inlined target will be different from the one in 
the table, so we need to deoptimize the program in its current state. For that the code contains so-called deoptimization exits . The target check conditionally jumps to such an exit, which itself is a call to 
the DeoptimizationEntry builtin. The builtin first saves all register values by spilling them to the stack |1] . Then it allocates the C++ Deoptimizer object and its FrameDescription input object. The builtin 
copies the spilled registers and all other stack slots from the optimized frame into the FrameDescription on the heap, and pops these values from the stack in the process. (Note that the execution is still in 
the builtin while it has already removed its own return address from the stack and started unwinding the calling frame!) Then the builtin computes the output frames. To do that, the deoptimizer loads the 
DeoptimizationData for the optimized frame, extracts the information for the deopt point, and recompiles each inlined function at this call site with Liftoff. Due to nested inlining there can be more than one 
inlined function, and with inlined tail calls the optimized function to which the optimized stack frame nominally belonged might not even be part of the unoptimized stack frames to be constructed. While 
compiling, Liftoff calculates the expected stack frame layout and the deoptimizer transforms the optimized frame description into the desired layouts reported by Liftoff. It returns to the builtin which then 
reads these output FrameDescription objects and pushes their values onto the stack. Finally the builtin fills in the registers from the top output FrameDescription . For the example above, our internal tracing 
with --trace-deopt-verbose shows the following: |bailout (kind: deopt-eager, reason: wrong call target, type: Wasm): begin. deoptimizing example, function index 2, bytecode offset 134, deopt exit 0, FP to SP 
delta 32, pc 0x14886e50cbb4] reading input for Liftoff frame => bailout_id=134, height=4, function_id=2 ; inputs: 0: 4 ; rdx (int32) 1: 0 ; rbx (int32) 2: (wasm int32 literal 7) 3: (wasm int32 literal 1) Liftoff
stack & register state for function index 2, frame size 48, total frame size 64 0: i32:rax 1: i32:s0x28 2: i32:c7 3: i32:c1 |bailout end. took 0.082 ms] First we can see that a deopt is triggered because of a 
wrong call target in Wasm for the function example . The trace then shows information about the input (optimized) frame, which has four values: The iterations parameter of the function (value 4 , stored in 
register rdx ), the sum local ( 0 stored in rbx ), the literal 7 and the literal 1, which are the two arguments to the call_indirect instruction. In this simple example, there is only one unoptimized output 
frame, so there is a 1:1 mapping between the two frames: The iterations value has to be stored in register rax while the sum value needs to end up in the stack slot s0x28 . The two constants are also recognized 
by Liftoff as constants and don\u2019t need to be transferred into stack slots or registers. |2] After these transformations have been done the builtin \u201creturns\u201d to the inner-most unoptimized frame 
which calls a final builtin to clean-up the Deoptimizer object and perform any needed allocations on the managed heap. |3] Finally, execution continues in the unoptimized code, in this case executing the 
call_indirect , which will also directly record the new call target in its feedback vector, so that any later tier-up is aware of this new target. Results # Besides the technical description and examples, we 
also want to demonstrate the usefulness of call_indirect inlining and Wasm deopt support with some measurements. |4] We first look at a collection of Dart microbenchmarks in the figure below. It compares three 
configurations against each other: All given numbers are speedups relative to V8 and Chrome\u2019s behavior before call_indirect inlining and Wasm deopts (i.e., a speedup of 2x means the runtime is half of that 
of the baseline). The blue bars show the configuration with call_indirect inlining enabled but no Wasm deopts, i.e., where the optimized code contains a generic slow path. On several of these microbenchmarks 
this already yields (sometimes substantial) speedups. |5] On average across all items, call_indirect inlining speeds up execution by 1.19x compared to the baseline without. Finally, the red bars show the 
configuration we actually ship, where both Wasm deopts and call_indirect inlining are enabled. With an average speedup of 1.59x over the baseline, this shows that in particular the combination of speculative 
optimizations and deoptimization support is highly beneficial. Naturally, microbenchmarks isolate and emphasize the effects of an optimization quite a bit. This is useful during development or to get a strong 
signal with noisy measurements. However, more realistic are results on larger applications and benchmarks, as shown in the following figure. To the very left, we see a 2% speedup in terms of runtime for 
richards-wasm , a workload from the JetStream benchmark suite . Next, we see a 1% speedup for a Wasm build of the widely-used SQLite 3 database, and 8% speedup for Dart Flute , a WasmGC benchmark that emulates a
UI workload similar to Flutter . The final two results are from an internal benchmark for the Google Sheets calc engine , which is powered by WasmGC, with speedups due to deopts of up to 7% (only deopts matter 
here as this last application only uses call_ref s for runtime dispatch, i.e., it has no call_indirect s). Conclusion and Outlook # This concludes our post about two new optimizations in the V8 engine for 
WebAssembly. To summarize, we have seen: How speculative inlining can inline functions even in the presence of indirect calls, what feedback is and how it is used and updated, what to do when assumptions made 
during optimizations are invalid at runtime, how a deoptimization can exit optimized code and enters baseline code in the middle of executing a function, and finally how this significantly improves the execution
of real-world workloads. In the future, we plan on adding more speculative optimizations based on deopt support for WebAssembly, e.g. bounds-check elimination or more extensive load elimination for WasmGC 
objects. And also in terms of inlining, there is more to be done: While we now have Wasm-into-Wasm inlining for all kinds of call instructions, we can still extend inlining across the language boundary, e.g., 
for JavaScript-to-Wasm calls. Check back on our V8 blog for exciting updates in the future! Footnotes # Due to the SIMD extension for Wasm this also includes all 128-bit vector registers used by TurboFan. 
\u21a9\ufe0e In other cases values might be constant in the optimized version after constant folding but need to be materialized into stack slots or registers for Liftoff, so the deoptimization data needs to 
store these constant values. \u21a9\ufe0e During the deoptimization itself we cannot allocate on the heap since allocations can trigger the garbage collector (GC) and the stack is not in a state that the GC can 
inspect. (The GC needs to visit all heap-references on the stack and potentially update them when moving an object.) \u21a9\ufe0e The measurements were taken on an x64 workstation; the figures show the median of
N=21 repetitions. \u21a9\ufe0e For the three Matrix4Benchmark items that slightly regress, enabling call_indirect inlining causes our heuristic to prefer 16 indirect call sites for inlining over other direct 
call sites. This leads to exhausting the inlining budget (i.e., stop inlining because the resulting code grows too large), so that fewer of the direct calls are inlined compared with before. In this particular 
case, the heuristic doesn\u2019t predict perfectly how beneficial one inlining decision is over the other and leads to a suboptimal result. Improving this heuristics is interesting future work. \u21a9\ufe0e 
Posted by Daniel Lehmann and Matthias Liedtke. Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style 
license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-decompile.txt",
    "similarity": 0.444,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-decompile.txt\n\nWhat\u2019s in that .wasm? Introducing: wasm-decompile \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm 
features Research What\u2019s in that .wasm ? Introducing: wasm-decompile Published 27 April 2020 \u00b7 Tagged with WebAssembly tooling We have a growing number of compilers and other tools that generate or 
manipulate .wasm files, and sometimes you might want to have a look inside. Maybe you\u2019re a developer of such a tool, or more directly, you\u2019re a programmer targeting Wasm, and wondering what the 
generated code looks like, for performance or other reasons. Problem is, Wasm is rather low-level, much like actual assembly code. In particular, unlike, say, the JVM, all data structures have been compiled down
to load/store operations, rather than conveniently named classes and fields. Compilers like LLVM can do an impressive amount of transformations that make the generated code look nothing like the code that went 
in. Disassemble or.. decompile? # You could use tools like wasm2wat (part of the WABT toolkit), to transform a .wasm into Wasm\u2019s standard text format, .wat , which is a very faithful but not particularly 
readable representation. For example, a simple C function like a dot product: typedef struct { float x , y , z ; } vec3 ; float dot ( const vec3 * a , const vec3 * b ) { return a -> x * b -> x + a -> y * b -> y 
+ a -> z * b -> z ; } We use clang dot.c -c -target wasm32 -O2 followed by wasm2wat -f dot.o to turn it into this .wat : ( func $dot ( type 0 ) ( param i32 i32 ) ( result f32 ) ( f32 . add ( f32 . add ( f32 . 
mul ( f32 . load ( local .get 0 ) ) ( f32 . load ( local .get 1 ) ) ) ( f32 . mul ( f32 . load offset = 4 ( local .get 0 ) ) ( f32 . load offset = 4 ( local .get 1 ) ) ) ) ( f32 . mul ( f32 . load offset = 8 ( 
local .get 0 ) ) ( f32 . load offset = 8 ( local .get 1 ) ) ) ) ) ) That is a tiny bit of code, but already not great to read for many reasons. Besides the lack of an expression based syntax and general 
verbosity, having to understand data structures as memory loads is not easy. Now imagine looking at the output of a large program, and things will get incomprehensible fast. Instead of wasm2wat , run 
wasm-decompile dot.o , and you get: function dot ( a : { a : float , b : float , c : float } , b : { a : float , b : float , c : float } ) : float { return a . a * b . a + a . b * b . b + a . c * b . c } This 
looks a lot more familiar. Besides an expression based syntax that mimics programming languages you may be familiar with, the decompiler looks at all loads and stores in a function, and tries to infer their 
structure. It then annotates each variable that is used as a pointer with an \"inline\" struct declaration. It does not create named struct declarations since it doesn\u2019t necessarily know which uses of 3 
floats represent the same concept. Decompile to what? # wasm-decompile produces output that tries to look like a \"very average programming language\" while still staying close to the Wasm it represents. Its #1 
goal is readability: help guide readers understand what is in a .wasm with as easy to follow code as possible. Its #2 goal is to still represent Wasm as 1:1 as possible, to not lose its utility as a 
disassembler. Obviously these two goals are not always unifiable. This output is not meant to be an actual programming language and there is currently no way to compile it back into Wasm. Loads and stores # As 
demonstrated above, wasm-decompile looks at all loads and stores over a particular pointer. If they form a continuous set of accesses, it will output one of these \"inline\" struct declarations. If not all 
\"fields\" are accessed, it can\u2019t tell for sure whether this is meant to be a struct, or some other form of unrelated memory access. In that case it falls back to simpler types like float_ptr (if the types 
are the same), or, in the worst case, will output an array access like o|2]:int , which says: o points to int values, and we\u2019re accessing the third one. That last case happens more often than you\u2019d 
think, since Wasm locals function more like registers than variables, so optimized code may share the same pointer for unrelated objects. The decompiler tries to be smart about indexing, and detects patterns 
like (base + (index << 2))|0]:int that result from regular C array indexing operations like base|index] where base points to a 4-byte type. These are very common in code since Wasm has only constant offsets on 
loads and stores. wasm-decompile output transforms them back into base|index]:int . Additionally it knows when absolute addresses refer to the data section. Control flow # Most familiar is Wasm\u2019s if-then 
construct, which translates to a familiar if (cond) { A } else { B } syntax, with the addition that in Wasm it can actually return a value, so it can also represent the ternary cond ? A : B syntax available in 
some languages. The rest of Wasm\u2019s control flow is based on the block and loop blocks, and the br , br_if and br_table jumps. The decompiler stays decently close to these constructs rather than trying to 
infer the while/for/switch constructs they may have come from, since this tends to work better with optimized output. For example, a typical loop in the wasm-decompile output may look like: loop A { // body of 
the loop here. if ( cond ) continue A ; } Here, A is a label that allows multiple of these to be nested. Having an if and continue to control the loop may look slightly foreign compared to a while loop, but it 
corresponds directly to Wasm\u2019s br_if . Blocks are similar, but instead of branching backwards, they branch forwards: block { if ( cond ) break ; // body goes here. } This actually implements an if-then. 
Future versions of the decompiler may translate these into actual if-thens when possible. Wasm\u2019s most surprising control construct is br_table , which implements something like a switch , except using 
nested block s, which tends to be hard to read. The decompiler flattens these to make them slightly easier to follow, for example: br_table | A , B , C , . . D ] ( a ) ; label A : return 0 ; label B : return 1 ;
label C : return 2 ; label D : This is similar to switch on a , with D being the default case. Other fun features # The decompiler: Can pull names from debug or linking information, or generate names itself. 
When using existing names, it has special code to simplify C++ name mangled symbols. Already supports the multi-value proposal, which makes turning things into expressions and statements a bit harder. Additional
variables are used when multiple values are returned. It can even generate names from the contents of data sections. Outputs nice declarations for all Wasm section types, not just code. For example, it tries to 
make data sections readable by outputting them as text when possible. Supports operator precedence (common to most C-style languages) to reduce the () on common expressions. Limitations # Decompiling Wasm is 
fundamentally harder than, say, JVM bytecode. The latter is un-optimized, so relatively faithful to the structure of the original code, and even though names may be missing, refers to unique classes rather than 
just memory locations. In contrast, most .wasm output has been heavily optimized by LLVM and thus has often lost most of its original structure. The output code is very unlike what a programmer would write. That
makes a decompiler for Wasm a bigger challenge to make useful, but that doesn\u2019t mean we shouldn\u2019t try! More # The best way to see more is of course to decompile your own Wasm project! Additionally, a 
more in-depth guide to wasm-decompile is here . Its implementation is in the source files starting with decompiler here (feel free to contribute a PR to make it better!). Some test cases that show further 
examples of differences between .wat and the decompiler are here . Posted by Wouter van Oortmerssen ( @wvo ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise 
noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site 
policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/lazy-unlinking.txt",
    "similarity": 0.436,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/lazy-unlinking.txt\n\nAn internship on laziness: lazy unlinking of deoptimized functions \u00b7 V8 V8 Show navigation Home Blog Docs 
Tools JS/Wasm features Research An internship on laziness: lazy unlinking of deoptimized functions Published 04 October 2017 \u00b7 Tagged with memory internals Roughly three months ago, I joined the V8 team 
(Google Munich) as an intern and since then I\u2019ve been working on the VM\u2019s Deoptimizer \u2014 something completely new to me which proved to be an interesting and challenging project. The first part of 
my internship focused on improving the VM security-wise . The second part focused on performance improvements. Namely, on the removal of a data-structure used for the unlinking of previously deoptimized 
functions, which was a performance bottleneck during garbage collection. This blog post describes this second part of my internship. I\u2019ll explain how V8 used to unlink deoptimized functions, how we changed 
this, and what performance improvements were obtained. Let\u2019s (very) briefly recap the V8 pipeline for a JavaScript function: V8\u2019s interpreter, Ignition, collects profiling information about that 
function while interpreting it. Once the function becomes hot, this information is passed to V8\u2019s compiler, TurboFan, which generates optimized machine code. When the profiling information is no longer 
valid \u2014 for example because one of the profiled objects gets a different type during runtime \u2014 the optimized machine code might become invalid. In that case, V8 needs to deoptimize it. An overview of 
V8, as seen in JavaScript Start-up Performance Upon optimization, TurboFan generates a code object, i.e. the optimized machine code, for the function under optimization. When this function is invoked the next 
time, V8 follows the link to optimized code for that function and executes it. Upon deoptimization of this function, we need to unlink the code object in order to make sure that it won\u2019t be executed again. 
How does that happen? For example, in the following code, the function f1 will be invoked many times (always passing an integer as argument). TurboFan then generates machine code for that specific case. function
g ( ) { return ( i ) => i ; } // Create a closure. const f1 = g ( ) ; // Optimize f1. for ( var i = 0 ; i < 1000 ; i ++ ) f1 ( 0 ) ; Each function also has a trampoline to the interpreter \u2014 more details in 
these slides \u2014 and will keep a pointer to this trampoline in its SharedFunctionInfo (SFI). This trampoline will be used whenever V8 needs to go back to unoptimized code. Thus, upon deoptimization, triggered
by passing an argument of a different type, for example, the Deoptimizer can simply set the code field of the JavaScript function to this trampoline. An overview of V8, as seen in JavaScript Start-up Performance
Although this seems simple, it forces V8 to keep weak lists of optimized JavaScript functions. This is because it is possible to have different functions pointing to the same optimized code object. We can extend
our example as follows, and the functions f1 and f2 both point to the same optimized code. const f2 = g ( ) ; f2 ( 0 ) ; If the function f1 is deoptimized (for example by invoking it with an object of different 
type {x: 0} ) we need to make sure that the invalidated code will not be executed again by invoking f2 . Thus, upon deoptimization, V8 used to iterate over all the optimized JavaScript functions, and would 
unlink those that pointed to the code object being deoptimized. This iteration in applications with many optimized JavaScript functions became a performance bottleneck. Moreover, other than slowing down 
deoptimization, V8 used to iterate over these lists upon stop-the-world cycles of garbage collection, making it even worse. In order to have an idea of the impact of such data-structure in the performance of V8,
we wrote a micro-benchmark that stresses its usage, by triggering many scavenge cycles after creating many JavaScript functions. function g ( ) { return ( i ) => i + 1 ; } // Create an initial closure and 
optimize. var f = g ( ) ; f ( 0 ) ; f ( 0 ) ; % OptimizeFunctionOnNextCall ( f ) ; f ( 0 ) ; // Create 2M closures; those will get the previously optimized code. var a = | ] ; for ( var i = 0 ; i < 2000000 ; i 
++ ) { var h = g ( ) ; h ( ) ; a . push ( h ) ; } // Now cause scavenges; all of them are slow. for ( var i = 0 ; i < 1000 ; i ++ ) { new Array ( 50000 ) ; } When running this benchmark, we could observe that V8
spent around 98% of its execution time on garbage collection. We then removed this data structure, and instead used an approach for lazy unlinking , and this was what we observed on x64: Although this is just a 
micro-benchmark that creates many JavaScript functions and triggers many garbage collection cycles, it gives us an idea of the overhead introduced by this data structure. Other more realistic applications where 
we saw some overhead, and which motivated this work, were the router benchmark implemented in Node.js and ARES-6 benchmark suite . Lazy unlinking # Rather than unlinking optimized code from JavaScript functions 
upon deoptimization, V8 postpones it for the next invocation of such functions. When such functions are invoked, V8 checks whether they have been deoptimized, unlinks them and then continues with their lazy 
compilation. If these functions are never invoked again, then they will never be unlinked and the deoptimized code objects will not be collected. However, given that during deoptimization, we invalidate all the 
embedded fields of the code object, we only keep that code object alive. The commit that removed this list of optimized JavaScript functions required changes in several parts of the VM, but the basic idea is as 
follows. When assembling the optimized code object, we check if this is the code of a JavaScript function. If so, in its prologue, we assemble machine code to bail out if the code object has been deoptimized. 
Upon deoptimization we don\u2019t modify the deoptimized code \u2014 code patching is gone. Thus, its bit marked_for_deoptimization is still set when invoking the function again. TurboFan generates code to check
it, and if it is set, then V8 jumps to a new builtin, CompileLazyDeoptimizedCode , that unlinks the deoptimized code from the JavaScript function and then continues with lazy compilation. In more detail, the 
first step is to generate instructions that load the address of the code being currently assembled. We can do that in x64, with the following code: Label current ; // Load effective address of current 
instruction into rcx. __ leaq ( rcx , Operand ( & current ) ) ; __ bind ( & current ) ; After that we need to obtain where in the code object the marked_for_deoptimization bit lives. int pc = __ pc_offset ( ) ; 
int offset = Code :: kKindSpecificFlags1Offset - ( Code :: kHeaderSize + pc ) ; We can then test the bit and if it is set, we jump to the CompileLazyDeoptimizedCode built in. // Test if the bit is set, that is, 
if the code is marked for deoptimization. __ testl ( Operand ( rcx , offset ) , Immediate ( 1 << Code :: kMarkedForDeoptimizationBit ) ) ; // Jump to builtin if it is. __ j ( not_zero , /* handle to builtin code
here */ , RelocInfo :: CODE_TARGET ) ; On the side of this CompileLazyDeoptimizedCode builtin, all that\u2019s left to do is to unlink the code field from the JavaScript function and set it to the trampoline to 
the Interpreter entry. So, considering that the address of the JavaScript function is in the register rdi , we can obtain the pointer to the SharedFunctionInfo with: // Field read to obtain the 
SharedFunctionInfo. __ movq ( rcx , FieldOperand ( rdi , JSFunction :: kSharedFunctionInfoOffset ) ) ; \u2026and similarly the trampoline with: // Field read to obtain the code object. __ movq ( rcx , 
FieldOperand ( rcx , SharedFunctionInfo :: kCodeOffset ) ) ; Then we can use it to update the function slot for the code pointer: // Update the code field of the function with the trampoline. __ movq ( 
FieldOperand ( rdi , JSFunction :: kCodeOffset ) , rcx ) ; // Write barrier to protect the field. __ RecordWriteField ( rdi , JSFunction :: kCodeOffset , rcx , r15 , kDontSaveFPRegs , OMIT_REMEMBERED_SET , 
OMIT_SMI_CHECK ) ; This produces the same result as before. However, rather than taking care of the unlinking in the Deoptimizer, we need to worry about it during code generation. Hence the handwritten assembly.
The above is how it works in the x64 architecture . We have implemented it for ia32 , arm , arm64 , mips , and mips64 as well. This new technique is already integrated in V8 and, as we\u2019ll discuss later, 
allows for performance improvements. However, it comes with a minor disadvantage: Before, V8 would consider unlinking only upon deoptimization. Now, it has to do so in the activation of all optimized functions. 
Moreover, the approach to check the marked_for_deoptimization bit is not as efficient as it could be, given that we need to do some work to obtain the address of the code object. Note that this happens when 
entering every optimized function. A possible solution for this issue is to keep in a code object a pointer to itself. Rather than doing work to find the address of the code object whenever the function is 
invoked, V8 would do it only once, after its construction. Results # We now look at the performance gains and regressions obtained with this project. General improvements on x64 # The following plot shows us 
some improvements and regressions, relative to the previous commit. Note that the higher, the better. The promises benchmarks are the ones where we see greater improvements, observing almost 33% gain for the 
bluebird-parallel benchmark, and 22.40% for wikipedia . We also observed a few regressions in some benchmarks. This is related to the issue explained above, on checking whether the code is marked for 
deoptimization. We also see improvements in the ARES-6 benchmark suite. Note that in this chart too, the higher the better. These programs used to spend considerable amount of time in GC-related activities. With
lazy unlinking we improve performance by 1.9% overall. The most notable case is the Air steadyState where we get an improvement of around 5.36%. AreWeFastYet results # The performance results for the Octane and 
ARES-6 benchmark suites also showed up on the AreWeFastYet tracker. We looked at these performance results on September 5th, 2017, using the provided default machine (macOS 10.10 64-bit, Mac Pro, shell). 
Cross-browser results on Octane as seen on AreWeFastYet Cross-browser results on ARES-6 as seen on AreWeFastYet Impact on Node.js # We can also see performance improvements in the router-benchmark . The 
following two plots show the number of operations per second of each tested router. Thus the higher the better. We have performed two kinds of experiments with this benchmark suite. Firstly, we ran each test in 
isolation, so that we could see the performance improvement, independently from the remaining tests. Secondly, we ran all tests at once, without switching of the VM, thus simulating an environment where each 
test is integrated with other functionalities. For the first experiment, we saw that the router and express tests perform about twice as many operations than before, in the same amount of time. For the second 
experiment, we saw even greater improvement. In some of the cases, such as routr , server-router and router , the benchmark performs approximately 3.80\u00d7, 3\u00d7 and 2\u00d7 more operations, respectively. 
This happens because V8 accumulates more optimized JavaScript functions, test after test. Thus, whenever executing a given test, if a garbage collection cycle is triggered, V8 has to visit the optimized 
functions from the current test and from the previous ones. Further optimization # Now that V8 does not keep the linked-list of JavaScript functions in the context, we can remove the field next from the 
JSFunction class. Although this is a simple modification, it allows us to save the size of a pointer per function, which represent significant savings in several web pages: Benchmark Kind Memory savings 
(absolute) Memory savings (relative) facebook.com Average effective size 170 KB 3.70% twitter.com Average size of allocated objects 284 KB 1.20% cnn.com Average size of allocated objects 788 KB 1.53% youtube.com
Average size of allocated objects 129 KB 0.79% Acknowledgments # Throughout my internship, I had lots of help from several people, who were always available to answer my many questions. Thus I would like to 
thank the following people: Benedikt Meurer, Jaroslav Sevcik, and Michael Starzinger for discussions on how the compiler and the deoptimizer work, Ulan Degenbaev for helping with the garbage collector whenever I
broke it, and Mathias Bynens, Peter Marshall, Camillo Bruni, and Maya Armyanova for proofreading this article. Finally, this article is my last contribution as a Google intern and I would like to take the 
opportunity to thank everyone in the V8 team, and especially my host, Benedikt Meurer, for hosting me and for giving me the opportunity to work on such an interesting project \u2014 I definitely learned a lot 
and enjoyed my time at Google! Posted by Juliana Franco ( @jupvfranco ), Laziness Expert. Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples
from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  }
]
[Step 10: Duration 8.33 seconds| Input tokens: 75,816 | Output tokens: 3,860]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 11 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'deopt'}                                                                                                                             │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: deopt-recursive-eager-once
const-tracking-let-already-not-constant-global-no-deopt-maglev
deopt-unlinked
deopt-recursive-soft-once
deopt-global-accessor
arguments-apply-deopt
const-tracking-let-already-not-constant-no-deopt-maglev
deopt-recursive-lazy-once
const-tracking-let-no-deopt-write-if-already-not-const
deopt-with-fp-regs
string-constructor-inlining-lazy-deopt
math-deopt
unary-minus-deopt
deopt-with-outer-context
string-deopt
arguments-deopt
deopt-minus-zero
wasm/asm-wasm-deopt
wasm/wrapper-inlining-lazy-deopt
wasm/deopt/deopt-untagged-parameters
wasm/deopt/deopt-multi-instance-inlined
wasm/deopt/deopt-non-inlined-target
wasm/deopt/deopt-serialized-module
wasm/deopt/deopt-value-types
wasm/deopt/deopt-tail-call-parent-frame
wasm/deopt/deopt-inlined-param-stack-slots
wasm/deopt/deopt-minimal
wasm/deopt/deopt-inlined-stacktrace
wasm/deopt/deopt-too-much-feedback
wasm/deopt/deopt-many-params-tagged
wasm/deopt/deopt-signal-handler
wasm/deopt/deopt-large-i31ref-constant
wasm/deopt/deopt-dynamic-tierup
wasm/deopt/deopt-multiple
wasm/deopt/deopt-multi-instance-call-indirect
wasm/deopt/deopt-many-params-many-locals-s128
wasm/deopt/deopt-untagged-parameters-s128
wasm/deopt/deopt-feedback-states
wasm/deopt/deopt-call-indirect
wasm/deopt/deopt-multi-instance-different-callee
wasm/deopt/deopt-non-inlineable-target
wasm/deopt/deopt-metrics
wasm/deopt/deopt-int64-values
wasm/deopt/deopt-many-params
wasm/deopt/deopt-inlined
wasm/deopt/deopt-memory-access
wasm/deopt/deopt-debugging
wasm/deopt/deopt-nan
wasm/deopt/deopt-many-results
compiler/deopt-eager-var-mutation-ite
compiler/deopt-closure
compiler/bigint64-mod-no-deopt-loop
compiler/deopt-lazy-var-mutation
compiler/deopt-array-prototype-includes
compiler/deopt-array-prototype-map
compiler/deopt-eager-and-lazy
compiler/checkmaps-with-migration-and-deopt-mono2
compiler/inline-context-deopt
compiler/deopt-inlined-smi
compiler/deopt-followed-by-gc
compiler/checkmaps-with-migration-and-deopt-poly2
compiler/deopt-literal-string
compiler/deopt-array-prototype-indexof
compiler/deopt-twice-on-call
compiler/deopt-tonumber-shift
compiler/deopt-accessors5
compiler/deopt-array-prototype-slice
compiler/deopt-args
compiler/deopt-simple-try-catch
compiler/deopt-bool2
compiler/deopt-during-eval-lookup
compiler/escape-analysis-deopt-2
compiler/deopt-simple-lazy
compiler/deopt-inlined-from-call
compiler/assignment-deopt
compiler/deopt-accessors4
compiler/deopt-array-prototype-reduce
compiler/bigint-add-no-deopt-loop
compiler/lazy-deopt-in-literal
compiler/deopt-array-prototype-lastindexof
compiler/deopt-literal-receiver
compiler/try-finally-deopt
compiler/deopt-twice
compiler/deopt-numberoroddball-binop
compiler/number-constructor-deopt
compiler/simple-deopt
compiler/bigint64-mul-no-deopt-loop
compiler/misc-ensure-no-deopt
compiler/escape-analysis-deopt-1
compiler/deopt-tonumber-binop
compiler/escape-analysis-deopt-5
compiler/escape-analysis-deopt-3
compiler/string-concat-deopt
compiler/deopt-accessors3
compiler/deopt-eager-with-freeze
compiler/regress-loop-deopt
compiler/regress-shared-deopt
compiler/deopt-now-lazy
compiler/escape-analysis-deopt-6
compiler/count-deopt
compiler/regress-to-number-binop-deopt
compiler/inline-omit-arguments-deopt
compiler/deopt-array-prototype-find
compiler/stress-deopt-count-1
compiler/stress-deopt-count-2
compiler/deoptimize-lazy-weak
compiler/try-catch-deopt
compiler/checkmaps-with-migration-and-deopt-poly3
compiler/checkmaps-with-migration-and-deopt-mono
compiler/deopt-string-outofbounds
compiler/dataview-deopt
compiler/dont-constant-fold-deopting-checks
compiler/deopt-array-push
compiler/deopt-soft-simple
compiler/escape-analysis-deopt-4
compiler/fast-api-deopt
compiler/deopt-array-prototype-foreach
compiler/deopt-array-prototype-findindex
compiler/deopt-array-iterator-prototype-next
compiler/deopt-array-prototype-some
compiler/regress-lazy-deopt
compiler/deopt-tonumber-compare
compiler/deopt-accessors7
compiler/bigint64-add-no-deopt-loop
compiler/deopt-many-lazy
compiler/deopt-lazy-freeze
compiler/try-deopt
compiler/eager-deopt-simple
compiler/deopt-lazy-shape-mutation
compiler/deopt-pretenure
compiler/deopt-array-prototype-pop
compiler/bigint64-sub-no-deopt-loop
compiler/bigint64-div-no-deopt-loop
compiler/checkmaps-with-migration-and-deopt-poly
compiler/inline-surplus-arguments-deopt
compiler/recursive-deopt
compiler/deopt-accessors1
compiler/deopt-simple-eager
compiler/deopt-array-prototype-filter
compiler/regress-deopt-call-as-function
compiler/lazy-deopt-async-function-resolve
compiler/truncating-store-deopt
compiler/deopt-accessors2
compiler/deopt-accessors6
compiler/deopt-array-builtins
compiler/deopt-array-prototype-every
compiler/deopt-bool
compiler/deopt-materialize-accumulator
regress/regress-4578-deopt
regress/regress-deoptimize-constant-keyed-load
regress/regress-lazy-deopt-inlining2
regress/regress-deopt-in-array-literal-spread
regress/comparison-in-effect-context-deopt
regress/regress-bind-deoptimize
regress/regress-deopt-gcb
regress/regress-lazy-deopt-reloc
regress/regress-unlink-closures-on-deopt
regress/regress-deopt-gc
regress/binop-in-effect-context-deopt
regress/number-named-call-deopt
regress/call-function-in-effect-context-deopt
regress/regress-array-pop-deopt
regress/regress-grow-deopt
regress/regress-deopt-store-effect
regress/regress-lazy-deopt-inlining
harmony/bigint/rematerialize-on-deopt
es6/computed-property-names-deopt
es6/iterator-eager-deopt
es6/iterator-call-lazy-deopt
es6/iterator-get-lazy-deopt
turbolev/constant-typed-array-load-deopt-oob
turbolev/constant-typed-array-store-deopt-oob
turbolev/constant-typed-array-store-deopt-detach
turbolev/constant-typed-array-load-deopt-detach
maglev/typed-array-length-deopt
maglev/checkmaps-with-migration-and-deopt-mono2
maglev/checkmaps-with-migration-and-deopt-poly2
maglev/typed-array-length-deopt-large
maglev/eager-deopt-in-inline
maglev/constant-typed-array-load-deopt-oob
maglev/lazy-deopt-with-onstack-activation
maglev/lazy-deopt-without-onstack-activation
maglev/constant-typed-array-store-deopt-oob
maglev/checkmaps-with-migration-and-deopt-poly3
maglev/checkmaps-with-migration-and-deopt-mono
maglev/inline-fresh-parent-deopt-frame
maglev/no-deopt-deprecated-map
maglev/constant-typed-array-store-deopt-detach
maglev/checkmaps-with-migration-and-deopt-poly
maglev/constant-typed-array-load-deopt-detach
turboshaft/checkmaps-with-migration-and-deopt-mono2
turboshaft/checkmaps-with-migration-and-deopt-poly2
turboshaft/checkmaps-with-migration-and-deopt-poly3
turboshaft/checkmaps-with-migration-and-deopt-mono
turboshaft/checkmaps-with-migration-and-deopt-poly
turboshaft/maglev-frontend/typed-array-length-deopt
turboshaft/maglev-frontend/js-call-lazy-deopt
turboshaft/maglev-frontend/unconditional-deopt
turboshaft/maglev-frontend/deopt-raw-float-int32
turboshaft/maglev-frontend/uint8-clamped-arr-deopt
turboshaft/maglev-frontend/typed-array-length-deopt-large
turboshaft/maglev-frontend/char-code-at-uint32max-deopt
turboshaft/maglev-frontend/new-obj-deopt
turboshaft/maglev-frontend/holey-float64-phi-deopt
turboshaft/maglev-frontend/literal-nan-hole-deopt
turboshaft/maglev-frontend/lazy-deopt-multi-return
turboshaft/maglev-frontend/holey-float64-deopt
turboshaft/maglev-frontend/builtin-continuation-deopt
[Step 11: Duration 6.93 seconds| Input tokens: 111,762 | Output tokens: 4,208]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 12 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'deoptimizer.cc Deoptimizer FrameDescription CompileLazyDeoptimizedCode CompileLazyDeoptimizedCode builtin src/deoptimizer.cc deopt frame       │
│ description', 'top_k': 5, 'topic_filter': 'v8'}                                                                                                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.468,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/webassembly-opcode.txt",
    "similarity": 0.455,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/webassembly-opcode.txt\n\nWebAssembly - adding a new opcode \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
WebAssembly - adding a new opcode WebAssembly (Wasm) is a binary instruction format for a stack-based virtual machine. This tutorial walks the reader through implementing a new WebAssembly instruction in V8. 
WebAssembly is implemented in V8 in three parts: the interpreter the baseline compiler (Liftoff) the optimizing compiler (TurboFan) The rest of this document focuses on the TurboFan pipeline, walking through how
to add a new Wasm instruction and implement it in TurboFan. At a high level, Wasm instructions are compiled into a TurboFan graph, and we rely on the TurboFan pipeline to compile the graph into (ultimately) 
machine code. For more on TurboFan, check out the V8 docs . Opcodes/Instructions # Let\u2019s define a new instruction that adds 1 to an int32 (on the top of the stack). Note: A list of instructions supported by
all Wasm implementations can be found in the spec . All Wasm instructions are defined in src/wasm/wasm-opcodes.h . The instructions are grouped roughly by what they do, e.g. control, memory, SIMD, atomic, etc. 
Let\u2019s add our new instruction, I32Add1 , to the FOREACH_SIMPLE_OPCODE section: diff --git a/src/wasm/wasm-opcodes.h b/src/wasm/wasm-opcodes.h index 6970c667e7..867cbf451a 100644 --- 
a/src/wasm/wasm-opcodes.h +++ b/src/wasm/wasm-opcodes.h @@ -96,6 +96,7 @@ bool IsJSCompatibleSignature(const FunctionSig* sig, bool hasBigIntFeature); // Expressions with signatures. #define 
FOREACH_SIMPLE_OPCODE(V) \\ + V(I32Add1, 0xee, i_i) \\ V(I32Eqz, 0x45, i_i) \\ V(I32Eq, 0x46, i_ii) \\ V(I32Ne, 0x47, i_ii) \\ WebAssembly is a binary format, so 0xee specifies the encoding of this instruction. 
In this tutorial we chose 0xee as it is currently unused. Note: Actually adding an instruction to the spec involves work beyond what is described here. We can run a simple unit test for opcodes with: $ 
tools/dev/gm.py x64.debug unittests/WasmOpcodesTest* ... |==========] Running 1 test from 1 test suite. |----------] Global test environment set-up. |----------] 1 test from WasmOpcodesTest | RUN ] 
WasmOpcodesTest.EveryOpcodeHasAName ../../test/unittests/wasm/wasm-opcodes-unittest.cc:27: Failure Value of: false Actual: false Expected: true WasmOpcodes::OpcodeName(kExprI32Add1) == \"unknown\"; plazz halp in
src/wasm/wasm-opcodes.cc | FAILED ] WasmOpcodesTest.EveryOpcodeHasAName This error indicates that we don\u2019t have a name for our new instruction. Adding a name for the new opcode can be done in 
src/wasm/wasm-opcodes.cc : diff --git a/src/wasm/wasm-opcodes.cc b/src/wasm/wasm-opcodes.cc index 5ed664441d..2d4e9554fe 100644 --- a/src/wasm/wasm-opcodes.cc +++ b/src/wasm/wasm-opcodes.cc @@ -75,6 +75,7 @@ 
const char* WasmOpcodes::OpcodeName(WasmOpcode opcode) { // clang-format off // Standard opcodes + CASE_I32_OP(Add1, \"add1\") CASE_INT_OP(Eqz, \"eqz\") CASE_ALL_OP(Eq, \"eq\") CASE_I64x2_OP(Eq, \"eq\") By 
adding our new instruction in FOREACH_SIMPLE_OPCODE , we are skipping a fair amount of work that is done in src/wasm/function-body-decoder-impl.h , which decodes Wasm opcodes and calls into the TurboFan graph 
generator. Thus, depending on what your opcode does, you might have more work to do. We skip this in the interest of brevity. Writing a test for the new opcode # Wasm tests can be found in test/cctest/wasm/ . 
Let\u2019s take a look at test/cctest/wasm/test-run-wasm.cc , where many \u201csimple\u201d opcodes are tested. There are many examples in this file that we can follow. The general setup is: create a WasmRunner 
set up globals to hold result (optional) set up locals as parameters to instruction (optional) build the wasm module run it and compare with an expected output Here\u2019s a simple test for our new opcode: diff 
--git a/test/cctest/wasm/test-run-wasm.cc b/test/cctest/wasm/test-run-wasm.cc index 26df61ceb8..b1ee6edd71 100644 --- a/test/cctest/wasm/test-run-wasm.cc +++ b/test/cctest/wasm/test-run-wasm.cc @@ -28,6 +28,15 
@@ namespace test_run_wasm { #define RET(x) x, kExprReturn #define RET_I8(x) WASM_I32V_2(x), kExprReturn + #define WASM_I32_ADD1(x) x, kExprI32Add1 + + WASM_EXEC_TEST(Int32Add1) { + WasmRunner<int32_t> 
r(execution_tier); + // 10 + 1 + BUILD(r, WASM_I32_ADD1(WASM_I32V_1(10))); + CHECK_EQ(11, r.Call()); + } + WASM_EXEC_TEST(Int32Const) { WasmRunner<int32_t> r(execution_tier); const int32_t kExpectedValue = 
0x11223344; Run the test: $ tools/dev/gm.py x64.debug 'cctest/test-run-wasm-simd/RunWasmTurbofan_I32Add1' ... === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in 
../../src/compiler/wasm-compiler.cc, line 988 # Unsupported opcode 0xee:i32.add1 Tip: Finding the test name can be tricky, since the test definition is behind a macro. Use Code Search to click around to discover
the macro definitions. This error indicates that the compiler does not know of our new instruction. That will change in the next section. Compiling Wasm into TurboFan # In the introduction, we mentioned that 
Wasm instructions are compiled into a TurboFan graph. wasm-compiler.cc is where this happens. Let\u2019s take a look at an example opcode, I32Eqz : switch ( opcode ) { case wasm :: kExprI32Eqz : op = m -> 
Word32Equal ( ) ; return graph ( ) -> NewNode ( op , input , mcgraph ( ) -> Int32Constant ( 0 ) ) ; This switches on the Wasm opcode wasm::kExprI32Eqz , and builds a TurboFan graph consisting of the operation 
Word32Equal with the inputs input , which is the argument to the Wasm instruction, and a constant 0 . The Word32Equal operator is provided by the underlying V8 abstract machine, which is 
architecture-independent. Later in the pipeline, this abstract machine operator will be translated into architecture-dependent assembly. For our new opcode, I32Add1 , we need a graph that adds a constant 1 to 
the input, so we can resuse an existing machine operator, Int32Add , passing it the input, and a constant 1: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index 
f666bbb7c1..399293c03b 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 @@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; 
MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return graph()->NewNode(m->Int32Add(), input, mcgraph()->Int32Constant(1)); case wasm::kExprI32Eqz: op = 
m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); This is enough to get the test passing. However, not all instructions have an existing TurboFan machine operator. In that case 
we have to add this new operator to the machine. Let\u2019s try that. TurboFan machine operators # We want to add the knowledge of Int32Add1 to the TurboFan machine. So let\u2019s pretend that it exists and use 
it first: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index f666bbb7c1..1d93601584 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 
@@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return 
graph()->NewNode(m->Int32Add1(), input); case wasm::kExprI32Eqz: op = m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); Trying to run the same test leads to a compilation failure
that hints at where to make changes: ../../src/compiler/wasm-compiler.cc:717:34: error: no member named 'Int32Add1' in 'v8::internal::compiler::MachineOperatorBuilder'; did you mean 'Int32Add'? return 
graph()->NewNode(m->Int32Add1(), input); ^~~~~~~~~ Int32Add There are a couple of places that needs to be modified to add an operator: src/compiler/machine-operator.cc header src/compiler/machine-operator.h list
of opcodes that the machine understands src/compiler/opcodes.h verifier src/compiler/verifier.cc diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc index 16e838c2aa..fdd6d951f0 
100644 --- a/src/compiler/machine-operator.cc +++ b/src/compiler/machine-operator.cc @@ -136,6 +136,7 @@ MachineType AtomicOpType(Operator const* op) { #define MACHINE_PURE_OP_LIST(V) \\ 
PURE_BINARY_OP_LIST_32(V) \\ PURE_BINARY_OP_LIST_64(V) \\ + V(Int32Add1, Operator::kNoProperties, 1, 0, 1) \\ V(Word32Clz, Operator::kNoProperties, 1, 0, 1) \\ V(Word64Clz, Operator::kNoProperties, 1, 0, 1) \\ 
V(Word32ReverseBytes, Operator::kNoProperties, 1, 0, 1) \\ diff --git a/src/compiler/machine-operator.h b/src/compiler/machine-operator.h index a2b9fce0ee..f95e75a445 100644 --- a/src/compiler/machine-operator.h
+++ b/src/compiler/machine-operator.h @@ -265,6 +265,8 @@ class V8_EXPORT_PRIVATE MachineOperatorBuilder final const Operator* Word32PairShr(); const Operator* Word32PairSar(); + const Operator* Int32Add1(); + 
const Operator* Int32Add(); const Operator* Int32AddWithOverflow(); const Operator* Int32Sub(); diff --git a/src/compiler/opcodes.h b/src/compiler/opcodes.h index ce24a0bd3f..2c8c5ebaca 100644 --- 
a/src/compiler/opcodes.h +++ b/src/compiler/opcodes.h @@ -506,6 +506,7 @@ V(Float64LessThanOrEqual) #define MACHINE_UNOP_32_LIST(V) \\ + V(Int32Add1) \\ V(Word32Clz) \\ V(Word32Ctz) \\ V(Int32AbsWithOverflow) \\
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc index 461aef0023..95251934ce 100644 --- a/src/compiler/verifier.cc +++ b/src/compiler/verifier.cc @@ -1861,6 +1861,7 @@ void 
Verifier::Visitor::Check(Node* node, const AllNodes& all) { case IrOpcode::kSignExtendWord16ToInt64: case IrOpcode::kSignExtendWord32ToInt64: case IrOpcode::kStaticAssert: + case IrOpcode::kInt32Add1: #define 
SIMD_MACHINE_OP_CASE(Name) case IrOpcode::k##Name: MACHINE_SIMD_OP_LIST(SIMD_MACHINE_OP_CASE) Running the test again now gives us a different failure: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # 
Fatal error in ../../src/compiler/backend/instruction-selector.cc, line 2072 # Unexpected operator #289:Int32Add1 @ node #7 Instruction selection # So far we have been working at the TurboFan level, dealing with
(a sea of) nodes in the TurboFan graph. However, at the assembly level, we have instructions and operands. Instruction selection is the process of translating this graph to instructions and operands. The last 
test error indicated that we need something in src/compiler/backend/instruction-selector.cc . This is a big file with a giant switch statement over all the machine opcodes. It calls into architecture specific 
instruction selection, using the visitor pattern to emit instructions for each type of node. Since we added a new TurboFan machine opcode, we need to add it here as well: diff --git 
a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc index 3152b2d41e..7375085649 100644 --- a/src/compiler/backend/instruction-selector.cc +++ 
b/src/compiler/backend/instruction-selector.cc @@ -2067,6 +2067,8 @@ void InstructionSelector::VisitNode(Node* node) { return MarkAsWord32(node), VisitS1x16AnyTrue(node); case IrOpcode::kS1x16AllTrue: return 
MarkAsWord32(node), VisitS1x16AllTrue(node); + case IrOpcode::kInt32Add1: + return MarkAsWord32(node), VisitInt32Add1(node); default: FATAL(\"Unexpected operator #%d:%s @ node #%d\", node->opcode(), 
node->op()->mnemonic(), node->id()); Instruction selection is architecture dependent, so we have to add it to the architecture specific instruction selector files too. For this codelab we only focus on the x64 
architecture, so src/compiler/backend/x64/instruction-selector-x64.cc needs to be modified: diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc 
b/src/compiler/backend/x64/instruction-selector-x64.cc index 2324e119a6..4b55671243 100644 --- a/src/compiler/backend/x64/instruction-selector-x64.cc +++ b/src/compiler/backend/x64/instruction-selector-x64.cc @@
-841,6 +841,11 @@ void InstructionSelector::VisitWord32ReverseBytes(Node* node) { Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); } + void InstructionSelector::VisitInt32Add1(Node*
node) { + X64OperandGenerator g(this); + Emit(kX64Int32Add1, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); + } + And we also need to add this new x64-specific opcode, kX64Int32Add1 to 
src/compiler/backend/x64/instruction-codes-x64.h : diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h index 9b8be0e0b5..7f5faeb87b 100644 --- 
a/src/compiler/backend/x64/instruction-codes-x64.h +++ b/src/compiler/backend/x64/instruction-codes-x64.h @@ -12,6 +12,7 @@ namespace compiler { // X64-specific opcodes that specify which assembly sequence to 
emit. // Most opcodes specify a single instruction. #define TARGET_ARCH_OPCODE_LIST(V) \\ + V(X64Int32Add1) \\ V(X64Add) \\ V(X64Add32) \\ V(X64And) \\ Instruction scheduling and code generation # Running our 
test, we see new compilation errors: ../../src/compiler/backend/x64/instruction-scheduler-x64.cc:15:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(instr->arch_opcode()) { ^ 1 error generated. ... ../../src/compiler/backend/x64/code-generator-x64.cc:733:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(arch_opcode) { ^ 1 error generated. Instruction scheduling takes care of dependencies that instructions may have to allow for more optimization (e.g. instruction reordering). Our new opcode has no data 
dependency, so we can add it simply to: src/compiler/backend/x64/instruction-scheduler-x64.cc : diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc 
b/src/compiler/backend/x64/instruction-scheduler-x64.cc index 79eda7e78d..3667a84577 100644 --- a/src/compiler/backend/x64/instruction-scheduler-x64.cc +++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -13,6 +13,7 @@ bool InstructionScheduler::SchedulerSupported() { return true; } int InstructionScheduler::GetTargetInstructionFlags( const Instruction* instr) const { switch (instr->arch_opcode()) { + case 
kX64Int32Add1: case kX64Add: case kX64Add32: case kX64And: Code generation is where we translate our architecture specific opcodes into assembly. Let\u2019s add a clause to 
src/compiler/backend/x64/code-generator-x64.cc : diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc index 61c3a45a16..9c37ed7464 100644 --- 
a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -731,6 +731,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction( InstructionCode 
opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { 
Handle<Code> code = i.InputCode(0); For now we leave our code generation empty, and we can run the test to make sure everything compiles: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in
../../test/cctest/wasm/test-run-wasm.cc, line 37 # Check failed: 11 == r.Call() (11 vs. 10). This failure is expected, since our new instruction is not implemented yet \u2014 it is essentially a no-op, so our 
actual value was unchanged ( 10 ). To implement our opcode, we can use the add assembly instruction: diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc 
index 6c828d6bc4..260c8619f2 100644 --- a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -744,6 +744,11 @@ CodeGenerator::CodeGenResult 
CodeGenerator::AssembleArchInstruction( InstructionCode opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + 
DCHECK_EQ(i.OutputRegister(), i.InputRegister(0)); + __ addl(i.InputRegister(0), Immediate(1)); + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { Handle<Code> code = i.InputCode(0); And
this makes the test pass: Luckily for us addl is already implemented. If our new opcode required writing a new assembly instruction implementation, we would add it to src/compiler/backend/x64/assembler-x64.cc , 
where the assembly instruction is encoded into bytes and emitted. Tip: To inspect the generated code, we can pass --print-code to cctest . Other architectures # In this codelab we only implemented this new 
instruction for x64. The steps required for other architectures are similar: add TurboFan machine operators, use the platform-dependent files for instruction selection, scheduling, code generation, assembler. 
Tip: if we compile what we have done so far on another target, e.g. arm64, we are likely to get errors in linking. To resolve those errors, add UNIMPLEMENTED() stubs. Branding Terms Privacy Twitter Edit this 
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-decompile.txt",
    "similarity": 0.433,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-decompile.txt\n\nWhat\u2019s in that .wasm? Introducing: wasm-decompile \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm 
features Research What\u2019s in that .wasm ? Introducing: wasm-decompile Published 27 April 2020 \u00b7 Tagged with WebAssembly tooling We have a growing number of compilers and other tools that generate or 
manipulate .wasm files, and sometimes you might want to have a look inside. Maybe you\u2019re a developer of such a tool, or more directly, you\u2019re a programmer targeting Wasm, and wondering what the 
generated code looks like, for performance or other reasons. Problem is, Wasm is rather low-level, much like actual assembly code. In particular, unlike, say, the JVM, all data structures have been compiled down
to load/store operations, rather than conveniently named classes and fields. Compilers like LLVM can do an impressive amount of transformations that make the generated code look nothing like the code that went 
in. Disassemble or.. decompile? # You could use tools like wasm2wat (part of the WABT toolkit), to transform a .wasm into Wasm\u2019s standard text format, .wat , which is a very faithful but not particularly 
readable representation. For example, a simple C function like a dot product: typedef struct { float x , y , z ; } vec3 ; float dot ( const vec3 * a , const vec3 * b ) { return a -> x * b -> x + a -> y * b -> y 
+ a -> z * b -> z ; } We use clang dot.c -c -target wasm32 -O2 followed by wasm2wat -f dot.o to turn it into this .wat : ( func $dot ( type 0 ) ( param i32 i32 ) ( result f32 ) ( f32 . add ( f32 . add ( f32 . 
mul ( f32 . load ( local .get 0 ) ) ( f32 . load ( local .get 1 ) ) ) ( f32 . mul ( f32 . load offset = 4 ( local .get 0 ) ) ( f32 . load offset = 4 ( local .get 1 ) ) ) ) ( f32 . mul ( f32 . load offset = 8 ( 
local .get 0 ) ) ( f32 . load offset = 8 ( local .get 1 ) ) ) ) ) ) That is a tiny bit of code, but already not great to read for many reasons. Besides the lack of an expression based syntax and general 
verbosity, having to understand data structures as memory loads is not easy. Now imagine looking at the output of a large program, and things will get incomprehensible fast. Instead of wasm2wat , run 
wasm-decompile dot.o , and you get: function dot ( a : { a : float , b : float , c : float } , b : { a : float , b : float , c : float } ) : float { return a . a * b . a + a . b * b . b + a . c * b . c } This 
looks a lot more familiar. Besides an expression based syntax that mimics programming languages you may be familiar with, the decompiler looks at all loads and stores in a function, and tries to infer their 
structure. It then annotates each variable that is used as a pointer with an \"inline\" struct declaration. It does not create named struct declarations since it doesn\u2019t necessarily know which uses of 3 
floats represent the same concept. Decompile to what? # wasm-decompile produces output that tries to look like a \"very average programming language\" while still staying close to the Wasm it represents. Its #1 
goal is readability: help guide readers understand what is in a .wasm with as easy to follow code as possible. Its #2 goal is to still represent Wasm as 1:1 as possible, to not lose its utility as a 
disassembler. Obviously these two goals are not always unifiable. This output is not meant to be an actual programming language and there is currently no way to compile it back into Wasm. Loads and stores # As 
demonstrated above, wasm-decompile looks at all loads and stores over a particular pointer. If they form a continuous set of accesses, it will output one of these \"inline\" struct declarations. If not all 
\"fields\" are accessed, it can\u2019t tell for sure whether this is meant to be a struct, or some other form of unrelated memory access. In that case it falls back to simpler types like float_ptr (if the types 
are the same), or, in the worst case, will output an array access like o|2]:int , which says: o points to int values, and we\u2019re accessing the third one. That last case happens more often than you\u2019d 
think, since Wasm locals function more like registers than variables, so optimized code may share the same pointer for unrelated objects. The decompiler tries to be smart about indexing, and detects patterns 
like (base + (index << 2))|0]:int that result from regular C array indexing operations like base|index] where base points to a 4-byte type. These are very common in code since Wasm has only constant offsets on 
loads and stores. wasm-decompile output transforms them back into base|index]:int . Additionally it knows when absolute addresses refer to the data section. Control flow # Most familiar is Wasm\u2019s if-then 
construct, which translates to a familiar if (cond) { A } else { B } syntax, with the addition that in Wasm it can actually return a value, so it can also represent the ternary cond ? A : B syntax available in 
some languages. The rest of Wasm\u2019s control flow is based on the block and loop blocks, and the br , br_if and br_table jumps. The decompiler stays decently close to these constructs rather than trying to 
infer the while/for/switch constructs they may have come from, since this tends to work better with optimized output. For example, a typical loop in the wasm-decompile output may look like: loop A { // body of 
the loop here. if ( cond ) continue A ; } Here, A is a label that allows multiple of these to be nested. Having an if and continue to control the loop may look slightly foreign compared to a while loop, but it 
corresponds directly to Wasm\u2019s br_if . Blocks are similar, but instead of branching backwards, they branch forwards: block { if ( cond ) break ; // body goes here. } This actually implements an if-then. 
Future versions of the decompiler may translate these into actual if-thens when possible. Wasm\u2019s most surprising control construct is br_table , which implements something like a switch , except using 
nested block s, which tends to be hard to read. The decompiler flattens these to make them slightly easier to follow, for example: br_table | A , B , C , . . D ] ( a ) ; label A : return 0 ; label B : return 1 ;
label C : return 2 ; label D : This is similar to switch on a , with D being the default case. Other fun features # The decompiler: Can pull names from debug or linking information, or generate names itself. 
When using existing names, it has special code to simplify C++ name mangled symbols. Already supports the multi-value proposal, which makes turning things into expressions and statements a bit harder. Additional
variables are used when multiple values are returned. It can even generate names from the contents of data sections. Outputs nice declarations for all Wasm section types, not just code. For example, it tries to 
make data sections readable by outputting them as text when possible. Supports operator precedence (common to most C-style languages) to reduce the () on common expressions. Limitations # Decompiling Wasm is 
fundamentally harder than, say, JVM bytecode. The latter is un-optimized, so relatively faithful to the structure of the original code, and even though names may be missing, refers to unique classes rather than 
just memory locations. In contrast, most .wasm output has been heavily optimized by LLVM and thus has often lost most of its original structure. The output code is very unlike what a programmer would write. That
makes a decompiler for Wasm a bigger challenge to make useful, but that doesn\u2019t mean we shouldn\u2019t try! More # The best way to see more is of course to decompile your own Wasm project! Additionally, a 
more in-depth guide to wasm-decompile is here . Its implementation is in the source files starting with decompiler here (feel free to contribute a PR to make it better!). Some test cases that show further 
examples of differences between .wat and the decompiler are here . Posted by Wouter van Oortmerssen ( @wvo ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise 
noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site 
policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-speculative-optimizations.txt",
    "similarity": 0.426,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-speculative-optimizations.txt\n\nSpeculative Optimizations for WebAssembly using Deopts and Inlining \u00b7 V8 V8 Show navigation 
Home Blog Docs Tools JS/Wasm features Research Speculative Optimizations for WebAssembly using Deopts and Inlining Published 24 June 2025 \u00b7 Tagged with WebAssembly internals In this blog post, we explain 
two optimizations for WebAssembly that we recently implemented in V8 and that shipped with Google Chrome M137, namely speculative call_indirect inlining and deoptimization support for WebAssembly. In 
combination, they allow us to generate better machine code by making assumptions based on runtime feedback. This speeds up WebAssembly execution, in particular for WasmGC programs. On a set of Dart 
microbenchmarks for example, the speedup by the combination of both optimizations is more than 50% on average, and on larger, realistic applications and benchmarks shown below the speedup is between 1% and 8%. 
Deoptimizations are also an important building block for further optimizations in the future. Background # Fast execution of JavaScript relies heavily on speculative optimizations . That is, JIT-compilers make 
assumptions when generating machine code based on feedback that was collected during earlier executions. For example, given the expression a + b , the compiler can generate machine code for an integer addition 
if past feedback indicates that a and b are integers (and not strings, floating point numbers, or other objects). Without making such assumptions, the compiler would have to emit generic code that handles the 
full behavior of the + operator in JavaScript , which is complex and thus much slower. If the program later behaves differently and thus violates assumptions made when generating the optimized code, V8 performs 
a deoptimization (or deopt, for short). That means throwing away the optimized code and continuing execution in unoptimized code (and collecting more feedback to possibly tier-up again later). In contrast to 
JavaScript, fast execution of WebAssembly hasn\u2019t required speculative optimizations and deopts. One reason is that WebAssembly programs can already be optimized quite well because more information is 
statically available as e.g., functions, instructions, and variables are all statically typed. Another reason is that WebAssembly binaries are often compiled from C, C++, or Rust. These source languages are also
more amenable to static analysis than JavaScript, and thus toolchains such as Emscripten (based on LLVM) or Binaryen can already optimize the program ahead-of-time. This results in fairly well-optimized 
binaries, at least when targeting WebAssembly 1.0, which launched in 2017 . Motivation # Then why do we employ more speculative optimizations for WebAssembly in V8 now? One reason is that WebAssembly has evolved
with the introduction of WasmGC , the WebAssembly Garbage Collection proposal . It better supports compiling \u201cmanaged\u201d languages such as Java , Kotlin , or Dart to WebAssembly. The resulting WasmGC 
bytecode is more high-level than Wasm 1.0, e.g., it supports rich types, such as structs and arrays, subtyping, and operations on such types. The generated machine code for WasmGC thus benefits more from 
speculative optimizations. One particularly important optimization is inlining , that is, replacing a call instruction with the body of the callee function. Not only does this get rid of the administrative 
overhead associated with the call itself (which might be higher than the actual work for very small functions), but it also enables many other, subsequent optimizations to \u201csee through\u201d the function 
call, even if those optimizations are not inter-procedural. No wonder inlining was already recognized in 1971 as one of the most important optimizations in Frances Allen\u2019s seminal \u201cCatalogue of 
Optimizing Transformations\u201d . One complication for inlining are indirect calls , i.e., calls where the callee is only known at runtime and can be one of many potential targets. This is particularly true for
the languages that compile to WasmGC. Consider Java or Kotlin where methods are virtual by default, whereas in C++ one has to opt-in by marking them explicitly. If there is not a single statically known callee, 
inlining is obviously not as straightforward. Speculative Inlining # This is where speculative inlining comes into play. In theory, indirect calls can target many different functions, but in practice, they still
often go to a single target (called a monomorphic function call) or a few select cases (called polymorphic ). We record those targets when executing the unoptimized code, and then inline up to four target 
functions when generating the optimized code. Speculative inlining overview The figure above shows the high-level picture. We start at the top left, with the unoptimized code for function func_a , generated by 
Liftoff , our baseline compiler for WebAssembly. At each call site Liftoff also emits code to update the feedback vector . This metadata array exists once per function, and it contains one entry per call 
instruction in the given function. Each entry records the call targets and counts for this particular call site. The example feedback at the bottom of the figure shows a monomorphic entry for the call_indirect 
in func_a ; here the call target was 1337 times func_b . When a function is hot enough to tier-up to TurboFan , i.e., gets compiled with our optimizing compiler, we come to the second step. TurboFan reads the 
corresponding feedback vector and decides whether and which targets to inline at each call site. Whether it is worthwhile inlining one or multiple callees depends on a variety of heuristics . E.g., large 
functions are never inlined, tiny functions are almost always inlined, and generally there is a maximum inlining budget after which no more inlining into a function happens, as that also has costs in terms of 
compile time and generated machine code size. As in many places in compilers, particularly multi-tier JITs, these trade-offs are quite complex and get tuned over time. In this example, TurboFan decides to inline
func_b into func_a . On the upper right hand side of the figure, we see the result of speculative inlining in the generated optimized code. Instead of an indirect call, the code first checks if the target at 
runtime matches what we have assumed during compilation. If that\u2019s the case, we continue executing the inlined body of the corresponding function. Subsequent optimizations can also transform the inlined 
code further, given the now available surrounding context. E.g., constant propagation and constant folding could specialize the code to this particular call site or GVN could hoist out repeated computations. In 
the case of polymorphic feedback, TurboFan can also emit a series of target checks and inlined bodies, not just one as in this example. Technical Deep Dive # So much for the high-level picture. For readers 
interested in the implementation, we look at some more details and concrete code in this section. In the figure above, the feedback vector is only shown conceptually as an array of entries and only one kind of 
entry is shown. Below, we see that each entry can go through four stages over the course of the execution: Initially, all entries are uninitialized (all call counts are zero), potentially transitioning to 
monomorphic (a single call target was recorded), polymorphic (up to four call targets), and finally megamorphic (more than four targets, where we don\u2019t inline any more and thus don\u2019t need to record 
call counts either). Each entry is actually a pair of objects such that the most common monomorphic case can store both the call count and the target inline in the vector, i.e., without an additional allocation.
For the polymorphic case, feedback is stored in an out-of-line array, as shown below. The builtin that updates a feedback entry (corresponding to update_feedback() in the first figure) is written in Torque . 
(It\u2019s quite easy to read, give it a try!) It first checks for monomorphic or polymorphic \u201chits\u201d, where just the count has to be incremented. This is again because they are the most common cases 
and thus performance-sensitive. The feedback vector and its entries are JavaScript objects (e.g., the call counts are Smis ), so they live on the managed heap. As such, it is part of the V8 sandbox and is 
automatically cleaned up if the corresponding Wasm instance (see below) is no longer reachable. Details of the feedback vector Next, let\u2019s look at the effect of inlining on an actual WebAssembly program. 
The example function below does 200M indirect calls in a loop to a single target inlinee that contains an addition. Obviously, this is a somewhat simplified microbenchmark, but it demonstrates the benefits of 
speculative inlining well. ( func $example ( param $iterations i32 ) ( result i32 ) ( local $sum i32 ) block $ block loop $ loop local .get $iterations i32 . eqz br_if $ block ;; terminate loop local .get 
$iterations ;; update loop counter i32 . const 1 i32 . sub local .set $iterations i32 . const 7 ;; argument for the function call i32 . const 1 ;; table index, refering to $inlinee call_indirect ( param i32 ) ( 
result i32 ) local .get $sum i32 . add local .set $sum br $ loop ;; repeat end end local .get $sum ) ... ( func $inlinee ( param $x i32 ) ( result i32 ) local .get $x i32 . const 37 i32 . add ) For readers not 
familiar with the WebAssembly text format, here is a rough C equivalent of the above program: int inlinee ( int x ) { return x + 37 ; } int ( * func_ptr ) ( int ) = inlinee ; int example ( int iterations ) { int
sum = 0 ; while ( iterations != 0 ) { iterations -- ; sum += func_ptr ( 7 ) ; } return sum ; } The next figure shows excerpts of TurboFan\u2019s intermediate representation for the example function, visualized 
with Turbolizer : Speculative inlining and Wasm deopts are enabled on the right, and disabled on the left. In both versions, we have to check whether the table index argument to the call_indirect instruction is 
in-bounds, as per the WebAssembly semantics (first red box in both cases). Without inlining, we also have to check whether the function at this index has the correct signature before actually calling it (second 
red box on the left). Finally, the first green box on the left is the indirect call, and the second green box is the addition of the result of said call. In the green box on the right, we see that after inlining
and further optimizations, the call is completely gone and the addition in inlinee and the addition in example were constant-folded into a single addition with a constant. Altogether, on this particular 
microbenchmark, inlining, deopts, and subsequent optimizations speed up the program from around 675 ms to 90 ms execution time on an x64 workstation. In this case, the optimized machine code with inlining is 
even smaller than without (968 vs. 1572 bytes), although that certainly need not be. TurboFan IR without and with inlining Finally, we want to briefly explain the Wasm instance check and target check that the 
code with speculative inlining does on the right. Semantically, Wasm functions are closures over a Wasm instance (which \u201cholds\u201d the current state of globals, tables, imports from the host, etc.). 
Correctly inlining functions that belong to a different instance (e.g., which are called via an imported table) would hence require additional compiler machinery as well as solving a few obstacles in our general
handling of generated code. Luckily, most calls are within a single instance anyway, so for the time being we check that the call target\u2019s instance matches the current instance, which lets the compiler make
the simplifying assumption that both instances are the same. If not, we deoptimize in block 8 (due to wrong instance) or block 6 (due to wrong target). This additional Wasm instance check was specifically 
introduced for the new call_indirect inlining. WebAssembly also has another kind of indirect call, call_ref , for which we already added inlining support when launching our WasmGC implementation . The fast path 
for call_ref inlining doesn\u2019t require an explicit instance check, since the WasmFuncRef object that is the call_ref input already includes the instance the function closes over, so comparing the target for 
equality subsumes both checks. With the new call_indirect inlining, V8 now supports inlining Wasm-to-Wasm calls for all types of call instructions: direct call s, call_ref , call_indirect , and their respective 
tail-call variants return_call , return_call_ref , and return_call_indirect . Deoptimization # So far, we have focused on inlining and how that improves the optimized code. But what happens if we cannot stay on 
a fast path, i.e., if one of the assumptions made during optimization turns out to be false at runtime? This is where deopts come into play. The very first figure of this post already shows the high-level idea 
of a deopt: We cannot continue execution in the optimized code because it has made some assumptions that are now invalidated. So instead, we \u201cgo back\u201d to the unoptimized baseline code. Crucially, this 
transition to unoptimized code happens in the middle of executing the current function , i.e., when the optimized code has already performed operations with side-effects (say, called the underlying operating 
system), which we cannot undo, and while it is holding intermediate values in registers and on the stack. So a deopt cannot just jump to the beginning of the unoptimized function, but instead does something much
more interesting: First, we save the current program state . We do this by calling into the runtime from optimized code. The Deoptimizer then serializes the current program state into an internal data structure,
the FrameDescription . This entails reading out CPU registers and inspecting the stack frame of the function to be deoptimized. Next, we transform this state such that it fits the unoptimized code , that is, 
placing values in the correct registers and stack slots that the Liftoff-generated code expects at the deoptimization point. E.g., a value that TurboFan code has put in a register may now end up in a stack slot.
The stack layout and expectations for the baseline code (its \u201ccalling convention\u201d for the deopt point, if you will) are read out of metadata generated by Liftoff during compilation. And finally, we 
replace the optimized stack frame with the corresponding unoptimized stack frame(s) and jump to the middle of the baseline code , corresponding to the point in the optimized code where the deopt was triggered. 
Obviously, this is quite complex machinery, which begs the question: Why do we go through the hassle of it all and not just generate a slow path in optimized code? Let\u2019s compare the intermediate 
representation for the earlier example WebAssembly code, left without and right with deopts. The three red boxes (table bounds / Wasm instance / target check) are fundamentally the same. The difference is in the
code for the slow path. Without deopts, we don\u2019t have the option of returning to baseline code, so the optimized code must handle the full, generic behavior of an indirect call, as shown in the yellow box 
on the left. This unfortunately impedes further optimizations and thus results in slower execution. TurboFan IR without and with deopts There are two reasons why the non-deopt slow path impedes optimizations. 
First, it contains more operations, in particular a call, which we have to assume goes anywhere and has arbitrary side-effects. Second, note how the Deoptimize operation in block 8 on the right has no successor 
in the control-flow graph, whereas the yellow slow path on the left has a successor. In particular for loops, the deoptimization node/block will not produce data-flow facts (in the sense of data-flow analyses 
and optimizations, such as live variables , load elimination, or escape analysis ) that propagate to the next iteration of the loop. In essence, the deopt point \u201cjust\u201d terminates the execution of the 
function, without much effect on the surrounding code, which can be utilized nicely by subsequent optimizations. Finally, this also explains why the combination of speculative optimizations (e.g., inlining) and 
deoptimization is so useful: The first adds a fast path based on speculative assumptions, and deopts allow the compiler to not worry much about the cases where the assumptions turn out to be false. Concretely, 
for the earlier microbenchmark with 200M indirect calls, performing just speculative inlining without deopts speeds up the program \u201conly\u201d to about 180 ms, compared to 90 ms with both inlining and 
deopts (and 675 ms without either). Technical Deep Dive # For the interested reader, we now again look at a concrete example and technical details, this time for when we deopt. Let's assume we execute the 
optimized code from above but the function stored at table index 1 has changed in the meantime. The table bounds check and Wasm instance check will pass, but the inlined target will be different from the one in 
the table, so we need to deoptimize the program in its current state. For that the code contains so-called deoptimization exits . The target check conditionally jumps to such an exit, which itself is a call to 
the DeoptimizationEntry builtin. The builtin first saves all register values by spilling them to the stack |1] . Then it allocates the C++ Deoptimizer object and its FrameDescription input object. The builtin 
copies the spilled registers and all other stack slots from the optimized frame into the FrameDescription on the heap, and pops these values from the stack in the process. (Note that the execution is still in 
the builtin while it has already removed its own return address from the stack and started unwinding the calling frame!) Then the builtin computes the output frames. To do that, the deoptimizer loads the 
DeoptimizationData for the optimized frame, extracts the information for the deopt point, and recompiles each inlined function at this call site with Liftoff. Due to nested inlining there can be more than one 
inlined function, and with inlined tail calls the optimized function to which the optimized stack frame nominally belonged might not even be part of the unoptimized stack frames to be constructed. While 
compiling, Liftoff calculates the expected stack frame layout and the deoptimizer transforms the optimized frame description into the desired layouts reported by Liftoff. It returns to the builtin which then 
reads these output FrameDescription objects and pushes their values onto the stack. Finally the builtin fills in the registers from the top output FrameDescription . For the example above, our internal tracing 
with --trace-deopt-verbose shows the following: |bailout (kind: deopt-eager, reason: wrong call target, type: Wasm): begin. deoptimizing example, function index 2, bytecode offset 134, deopt exit 0, FP to SP 
delta 32, pc 0x14886e50cbb4] reading input for Liftoff frame => bailout_id=134, height=4, function_id=2 ; inputs: 0: 4 ; rdx (int32) 1: 0 ; rbx (int32) 2: (wasm int32 literal 7) 3: (wasm int32 literal 1) Liftoff
stack & register state for function index 2, frame size 48, total frame size 64 0: i32:rax 1: i32:s0x28 2: i32:c7 3: i32:c1 |bailout end. took 0.082 ms] First we can see that a deopt is triggered because of a 
wrong call target in Wasm for the function example . The trace then shows information about the input (optimized) frame, which has four values: The iterations parameter of the function (value 4 , stored in 
register rdx ), the sum local ( 0 stored in rbx ), the literal 7 and the literal 1, which are the two arguments to the call_indirect instruction. In this simple example, there is only one unoptimized output 
frame, so there is a 1:1 mapping between the two frames: The iterations value has to be stored in register rax while the sum value needs to end up in the stack slot s0x28 . The two constants are also recognized 
by Liftoff as constants and don\u2019t need to be transferred into stack slots or registers. |2] After these transformations have been done the builtin \u201creturns\u201d to the inner-most unoptimized frame 
which calls a final builtin to clean-up the Deoptimizer object and perform any needed allocations on the managed heap. |3] Finally, execution continues in the unoptimized code, in this case executing the 
call_indirect , which will also directly record the new call target in its feedback vector, so that any later tier-up is aware of this new target. Results # Besides the technical description and examples, we 
also want to demonstrate the usefulness of call_indirect inlining and Wasm deopt support with some measurements. |4] We first look at a collection of Dart microbenchmarks in the figure below. It compares three 
configurations against each other: All given numbers are speedups relative to V8 and Chrome\u2019s behavior before call_indirect inlining and Wasm deopts (i.e., a speedup of 2x means the runtime is half of that 
of the baseline). The blue bars show the configuration with call_indirect inlining enabled but no Wasm deopts, i.e., where the optimized code contains a generic slow path. On several of these microbenchmarks 
this already yields (sometimes substantial) speedups. |5] On average across all items, call_indirect inlining speeds up execution by 1.19x compared to the baseline without. Finally, the red bars show the 
configuration we actually ship, where both Wasm deopts and call_indirect inlining are enabled. With an average speedup of 1.59x over the baseline, this shows that in particular the combination of speculative 
optimizations and deoptimization support is highly beneficial. Naturally, microbenchmarks isolate and emphasize the effects of an optimization quite a bit. This is useful during development or to get a strong 
signal with noisy measurements. However, more realistic are results on larger applications and benchmarks, as shown in the following figure. To the very left, we see a 2% speedup in terms of runtime for 
richards-wasm , a workload from the JetStream benchmark suite . Next, we see a 1% speedup for a Wasm build of the widely-used SQLite 3 database, and 8% speedup for Dart Flute , a WasmGC benchmark that emulates a
UI workload similar to Flutter . The final two results are from an internal benchmark for the Google Sheets calc engine , which is powered by WasmGC, with speedups due to deopts of up to 7% (only deopts matter 
here as this last application only uses call_ref s for runtime dispatch, i.e., it has no call_indirect s). Conclusion and Outlook # This concludes our post about two new optimizations in the V8 engine for 
WebAssembly. To summarize, we have seen: How speculative inlining can inline functions even in the presence of indirect calls, what feedback is and how it is used and updated, what to do when assumptions made 
during optimizations are invalid at runtime, how a deoptimization can exit optimized code and enters baseline code in the middle of executing a function, and finally how this significantly improves the execution
of real-world workloads. In the future, we plan on adding more speculative optimizations based on deopt support for WebAssembly, e.g. bounds-check elimination or more extensive load elimination for WasmGC 
objects. And also in terms of inlining, there is more to be done: While we now have Wasm-into-Wasm inlining for all kinds of call instructions, we can still extend inlining across the language boundary, e.g., 
for JavaScript-to-Wasm calls. Check back on our V8 blog for exciting updates in the future! Footnotes # Due to the SIMD extension for Wasm this also includes all 128-bit vector registers used by TurboFan. 
\u21a9\ufe0e In other cases values might be constant in the optimized version after constant folding but need to be materialized into stack slots or registers for Liftoff, so the deoptimization data needs to 
store these constant values. \u21a9\ufe0e During the deoptimization itself we cannot allocate on the heap since allocations can trigger the garbage collector (GC) and the stack is not in a state that the GC can 
inspect. (The GC needs to visit all heap-references on the stack and potentially update them when moving an object.) \u21a9\ufe0e The measurements were taken on an x64 workstation; the figures show the median of
N=21 repetitions. \u21a9\ufe0e For the three Matrix4Benchmark items that slightly regress, enabling call_indirect inlining causes our heuristic to prefer 16 indirect call sites for inlining over other direct 
call sites. This leads to exhausting the inlining budget (i.e., stop inlining because the resulting code grows too large), so that fewer of the direct calls are inlined compared with before. In this particular 
case, the heuristic doesn\u2019t predict perfectly how beneficial one inlining decision is over the other and leads to a suboptimal result. Improving this heuristics is interesting future work. \u21a9\ufe0e 
Posted by Daniel Lehmann and Matthias Liedtke. Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style 
license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/lazy-unlinking.txt",
    "similarity": 0.423,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/lazy-unlinking.txt\n\nAn internship on laziness: lazy unlinking of deoptimized functions \u00b7 V8 V8 Show navigation Home Blog Docs 
Tools JS/Wasm features Research An internship on laziness: lazy unlinking of deoptimized functions Published 04 October 2017 \u00b7 Tagged with memory internals Roughly three months ago, I joined the V8 team 
(Google Munich) as an intern and since then I\u2019ve been working on the VM\u2019s Deoptimizer \u2014 something completely new to me which proved to be an interesting and challenging project. The first part of 
my internship focused on improving the VM security-wise . The second part focused on performance improvements. Namely, on the removal of a data-structure used for the unlinking of previously deoptimized 
functions, which was a performance bottleneck during garbage collection. This blog post describes this second part of my internship. I\u2019ll explain how V8 used to unlink deoptimized functions, how we changed 
this, and what performance improvements were obtained. Let\u2019s (very) briefly recap the V8 pipeline for a JavaScript function: V8\u2019s interpreter, Ignition, collects profiling information about that 
function while interpreting it. Once the function becomes hot, this information is passed to V8\u2019s compiler, TurboFan, which generates optimized machine code. When the profiling information is no longer 
valid \u2014 for example because one of the profiled objects gets a different type during runtime \u2014 the optimized machine code might become invalid. In that case, V8 needs to deoptimize it. An overview of 
V8, as seen in JavaScript Start-up Performance Upon optimization, TurboFan generates a code object, i.e. the optimized machine code, for the function under optimization. When this function is invoked the next 
time, V8 follows the link to optimized code for that function and executes it. Upon deoptimization of this function, we need to unlink the code object in order to make sure that it won\u2019t be executed again. 
How does that happen? For example, in the following code, the function f1 will be invoked many times (always passing an integer as argument). TurboFan then generates machine code for that specific case. function
g ( ) { return ( i ) => i ; } // Create a closure. const f1 = g ( ) ; // Optimize f1. for ( var i = 0 ; i < 1000 ; i ++ ) f1 ( 0 ) ; Each function also has a trampoline to the interpreter \u2014 more details in 
these slides \u2014 and will keep a pointer to this trampoline in its SharedFunctionInfo (SFI). This trampoline will be used whenever V8 needs to go back to unoptimized code. Thus, upon deoptimization, triggered
by passing an argument of a different type, for example, the Deoptimizer can simply set the code field of the JavaScript function to this trampoline. An overview of V8, as seen in JavaScript Start-up Performance
Although this seems simple, it forces V8 to keep weak lists of optimized JavaScript functions. This is because it is possible to have different functions pointing to the same optimized code object. We can extend
our example as follows, and the functions f1 and f2 both point to the same optimized code. const f2 = g ( ) ; f2 ( 0 ) ; If the function f1 is deoptimized (for example by invoking it with an object of different 
type {x: 0} ) we need to make sure that the invalidated code will not be executed again by invoking f2 . Thus, upon deoptimization, V8 used to iterate over all the optimized JavaScript functions, and would 
unlink those that pointed to the code object being deoptimized. This iteration in applications with many optimized JavaScript functions became a performance bottleneck. Moreover, other than slowing down 
deoptimization, V8 used to iterate over these lists upon stop-the-world cycles of garbage collection, making it even worse. In order to have an idea of the impact of such data-structure in the performance of V8,
we wrote a micro-benchmark that stresses its usage, by triggering many scavenge cycles after creating many JavaScript functions. function g ( ) { return ( i ) => i + 1 ; } // Create an initial closure and 
optimize. var f = g ( ) ; f ( 0 ) ; f ( 0 ) ; % OptimizeFunctionOnNextCall ( f ) ; f ( 0 ) ; // Create 2M closures; those will get the previously optimized code. var a = | ] ; for ( var i = 0 ; i < 2000000 ; i 
++ ) { var h = g ( ) ; h ( ) ; a . push ( h ) ; } // Now cause scavenges; all of them are slow. for ( var i = 0 ; i < 1000 ; i ++ ) { new Array ( 50000 ) ; } When running this benchmark, we could observe that V8
spent around 98% of its execution time on garbage collection. We then removed this data structure, and instead used an approach for lazy unlinking , and this was what we observed on x64: Although this is just a 
micro-benchmark that creates many JavaScript functions and triggers many garbage collection cycles, it gives us an idea of the overhead introduced by this data structure. Other more realistic applications where 
we saw some overhead, and which motivated this work, were the router benchmark implemented in Node.js and ARES-6 benchmark suite . Lazy unlinking # Rather than unlinking optimized code from JavaScript functions 
upon deoptimization, V8 postpones it for the next invocation of such functions. When such functions are invoked, V8 checks whether they have been deoptimized, unlinks them and then continues with their lazy 
compilation. If these functions are never invoked again, then they will never be unlinked and the deoptimized code objects will not be collected. However, given that during deoptimization, we invalidate all the 
embedded fields of the code object, we only keep that code object alive. The commit that removed this list of optimized JavaScript functions required changes in several parts of the VM, but the basic idea is as 
follows. When assembling the optimized code object, we check if this is the code of a JavaScript function. If so, in its prologue, we assemble machine code to bail out if the code object has been deoptimized. 
Upon deoptimization we don\u2019t modify the deoptimized code \u2014 code patching is gone. Thus, its bit marked_for_deoptimization is still set when invoking the function again. TurboFan generates code to check
it, and if it is set, then V8 jumps to a new builtin, CompileLazyDeoptimizedCode , that unlinks the deoptimized code from the JavaScript function and then continues with lazy compilation. In more detail, the 
first step is to generate instructions that load the address of the code being currently assembled. We can do that in x64, with the following code: Label current ; // Load effective address of current 
instruction into rcx. __ leaq ( rcx , Operand ( & current ) ) ; __ bind ( & current ) ; After that we need to obtain where in the code object the marked_for_deoptimization bit lives. int pc = __ pc_offset ( ) ; 
int offset = Code :: kKindSpecificFlags1Offset - ( Code :: kHeaderSize + pc ) ; We can then test the bit and if it is set, we jump to the CompileLazyDeoptimizedCode built in. // Test if the bit is set, that is, 
if the code is marked for deoptimization. __ testl ( Operand ( rcx , offset ) , Immediate ( 1 << Code :: kMarkedForDeoptimizationBit ) ) ; // Jump to builtin if it is. __ j ( not_zero , /* handle to builtin code
here */ , RelocInfo :: CODE_TARGET ) ; On the side of this CompileLazyDeoptimizedCode builtin, all that\u2019s left to do is to unlink the code field from the JavaScript function and set it to the trampoline to 
the Interpreter entry. So, considering that the address of the JavaScript function is in the register rdi , we can obtain the pointer to the SharedFunctionInfo with: // Field read to obtain the 
SharedFunctionInfo. __ movq ( rcx , FieldOperand ( rdi , JSFunction :: kSharedFunctionInfoOffset ) ) ; \u2026and similarly the trampoline with: // Field read to obtain the code object. __ movq ( rcx , 
FieldOperand ( rcx , SharedFunctionInfo :: kCodeOffset ) ) ; Then we can use it to update the function slot for the code pointer: // Update the code field of the function with the trampoline. __ movq ( 
FieldOperand ( rdi , JSFunction :: kCodeOffset ) , rcx ) ; // Write barrier to protect the field. __ RecordWriteField ( rdi , JSFunction :: kCodeOffset , rcx , r15 , kDontSaveFPRegs , OMIT_REMEMBERED_SET , 
OMIT_SMI_CHECK ) ; This produces the same result as before. However, rather than taking care of the unlinking in the Deoptimizer, we need to worry about it during code generation. Hence the handwritten assembly.
The above is how it works in the x64 architecture . We have implemented it for ia32 , arm , arm64 , mips , and mips64 as well. This new technique is already integrated in V8 and, as we\u2019ll discuss later, 
allows for performance improvements. However, it comes with a minor disadvantage: Before, V8 would consider unlinking only upon deoptimization. Now, it has to do so in the activation of all optimized functions. 
Moreover, the approach to check the marked_for_deoptimization bit is not as efficient as it could be, given that we need to do some work to obtain the address of the code object. Note that this happens when 
entering every optimized function. A possible solution for this issue is to keep in a code object a pointer to itself. Rather than doing work to find the address of the code object whenever the function is 
invoked, V8 would do it only once, after its construction. Results # We now look at the performance gains and regressions obtained with this project. General improvements on x64 # The following plot shows us 
some improvements and regressions, relative to the previous commit. Note that the higher, the better. The promises benchmarks are the ones where we see greater improvements, observing almost 33% gain for the 
bluebird-parallel benchmark, and 22.40% for wikipedia . We also observed a few regressions in some benchmarks. This is related to the issue explained above, on checking whether the code is marked for 
deoptimization. We also see improvements in the ARES-6 benchmark suite. Note that in this chart too, the higher the better. These programs used to spend considerable amount of time in GC-related activities. With
lazy unlinking we improve performance by 1.9% overall. The most notable case is the Air steadyState where we get an improvement of around 5.36%. AreWeFastYet results # The performance results for the Octane and 
ARES-6 benchmark suites also showed up on the AreWeFastYet tracker. We looked at these performance results on September 5th, 2017, using the provided default machine (macOS 10.10 64-bit, Mac Pro, shell). 
Cross-browser results on Octane as seen on AreWeFastYet Cross-browser results on ARES-6 as seen on AreWeFastYet Impact on Node.js # We can also see performance improvements in the router-benchmark . The 
following two plots show the number of operations per second of each tested router. Thus the higher the better. We have performed two kinds of experiments with this benchmark suite. Firstly, we ran each test in 
isolation, so that we could see the performance improvement, independently from the remaining tests. Secondly, we ran all tests at once, without switching of the VM, thus simulating an environment where each 
test is integrated with other functionalities. For the first experiment, we saw that the router and express tests perform about twice as many operations than before, in the same amount of time. For the second 
experiment, we saw even greater improvement. In some of the cases, such as routr , server-router and router , the benchmark performs approximately 3.80\u00d7, 3\u00d7 and 2\u00d7 more operations, respectively. 
This happens because V8 accumulates more optimized JavaScript functions, test after test. Thus, whenever executing a given test, if a garbage collection cycle is triggered, V8 has to visit the optimized 
functions from the current test and from the previous ones. Further optimization # Now that V8 does not keep the linked-list of JavaScript functions in the context, we can remove the field next from the 
JSFunction class. Although this is a simple modification, it allows us to save the size of a pointer per function, which represent significant savings in several web pages: Benchmark Kind Memory savings 
(absolute) Memory savings (relative) facebook.com Average effective size 170 KB 3.70% twitter.com Average size of allocated objects 284 KB 1.20% cnn.com Average size of allocated objects 788 KB 1.53% youtube.com
Average size of allocated objects 129 KB 0.79% Acknowledgments # Throughout my internship, I had lots of help from several people, who were always available to answer my many questions. Thus I would like to 
thank the following people: Benedikt Meurer, Jaroslav Sevcik, and Michael Starzinger for discussions on how the compiler and the deoptimizer work, Ulan Degenbaev for helping with the garbage collector whenever I
broke it, and Mathias Bynens, Peter Marshall, Camillo Bruni, and Maya Armyanova for proofreading this article. Finally, this article is my last contribution as a Google intern and I would like to take the 
opportunity to thank everyone in the V8 team, and especially my host, Benedikt Meurer, for hosting me and for giving me the opportunity to work on such an interesting project \u2014 I definitely learned a lot 
and enjoyed my time at Google! Posted by Juliana Franco ( @jupvfranco ), Laziness Expert. Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples
from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  }
]
[Step 12: Duration 11.97 seconds| Input tokens: 149,845 | Output tokens: 4,783]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 13 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'CompileLazyDeoptimizedCode builtin CompileLazyDeoptimizedCode file src/CompileLazyDeoptimizedCode CompileLazyDeoptimizedCode builtin           │
│ deoptimizer Code::kMarkedForDeoptimizationBit', 'top_k': 5, 'topic_filter': 'v8'}                                                                                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/webassembly-opcode.txt",
    "similarity": 0.461,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/webassembly-opcode.txt\n\nWebAssembly - adding a new opcode \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
WebAssembly - adding a new opcode WebAssembly (Wasm) is a binary instruction format for a stack-based virtual machine. This tutorial walks the reader through implementing a new WebAssembly instruction in V8. 
WebAssembly is implemented in V8 in three parts: the interpreter the baseline compiler (Liftoff) the optimizing compiler (TurboFan) The rest of this document focuses on the TurboFan pipeline, walking through how
to add a new Wasm instruction and implement it in TurboFan. At a high level, Wasm instructions are compiled into a TurboFan graph, and we rely on the TurboFan pipeline to compile the graph into (ultimately) 
machine code. For more on TurboFan, check out the V8 docs . Opcodes/Instructions # Let\u2019s define a new instruction that adds 1 to an int32 (on the top of the stack). Note: A list of instructions supported by
all Wasm implementations can be found in the spec . All Wasm instructions are defined in src/wasm/wasm-opcodes.h . The instructions are grouped roughly by what they do, e.g. control, memory, SIMD, atomic, etc. 
Let\u2019s add our new instruction, I32Add1 , to the FOREACH_SIMPLE_OPCODE section: diff --git a/src/wasm/wasm-opcodes.h b/src/wasm/wasm-opcodes.h index 6970c667e7..867cbf451a 100644 --- 
a/src/wasm/wasm-opcodes.h +++ b/src/wasm/wasm-opcodes.h @@ -96,6 +96,7 @@ bool IsJSCompatibleSignature(const FunctionSig* sig, bool hasBigIntFeature); // Expressions with signatures. #define 
FOREACH_SIMPLE_OPCODE(V) \\ + V(I32Add1, 0xee, i_i) \\ V(I32Eqz, 0x45, i_i) \\ V(I32Eq, 0x46, i_ii) \\ V(I32Ne, 0x47, i_ii) \\ WebAssembly is a binary format, so 0xee specifies the encoding of this instruction. 
In this tutorial we chose 0xee as it is currently unused. Note: Actually adding an instruction to the spec involves work beyond what is described here. We can run a simple unit test for opcodes with: $ 
tools/dev/gm.py x64.debug unittests/WasmOpcodesTest* ... |==========] Running 1 test from 1 test suite. |----------] Global test environment set-up. |----------] 1 test from WasmOpcodesTest | RUN ] 
WasmOpcodesTest.EveryOpcodeHasAName ../../test/unittests/wasm/wasm-opcodes-unittest.cc:27: Failure Value of: false Actual: false Expected: true WasmOpcodes::OpcodeName(kExprI32Add1) == \"unknown\"; plazz halp in
src/wasm/wasm-opcodes.cc | FAILED ] WasmOpcodesTest.EveryOpcodeHasAName This error indicates that we don\u2019t have a name for our new instruction. Adding a name for the new opcode can be done in 
src/wasm/wasm-opcodes.cc : diff --git a/src/wasm/wasm-opcodes.cc b/src/wasm/wasm-opcodes.cc index 5ed664441d..2d4e9554fe 100644 --- a/src/wasm/wasm-opcodes.cc +++ b/src/wasm/wasm-opcodes.cc @@ -75,6 +75,7 @@ 
const char* WasmOpcodes::OpcodeName(WasmOpcode opcode) { // clang-format off // Standard opcodes + CASE_I32_OP(Add1, \"add1\") CASE_INT_OP(Eqz, \"eqz\") CASE_ALL_OP(Eq, \"eq\") CASE_I64x2_OP(Eq, \"eq\") By 
adding our new instruction in FOREACH_SIMPLE_OPCODE , we are skipping a fair amount of work that is done in src/wasm/function-body-decoder-impl.h , which decodes Wasm opcodes and calls into the TurboFan graph 
generator. Thus, depending on what your opcode does, you might have more work to do. We skip this in the interest of brevity. Writing a test for the new opcode # Wasm tests can be found in test/cctest/wasm/ . 
Let\u2019s take a look at test/cctest/wasm/test-run-wasm.cc , where many \u201csimple\u201d opcodes are tested. There are many examples in this file that we can follow. The general setup is: create a WasmRunner 
set up globals to hold result (optional) set up locals as parameters to instruction (optional) build the wasm module run it and compare with an expected output Here\u2019s a simple test for our new opcode: diff 
--git a/test/cctest/wasm/test-run-wasm.cc b/test/cctest/wasm/test-run-wasm.cc index 26df61ceb8..b1ee6edd71 100644 --- a/test/cctest/wasm/test-run-wasm.cc +++ b/test/cctest/wasm/test-run-wasm.cc @@ -28,6 +28,15 
@@ namespace test_run_wasm { #define RET(x) x, kExprReturn #define RET_I8(x) WASM_I32V_2(x), kExprReturn + #define WASM_I32_ADD1(x) x, kExprI32Add1 + + WASM_EXEC_TEST(Int32Add1) { + WasmRunner<int32_t> 
r(execution_tier); + // 10 + 1 + BUILD(r, WASM_I32_ADD1(WASM_I32V_1(10))); + CHECK_EQ(11, r.Call()); + } + WASM_EXEC_TEST(Int32Const) { WasmRunner<int32_t> r(execution_tier); const int32_t kExpectedValue = 
0x11223344; Run the test: $ tools/dev/gm.py x64.debug 'cctest/test-run-wasm-simd/RunWasmTurbofan_I32Add1' ... === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in 
../../src/compiler/wasm-compiler.cc, line 988 # Unsupported opcode 0xee:i32.add1 Tip: Finding the test name can be tricky, since the test definition is behind a macro. Use Code Search to click around to discover
the macro definitions. This error indicates that the compiler does not know of our new instruction. That will change in the next section. Compiling Wasm into TurboFan # In the introduction, we mentioned that 
Wasm instructions are compiled into a TurboFan graph. wasm-compiler.cc is where this happens. Let\u2019s take a look at an example opcode, I32Eqz : switch ( opcode ) { case wasm :: kExprI32Eqz : op = m -> 
Word32Equal ( ) ; return graph ( ) -> NewNode ( op , input , mcgraph ( ) -> Int32Constant ( 0 ) ) ; This switches on the Wasm opcode wasm::kExprI32Eqz , and builds a TurboFan graph consisting of the operation 
Word32Equal with the inputs input , which is the argument to the Wasm instruction, and a constant 0 . The Word32Equal operator is provided by the underlying V8 abstract machine, which is 
architecture-independent. Later in the pipeline, this abstract machine operator will be translated into architecture-dependent assembly. For our new opcode, I32Add1 , we need a graph that adds a constant 1 to 
the input, so we can resuse an existing machine operator, Int32Add , passing it the input, and a constant 1: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index 
f666bbb7c1..399293c03b 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 @@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; 
MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return graph()->NewNode(m->Int32Add(), input, mcgraph()->Int32Constant(1)); case wasm::kExprI32Eqz: op = 
m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); This is enough to get the test passing. However, not all instructions have an existing TurboFan machine operator. In that case 
we have to add this new operator to the machine. Let\u2019s try that. TurboFan machine operators # We want to add the knowledge of Int32Add1 to the TurboFan machine. So let\u2019s pretend that it exists and use 
it first: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index f666bbb7c1..1d93601584 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 
@@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return 
graph()->NewNode(m->Int32Add1(), input); case wasm::kExprI32Eqz: op = m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); Trying to run the same test leads to a compilation failure
that hints at where to make changes: ../../src/compiler/wasm-compiler.cc:717:34: error: no member named 'Int32Add1' in 'v8::internal::compiler::MachineOperatorBuilder'; did you mean 'Int32Add'? return 
graph()->NewNode(m->Int32Add1(), input); ^~~~~~~~~ Int32Add There are a couple of places that needs to be modified to add an operator: src/compiler/machine-operator.cc header src/compiler/machine-operator.h list
of opcodes that the machine understands src/compiler/opcodes.h verifier src/compiler/verifier.cc diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc index 16e838c2aa..fdd6d951f0 
100644 --- a/src/compiler/machine-operator.cc +++ b/src/compiler/machine-operator.cc @@ -136,6 +136,7 @@ MachineType AtomicOpType(Operator const* op) { #define MACHINE_PURE_OP_LIST(V) \\ 
PURE_BINARY_OP_LIST_32(V) \\ PURE_BINARY_OP_LIST_64(V) \\ + V(Int32Add1, Operator::kNoProperties, 1, 0, 1) \\ V(Word32Clz, Operator::kNoProperties, 1, 0, 1) \\ V(Word64Clz, Operator::kNoProperties, 1, 0, 1) \\ 
V(Word32ReverseBytes, Operator::kNoProperties, 1, 0, 1) \\ diff --git a/src/compiler/machine-operator.h b/src/compiler/machine-operator.h index a2b9fce0ee..f95e75a445 100644 --- a/src/compiler/machine-operator.h
+++ b/src/compiler/machine-operator.h @@ -265,6 +265,8 @@ class V8_EXPORT_PRIVATE MachineOperatorBuilder final const Operator* Word32PairShr(); const Operator* Word32PairSar(); + const Operator* Int32Add1(); + 
const Operator* Int32Add(); const Operator* Int32AddWithOverflow(); const Operator* Int32Sub(); diff --git a/src/compiler/opcodes.h b/src/compiler/opcodes.h index ce24a0bd3f..2c8c5ebaca 100644 --- 
a/src/compiler/opcodes.h +++ b/src/compiler/opcodes.h @@ -506,6 +506,7 @@ V(Float64LessThanOrEqual) #define MACHINE_UNOP_32_LIST(V) \\ + V(Int32Add1) \\ V(Word32Clz) \\ V(Word32Ctz) \\ V(Int32AbsWithOverflow) \\
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc index 461aef0023..95251934ce 100644 --- a/src/compiler/verifier.cc +++ b/src/compiler/verifier.cc @@ -1861,6 +1861,7 @@ void 
Verifier::Visitor::Check(Node* node, const AllNodes& all) { case IrOpcode::kSignExtendWord16ToInt64: case IrOpcode::kSignExtendWord32ToInt64: case IrOpcode::kStaticAssert: + case IrOpcode::kInt32Add1: #define 
SIMD_MACHINE_OP_CASE(Name) case IrOpcode::k##Name: MACHINE_SIMD_OP_LIST(SIMD_MACHINE_OP_CASE) Running the test again now gives us a different failure: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # 
Fatal error in ../../src/compiler/backend/instruction-selector.cc, line 2072 # Unexpected operator #289:Int32Add1 @ node #7 Instruction selection # So far we have been working at the TurboFan level, dealing with
(a sea of) nodes in the TurboFan graph. However, at the assembly level, we have instructions and operands. Instruction selection is the process of translating this graph to instructions and operands. The last 
test error indicated that we need something in src/compiler/backend/instruction-selector.cc . This is a big file with a giant switch statement over all the machine opcodes. It calls into architecture specific 
instruction selection, using the visitor pattern to emit instructions for each type of node. Since we added a new TurboFan machine opcode, we need to add it here as well: diff --git 
a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc index 3152b2d41e..7375085649 100644 --- a/src/compiler/backend/instruction-selector.cc +++ 
b/src/compiler/backend/instruction-selector.cc @@ -2067,6 +2067,8 @@ void InstructionSelector::VisitNode(Node* node) { return MarkAsWord32(node), VisitS1x16AnyTrue(node); case IrOpcode::kS1x16AllTrue: return 
MarkAsWord32(node), VisitS1x16AllTrue(node); + case IrOpcode::kInt32Add1: + return MarkAsWord32(node), VisitInt32Add1(node); default: FATAL(\"Unexpected operator #%d:%s @ node #%d\", node->opcode(), 
node->op()->mnemonic(), node->id()); Instruction selection is architecture dependent, so we have to add it to the architecture specific instruction selector files too. For this codelab we only focus on the x64 
architecture, so src/compiler/backend/x64/instruction-selector-x64.cc needs to be modified: diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc 
b/src/compiler/backend/x64/instruction-selector-x64.cc index 2324e119a6..4b55671243 100644 --- a/src/compiler/backend/x64/instruction-selector-x64.cc +++ b/src/compiler/backend/x64/instruction-selector-x64.cc @@
-841,6 +841,11 @@ void InstructionSelector::VisitWord32ReverseBytes(Node* node) { Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); } + void InstructionSelector::VisitInt32Add1(Node*
node) { + X64OperandGenerator g(this); + Emit(kX64Int32Add1, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); + } + And we also need to add this new x64-specific opcode, kX64Int32Add1 to 
src/compiler/backend/x64/instruction-codes-x64.h : diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h index 9b8be0e0b5..7f5faeb87b 100644 --- 
a/src/compiler/backend/x64/instruction-codes-x64.h +++ b/src/compiler/backend/x64/instruction-codes-x64.h @@ -12,6 +12,7 @@ namespace compiler { // X64-specific opcodes that specify which assembly sequence to 
emit. // Most opcodes specify a single instruction. #define TARGET_ARCH_OPCODE_LIST(V) \\ + V(X64Int32Add1) \\ V(X64Add) \\ V(X64Add32) \\ V(X64And) \\ Instruction scheduling and code generation # Running our 
test, we see new compilation errors: ../../src/compiler/backend/x64/instruction-scheduler-x64.cc:15:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(instr->arch_opcode()) { ^ 1 error generated. ... ../../src/compiler/backend/x64/code-generator-x64.cc:733:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(arch_opcode) { ^ 1 error generated. Instruction scheduling takes care of dependencies that instructions may have to allow for more optimization (e.g. instruction reordering). Our new opcode has no data 
dependency, so we can add it simply to: src/compiler/backend/x64/instruction-scheduler-x64.cc : diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc 
b/src/compiler/backend/x64/instruction-scheduler-x64.cc index 79eda7e78d..3667a84577 100644 --- a/src/compiler/backend/x64/instruction-scheduler-x64.cc +++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -13,6 +13,7 @@ bool InstructionScheduler::SchedulerSupported() { return true; } int InstructionScheduler::GetTargetInstructionFlags( const Instruction* instr) const { switch (instr->arch_opcode()) { + case 
kX64Int32Add1: case kX64Add: case kX64Add32: case kX64And: Code generation is where we translate our architecture specific opcodes into assembly. Let\u2019s add a clause to 
src/compiler/backend/x64/code-generator-x64.cc : diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc index 61c3a45a16..9c37ed7464 100644 --- 
a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -731,6 +731,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction( InstructionCode 
opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { 
Handle<Code> code = i.InputCode(0); For now we leave our code generation empty, and we can run the test to make sure everything compiles: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in
../../test/cctest/wasm/test-run-wasm.cc, line 37 # Check failed: 11 == r.Call() (11 vs. 10). This failure is expected, since our new instruction is not implemented yet \u2014 it is essentially a no-op, so our 
actual value was unchanged ( 10 ). To implement our opcode, we can use the add assembly instruction: diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc 
index 6c828d6bc4..260c8619f2 100644 --- a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -744,6 +744,11 @@ CodeGenerator::CodeGenResult 
CodeGenerator::AssembleArchInstruction( InstructionCode opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + 
DCHECK_EQ(i.OutputRegister(), i.InputRegister(0)); + __ addl(i.InputRegister(0), Immediate(1)); + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { Handle<Code> code = i.InputCode(0); And
this makes the test pass: Luckily for us addl is already implemented. If our new opcode required writing a new assembly instruction implementation, we would add it to src/compiler/backend/x64/assembler-x64.cc , 
where the assembly instruction is encoded into bytes and emitted. Tip: To inspect the generated code, we can pass --print-code to cctest . Other architectures # In this codelab we only implemented this new 
instruction for x64. The steps required for other architectures are similar: add TurboFan machine operators, use the platform-dependent files for instruction selection, scheduling, code generation, assembler. 
Tip: if we compile what we have done so far on another target, e.g. arm64, we are likely to get errors in linking. To resolve those errors, add UNIMPLEMENTED() stubs. Branding Terms Privacy Twitter Edit this 
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.46,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-decompile.txt",
    "similarity": 0.448,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-decompile.txt\n\nWhat\u2019s in that .wasm? Introducing: wasm-decompile \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm 
features Research What\u2019s in that .wasm ? Introducing: wasm-decompile Published 27 April 2020 \u00b7 Tagged with WebAssembly tooling We have a growing number of compilers and other tools that generate or 
manipulate .wasm files, and sometimes you might want to have a look inside. Maybe you\u2019re a developer of such a tool, or more directly, you\u2019re a programmer targeting Wasm, and wondering what the 
generated code looks like, for performance or other reasons. Problem is, Wasm is rather low-level, much like actual assembly code. In particular, unlike, say, the JVM, all data structures have been compiled down
to load/store operations, rather than conveniently named classes and fields. Compilers like LLVM can do an impressive amount of transformations that make the generated code look nothing like the code that went 
in. Disassemble or.. decompile? # You could use tools like wasm2wat (part of the WABT toolkit), to transform a .wasm into Wasm\u2019s standard text format, .wat , which is a very faithful but not particularly 
readable representation. For example, a simple C function like a dot product: typedef struct { float x , y , z ; } vec3 ; float dot ( const vec3 * a , const vec3 * b ) { return a -> x * b -> x + a -> y * b -> y 
+ a -> z * b -> z ; } We use clang dot.c -c -target wasm32 -O2 followed by wasm2wat -f dot.o to turn it into this .wat : ( func $dot ( type 0 ) ( param i32 i32 ) ( result f32 ) ( f32 . add ( f32 . add ( f32 . 
mul ( f32 . load ( local .get 0 ) ) ( f32 . load ( local .get 1 ) ) ) ( f32 . mul ( f32 . load offset = 4 ( local .get 0 ) ) ( f32 . load offset = 4 ( local .get 1 ) ) ) ) ( f32 . mul ( f32 . load offset = 8 ( 
local .get 0 ) ) ( f32 . load offset = 8 ( local .get 1 ) ) ) ) ) ) That is a tiny bit of code, but already not great to read for many reasons. Besides the lack of an expression based syntax and general 
verbosity, having to understand data structures as memory loads is not easy. Now imagine looking at the output of a large program, and things will get incomprehensible fast. Instead of wasm2wat , run 
wasm-decompile dot.o , and you get: function dot ( a : { a : float , b : float , c : float } , b : { a : float , b : float , c : float } ) : float { return a . a * b . a + a . b * b . b + a . c * b . c } This 
looks a lot more familiar. Besides an expression based syntax that mimics programming languages you may be familiar with, the decompiler looks at all loads and stores in a function, and tries to infer their 
structure. It then annotates each variable that is used as a pointer with an \"inline\" struct declaration. It does not create named struct declarations since it doesn\u2019t necessarily know which uses of 3 
floats represent the same concept. Decompile to what? # wasm-decompile produces output that tries to look like a \"very average programming language\" while still staying close to the Wasm it represents. Its #1 
goal is readability: help guide readers understand what is in a .wasm with as easy to follow code as possible. Its #2 goal is to still represent Wasm as 1:1 as possible, to not lose its utility as a 
disassembler. Obviously these two goals are not always unifiable. This output is not meant to be an actual programming language and there is currently no way to compile it back into Wasm. Loads and stores # As 
demonstrated above, wasm-decompile looks at all loads and stores over a particular pointer. If they form a continuous set of accesses, it will output one of these \"inline\" struct declarations. If not all 
\"fields\" are accessed, it can\u2019t tell for sure whether this is meant to be a struct, or some other form of unrelated memory access. In that case it falls back to simpler types like float_ptr (if the types 
are the same), or, in the worst case, will output an array access like o|2]:int , which says: o points to int values, and we\u2019re accessing the third one. That last case happens more often than you\u2019d 
think, since Wasm locals function more like registers than variables, so optimized code may share the same pointer for unrelated objects. The decompiler tries to be smart about indexing, and detects patterns 
like (base + (index << 2))|0]:int that result from regular C array indexing operations like base|index] where base points to a 4-byte type. These are very common in code since Wasm has only constant offsets on 
loads and stores. wasm-decompile output transforms them back into base|index]:int . Additionally it knows when absolute addresses refer to the data section. Control flow # Most familiar is Wasm\u2019s if-then 
construct, which translates to a familiar if (cond) { A } else { B } syntax, with the addition that in Wasm it can actually return a value, so it can also represent the ternary cond ? A : B syntax available in 
some languages. The rest of Wasm\u2019s control flow is based on the block and loop blocks, and the br , br_if and br_table jumps. The decompiler stays decently close to these constructs rather than trying to 
infer the while/for/switch constructs they may have come from, since this tends to work better with optimized output. For example, a typical loop in the wasm-decompile output may look like: loop A { // body of 
the loop here. if ( cond ) continue A ; } Here, A is a label that allows multiple of these to be nested. Having an if and continue to control the loop may look slightly foreign compared to a while loop, but it 
corresponds directly to Wasm\u2019s br_if . Blocks are similar, but instead of branching backwards, they branch forwards: block { if ( cond ) break ; // body goes here. } This actually implements an if-then. 
Future versions of the decompiler may translate these into actual if-thens when possible. Wasm\u2019s most surprising control construct is br_table , which implements something like a switch , except using 
nested block s, which tends to be hard to read. The decompiler flattens these to make them slightly easier to follow, for example: br_table | A , B , C , . . D ] ( a ) ; label A : return 0 ; label B : return 1 ;
label C : return 2 ; label D : This is similar to switch on a , with D being the default case. Other fun features # The decompiler: Can pull names from debug or linking information, or generate names itself. 
When using existing names, it has special code to simplify C++ name mangled symbols. Already supports the multi-value proposal, which makes turning things into expressions and statements a bit harder. Additional
variables are used when multiple values are returned. It can even generate names from the contents of data sections. Outputs nice declarations for all Wasm section types, not just code. For example, it tries to 
make data sections readable by outputting them as text when possible. Supports operator precedence (common to most C-style languages) to reduce the () on common expressions. Limitations # Decompiling Wasm is 
fundamentally harder than, say, JVM bytecode. The latter is un-optimized, so relatively faithful to the structure of the original code, and even though names may be missing, refers to unique classes rather than 
just memory locations. In contrast, most .wasm output has been heavily optimized by LLVM and thus has often lost most of its original structure. The output code is very unlike what a programmer would write. That
makes a decompiler for Wasm a bigger challenge to make useful, but that doesn\u2019t mean we shouldn\u2019t try! More # The best way to see more is of course to decompile your own Wasm project! Additionally, a 
more in-depth guide to wasm-decompile is here . Its implementation is in the source files starting with decompiler here (feel free to contribute a PR to make it better!). Some test cases that show further 
examples of differences between .wat and the decompiler are here . Posted by Wouter van Oortmerssen ( @wvo ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise 
noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site 
policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/short-builtin-calls.txt",
    "similarity": 0.442,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/short-builtin-calls.txt\n\nShort builtin calls \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Short builtin 
calls Published 06 May 2021 \u00b7 Tagged with JavaScript In V8 v9.1 we\u2019ve temporarily disabled embedded builtins on desktop. While embedding builtins significantly improves memory usage, we\u2019ve 
realized that function calls between embedded builtins and JIT compiled code can come at a considerable performance penalty. This cost depends on the microarchitecture of the CPU. In this post we\u2019ll explain
why this is happening, what the performance looks like, and what we\u2019re planning to do to resolve this long-term. Code allocation # Machine code generated by V8\u2019s just-in-time (JIT) compilers is 
allocated dynamically on memory pages owned by the VM. V8 allocates memory pages within a contiguous address space region, which itself either lies somewhere randomly in memory (for address space layout 
randomization reasons), or somewhere inside of the 4-GiB virtual memory cage we allocate for pointer compression . V8 JIT code very commonly calls into builtins. Builtins are essentially snippets of machine code
that are shipped as part of the VM. There are builtins that implement full JavaScript standard library functions, such as Function.prototype.bind , but many builtins are helper snippets of machine code that fill
in the gap between the higher-level semantics of JS and the low-level capabilities of the CPU. For example, if a JavaScript function wants to call another JavaScript function, it is common for the function 
implementation to call a CallFunction builtin that figures out how the target JavaScript function should be called; i.e., whether it\u2019s a proxy or a regular function, how many arguments it expects, etc. 
Since these snippets are known when we build the VM, they are \"embedded\" in the Chrome binary, which means that they end up within the Chrome binary code region. Direct vs. indirect calls # On 64-bit 
architectures, the Chrome binary, which includes these builtins, lies arbitrarily far away from JIT code. With the x86-64 instruction set, this means we can\u2019t use direct calls: they take a 32-bit signed 
immediate that\u2019s used as an offset to the address of the call, and the target may be more than 2 GiB away. Instead, we need to rely on indirect calls through a register or memory operand. Such calls rely 
more heavily on prediction since it\u2019s not immediately apparent from the call instruction itself what the target of the call is. On ARM64 we can\u2019t use direct calls at all since the range is limited to 
128 MiB. This means that in both cases we rely on the accuracy of the CPU's indirect branch predictor. Indirect branch prediction limitations # When targeting x86-64 it would be nice to rely on direct calls. It 
should reduce strain on the indirect branch predictor as the target is known after the instruction is decoded, but it also doesn't require the target to be loaded into a register from a constant or memory. But 
it's not just the obvious differences visible in the machine code. Due to Spectre v2 various device/OS combinations have turned off indirect branch prediction. This means that on such configurations we\u2019ll 
get very costly stalls on function calls from JIT code that rely on the CallFunction builtin. More importantly, even though 64-bit instruction set architectures (the \u201chigh-level language of the CPU\u201d) 
support indirect calls to far addresses, the microarchitecture is free to implement optimisations with arbitrary limitations. It appears common for indirect branch predictors to presume that call distances do 
not exceed a certain distance (e.g., 4GiB), requiring less memory per prediction. E.g., the Intel Optimization Manual explicitly states: For 64-bit applications, branch prediction performance can be negatively 
impacted when the target of a branch is more than 4 GB away from the branch. While on ARM64 the architectural call range for direct calls is limited to 128 MiB, it turns out that Apple\u2019s M1 chip has the 
same microarchitectural 4 GiB range limitation for indirect call prediction. Indirect calls to a call target further away than 4 GiB always seem to be mispredicted. Due to the particularly large re-order buffer 
of the M1, the component of the CPU that enables future predicted instructions to be executed speculatively out-of-order, frequent misprediction results in an exceptionally large performance penalty. Temporary 
solution: copy the builtins # To avoid the cost of frequent mispredictions, and to avoid unnecessarily relying on branch prediction where possible on x86-64, we\u2019ve decided to temporarily copy the builtins 
into V8's pointer compression cage on desktop machines with enough memory. This puts the copied builtin code close to dynamically generated code. The performance results heavily depend on the device 
configuration, but here are some results from our performance bots: Browsing benchmarks recorded from live pages Benchmark score improvement Unembedding builtins does increase memory usage on affected devices by
1.2 to 1.4 MiB per V8 instance. As a better long-term solution we\u2019re looking into allocating JIT code closer to the Chrome binary. That way we can re-embed the builtins to regain the memory benefits, while 
additionally improving the performance of calls from V8-generated code to C++ code. Posted by Toon Verwaest , The Big Short. Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except 
as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, 
see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/tags/cppgc.txt",
    "similarity": 0.441,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/tags/cppgc.txt\n\nBlog posts tagged \u201ccppgc\u201d \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Blog 
posts tagged \u201ccppgc\u201d Pointer compression in Oilpan 28 November 2022 internals memory cppgc Oilpan library 10 November 2021 internals memory cppgc High-performance garbage collection for C++ 26 May 2020
internals memory cppgc Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on 
this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  }
]
[Step 13: Duration 9.99 seconds| Input tokens: 204,515 | Output tokens: 5,367]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 14 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'src/deoptimizer.cc Deoptimizer FrameDescription Deoptimizer::Deoptimize CompileLazyDeoptimizedCode builtin CompileLazyDeoptimizedCode',        │
│ 'top_k': 5, 'topic_filter': 'v8'}                                                                                                                                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.47,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/webassembly-opcode.txt",
    "similarity": 0.456,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/webassembly-opcode.txt\n\nWebAssembly - adding a new opcode \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
WebAssembly - adding a new opcode WebAssembly (Wasm) is a binary instruction format for a stack-based virtual machine. This tutorial walks the reader through implementing a new WebAssembly instruction in V8. 
WebAssembly is implemented in V8 in three parts: the interpreter the baseline compiler (Liftoff) the optimizing compiler (TurboFan) The rest of this document focuses on the TurboFan pipeline, walking through how
to add a new Wasm instruction and implement it in TurboFan. At a high level, Wasm instructions are compiled into a TurboFan graph, and we rely on the TurboFan pipeline to compile the graph into (ultimately) 
machine code. For more on TurboFan, check out the V8 docs . Opcodes/Instructions # Let\u2019s define a new instruction that adds 1 to an int32 (on the top of the stack). Note: A list of instructions supported by
all Wasm implementations can be found in the spec . All Wasm instructions are defined in src/wasm/wasm-opcodes.h . The instructions are grouped roughly by what they do, e.g. control, memory, SIMD, atomic, etc. 
Let\u2019s add our new instruction, I32Add1 , to the FOREACH_SIMPLE_OPCODE section: diff --git a/src/wasm/wasm-opcodes.h b/src/wasm/wasm-opcodes.h index 6970c667e7..867cbf451a 100644 --- 
a/src/wasm/wasm-opcodes.h +++ b/src/wasm/wasm-opcodes.h @@ -96,6 +96,7 @@ bool IsJSCompatibleSignature(const FunctionSig* sig, bool hasBigIntFeature); // Expressions with signatures. #define 
FOREACH_SIMPLE_OPCODE(V) \\ + V(I32Add1, 0xee, i_i) \\ V(I32Eqz, 0x45, i_i) \\ V(I32Eq, 0x46, i_ii) \\ V(I32Ne, 0x47, i_ii) \\ WebAssembly is a binary format, so 0xee specifies the encoding of this instruction. 
In this tutorial we chose 0xee as it is currently unused. Note: Actually adding an instruction to the spec involves work beyond what is described here. We can run a simple unit test for opcodes with: $ 
tools/dev/gm.py x64.debug unittests/WasmOpcodesTest* ... |==========] Running 1 test from 1 test suite. |----------] Global test environment set-up. |----------] 1 test from WasmOpcodesTest | RUN ] 
WasmOpcodesTest.EveryOpcodeHasAName ../../test/unittests/wasm/wasm-opcodes-unittest.cc:27: Failure Value of: false Actual: false Expected: true WasmOpcodes::OpcodeName(kExprI32Add1) == \"unknown\"; plazz halp in
src/wasm/wasm-opcodes.cc | FAILED ] WasmOpcodesTest.EveryOpcodeHasAName This error indicates that we don\u2019t have a name for our new instruction. Adding a name for the new opcode can be done in 
src/wasm/wasm-opcodes.cc : diff --git a/src/wasm/wasm-opcodes.cc b/src/wasm/wasm-opcodes.cc index 5ed664441d..2d4e9554fe 100644 --- a/src/wasm/wasm-opcodes.cc +++ b/src/wasm/wasm-opcodes.cc @@ -75,6 +75,7 @@ 
const char* WasmOpcodes::OpcodeName(WasmOpcode opcode) { // clang-format off // Standard opcodes + CASE_I32_OP(Add1, \"add1\") CASE_INT_OP(Eqz, \"eqz\") CASE_ALL_OP(Eq, \"eq\") CASE_I64x2_OP(Eq, \"eq\") By 
adding our new instruction in FOREACH_SIMPLE_OPCODE , we are skipping a fair amount of work that is done in src/wasm/function-body-decoder-impl.h , which decodes Wasm opcodes and calls into the TurboFan graph 
generator. Thus, depending on what your opcode does, you might have more work to do. We skip this in the interest of brevity. Writing a test for the new opcode # Wasm tests can be found in test/cctest/wasm/ . 
Let\u2019s take a look at test/cctest/wasm/test-run-wasm.cc , where many \u201csimple\u201d opcodes are tested. There are many examples in this file that we can follow. The general setup is: create a WasmRunner 
set up globals to hold result (optional) set up locals as parameters to instruction (optional) build the wasm module run it and compare with an expected output Here\u2019s a simple test for our new opcode: diff 
--git a/test/cctest/wasm/test-run-wasm.cc b/test/cctest/wasm/test-run-wasm.cc index 26df61ceb8..b1ee6edd71 100644 --- a/test/cctest/wasm/test-run-wasm.cc +++ b/test/cctest/wasm/test-run-wasm.cc @@ -28,6 +28,15 
@@ namespace test_run_wasm { #define RET(x) x, kExprReturn #define RET_I8(x) WASM_I32V_2(x), kExprReturn + #define WASM_I32_ADD1(x) x, kExprI32Add1 + + WASM_EXEC_TEST(Int32Add1) { + WasmRunner<int32_t> 
r(execution_tier); + // 10 + 1 + BUILD(r, WASM_I32_ADD1(WASM_I32V_1(10))); + CHECK_EQ(11, r.Call()); + } + WASM_EXEC_TEST(Int32Const) { WasmRunner<int32_t> r(execution_tier); const int32_t kExpectedValue = 
0x11223344; Run the test: $ tools/dev/gm.py x64.debug 'cctest/test-run-wasm-simd/RunWasmTurbofan_I32Add1' ... === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in 
../../src/compiler/wasm-compiler.cc, line 988 # Unsupported opcode 0xee:i32.add1 Tip: Finding the test name can be tricky, since the test definition is behind a macro. Use Code Search to click around to discover
the macro definitions. This error indicates that the compiler does not know of our new instruction. That will change in the next section. Compiling Wasm into TurboFan # In the introduction, we mentioned that 
Wasm instructions are compiled into a TurboFan graph. wasm-compiler.cc is where this happens. Let\u2019s take a look at an example opcode, I32Eqz : switch ( opcode ) { case wasm :: kExprI32Eqz : op = m -> 
Word32Equal ( ) ; return graph ( ) -> NewNode ( op , input , mcgraph ( ) -> Int32Constant ( 0 ) ) ; This switches on the Wasm opcode wasm::kExprI32Eqz , and builds a TurboFan graph consisting of the operation 
Word32Equal with the inputs input , which is the argument to the Wasm instruction, and a constant 0 . The Word32Equal operator is provided by the underlying V8 abstract machine, which is 
architecture-independent. Later in the pipeline, this abstract machine operator will be translated into architecture-dependent assembly. For our new opcode, I32Add1 , we need a graph that adds a constant 1 to 
the input, so we can resuse an existing machine operator, Int32Add , passing it the input, and a constant 1: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index 
f666bbb7c1..399293c03b 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 @@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; 
MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return graph()->NewNode(m->Int32Add(), input, mcgraph()->Int32Constant(1)); case wasm::kExprI32Eqz: op = 
m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); This is enough to get the test passing. However, not all instructions have an existing TurboFan machine operator. In that case 
we have to add this new operator to the machine. Let\u2019s try that. TurboFan machine operators # We want to add the knowledge of Int32Add1 to the TurboFan machine. So let\u2019s pretend that it exists and use 
it first: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index f666bbb7c1..1d93601584 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 
@@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return 
graph()->NewNode(m->Int32Add1(), input); case wasm::kExprI32Eqz: op = m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); Trying to run the same test leads to a compilation failure
that hints at where to make changes: ../../src/compiler/wasm-compiler.cc:717:34: error: no member named 'Int32Add1' in 'v8::internal::compiler::MachineOperatorBuilder'; did you mean 'Int32Add'? return 
graph()->NewNode(m->Int32Add1(), input); ^~~~~~~~~ Int32Add There are a couple of places that needs to be modified to add an operator: src/compiler/machine-operator.cc header src/compiler/machine-operator.h list
of opcodes that the machine understands src/compiler/opcodes.h verifier src/compiler/verifier.cc diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc index 16e838c2aa..fdd6d951f0 
100644 --- a/src/compiler/machine-operator.cc +++ b/src/compiler/machine-operator.cc @@ -136,6 +136,7 @@ MachineType AtomicOpType(Operator const* op) { #define MACHINE_PURE_OP_LIST(V) \\ 
PURE_BINARY_OP_LIST_32(V) \\ PURE_BINARY_OP_LIST_64(V) \\ + V(Int32Add1, Operator::kNoProperties, 1, 0, 1) \\ V(Word32Clz, Operator::kNoProperties, 1, 0, 1) \\ V(Word64Clz, Operator::kNoProperties, 1, 0, 1) \\ 
V(Word32ReverseBytes, Operator::kNoProperties, 1, 0, 1) \\ diff --git a/src/compiler/machine-operator.h b/src/compiler/machine-operator.h index a2b9fce0ee..f95e75a445 100644 --- a/src/compiler/machine-operator.h
+++ b/src/compiler/machine-operator.h @@ -265,6 +265,8 @@ class V8_EXPORT_PRIVATE MachineOperatorBuilder final const Operator* Word32PairShr(); const Operator* Word32PairSar(); + const Operator* Int32Add1(); + 
const Operator* Int32Add(); const Operator* Int32AddWithOverflow(); const Operator* Int32Sub(); diff --git a/src/compiler/opcodes.h b/src/compiler/opcodes.h index ce24a0bd3f..2c8c5ebaca 100644 --- 
a/src/compiler/opcodes.h +++ b/src/compiler/opcodes.h @@ -506,6 +506,7 @@ V(Float64LessThanOrEqual) #define MACHINE_UNOP_32_LIST(V) \\ + V(Int32Add1) \\ V(Word32Clz) \\ V(Word32Ctz) \\ V(Int32AbsWithOverflow) \\
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc index 461aef0023..95251934ce 100644 --- a/src/compiler/verifier.cc +++ b/src/compiler/verifier.cc @@ -1861,6 +1861,7 @@ void 
Verifier::Visitor::Check(Node* node, const AllNodes& all) { case IrOpcode::kSignExtendWord16ToInt64: case IrOpcode::kSignExtendWord32ToInt64: case IrOpcode::kStaticAssert: + case IrOpcode::kInt32Add1: #define 
SIMD_MACHINE_OP_CASE(Name) case IrOpcode::k##Name: MACHINE_SIMD_OP_LIST(SIMD_MACHINE_OP_CASE) Running the test again now gives us a different failure: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # 
Fatal error in ../../src/compiler/backend/instruction-selector.cc, line 2072 # Unexpected operator #289:Int32Add1 @ node #7 Instruction selection # So far we have been working at the TurboFan level, dealing with
(a sea of) nodes in the TurboFan graph. However, at the assembly level, we have instructions and operands. Instruction selection is the process of translating this graph to instructions and operands. The last 
test error indicated that we need something in src/compiler/backend/instruction-selector.cc . This is a big file with a giant switch statement over all the machine opcodes. It calls into architecture specific 
instruction selection, using the visitor pattern to emit instructions for each type of node. Since we added a new TurboFan machine opcode, we need to add it here as well: diff --git 
a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc index 3152b2d41e..7375085649 100644 --- a/src/compiler/backend/instruction-selector.cc +++ 
b/src/compiler/backend/instruction-selector.cc @@ -2067,6 +2067,8 @@ void InstructionSelector::VisitNode(Node* node) { return MarkAsWord32(node), VisitS1x16AnyTrue(node); case IrOpcode::kS1x16AllTrue: return 
MarkAsWord32(node), VisitS1x16AllTrue(node); + case IrOpcode::kInt32Add1: + return MarkAsWord32(node), VisitInt32Add1(node); default: FATAL(\"Unexpected operator #%d:%s @ node #%d\", node->opcode(), 
node->op()->mnemonic(), node->id()); Instruction selection is architecture dependent, so we have to add it to the architecture specific instruction selector files too. For this codelab we only focus on the x64 
architecture, so src/compiler/backend/x64/instruction-selector-x64.cc needs to be modified: diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc 
b/src/compiler/backend/x64/instruction-selector-x64.cc index 2324e119a6..4b55671243 100644 --- a/src/compiler/backend/x64/instruction-selector-x64.cc +++ b/src/compiler/backend/x64/instruction-selector-x64.cc @@
-841,6 +841,11 @@ void InstructionSelector::VisitWord32ReverseBytes(Node* node) { Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); } + void InstructionSelector::VisitInt32Add1(Node*
node) { + X64OperandGenerator g(this); + Emit(kX64Int32Add1, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); + } + And we also need to add this new x64-specific opcode, kX64Int32Add1 to 
src/compiler/backend/x64/instruction-codes-x64.h : diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h index 9b8be0e0b5..7f5faeb87b 100644 --- 
a/src/compiler/backend/x64/instruction-codes-x64.h +++ b/src/compiler/backend/x64/instruction-codes-x64.h @@ -12,6 +12,7 @@ namespace compiler { // X64-specific opcodes that specify which assembly sequence to 
emit. // Most opcodes specify a single instruction. #define TARGET_ARCH_OPCODE_LIST(V) \\ + V(X64Int32Add1) \\ V(X64Add) \\ V(X64Add32) \\ V(X64And) \\ Instruction scheduling and code generation # Running our 
test, we see new compilation errors: ../../src/compiler/backend/x64/instruction-scheduler-x64.cc:15:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(instr->arch_opcode()) { ^ 1 error generated. ... ../../src/compiler/backend/x64/code-generator-x64.cc:733:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(arch_opcode) { ^ 1 error generated. Instruction scheduling takes care of dependencies that instructions may have to allow for more optimization (e.g. instruction reordering). Our new opcode has no data 
dependency, so we can add it simply to: src/compiler/backend/x64/instruction-scheduler-x64.cc : diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc 
b/src/compiler/backend/x64/instruction-scheduler-x64.cc index 79eda7e78d..3667a84577 100644 --- a/src/compiler/backend/x64/instruction-scheduler-x64.cc +++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -13,6 +13,7 @@ bool InstructionScheduler::SchedulerSupported() { return true; } int InstructionScheduler::GetTargetInstructionFlags( const Instruction* instr) const { switch (instr->arch_opcode()) { + case 
kX64Int32Add1: case kX64Add: case kX64Add32: case kX64And: Code generation is where we translate our architecture specific opcodes into assembly. Let\u2019s add a clause to 
src/compiler/backend/x64/code-generator-x64.cc : diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc index 61c3a45a16..9c37ed7464 100644 --- 
a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -731,6 +731,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction( InstructionCode 
opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { 
Handle<Code> code = i.InputCode(0); For now we leave our code generation empty, and we can run the test to make sure everything compiles: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in
../../test/cctest/wasm/test-run-wasm.cc, line 37 # Check failed: 11 == r.Call() (11 vs. 10). This failure is expected, since our new instruction is not implemented yet \u2014 it is essentially a no-op, so our 
actual value was unchanged ( 10 ). To implement our opcode, we can use the add assembly instruction: diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc 
index 6c828d6bc4..260c8619f2 100644 --- a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -744,6 +744,11 @@ CodeGenerator::CodeGenResult 
CodeGenerator::AssembleArchInstruction( InstructionCode opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + 
DCHECK_EQ(i.OutputRegister(), i.InputRegister(0)); + __ addl(i.InputRegister(0), Immediate(1)); + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { Handle<Code> code = i.InputCode(0); And
this makes the test pass: Luckily for us addl is already implemented. If our new opcode required writing a new assembly instruction implementation, we would add it to src/compiler/backend/x64/assembler-x64.cc , 
where the assembly instruction is encoded into bytes and emitted. Tip: To inspect the generated code, we can pass --print-code to cctest . Other architectures # In this codelab we only implemented this new 
instruction for x64. The steps required for other architectures are similar: add TurboFan machine operators, use the platform-dependent files for instruction selection, scheduling, code generation, assembler. 
Tip: if we compile what we have done so far on another target, e.g. arm64, we are likely to get errors in linking. To resolve those errors, add UNIMPLEMENTED() stubs. Branding Terms Privacy Twitter Edit this 
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-speculative-optimizations.txt",
    "similarity": 0.442,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-speculative-optimizations.txt\n\nSpeculative Optimizations for WebAssembly using Deopts and Inlining \u00b7 V8 V8 Show navigation 
Home Blog Docs Tools JS/Wasm features Research Speculative Optimizations for WebAssembly using Deopts and Inlining Published 24 June 2025 \u00b7 Tagged with WebAssembly internals In this blog post, we explain 
two optimizations for WebAssembly that we recently implemented in V8 and that shipped with Google Chrome M137, namely speculative call_indirect inlining and deoptimization support for WebAssembly. In 
combination, they allow us to generate better machine code by making assumptions based on runtime feedback. This speeds up WebAssembly execution, in particular for WasmGC programs. On a set of Dart 
microbenchmarks for example, the speedup by the combination of both optimizations is more than 50% on average, and on larger, realistic applications and benchmarks shown below the speedup is between 1% and 8%. 
Deoptimizations are also an important building block for further optimizations in the future. Background # Fast execution of JavaScript relies heavily on speculative optimizations . That is, JIT-compilers make 
assumptions when generating machine code based on feedback that was collected during earlier executions. For example, given the expression a + b , the compiler can generate machine code for an integer addition 
if past feedback indicates that a and b are integers (and not strings, floating point numbers, or other objects). Without making such assumptions, the compiler would have to emit generic code that handles the 
full behavior of the + operator in JavaScript , which is complex and thus much slower. If the program later behaves differently and thus violates assumptions made when generating the optimized code, V8 performs 
a deoptimization (or deopt, for short). That means throwing away the optimized code and continuing execution in unoptimized code (and collecting more feedback to possibly tier-up again later). In contrast to 
JavaScript, fast execution of WebAssembly hasn\u2019t required speculative optimizations and deopts. One reason is that WebAssembly programs can already be optimized quite well because more information is 
statically available as e.g., functions, instructions, and variables are all statically typed. Another reason is that WebAssembly binaries are often compiled from C, C++, or Rust. These source languages are also
more amenable to static analysis than JavaScript, and thus toolchains such as Emscripten (based on LLVM) or Binaryen can already optimize the program ahead-of-time. This results in fairly well-optimized 
binaries, at least when targeting WebAssembly 1.0, which launched in 2017 . Motivation # Then why do we employ more speculative optimizations for WebAssembly in V8 now? One reason is that WebAssembly has evolved
with the introduction of WasmGC , the WebAssembly Garbage Collection proposal . It better supports compiling \u201cmanaged\u201d languages such as Java , Kotlin , or Dart to WebAssembly. The resulting WasmGC 
bytecode is more high-level than Wasm 1.0, e.g., it supports rich types, such as structs and arrays, subtyping, and operations on such types. The generated machine code for WasmGC thus benefits more from 
speculative optimizations. One particularly important optimization is inlining , that is, replacing a call instruction with the body of the callee function. Not only does this get rid of the administrative 
overhead associated with the call itself (which might be higher than the actual work for very small functions), but it also enables many other, subsequent optimizations to \u201csee through\u201d the function 
call, even if those optimizations are not inter-procedural. No wonder inlining was already recognized in 1971 as one of the most important optimizations in Frances Allen\u2019s seminal \u201cCatalogue of 
Optimizing Transformations\u201d . One complication for inlining are indirect calls , i.e., calls where the callee is only known at runtime and can be one of many potential targets. This is particularly true for
the languages that compile to WasmGC. Consider Java or Kotlin where methods are virtual by default, whereas in C++ one has to opt-in by marking them explicitly. If there is not a single statically known callee, 
inlining is obviously not as straightforward. Speculative Inlining # This is where speculative inlining comes into play. In theory, indirect calls can target many different functions, but in practice, they still
often go to a single target (called a monomorphic function call) or a few select cases (called polymorphic ). We record those targets when executing the unoptimized code, and then inline up to four target 
functions when generating the optimized code. Speculative inlining overview The figure above shows the high-level picture. We start at the top left, with the unoptimized code for function func_a , generated by 
Liftoff , our baseline compiler for WebAssembly. At each call site Liftoff also emits code to update the feedback vector . This metadata array exists once per function, and it contains one entry per call 
instruction in the given function. Each entry records the call targets and counts for this particular call site. The example feedback at the bottom of the figure shows a monomorphic entry for the call_indirect 
in func_a ; here the call target was 1337 times func_b . When a function is hot enough to tier-up to TurboFan , i.e., gets compiled with our optimizing compiler, we come to the second step. TurboFan reads the 
corresponding feedback vector and decides whether and which targets to inline at each call site. Whether it is worthwhile inlining one or multiple callees depends on a variety of heuristics . E.g., large 
functions are never inlined, tiny functions are almost always inlined, and generally there is a maximum inlining budget after which no more inlining into a function happens, as that also has costs in terms of 
compile time and generated machine code size. As in many places in compilers, particularly multi-tier JITs, these trade-offs are quite complex and get tuned over time. In this example, TurboFan decides to inline
func_b into func_a . On the upper right hand side of the figure, we see the result of speculative inlining in the generated optimized code. Instead of an indirect call, the code first checks if the target at 
runtime matches what we have assumed during compilation. If that\u2019s the case, we continue executing the inlined body of the corresponding function. Subsequent optimizations can also transform the inlined 
code further, given the now available surrounding context. E.g., constant propagation and constant folding could specialize the code to this particular call site or GVN could hoist out repeated computations. In 
the case of polymorphic feedback, TurboFan can also emit a series of target checks and inlined bodies, not just one as in this example. Technical Deep Dive # So much for the high-level picture. For readers 
interested in the implementation, we look at some more details and concrete code in this section. In the figure above, the feedback vector is only shown conceptually as an array of entries and only one kind of 
entry is shown. Below, we see that each entry can go through four stages over the course of the execution: Initially, all entries are uninitialized (all call counts are zero), potentially transitioning to 
monomorphic (a single call target was recorded), polymorphic (up to four call targets), and finally megamorphic (more than four targets, where we don\u2019t inline any more and thus don\u2019t need to record 
call counts either). Each entry is actually a pair of objects such that the most common monomorphic case can store both the call count and the target inline in the vector, i.e., without an additional allocation.
For the polymorphic case, feedback is stored in an out-of-line array, as shown below. The builtin that updates a feedback entry (corresponding to update_feedback() in the first figure) is written in Torque . 
(It\u2019s quite easy to read, give it a try!) It first checks for monomorphic or polymorphic \u201chits\u201d, where just the count has to be incremented. This is again because they are the most common cases 
and thus performance-sensitive. The feedback vector and its entries are JavaScript objects (e.g., the call counts are Smis ), so they live on the managed heap. As such, it is part of the V8 sandbox and is 
automatically cleaned up if the corresponding Wasm instance (see below) is no longer reachable. Details of the feedback vector Next, let\u2019s look at the effect of inlining on an actual WebAssembly program. 
The example function below does 200M indirect calls in a loop to a single target inlinee that contains an addition. Obviously, this is a somewhat simplified microbenchmark, but it demonstrates the benefits of 
speculative inlining well. ( func $example ( param $iterations i32 ) ( result i32 ) ( local $sum i32 ) block $ block loop $ loop local .get $iterations i32 . eqz br_if $ block ;; terminate loop local .get 
$iterations ;; update loop counter i32 . const 1 i32 . sub local .set $iterations i32 . const 7 ;; argument for the function call i32 . const 1 ;; table index, refering to $inlinee call_indirect ( param i32 ) ( 
result i32 ) local .get $sum i32 . add local .set $sum br $ loop ;; repeat end end local .get $sum ) ... ( func $inlinee ( param $x i32 ) ( result i32 ) local .get $x i32 . const 37 i32 . add ) For readers not 
familiar with the WebAssembly text format, here is a rough C equivalent of the above program: int inlinee ( int x ) { return x + 37 ; } int ( * func_ptr ) ( int ) = inlinee ; int example ( int iterations ) { int
sum = 0 ; while ( iterations != 0 ) { iterations -- ; sum += func_ptr ( 7 ) ; } return sum ; } The next figure shows excerpts of TurboFan\u2019s intermediate representation for the example function, visualized 
with Turbolizer : Speculative inlining and Wasm deopts are enabled on the right, and disabled on the left. In both versions, we have to check whether the table index argument to the call_indirect instruction is 
in-bounds, as per the WebAssembly semantics (first red box in both cases). Without inlining, we also have to check whether the function at this index has the correct signature before actually calling it (second 
red box on the left). Finally, the first green box on the left is the indirect call, and the second green box is the addition of the result of said call. In the green box on the right, we see that after inlining
and further optimizations, the call is completely gone and the addition in inlinee and the addition in example were constant-folded into a single addition with a constant. Altogether, on this particular 
microbenchmark, inlining, deopts, and subsequent optimizations speed up the program from around 675 ms to 90 ms execution time on an x64 workstation. In this case, the optimized machine code with inlining is 
even smaller than without (968 vs. 1572 bytes), although that certainly need not be. TurboFan IR without and with inlining Finally, we want to briefly explain the Wasm instance check and target check that the 
code with speculative inlining does on the right. Semantically, Wasm functions are closures over a Wasm instance (which \u201cholds\u201d the current state of globals, tables, imports from the host, etc.). 
Correctly inlining functions that belong to a different instance (e.g., which are called via an imported table) would hence require additional compiler machinery as well as solving a few obstacles in our general
handling of generated code. Luckily, most calls are within a single instance anyway, so for the time being we check that the call target\u2019s instance matches the current instance, which lets the compiler make
the simplifying assumption that both instances are the same. If not, we deoptimize in block 8 (due to wrong instance) or block 6 (due to wrong target). This additional Wasm instance check was specifically 
introduced for the new call_indirect inlining. WebAssembly also has another kind of indirect call, call_ref , for which we already added inlining support when launching our WasmGC implementation . The fast path 
for call_ref inlining doesn\u2019t require an explicit instance check, since the WasmFuncRef object that is the call_ref input already includes the instance the function closes over, so comparing the target for 
equality subsumes both checks. With the new call_indirect inlining, V8 now supports inlining Wasm-to-Wasm calls for all types of call instructions: direct call s, call_ref , call_indirect , and their respective 
tail-call variants return_call , return_call_ref , and return_call_indirect . Deoptimization # So far, we have focused on inlining and how that improves the optimized code. But what happens if we cannot stay on 
a fast path, i.e., if one of the assumptions made during optimization turns out to be false at runtime? This is where deopts come into play. The very first figure of this post already shows the high-level idea 
of a deopt: We cannot continue execution in the optimized code because it has made some assumptions that are now invalidated. So instead, we \u201cgo back\u201d to the unoptimized baseline code. Crucially, this 
transition to unoptimized code happens in the middle of executing the current function , i.e., when the optimized code has already performed operations with side-effects (say, called the underlying operating 
system), which we cannot undo, and while it is holding intermediate values in registers and on the stack. So a deopt cannot just jump to the beginning of the unoptimized function, but instead does something much
more interesting: First, we save the current program state . We do this by calling into the runtime from optimized code. The Deoptimizer then serializes the current program state into an internal data structure,
the FrameDescription . This entails reading out CPU registers and inspecting the stack frame of the function to be deoptimized. Next, we transform this state such that it fits the unoptimized code , that is, 
placing values in the correct registers and stack slots that the Liftoff-generated code expects at the deoptimization point. E.g., a value that TurboFan code has put in a register may now end up in a stack slot.
The stack layout and expectations for the baseline code (its \u201ccalling convention\u201d for the deopt point, if you will) are read out of metadata generated by Liftoff during compilation. And finally, we 
replace the optimized stack frame with the corresponding unoptimized stack frame(s) and jump to the middle of the baseline code , corresponding to the point in the optimized code where the deopt was triggered. 
Obviously, this is quite complex machinery, which begs the question: Why do we go through the hassle of it all and not just generate a slow path in optimized code? Let\u2019s compare the intermediate 
representation for the earlier example WebAssembly code, left without and right with deopts. The three red boxes (table bounds / Wasm instance / target check) are fundamentally the same. The difference is in the
code for the slow path. Without deopts, we don\u2019t have the option of returning to baseline code, so the optimized code must handle the full, generic behavior of an indirect call, as shown in the yellow box 
on the left. This unfortunately impedes further optimizations and thus results in slower execution. TurboFan IR without and with deopts There are two reasons why the non-deopt slow path impedes optimizations. 
First, it contains more operations, in particular a call, which we have to assume goes anywhere and has arbitrary side-effects. Second, note how the Deoptimize operation in block 8 on the right has no successor 
in the control-flow graph, whereas the yellow slow path on the left has a successor. In particular for loops, the deoptimization node/block will not produce data-flow facts (in the sense of data-flow analyses 
and optimizations, such as live variables , load elimination, or escape analysis ) that propagate to the next iteration of the loop. In essence, the deopt point \u201cjust\u201d terminates the execution of the 
function, without much effect on the surrounding code, which can be utilized nicely by subsequent optimizations. Finally, this also explains why the combination of speculative optimizations (e.g., inlining) and 
deoptimization is so useful: The first adds a fast path based on speculative assumptions, and deopts allow the compiler to not worry much about the cases where the assumptions turn out to be false. Concretely, 
for the earlier microbenchmark with 200M indirect calls, performing just speculative inlining without deopts speeds up the program \u201conly\u201d to about 180 ms, compared to 90 ms with both inlining and 
deopts (and 675 ms without either). Technical Deep Dive # For the interested reader, we now again look at a concrete example and technical details, this time for when we deopt. Let's assume we execute the 
optimized code from above but the function stored at table index 1 has changed in the meantime. The table bounds check and Wasm instance check will pass, but the inlined target will be different from the one in 
the table, so we need to deoptimize the program in its current state. For that the code contains so-called deoptimization exits . The target check conditionally jumps to such an exit, which itself is a call to 
the DeoptimizationEntry builtin. The builtin first saves all register values by spilling them to the stack |1] . Then it allocates the C++ Deoptimizer object and its FrameDescription input object. The builtin 
copies the spilled registers and all other stack slots from the optimized frame into the FrameDescription on the heap, and pops these values from the stack in the process. (Note that the execution is still in 
the builtin while it has already removed its own return address from the stack and started unwinding the calling frame!) Then the builtin computes the output frames. To do that, the deoptimizer loads the 
DeoptimizationData for the optimized frame, extracts the information for the deopt point, and recompiles each inlined function at this call site with Liftoff. Due to nested inlining there can be more than one 
inlined function, and with inlined tail calls the optimized function to which the optimized stack frame nominally belonged might not even be part of the unoptimized stack frames to be constructed. While 
compiling, Liftoff calculates the expected stack frame layout and the deoptimizer transforms the optimized frame description into the desired layouts reported by Liftoff. It returns to the builtin which then 
reads these output FrameDescription objects and pushes their values onto the stack. Finally the builtin fills in the registers from the top output FrameDescription . For the example above, our internal tracing 
with --trace-deopt-verbose shows the following: |bailout (kind: deopt-eager, reason: wrong call target, type: Wasm): begin. deoptimizing example, function index 2, bytecode offset 134, deopt exit 0, FP to SP 
delta 32, pc 0x14886e50cbb4] reading input for Liftoff frame => bailout_id=134, height=4, function_id=2 ; inputs: 0: 4 ; rdx (int32) 1: 0 ; rbx (int32) 2: (wasm int32 literal 7) 3: (wasm int32 literal 1) Liftoff
stack & register state for function index 2, frame size 48, total frame size 64 0: i32:rax 1: i32:s0x28 2: i32:c7 3: i32:c1 |bailout end. took 0.082 ms] First we can see that a deopt is triggered because of a 
wrong call target in Wasm for the function example . The trace then shows information about the input (optimized) frame, which has four values: The iterations parameter of the function (value 4 , stored in 
register rdx ), the sum local ( 0 stored in rbx ), the literal 7 and the literal 1, which are the two arguments to the call_indirect instruction. In this simple example, there is only one unoptimized output 
frame, so there is a 1:1 mapping between the two frames: The iterations value has to be stored in register rax while the sum value needs to end up in the stack slot s0x28 . The two constants are also recognized 
by Liftoff as constants and don\u2019t need to be transferred into stack slots or registers. |2] After these transformations have been done the builtin \u201creturns\u201d to the inner-most unoptimized frame 
which calls a final builtin to clean-up the Deoptimizer object and perform any needed allocations on the managed heap. |3] Finally, execution continues in the unoptimized code, in this case executing the 
call_indirect , which will also directly record the new call target in its feedback vector, so that any later tier-up is aware of this new target. Results # Besides the technical description and examples, we 
also want to demonstrate the usefulness of call_indirect inlining and Wasm deopt support with some measurements. |4] We first look at a collection of Dart microbenchmarks in the figure below. It compares three 
configurations against each other: All given numbers are speedups relative to V8 and Chrome\u2019s behavior before call_indirect inlining and Wasm deopts (i.e., a speedup of 2x means the runtime is half of that 
of the baseline). The blue bars show the configuration with call_indirect inlining enabled but no Wasm deopts, i.e., where the optimized code contains a generic slow path. On several of these microbenchmarks 
this already yields (sometimes substantial) speedups. |5] On average across all items, call_indirect inlining speeds up execution by 1.19x compared to the baseline without. Finally, the red bars show the 
configuration we actually ship, where both Wasm deopts and call_indirect inlining are enabled. With an average speedup of 1.59x over the baseline, this shows that in particular the combination of speculative 
optimizations and deoptimization support is highly beneficial. Naturally, microbenchmarks isolate and emphasize the effects of an optimization quite a bit. This is useful during development or to get a strong 
signal with noisy measurements. However, more realistic are results on larger applications and benchmarks, as shown in the following figure. To the very left, we see a 2% speedup in terms of runtime for 
richards-wasm , a workload from the JetStream benchmark suite . Next, we see a 1% speedup for a Wasm build of the widely-used SQLite 3 database, and 8% speedup for Dart Flute , a WasmGC benchmark that emulates a
UI workload similar to Flutter . The final two results are from an internal benchmark for the Google Sheets calc engine , which is powered by WasmGC, with speedups due to deopts of up to 7% (only deopts matter 
here as this last application only uses call_ref s for runtime dispatch, i.e., it has no call_indirect s). Conclusion and Outlook # This concludes our post about two new optimizations in the V8 engine for 
WebAssembly. To summarize, we have seen: How speculative inlining can inline functions even in the presence of indirect calls, what feedback is and how it is used and updated, what to do when assumptions made 
during optimizations are invalid at runtime, how a deoptimization can exit optimized code and enters baseline code in the middle of executing a function, and finally how this significantly improves the execution
of real-world workloads. In the future, we plan on adding more speculative optimizations based on deopt support for WebAssembly, e.g. bounds-check elimination or more extensive load elimination for WasmGC 
objects. And also in terms of inlining, there is more to be done: While we now have Wasm-into-Wasm inlining for all kinds of call instructions, we can still extend inlining across the language boundary, e.g., 
for JavaScript-to-Wasm calls. Check back on our V8 blog for exciting updates in the future! Footnotes # Due to the SIMD extension for Wasm this also includes all 128-bit vector registers used by TurboFan. 
\u21a9\ufe0e In other cases values might be constant in the optimized version after constant folding but need to be materialized into stack slots or registers for Liftoff, so the deoptimization data needs to 
store these constant values. \u21a9\ufe0e During the deoptimization itself we cannot allocate on the heap since allocations can trigger the garbage collector (GC) and the stack is not in a state that the GC can 
inspect. (The GC needs to visit all heap-references on the stack and potentially update them when moving an object.) \u21a9\ufe0e The measurements were taken on an x64 workstation; the figures show the median of
N=21 repetitions. \u21a9\ufe0e For the three Matrix4Benchmark items that slightly regress, enabling call_indirect inlining causes our heuristic to prefer 16 indirect call sites for inlining over other direct 
call sites. This leads to exhausting the inlining budget (i.e., stop inlining because the resulting code grows too large), so that fewer of the direct calls are inlined compared with before. In this particular 
case, the heuristic doesn\u2019t predict perfectly how beneficial one inlining decision is over the other and leads to a suboptimal result. Improving this heuristics is interesting future work. \u21a9\ufe0e 
Posted by Daniel Lehmann and Matthias Liedtke. Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style 
license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/lazy-unlinking.txt",
    "similarity": 0.441,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/lazy-unlinking.txt\n\nAn internship on laziness: lazy unlinking of deoptimized functions \u00b7 V8 V8 Show navigation Home Blog Docs 
Tools JS/Wasm features Research An internship on laziness: lazy unlinking of deoptimized functions Published 04 October 2017 \u00b7 Tagged with memory internals Roughly three months ago, I joined the V8 team 
(Google Munich) as an intern and since then I\u2019ve been working on the VM\u2019s Deoptimizer \u2014 something completely new to me which proved to be an interesting and challenging project. The first part of 
my internship focused on improving the VM security-wise . The second part focused on performance improvements. Namely, on the removal of a data-structure used for the unlinking of previously deoptimized 
functions, which was a performance bottleneck during garbage collection. This blog post describes this second part of my internship. I\u2019ll explain how V8 used to unlink deoptimized functions, how we changed 
this, and what performance improvements were obtained. Let\u2019s (very) briefly recap the V8 pipeline for a JavaScript function: V8\u2019s interpreter, Ignition, collects profiling information about that 
function while interpreting it. Once the function becomes hot, this information is passed to V8\u2019s compiler, TurboFan, which generates optimized machine code. When the profiling information is no longer 
valid \u2014 for example because one of the profiled objects gets a different type during runtime \u2014 the optimized machine code might become invalid. In that case, V8 needs to deoptimize it. An overview of 
V8, as seen in JavaScript Start-up Performance Upon optimization, TurboFan generates a code object, i.e. the optimized machine code, for the function under optimization. When this function is invoked the next 
time, V8 follows the link to optimized code for that function and executes it. Upon deoptimization of this function, we need to unlink the code object in order to make sure that it won\u2019t be executed again. 
How does that happen? For example, in the following code, the function f1 will be invoked many times (always passing an integer as argument). TurboFan then generates machine code for that specific case. function
g ( ) { return ( i ) => i ; } // Create a closure. const f1 = g ( ) ; // Optimize f1. for ( var i = 0 ; i < 1000 ; i ++ ) f1 ( 0 ) ; Each function also has a trampoline to the interpreter \u2014 more details in 
these slides \u2014 and will keep a pointer to this trampoline in its SharedFunctionInfo (SFI). This trampoline will be used whenever V8 needs to go back to unoptimized code. Thus, upon deoptimization, triggered
by passing an argument of a different type, for example, the Deoptimizer can simply set the code field of the JavaScript function to this trampoline. An overview of V8, as seen in JavaScript Start-up Performance
Although this seems simple, it forces V8 to keep weak lists of optimized JavaScript functions. This is because it is possible to have different functions pointing to the same optimized code object. We can extend
our example as follows, and the functions f1 and f2 both point to the same optimized code. const f2 = g ( ) ; f2 ( 0 ) ; If the function f1 is deoptimized (for example by invoking it with an object of different 
type {x: 0} ) we need to make sure that the invalidated code will not be executed again by invoking f2 . Thus, upon deoptimization, V8 used to iterate over all the optimized JavaScript functions, and would 
unlink those that pointed to the code object being deoptimized. This iteration in applications with many optimized JavaScript functions became a performance bottleneck. Moreover, other than slowing down 
deoptimization, V8 used to iterate over these lists upon stop-the-world cycles of garbage collection, making it even worse. In order to have an idea of the impact of such data-structure in the performance of V8,
we wrote a micro-benchmark that stresses its usage, by triggering many scavenge cycles after creating many JavaScript functions. function g ( ) { return ( i ) => i + 1 ; } // Create an initial closure and 
optimize. var f = g ( ) ; f ( 0 ) ; f ( 0 ) ; % OptimizeFunctionOnNextCall ( f ) ; f ( 0 ) ; // Create 2M closures; those will get the previously optimized code. var a = | ] ; for ( var i = 0 ; i < 2000000 ; i 
++ ) { var h = g ( ) ; h ( ) ; a . push ( h ) ; } // Now cause scavenges; all of them are slow. for ( var i = 0 ; i < 1000 ; i ++ ) { new Array ( 50000 ) ; } When running this benchmark, we could observe that V8
spent around 98% of its execution time on garbage collection. We then removed this data structure, and instead used an approach for lazy unlinking , and this was what we observed on x64: Although this is just a 
micro-benchmark that creates many JavaScript functions and triggers many garbage collection cycles, it gives us an idea of the overhead introduced by this data structure. Other more realistic applications where 
we saw some overhead, and which motivated this work, were the router benchmark implemented in Node.js and ARES-6 benchmark suite . Lazy unlinking # Rather than unlinking optimized code from JavaScript functions 
upon deoptimization, V8 postpones it for the next invocation of such functions. When such functions are invoked, V8 checks whether they have been deoptimized, unlinks them and then continues with their lazy 
compilation. If these functions are never invoked again, then they will never be unlinked and the deoptimized code objects will not be collected. However, given that during deoptimization, we invalidate all the 
embedded fields of the code object, we only keep that code object alive. The commit that removed this list of optimized JavaScript functions required changes in several parts of the VM, but the basic idea is as 
follows. When assembling the optimized code object, we check if this is the code of a JavaScript function. If so, in its prologue, we assemble machine code to bail out if the code object has been deoptimized. 
Upon deoptimization we don\u2019t modify the deoptimized code \u2014 code patching is gone. Thus, its bit marked_for_deoptimization is still set when invoking the function again. TurboFan generates code to check
it, and if it is set, then V8 jumps to a new builtin, CompileLazyDeoptimizedCode , that unlinks the deoptimized code from the JavaScript function and then continues with lazy compilation. In more detail, the 
first step is to generate instructions that load the address of the code being currently assembled. We can do that in x64, with the following code: Label current ; // Load effective address of current 
instruction into rcx. __ leaq ( rcx , Operand ( & current ) ) ; __ bind ( & current ) ; After that we need to obtain where in the code object the marked_for_deoptimization bit lives. int pc = __ pc_offset ( ) ; 
int offset = Code :: kKindSpecificFlags1Offset - ( Code :: kHeaderSize + pc ) ; We can then test the bit and if it is set, we jump to the CompileLazyDeoptimizedCode built in. // Test if the bit is set, that is, 
if the code is marked for deoptimization. __ testl ( Operand ( rcx , offset ) , Immediate ( 1 << Code :: kMarkedForDeoptimizationBit ) ) ; // Jump to builtin if it is. __ j ( not_zero , /* handle to builtin code
here */ , RelocInfo :: CODE_TARGET ) ; On the side of this CompileLazyDeoptimizedCode builtin, all that\u2019s left to do is to unlink the code field from the JavaScript function and set it to the trampoline to 
the Interpreter entry. So, considering that the address of the JavaScript function is in the register rdi , we can obtain the pointer to the SharedFunctionInfo with: // Field read to obtain the 
SharedFunctionInfo. __ movq ( rcx , FieldOperand ( rdi , JSFunction :: kSharedFunctionInfoOffset ) ) ; \u2026and similarly the trampoline with: // Field read to obtain the code object. __ movq ( rcx , 
FieldOperand ( rcx , SharedFunctionInfo :: kCodeOffset ) ) ; Then we can use it to update the function slot for the code pointer: // Update the code field of the function with the trampoline. __ movq ( 
FieldOperand ( rdi , JSFunction :: kCodeOffset ) , rcx ) ; // Write barrier to protect the field. __ RecordWriteField ( rdi , JSFunction :: kCodeOffset , rcx , r15 , kDontSaveFPRegs , OMIT_REMEMBERED_SET , 
OMIT_SMI_CHECK ) ; This produces the same result as before. However, rather than taking care of the unlinking in the Deoptimizer, we need to worry about it during code generation. Hence the handwritten assembly.
The above is how it works in the x64 architecture . We have implemented it for ia32 , arm , arm64 , mips , and mips64 as well. This new technique is already integrated in V8 and, as we\u2019ll discuss later, 
allows for performance improvements. However, it comes with a minor disadvantage: Before, V8 would consider unlinking only upon deoptimization. Now, it has to do so in the activation of all optimized functions. 
Moreover, the approach to check the marked_for_deoptimization bit is not as efficient as it could be, given that we need to do some work to obtain the address of the code object. Note that this happens when 
entering every optimized function. A possible solution for this issue is to keep in a code object a pointer to itself. Rather than doing work to find the address of the code object whenever the function is 
invoked, V8 would do it only once, after its construction. Results # We now look at the performance gains and regressions obtained with this project. General improvements on x64 # The following plot shows us 
some improvements and regressions, relative to the previous commit. Note that the higher, the better. The promises benchmarks are the ones where we see greater improvements, observing almost 33% gain for the 
bluebird-parallel benchmark, and 22.40% for wikipedia . We also observed a few regressions in some benchmarks. This is related to the issue explained above, on checking whether the code is marked for 
deoptimization. We also see improvements in the ARES-6 benchmark suite. Note that in this chart too, the higher the better. These programs used to spend considerable amount of time in GC-related activities. With
lazy unlinking we improve performance by 1.9% overall. The most notable case is the Air steadyState where we get an improvement of around 5.36%. AreWeFastYet results # The performance results for the Octane and 
ARES-6 benchmark suites also showed up on the AreWeFastYet tracker. We looked at these performance results on September 5th, 2017, using the provided default machine (macOS 10.10 64-bit, Mac Pro, shell). 
Cross-browser results on Octane as seen on AreWeFastYet Cross-browser results on ARES-6 as seen on AreWeFastYet Impact on Node.js # We can also see performance improvements in the router-benchmark . The 
following two plots show the number of operations per second of each tested router. Thus the higher the better. We have performed two kinds of experiments with this benchmark suite. Firstly, we ran each test in 
isolation, so that we could see the performance improvement, independently from the remaining tests. Secondly, we ran all tests at once, without switching of the VM, thus simulating an environment where each 
test is integrated with other functionalities. For the first experiment, we saw that the router and express tests perform about twice as many operations than before, in the same amount of time. For the second 
experiment, we saw even greater improvement. In some of the cases, such as routr , server-router and router , the benchmark performs approximately 3.80\u00d7, 3\u00d7 and 2\u00d7 more operations, respectively. 
This happens because V8 accumulates more optimized JavaScript functions, test after test. Thus, whenever executing a given test, if a garbage collection cycle is triggered, V8 has to visit the optimized 
functions from the current test and from the previous ones. Further optimization # Now that V8 does not keep the linked-list of JavaScript functions in the context, we can remove the field next from the 
JSFunction class. Although this is a simple modification, it allows us to save the size of a pointer per function, which represent significant savings in several web pages: Benchmark Kind Memory savings 
(absolute) Memory savings (relative) facebook.com Average effective size 170 KB 3.70% twitter.com Average size of allocated objects 284 KB 1.20% cnn.com Average size of allocated objects 788 KB 1.53% youtube.com
Average size of allocated objects 129 KB 0.79% Acknowledgments # Throughout my internship, I had lots of help from several people, who were always available to answer my many questions. Thus I would like to 
thank the following people: Benedikt Meurer, Jaroslav Sevcik, and Michael Starzinger for discussions on how the compiler and the deoptimizer work, Ulan Degenbaev for helping with the garbage collector whenever I
broke it, and Mathias Bynens, Peter Marshall, Camillo Bruni, and Maya Armyanova for proofreading this article. Finally, this article is my last contribution as a Google intern and I would like to take the 
opportunity to thank everyone in the V8 team, and especially my host, Benedikt Meurer, for hosting me and for giving me the opportunity to work on such an interesting project \u2014 I definitely learned a lot 
and enjoyed my time at Google! Posted by Juliana Franco ( @jupvfranco ), Laziness Expert. Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples
from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-decompile.txt",
    "similarity": 0.441,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-decompile.txt\n\nWhat\u2019s in that .wasm? Introducing: wasm-decompile \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm 
features Research What\u2019s in that .wasm ? Introducing: wasm-decompile Published 27 April 2020 \u00b7 Tagged with WebAssembly tooling We have a growing number of compilers and other tools that generate or 
manipulate .wasm files, and sometimes you might want to have a look inside. Maybe you\u2019re a developer of such a tool, or more directly, you\u2019re a programmer targeting Wasm, and wondering what the 
generated code looks like, for performance or other reasons. Problem is, Wasm is rather low-level, much like actual assembly code. In particular, unlike, say, the JVM, all data structures have been compiled down
to load/store operations, rather than conveniently named classes and fields. Compilers like LLVM can do an impressive amount of transformations that make the generated code look nothing like the code that went 
in. Disassemble or.. decompile? # You could use tools like wasm2wat (part of the WABT toolkit), to transform a .wasm into Wasm\u2019s standard text format, .wat , which is a very faithful but not particularly 
readable representation. For example, a simple C function like a dot product: typedef struct { float x , y , z ; } vec3 ; float dot ( const vec3 * a , const vec3 * b ) { return a -> x * b -> x + a -> y * b -> y 
+ a -> z * b -> z ; } We use clang dot.c -c -target wasm32 -O2 followed by wasm2wat -f dot.o to turn it into this .wat : ( func $dot ( type 0 ) ( param i32 i32 ) ( result f32 ) ( f32 . add ( f32 . add ( f32 . 
mul ( f32 . load ( local .get 0 ) ) ( f32 . load ( local .get 1 ) ) ) ( f32 . mul ( f32 . load offset = 4 ( local .get 0 ) ) ( f32 . load offset = 4 ( local .get 1 ) ) ) ) ( f32 . mul ( f32 . load offset = 8 ( 
local .get 0 ) ) ( f32 . load offset = 8 ( local .get 1 ) ) ) ) ) ) That is a tiny bit of code, but already not great to read for many reasons. Besides the lack of an expression based syntax and general 
verbosity, having to understand data structures as memory loads is not easy. Now imagine looking at the output of a large program, and things will get incomprehensible fast. Instead of wasm2wat , run 
wasm-decompile dot.o , and you get: function dot ( a : { a : float , b : float , c : float } , b : { a : float , b : float , c : float } ) : float { return a . a * b . a + a . b * b . b + a . c * b . c } This 
looks a lot more familiar. Besides an expression based syntax that mimics programming languages you may be familiar with, the decompiler looks at all loads and stores in a function, and tries to infer their 
structure. It then annotates each variable that is used as a pointer with an \"inline\" struct declaration. It does not create named struct declarations since it doesn\u2019t necessarily know which uses of 3 
floats represent the same concept. Decompile to what? # wasm-decompile produces output that tries to look like a \"very average programming language\" while still staying close to the Wasm it represents. Its #1 
goal is readability: help guide readers understand what is in a .wasm with as easy to follow code as possible. Its #2 goal is to still represent Wasm as 1:1 as possible, to not lose its utility as a 
disassembler. Obviously these two goals are not always unifiable. This output is not meant to be an actual programming language and there is currently no way to compile it back into Wasm. Loads and stores # As 
demonstrated above, wasm-decompile looks at all loads and stores over a particular pointer. If they form a continuous set of accesses, it will output one of these \"inline\" struct declarations. If not all 
\"fields\" are accessed, it can\u2019t tell for sure whether this is meant to be a struct, or some other form of unrelated memory access. In that case it falls back to simpler types like float_ptr (if the types 
are the same), or, in the worst case, will output an array access like o|2]:int , which says: o points to int values, and we\u2019re accessing the third one. That last case happens more often than you\u2019d 
think, since Wasm locals function more like registers than variables, so optimized code may share the same pointer for unrelated objects. The decompiler tries to be smart about indexing, and detects patterns 
like (base + (index << 2))|0]:int that result from regular C array indexing operations like base|index] where base points to a 4-byte type. These are very common in code since Wasm has only constant offsets on 
loads and stores. wasm-decompile output transforms them back into base|index]:int . Additionally it knows when absolute addresses refer to the data section. Control flow # Most familiar is Wasm\u2019s if-then 
construct, which translates to a familiar if (cond) { A } else { B } syntax, with the addition that in Wasm it can actually return a value, so it can also represent the ternary cond ? A : B syntax available in 
some languages. The rest of Wasm\u2019s control flow is based on the block and loop blocks, and the br , br_if and br_table jumps. The decompiler stays decently close to these constructs rather than trying to 
infer the while/for/switch constructs they may have come from, since this tends to work better with optimized output. For example, a typical loop in the wasm-decompile output may look like: loop A { // body of 
the loop here. if ( cond ) continue A ; } Here, A is a label that allows multiple of these to be nested. Having an if and continue to control the loop may look slightly foreign compared to a while loop, but it 
corresponds directly to Wasm\u2019s br_if . Blocks are similar, but instead of branching backwards, they branch forwards: block { if ( cond ) break ; // body goes here. } This actually implements an if-then. 
Future versions of the decompiler may translate these into actual if-thens when possible. Wasm\u2019s most surprising control construct is br_table , which implements something like a switch , except using 
nested block s, which tends to be hard to read. The decompiler flattens these to make them slightly easier to follow, for example: br_table | A , B , C , . . D ] ( a ) ; label A : return 0 ; label B : return 1 ;
label C : return 2 ; label D : This is similar to switch on a , with D being the default case. Other fun features # The decompiler: Can pull names from debug or linking information, or generate names itself. 
When using existing names, it has special code to simplify C++ name mangled symbols. Already supports the multi-value proposal, which makes turning things into expressions and statements a bit harder. Additional
variables are used when multiple values are returned. It can even generate names from the contents of data sections. Outputs nice declarations for all Wasm section types, not just code. For example, it tries to 
make data sections readable by outputting them as text when possible. Supports operator precedence (common to most C-style languages) to reduce the () on common expressions. Limitations # Decompiling Wasm is 
fundamentally harder than, say, JVM bytecode. The latter is un-optimized, so relatively faithful to the structure of the original code, and even though names may be missing, refers to unique classes rather than 
just memory locations. In contrast, most .wasm output has been heavily optimized by LLVM and thus has often lost most of its original structure. The output code is very unlike what a programmer would write. That
makes a decompiler for Wasm a bigger challenge to make useful, but that doesn\u2019t mean we shouldn\u2019t try! More # The best way to see more is of course to decompile your own Wasm project! Additionally, a 
more in-depth guide to wasm-decompile is here . Its implementation is in the source files starting with decompiler here (feel free to contribute a PR to make it better!). Some test cases that show further 
examples of differences between .wat and the decompiler are here . Posted by Wouter van Oortmerssen ( @wvo ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise 
noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site 
policies ."
  }
]
[Step 14: Duration 11.35 seconds| Input tokens: 268,619 | Output tokens: 5,941]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 15 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'compiler/regress-841117'}                                                                                                           │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: compiler/regress-841117
[Step 15: Duration 49.34 seconds| Input tokens: 349,309 | Output tokens: 7,189]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 16 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'regress-841117'}                                                                                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: compiler/regress-841117
[Step 16: Duration 14.99 seconds| Input tokens: 430,046 | Output tokens: 8,052]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 17 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'src/deoptimizer.cc file path Deoptimizer FrameDescription CompileLazyDeoptimizedCode CompileLazyDeoptimizedCode builtin file path', 'top_k':   │
│ 5, 'topic_filter': 'v8'}                                                                                                                                                                                        │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.468,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/webassembly-opcode.txt",
    "similarity": 0.457,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/webassembly-opcode.txt\n\nWebAssembly - adding a new opcode \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
WebAssembly - adding a new opcode WebAssembly (Wasm) is a binary instruction format for a stack-based virtual machine. This tutorial walks the reader through implementing a new WebAssembly instruction in V8. 
WebAssembly is implemented in V8 in three parts: the interpreter the baseline compiler (Liftoff) the optimizing compiler (TurboFan) The rest of this document focuses on the TurboFan pipeline, walking through how
to add a new Wasm instruction and implement it in TurboFan. At a high level, Wasm instructions are compiled into a TurboFan graph, and we rely on the TurboFan pipeline to compile the graph into (ultimately) 
machine code. For more on TurboFan, check out the V8 docs . Opcodes/Instructions # Let\u2019s define a new instruction that adds 1 to an int32 (on the top of the stack). Note: A list of instructions supported by
all Wasm implementations can be found in the spec . All Wasm instructions are defined in src/wasm/wasm-opcodes.h . The instructions are grouped roughly by what they do, e.g. control, memory, SIMD, atomic, etc. 
Let\u2019s add our new instruction, I32Add1 , to the FOREACH_SIMPLE_OPCODE section: diff --git a/src/wasm/wasm-opcodes.h b/src/wasm/wasm-opcodes.h index 6970c667e7..867cbf451a 100644 --- 
a/src/wasm/wasm-opcodes.h +++ b/src/wasm/wasm-opcodes.h @@ -96,6 +96,7 @@ bool IsJSCompatibleSignature(const FunctionSig* sig, bool hasBigIntFeature); // Expressions with signatures. #define 
FOREACH_SIMPLE_OPCODE(V) \\ + V(I32Add1, 0xee, i_i) \\ V(I32Eqz, 0x45, i_i) \\ V(I32Eq, 0x46, i_ii) \\ V(I32Ne, 0x47, i_ii) \\ WebAssembly is a binary format, so 0xee specifies the encoding of this instruction. 
In this tutorial we chose 0xee as it is currently unused. Note: Actually adding an instruction to the spec involves work beyond what is described here. We can run a simple unit test for opcodes with: $ 
tools/dev/gm.py x64.debug unittests/WasmOpcodesTest* ... |==========] Running 1 test from 1 test suite. |----------] Global test environment set-up. |----------] 1 test from WasmOpcodesTest | RUN ] 
WasmOpcodesTest.EveryOpcodeHasAName ../../test/unittests/wasm/wasm-opcodes-unittest.cc:27: Failure Value of: false Actual: false Expected: true WasmOpcodes::OpcodeName(kExprI32Add1) == \"unknown\"; plazz halp in
src/wasm/wasm-opcodes.cc | FAILED ] WasmOpcodesTest.EveryOpcodeHasAName This error indicates that we don\u2019t have a name for our new instruction. Adding a name for the new opcode can be done in 
src/wasm/wasm-opcodes.cc : diff --git a/src/wasm/wasm-opcodes.cc b/src/wasm/wasm-opcodes.cc index 5ed664441d..2d4e9554fe 100644 --- a/src/wasm/wasm-opcodes.cc +++ b/src/wasm/wasm-opcodes.cc @@ -75,6 +75,7 @@ 
const char* WasmOpcodes::OpcodeName(WasmOpcode opcode) { // clang-format off // Standard opcodes + CASE_I32_OP(Add1, \"add1\") CASE_INT_OP(Eqz, \"eqz\") CASE_ALL_OP(Eq, \"eq\") CASE_I64x2_OP(Eq, \"eq\") By 
adding our new instruction in FOREACH_SIMPLE_OPCODE , we are skipping a fair amount of work that is done in src/wasm/function-body-decoder-impl.h , which decodes Wasm opcodes and calls into the TurboFan graph 
generator. Thus, depending on what your opcode does, you might have more work to do. We skip this in the interest of brevity. Writing a test for the new opcode # Wasm tests can be found in test/cctest/wasm/ . 
Let\u2019s take a look at test/cctest/wasm/test-run-wasm.cc , where many \u201csimple\u201d opcodes are tested. There are many examples in this file that we can follow. The general setup is: create a WasmRunner 
set up globals to hold result (optional) set up locals as parameters to instruction (optional) build the wasm module run it and compare with an expected output Here\u2019s a simple test for our new opcode: diff 
--git a/test/cctest/wasm/test-run-wasm.cc b/test/cctest/wasm/test-run-wasm.cc index 26df61ceb8..b1ee6edd71 100644 --- a/test/cctest/wasm/test-run-wasm.cc +++ b/test/cctest/wasm/test-run-wasm.cc @@ -28,6 +28,15 
@@ namespace test_run_wasm { #define RET(x) x, kExprReturn #define RET_I8(x) WASM_I32V_2(x), kExprReturn + #define WASM_I32_ADD1(x) x, kExprI32Add1 + + WASM_EXEC_TEST(Int32Add1) { + WasmRunner<int32_t> 
r(execution_tier); + // 10 + 1 + BUILD(r, WASM_I32_ADD1(WASM_I32V_1(10))); + CHECK_EQ(11, r.Call()); + } + WASM_EXEC_TEST(Int32Const) { WasmRunner<int32_t> r(execution_tier); const int32_t kExpectedValue = 
0x11223344; Run the test: $ tools/dev/gm.py x64.debug 'cctest/test-run-wasm-simd/RunWasmTurbofan_I32Add1' ... === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in 
../../src/compiler/wasm-compiler.cc, line 988 # Unsupported opcode 0xee:i32.add1 Tip: Finding the test name can be tricky, since the test definition is behind a macro. Use Code Search to click around to discover
the macro definitions. This error indicates that the compiler does not know of our new instruction. That will change in the next section. Compiling Wasm into TurboFan # In the introduction, we mentioned that 
Wasm instructions are compiled into a TurboFan graph. wasm-compiler.cc is where this happens. Let\u2019s take a look at an example opcode, I32Eqz : switch ( opcode ) { case wasm :: kExprI32Eqz : op = m -> 
Word32Equal ( ) ; return graph ( ) -> NewNode ( op , input , mcgraph ( ) -> Int32Constant ( 0 ) ) ; This switches on the Wasm opcode wasm::kExprI32Eqz , and builds a TurboFan graph consisting of the operation 
Word32Equal with the inputs input , which is the argument to the Wasm instruction, and a constant 0 . The Word32Equal operator is provided by the underlying V8 abstract machine, which is 
architecture-independent. Later in the pipeline, this abstract machine operator will be translated into architecture-dependent assembly. For our new opcode, I32Add1 , we need a graph that adds a constant 1 to 
the input, so we can resuse an existing machine operator, Int32Add , passing it the input, and a constant 1: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index 
f666bbb7c1..399293c03b 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 @@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; 
MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return graph()->NewNode(m->Int32Add(), input, mcgraph()->Int32Constant(1)); case wasm::kExprI32Eqz: op = 
m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); This is enough to get the test passing. However, not all instructions have an existing TurboFan machine operator. In that case 
we have to add this new operator to the machine. Let\u2019s try that. TurboFan machine operators # We want to add the knowledge of Int32Add1 to the TurboFan machine. So let\u2019s pretend that it exists and use 
it first: diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc index f666bbb7c1..1d93601584 100644 --- a/src/compiler/wasm-compiler.cc +++ b/src/compiler/wasm-compiler.cc @@ -713,6 +713,8 
@@ Node* WasmGraphBuilder::Unop(wasm::WasmOpcode opcode, Node* input, const Operator* op; MachineOperatorBuilder* m = mcgraph()->machine(); switch (opcode) { + case wasm::kExprI32Add1: + return 
graph()->NewNode(m->Int32Add1(), input); case wasm::kExprI32Eqz: op = m->Word32Equal(); return graph()->NewNode(op, input, mcgraph()->Int32Constant(0)); Trying to run the same test leads to a compilation failure
that hints at where to make changes: ../../src/compiler/wasm-compiler.cc:717:34: error: no member named 'Int32Add1' in 'v8::internal::compiler::MachineOperatorBuilder'; did you mean 'Int32Add'? return 
graph()->NewNode(m->Int32Add1(), input); ^~~~~~~~~ Int32Add There are a couple of places that needs to be modified to add an operator: src/compiler/machine-operator.cc header src/compiler/machine-operator.h list
of opcodes that the machine understands src/compiler/opcodes.h verifier src/compiler/verifier.cc diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc index 16e838c2aa..fdd6d951f0 
100644 --- a/src/compiler/machine-operator.cc +++ b/src/compiler/machine-operator.cc @@ -136,6 +136,7 @@ MachineType AtomicOpType(Operator const* op) { #define MACHINE_PURE_OP_LIST(V) \\ 
PURE_BINARY_OP_LIST_32(V) \\ PURE_BINARY_OP_LIST_64(V) \\ + V(Int32Add1, Operator::kNoProperties, 1, 0, 1) \\ V(Word32Clz, Operator::kNoProperties, 1, 0, 1) \\ V(Word64Clz, Operator::kNoProperties, 1, 0, 1) \\ 
V(Word32ReverseBytes, Operator::kNoProperties, 1, 0, 1) \\ diff --git a/src/compiler/machine-operator.h b/src/compiler/machine-operator.h index a2b9fce0ee..f95e75a445 100644 --- a/src/compiler/machine-operator.h
+++ b/src/compiler/machine-operator.h @@ -265,6 +265,8 @@ class V8_EXPORT_PRIVATE MachineOperatorBuilder final const Operator* Word32PairShr(); const Operator* Word32PairSar(); + const Operator* Int32Add1(); + 
const Operator* Int32Add(); const Operator* Int32AddWithOverflow(); const Operator* Int32Sub(); diff --git a/src/compiler/opcodes.h b/src/compiler/opcodes.h index ce24a0bd3f..2c8c5ebaca 100644 --- 
a/src/compiler/opcodes.h +++ b/src/compiler/opcodes.h @@ -506,6 +506,7 @@ V(Float64LessThanOrEqual) #define MACHINE_UNOP_32_LIST(V) \\ + V(Int32Add1) \\ V(Word32Clz) \\ V(Word32Ctz) \\ V(Int32AbsWithOverflow) \\
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc index 461aef0023..95251934ce 100644 --- a/src/compiler/verifier.cc +++ b/src/compiler/verifier.cc @@ -1861,6 +1861,7 @@ void 
Verifier::Visitor::Check(Node* node, const AllNodes& all) { case IrOpcode::kSignExtendWord16ToInt64: case IrOpcode::kSignExtendWord32ToInt64: case IrOpcode::kStaticAssert: + case IrOpcode::kInt32Add1: #define 
SIMD_MACHINE_OP_CASE(Name) case IrOpcode::k##Name: MACHINE_SIMD_OP_LIST(SIMD_MACHINE_OP_CASE) Running the test again now gives us a different failure: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # 
Fatal error in ../../src/compiler/backend/instruction-selector.cc, line 2072 # Unexpected operator #289:Int32Add1 @ node #7 Instruction selection # So far we have been working at the TurboFan level, dealing with
(a sea of) nodes in the TurboFan graph. However, at the assembly level, we have instructions and operands. Instruction selection is the process of translating this graph to instructions and operands. The last 
test error indicated that we need something in src/compiler/backend/instruction-selector.cc . This is a big file with a giant switch statement over all the machine opcodes. It calls into architecture specific 
instruction selection, using the visitor pattern to emit instructions for each type of node. Since we added a new TurboFan machine opcode, we need to add it here as well: diff --git 
a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc index 3152b2d41e..7375085649 100644 --- a/src/compiler/backend/instruction-selector.cc +++ 
b/src/compiler/backend/instruction-selector.cc @@ -2067,6 +2067,8 @@ void InstructionSelector::VisitNode(Node* node) { return MarkAsWord32(node), VisitS1x16AnyTrue(node); case IrOpcode::kS1x16AllTrue: return 
MarkAsWord32(node), VisitS1x16AllTrue(node); + case IrOpcode::kInt32Add1: + return MarkAsWord32(node), VisitInt32Add1(node); default: FATAL(\"Unexpected operator #%d:%s @ node #%d\", node->opcode(), 
node->op()->mnemonic(), node->id()); Instruction selection is architecture dependent, so we have to add it to the architecture specific instruction selector files too. For this codelab we only focus on the x64 
architecture, so src/compiler/backend/x64/instruction-selector-x64.cc needs to be modified: diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc 
b/src/compiler/backend/x64/instruction-selector-x64.cc index 2324e119a6..4b55671243 100644 --- a/src/compiler/backend/x64/instruction-selector-x64.cc +++ b/src/compiler/backend/x64/instruction-selector-x64.cc @@
-841,6 +841,11 @@ void InstructionSelector::VisitWord32ReverseBytes(Node* node) { Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); } + void InstructionSelector::VisitInt32Add1(Node*
node) { + X64OperandGenerator g(this); + Emit(kX64Int32Add1, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0))); + } + And we also need to add this new x64-specific opcode, kX64Int32Add1 to 
src/compiler/backend/x64/instruction-codes-x64.h : diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h index 9b8be0e0b5..7f5faeb87b 100644 --- 
a/src/compiler/backend/x64/instruction-codes-x64.h +++ b/src/compiler/backend/x64/instruction-codes-x64.h @@ -12,6 +12,7 @@ namespace compiler { // X64-specific opcodes that specify which assembly sequence to 
emit. // Most opcodes specify a single instruction. #define TARGET_ARCH_OPCODE_LIST(V) \\ + V(X64Int32Add1) \\ V(X64Add) \\ V(X64Add32) \\ V(X64And) \\ Instruction scheduling and code generation # Running our 
test, we see new compilation errors: ../../src/compiler/backend/x64/instruction-scheduler-x64.cc:15:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(instr->arch_opcode()) { ^ 1 error generated. ... ../../src/compiler/backend/x64/code-generator-x64.cc:733:11: error: enumeration value 'kX64Int32Add1' not handled in switch |-Werror,-Wswitch] switch 
(arch_opcode) { ^ 1 error generated. Instruction scheduling takes care of dependencies that instructions may have to allow for more optimization (e.g. instruction reordering). Our new opcode has no data 
dependency, so we can add it simply to: src/compiler/backend/x64/instruction-scheduler-x64.cc : diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc 
b/src/compiler/backend/x64/instruction-scheduler-x64.cc index 79eda7e78d..3667a84577 100644 --- a/src/compiler/backend/x64/instruction-scheduler-x64.cc +++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -13,6 +13,7 @@ bool InstructionScheduler::SchedulerSupported() { return true; } int InstructionScheduler::GetTargetInstructionFlags( const Instruction* instr) const { switch (instr->arch_opcode()) { + case 
kX64Int32Add1: case kX64Add: case kX64Add32: case kX64And: Code generation is where we translate our architecture specific opcodes into assembly. Let\u2019s add a clause to 
src/compiler/backend/x64/code-generator-x64.cc : diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc index 61c3a45a16..9c37ed7464 100644 --- 
a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -731,6 +731,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction( InstructionCode 
opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { 
Handle<Code> code = i.InputCode(0); For now we leave our code generation empty, and we can run the test to make sure everything compiles: === cctest/test-run-wasm/RunWasmTurbofan_Int32Add1 === # # Fatal error in
../../test/cctest/wasm/test-run-wasm.cc, line 37 # Check failed: 11 == r.Call() (11 vs. 10). This failure is expected, since our new instruction is not implemented yet \u2014 it is essentially a no-op, so our 
actual value was unchanged ( 10 ). To implement our opcode, we can use the add assembly instruction: diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc 
index 6c828d6bc4..260c8619f2 100644 --- a/src/compiler/backend/x64/code-generator-x64.cc +++ b/src/compiler/backend/x64/code-generator-x64.cc @@ -744,6 +744,11 @@ CodeGenerator::CodeGenResult 
CodeGenerator::AssembleArchInstruction( InstructionCode opcode = instr->opcode(); ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode); switch (arch_opcode) { + case kX64Int32Add1: { + 
DCHECK_EQ(i.OutputRegister(), i.InputRegister(0)); + __ addl(i.InputRegister(0), Immediate(1)); + break; + } case kArchCallCodeObject: { if (HasImmediateInput(instr, 0)) { Handle<Code> code = i.InputCode(0); And
this makes the test pass: Luckily for us addl is already implemented. If our new opcode required writing a new assembly instruction implementation, we would add it to src/compiler/backend/x64/assembler-x64.cc , 
where the assembly instruction is encoded into bytes and emitted. Tip: To inspect the generated code, we can pass --print-code to cctest . Other architectures # In this codelab we only implemented this new 
instruction for x64. The steps required for other architectures are similar: add TurboFan machine operators, use the platform-dependent files for instruction selection, scheduling, code generation, assembler. 
Tip: if we compile what we have done so far on another target, e.g. arm64, we are likely to get errors in linking. To resolve those errors, add UNIMPLEMENTED() stubs. Branding Terms Privacy Twitter Edit this 
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-decompile.txt",
    "similarity": 0.452,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-decompile.txt\n\nWhat\u2019s in that .wasm? Introducing: wasm-decompile \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm 
features Research What\u2019s in that .wasm ? Introducing: wasm-decompile Published 27 April 2020 \u00b7 Tagged with WebAssembly tooling We have a growing number of compilers and other tools that generate or 
manipulate .wasm files, and sometimes you might want to have a look inside. Maybe you\u2019re a developer of such a tool, or more directly, you\u2019re a programmer targeting Wasm, and wondering what the 
generated code looks like, for performance or other reasons. Problem is, Wasm is rather low-level, much like actual assembly code. In particular, unlike, say, the JVM, all data structures have been compiled down
to load/store operations, rather than conveniently named classes and fields. Compilers like LLVM can do an impressive amount of transformations that make the generated code look nothing like the code that went 
in. Disassemble or.. decompile? # You could use tools like wasm2wat (part of the WABT toolkit), to transform a .wasm into Wasm\u2019s standard text format, .wat , which is a very faithful but not particularly 
readable representation. For example, a simple C function like a dot product: typedef struct { float x , y , z ; } vec3 ; float dot ( const vec3 * a , const vec3 * b ) { return a -> x * b -> x + a -> y * b -> y 
+ a -> z * b -> z ; } We use clang dot.c -c -target wasm32 -O2 followed by wasm2wat -f dot.o to turn it into this .wat : ( func $dot ( type 0 ) ( param i32 i32 ) ( result f32 ) ( f32 . add ( f32 . add ( f32 . 
mul ( f32 . load ( local .get 0 ) ) ( f32 . load ( local .get 1 ) ) ) ( f32 . mul ( f32 . load offset = 4 ( local .get 0 ) ) ( f32 . load offset = 4 ( local .get 1 ) ) ) ) ( f32 . mul ( f32 . load offset = 8 ( 
local .get 0 ) ) ( f32 . load offset = 8 ( local .get 1 ) ) ) ) ) ) That is a tiny bit of code, but already not great to read for many reasons. Besides the lack of an expression based syntax and general 
verbosity, having to understand data structures as memory loads is not easy. Now imagine looking at the output of a large program, and things will get incomprehensible fast. Instead of wasm2wat , run 
wasm-decompile dot.o , and you get: function dot ( a : { a : float , b : float , c : float } , b : { a : float , b : float , c : float } ) : float { return a . a * b . a + a . b * b . b + a . c * b . c } This 
looks a lot more familiar. Besides an expression based syntax that mimics programming languages you may be familiar with, the decompiler looks at all loads and stores in a function, and tries to infer their 
structure. It then annotates each variable that is used as a pointer with an \"inline\" struct declaration. It does not create named struct declarations since it doesn\u2019t necessarily know which uses of 3 
floats represent the same concept. Decompile to what? # wasm-decompile produces output that tries to look like a \"very average programming language\" while still staying close to the Wasm it represents. Its #1 
goal is readability: help guide readers understand what is in a .wasm with as easy to follow code as possible. Its #2 goal is to still represent Wasm as 1:1 as possible, to not lose its utility as a 
disassembler. Obviously these two goals are not always unifiable. This output is not meant to be an actual programming language and there is currently no way to compile it back into Wasm. Loads and stores # As 
demonstrated above, wasm-decompile looks at all loads and stores over a particular pointer. If they form a continuous set of accesses, it will output one of these \"inline\" struct declarations. If not all 
\"fields\" are accessed, it can\u2019t tell for sure whether this is meant to be a struct, or some other form of unrelated memory access. In that case it falls back to simpler types like float_ptr (if the types 
are the same), or, in the worst case, will output an array access like o|2]:int , which says: o points to int values, and we\u2019re accessing the third one. That last case happens more often than you\u2019d 
think, since Wasm locals function more like registers than variables, so optimized code may share the same pointer for unrelated objects. The decompiler tries to be smart about indexing, and detects patterns 
like (base + (index << 2))|0]:int that result from regular C array indexing operations like base|index] where base points to a 4-byte type. These are very common in code since Wasm has only constant offsets on 
loads and stores. wasm-decompile output transforms them back into base|index]:int . Additionally it knows when absolute addresses refer to the data section. Control flow # Most familiar is Wasm\u2019s if-then 
construct, which translates to a familiar if (cond) { A } else { B } syntax, with the addition that in Wasm it can actually return a value, so it can also represent the ternary cond ? A : B syntax available in 
some languages. The rest of Wasm\u2019s control flow is based on the block and loop blocks, and the br , br_if and br_table jumps. The decompiler stays decently close to these constructs rather than trying to 
infer the while/for/switch constructs they may have come from, since this tends to work better with optimized output. For example, a typical loop in the wasm-decompile output may look like: loop A { // body of 
the loop here. if ( cond ) continue A ; } Here, A is a label that allows multiple of these to be nested. Having an if and continue to control the loop may look slightly foreign compared to a while loop, but it 
corresponds directly to Wasm\u2019s br_if . Blocks are similar, but instead of branching backwards, they branch forwards: block { if ( cond ) break ; // body goes here. } This actually implements an if-then. 
Future versions of the decompiler may translate these into actual if-thens when possible. Wasm\u2019s most surprising control construct is br_table , which implements something like a switch , except using 
nested block s, which tends to be hard to read. The decompiler flattens these to make them slightly easier to follow, for example: br_table | A , B , C , . . D ] ( a ) ; label A : return 0 ; label B : return 1 ;
label C : return 2 ; label D : This is similar to switch on a , with D being the default case. Other fun features # The decompiler: Can pull names from debug or linking information, or generate names itself. 
When using existing names, it has special code to simplify C++ name mangled symbols. Already supports the multi-value proposal, which makes turning things into expressions and statements a bit harder. Additional
variables are used when multiple values are returned. It can even generate names from the contents of data sections. Outputs nice declarations for all Wasm section types, not just code. For example, it tries to 
make data sections readable by outputting them as text when possible. Supports operator precedence (common to most C-style languages) to reduce the () on common expressions. Limitations # Decompiling Wasm is 
fundamentally harder than, say, JVM bytecode. The latter is un-optimized, so relatively faithful to the structure of the original code, and even though names may be missing, refers to unique classes rather than 
just memory locations. In contrast, most .wasm output has been heavily optimized by LLVM and thus has often lost most of its original structure. The output code is very unlike what a programmer would write. That
makes a decompiler for Wasm a bigger challenge to make useful, but that doesn\u2019t mean we shouldn\u2019t try! More # The best way to see more is of course to decompile your own Wasm project! Additionally, a 
more in-depth guide to wasm-decompile is here . Its implementation is in the source files starting with decompiler here (feel free to contribute a PR to make it better!). Some test cases that show further 
examples of differences between .wat and the decompiler are here . Posted by Wouter van Oortmerssen ( @wvo ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise 
noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site 
policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/tags/cppgc.txt",
    "similarity": 0.442,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/tags/cppgc.txt\n\nBlog posts tagged \u201ccppgc\u201d \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Blog 
posts tagged \u201ccppgc\u201d Pointer compression in Oilpan 28 November 2022 internals memory cppgc Oilpan library 10 November 2021 internals memory cppgc High-performance garbage collection for C++ 26 May 2020
internals memory cppgc Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on 
this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/profile.txt",
    "similarity": 0.439,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/profile.txt\n\nUsing V8\u2019s sample-based profiler \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Using 
V8\u2019s sample-based profiler V8 has built-in sample-based profiling. Profiling is turned off by default, but can be enabled via the --prof command-line option. The sampler records stacks of both JavaScript 
and C/C++ code. Build # Build the d8 shell following the instructions at Building with GN . Command line # To start profiling, use the --prof option. When profiling, V8 generates a v8.log file which contains 
profiling data. Windows: build \\ Release \\ d8 --prof script.js Other platforms (replace ia32 with x64 if you want to profile the x64 build): out/ia32.release/d8 --prof script.js Process the generated output # 
Log file processing is done using JS scripts running by the d8 shell. For this to work, a d8 binary (or symlink, or d8.exe on Windows) must be in the root of your V8 checkout, or in the path specified by the 
environment variable D8_PATH . Note: this binary is just used to process the log, but not for the actual profiling, so it doesn\u2019t matter which version etc. it is. Make sure d8 used for analysis was not 
built with is_component_build ! Windows: tools \\ windows-tick-processor.bat v8.log Linux: tools/linux-tick-processor v8.log macOS: tools/mac-tick-processor v8.log Web UI for --prof # Preprocess the log with 
--preprocess (to resolve C++ symbols, etc). $V8_PATH /tools/linux-tick-processor --preprocess > v8.json Open tools/profview/index.html in your browser and select the v8.json file there. Example output # 
Statistical profiling result from benchmarks\\v8.log, (4192 ticks, 0 unaccounted, 0 excluded). |Shared libraries]: ticks total nonlib name 9 0.2% 0.0% C:\\WINDOWS\\system32\\ntdll.dll 2 0.0% 0.0% 
C:\\WINDOWS\\system32\\kernel32.dll |JavaScript]: ticks total nonlib name 741 17.7% 17.7% LazyCompile: am3 crypto.js:108 113 2.7% 2.7% LazyCompile: Scheduler.schedule richards.js:188 103 2.5% 2.5% LazyCompile: 
rewrite_nboyer earley-boyer.js:3604 103 2.5% 2.5% LazyCompile: TaskControlBlock.run richards.js:324 96 2.3% 2.3% Builtin: JSConstructCall ... |C++]: ticks total nonlib name 94 2.2% 2.2% 
v8::internal::ScavengeVisitor::VisitPointers 33 0.8% 0.8% v8::internal::SweepSpace 32 0.8% 0.8% v8::internal::Heap::MigrateObject 30 0.7% 0.7% v8::internal::Heap::AllocateArgumentsObject ... |GC]: ticks total 
nonlib name 458 10.9% |Bottom up (heavy) profile]: Note: percentage shows a share of a particular caller in the total amount of its parent calls. Callers occupying less than 2.0% are not shown. ticks parent name
741 17.7% LazyCompile: am3 crypto.js:108 449 60.6% LazyCompile: montReduce crypto.js:583 393 87.5% LazyCompile: montSqrTo crypto.js:603 212 53.9% LazyCompile: bnpExp crypto.js:621 212 100.0% LazyCompile: 
bnModPowInt crypto.js:634 212 100.0% LazyCompile: RSADoPublic crypto.js:1521 181 46.1% LazyCompile: bnModPow crypto.js:1098 181 100.0% LazyCompile: RSADoPrivate crypto.js:1628 ... Profiling web applications # 
Today\u2019s highly optimized virtual machines can run web apps at blazing speed. But one shouldn\u2019t rely only on them to achieve great performance: a carefully optimized algorithm or a less expensive 
function can often reach many-fold speed improvements on all browsers. Chrome DevTools \u2019 CPU Profiler helps you analyze your code\u2019s bottlenecks. But sometimes, you need to go deeper and more granular: 
this is where V8\u2019s internal profiler comes in handy. Let\u2019s use that profiler to examine the Mandelbrot explorer demo that Microsoft released together with IE10. After the demo release, V8 has fixed a 
bug that slowed down the computation unnecessarily (hence the poor performance of Chrome in the demo\u2019s blog post) and further optimized the engine, implementing a faster exp() approximation than what the 
standard system libraries provide. Following these changes, the demo ran 8\u00d7 faster than previously measured in Chrome. But what if you want the code to run faster on all browsers? You should first 
understand what keeps your CPU busy . Run Chrome (Windows and Linux Canary ) with the following command line switches, which causes it to output profiler tick information (in the v8.log file) for the URL you 
specify, which in our case was a local version of the Mandelbrot demo without web workers: ./chrome --js-flags = '--prof' --no-sandbox 'http://localhost:8080/' When preparing the test case, make sure it begins 
its work immediately upon load, and close Chrome when the computation is done (hit Alt+F4), so that you only have the ticks you care about in the log file. Also note that web workers aren\u2019t yet profiled 
correctly with this technique. Then, process the v8.log file with the tick-processor script that ships with V8 (or the new practical web version): v8/tools/linux-tick-processor v8.log Here\u2019s an interesting 
snippet of the processed output that should catch your attention: Statistical profiling result from null, (14306 ticks, 0 unaccounted, 0 excluded). |Shared libraries]: ticks total nonlib name 6326 44.2% 0.0% 
/lib/x86_64-linux-gnu/libm-2.15.so 3258 22.8% 0.0% /.../chrome/src/out/Release/lib/libv8.so 1411 9.9% 0.0% /lib/x86_64-linux-gnu/libpthread-2.15.so 27 0.2% 0.0% /.../chrome/src/out/Release/lib/libwebkit.so The 
top section shows that V8 is spending more time inside an OS-specific system library than in its own code. Let\u2019s look at what\u2019s responsible for it by examining the \u201cbottom up\u201d output section,
where you can read indented lines as \u201cwas called by\u201d (and lines starting with a * mean that the function has been optimized by TurboFan): |Bottom up (heavy) profile]: Note: percentage shows a share of 
a particular caller in the total amount of its parent calls. Callers occupying less than 2.0% are not shown. ticks parent name 6326 44.2% /lib/x86_64-linux-gnu/libm-2.15.so 6325 100.0% LazyCompile: *exp native 
math.js:91 6314 99.8% LazyCompile: *calculateMandelbrot http://localhost:8080/Demo.js:215 More than 44% of the total time is spent executing the exp() function inside a system library ! Adding some overhead for 
calling system libraries, that means about two thirds of the overall time are spent evaluating Math.exp() . If you look at the JavaScript code, you\u2019ll see that exp() is used solely to produce a smooth 
grayscale palette. There are countless ways to produce a smooth grayscale palette, but let\u2019s suppose you really really like exponential gradients. Here is where algorithmic optimization comes into play. 
You\u2019ll notice that exp() is called with an argument in the range -4 < x < 0 , so we can safely replace it with its Taylor approximation for that range, which delivers the same smooth gradient with only a 
multiplication and a couple of divisions: exp(x) \u2248 1 / ( 1 - x + x * x / 2) for -4 < x < 0 Tweaking the algorithm this way boosts the performance by an extra 30% compared to latest Canary and 5\u00d7 to the
system library based Math.exp() on Chrome Canary. This example shows how V8\u2019s internal profiler can help you go deeper into understanding your code bottlenecks, and that a smarter algorithm can push 
performance even further. To find out more about how benchmark that represent today\u2019s complex and demanding web applications, read How V8 measures real-world performance . Branding Terms Privacy Twitter 
Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons 
Attribution 3.0 License . For details, see our site policies ."
  }
]
[Step 17: Duration 7.74 seconds| Input tokens: 510,829 | Output tokens: 8,368]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 18 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'regress-841117'}                                                                                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: compiler/regress-841117
[Step 18: Duration 6.33 seconds| Input tokens: 601,828 | Output tokens: 8,591]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 19 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_knowledge_base' with arguments: {'query': 'file:src/deoptimizer.cc Deoptimizer FrameDescription "FrameDescription" "Deoptimizer" source file path', 'top_k': 5, 'topic_filter': 'v8'}     │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: |
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/gdb-jit.txt",
    "similarity": 0.436,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/gdb-jit.txt\n\nGDB JIT Compilation Interface integration \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research GDB 
JIT Compilation Interface integration GDB JIT Compilation Interface integration allows V8 to provide GDB with the symbol and debugging information for native code emitted from the V8 runtime. When GDB JIT 
Compilation Interface is disabled a typical backtrace in GDB contains frames marked with ?? . These frames correspond to dynamically generated code: #8 0x08281674 in v8::internal::Runtime_SetProperty (args=...) 
at src/runtime.cc:3758 #9 0xf5cae28e in ?? () #10 0xf5cc3a0a in ?? () #11 0xf5cc38f4 in ?? () #12 0xf5cbef19 in ?? () #13 0xf5cb09a2 in ?? () #14 0x0809e0a5 in v8::internal::Invoke (construct=false, func=..., 
receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd46f) at src/execution.cc:97 However enabling GDB JIT Compilation Interface allows GDB to produce more informative stack trace: #6 0x082857fc in 
v8::internal::Runtime_SetProperty (args=...) at src/runtime.cc:3758 #7 0xf5cae28e in ?? () #8 0xf5cc3a0a in loop () at test.js:6 #9 0xf5cc38f4 in test.js () at test.js:13 #10 0xf5cbef19 in ?? () #11 0xf5cb09a2 
in ?? () #12 0x0809e1f9 in v8::internal::Invoke (construct=false, func=..., receiver=..., argc=0, args=0x0, has_pending_exception=0xffffd44f) at src/execution.cc:97 Frames still unknown to GDB correspond to 
native code without source information. See known limitations for more details. GDB JIT Compilation Interface is specified in the GDB documentation: 
https://sourceware.org/gdb/current/onlinedocs/gdb/JIT-Interface.html Prerequisites # V8 v3.0.9 or newer GDB 7.0 or newer Linux OS CPU with Intel-compatible architecture (ia32 or x64) Enabling GDB JIT Compilation
Interface # GDB JIT Compilation Interface is currently excluded from the compilation by default and disabled in runtime. To enable it: Build V8 library with ENABLE_GDB_JIT_INTERFACE defined. If you are using 
scons to build V8 run it with gdbjit=on . Pass --gdbjit flag when starting V8. To check that you have enabled GDB JIT integration correctly try setting a breakpoint on __jit_debug_register_code . This function 
is invoked to notify GDB about new code objects. Known limitations # GDB side of JIT Interface currently (as of GDB 7.2) does not handle registration of code objects very effectively. Each next registration 
takes more time: with 500 registered objects each next registration takes more than 50ms, with 1000 registered code objects - more than 300 ms. This problem was reported to GDB developers but currently there is 
no solution available. To reduce pressure on GDB current implementation of GDB JIT integration operates in two modes: default and full (enabled by --gdbjit-full flag). In default mode V8 notifies GDB only about 
code objects that have source information attached (this usually includes all user scripts). In full - about all generated code objects (stubs, ICs, trampolines). On x64 GDB is unable to properly unwind stack 
without .eh_frame section ( Issue 1053 ) GDB is not notified about code deserialized from the snapshot ( Issue 1054 ) Only Linux OS on Intel-compatible CPUs is supported. For different OSes either a different 
ELF-header should be generated or a completely different object format should be used. Enabling GDB JIT interface disables compacting GC. This is done to reduce pressure on GDB as unregistering and registering 
each moved code object will incur considerable overhead. GDB JIT integration provides only approximate source information. It does not provide any information about local variables, function\u2019s arguments, 
stack layout etc. It does not enable stepping through JavaScript code or setting breakpoint on the given line. However one can set a breakpoint on a function by its name. Branding Terms Privacy Twitter Edit this
page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 
License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/tools/v8.7/ic-explorer.txt",
    "similarity": 0.409,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/tools/v8.7/ic-explorer.txt\n\nV8 IC explorer I C E Your IC-Explorer. 0 uninitialized X no feedback 1 monomorphic ^ recompute handler P 
polymorphic N megamorphic G generic Usage Run your script with --trace_ic and upload v8.log on this page: /path/to/d8 --trace_ic your_script.js Data trace entries: 0 Result Group-Key:"
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/tags/tooling.txt",
    "similarity": 0.404,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/tags/tooling.txt\n\nBlog posts tagged \u201ctooling\u201d \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research 
Blog posts tagged \u201ctooling\u201d Up to 4GB of memory in WebAssembly 14 May 2020 WebAssembly JavaScript tooling What\u2019s in that .wasm ? Introducing: wasm-decompile 27 April 2020 WebAssembly tooling 
Outside the web: standalone WebAssembly binaries using Emscripten 21 November 2019 WebAssembly tooling Emscripten and the LLVM WebAssembly backend 01 July 2019 WebAssembly tooling Branding Terms Privacy Twitter 
Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons 
Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/docs/profile.txt",
    "similarity": 0.404,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/docs/profile.txt\n\nUsing V8\u2019s sample-based profiler \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm features Research Using 
V8\u2019s sample-based profiler V8 has built-in sample-based profiling. Profiling is turned off by default, but can be enabled via the --prof command-line option. The sampler records stacks of both JavaScript 
and C/C++ code. Build # Build the d8 shell following the instructions at Building with GN . Command line # To start profiling, use the --prof option. When profiling, V8 generates a v8.log file which contains 
profiling data. Windows: build \\ Release \\ d8 --prof script.js Other platforms (replace ia32 with x64 if you want to profile the x64 build): out/ia32.release/d8 --prof script.js Process the generated output # 
Log file processing is done using JS scripts running by the d8 shell. For this to work, a d8 binary (or symlink, or d8.exe on Windows) must be in the root of your V8 checkout, or in the path specified by the 
environment variable D8_PATH . Note: this binary is just used to process the log, but not for the actual profiling, so it doesn\u2019t matter which version etc. it is. Make sure d8 used for analysis was not 
built with is_component_build ! Windows: tools \\ windows-tick-processor.bat v8.log Linux: tools/linux-tick-processor v8.log macOS: tools/mac-tick-processor v8.log Web UI for --prof # Preprocess the log with 
--preprocess (to resolve C++ symbols, etc). $V8_PATH /tools/linux-tick-processor --preprocess > v8.json Open tools/profview/index.html in your browser and select the v8.json file there. Example output # 
Statistical profiling result from benchmarks\\v8.log, (4192 ticks, 0 unaccounted, 0 excluded). |Shared libraries]: ticks total nonlib name 9 0.2% 0.0% C:\\WINDOWS\\system32\\ntdll.dll 2 0.0% 0.0% 
C:\\WINDOWS\\system32\\kernel32.dll |JavaScript]: ticks total nonlib name 741 17.7% 17.7% LazyCompile: am3 crypto.js:108 113 2.7% 2.7% LazyCompile: Scheduler.schedule richards.js:188 103 2.5% 2.5% LazyCompile: 
rewrite_nboyer earley-boyer.js:3604 103 2.5% 2.5% LazyCompile: TaskControlBlock.run richards.js:324 96 2.3% 2.3% Builtin: JSConstructCall ... |C++]: ticks total nonlib name 94 2.2% 2.2% 
v8::internal::ScavengeVisitor::VisitPointers 33 0.8% 0.8% v8::internal::SweepSpace 32 0.8% 0.8% v8::internal::Heap::MigrateObject 30 0.7% 0.7% v8::internal::Heap::AllocateArgumentsObject ... |GC]: ticks total 
nonlib name 458 10.9% |Bottom up (heavy) profile]: Note: percentage shows a share of a particular caller in the total amount of its parent calls. Callers occupying less than 2.0% are not shown. ticks parent name
741 17.7% LazyCompile: am3 crypto.js:108 449 60.6% LazyCompile: montReduce crypto.js:583 393 87.5% LazyCompile: montSqrTo crypto.js:603 212 53.9% LazyCompile: bnpExp crypto.js:621 212 100.0% LazyCompile: 
bnModPowInt crypto.js:634 212 100.0% LazyCompile: RSADoPublic crypto.js:1521 181 46.1% LazyCompile: bnModPow crypto.js:1098 181 100.0% LazyCompile: RSADoPrivate crypto.js:1628 ... Profiling web applications # 
Today\u2019s highly optimized virtual machines can run web apps at blazing speed. But one shouldn\u2019t rely only on them to achieve great performance: a carefully optimized algorithm or a less expensive 
function can often reach many-fold speed improvements on all browsers. Chrome DevTools \u2019 CPU Profiler helps you analyze your code\u2019s bottlenecks. But sometimes, you need to go deeper and more granular: 
this is where V8\u2019s internal profiler comes in handy. Let\u2019s use that profiler to examine the Mandelbrot explorer demo that Microsoft released together with IE10. After the demo release, V8 has fixed a 
bug that slowed down the computation unnecessarily (hence the poor performance of Chrome in the demo\u2019s blog post) and further optimized the engine, implementing a faster exp() approximation than what the 
standard system libraries provide. Following these changes, the demo ran 8\u00d7 faster than previously measured in Chrome. But what if you want the code to run faster on all browsers? You should first 
understand what keeps your CPU busy . Run Chrome (Windows and Linux Canary ) with the following command line switches, which causes it to output profiler tick information (in the v8.log file) for the URL you 
specify, which in our case was a local version of the Mandelbrot demo without web workers: ./chrome --js-flags = '--prof' --no-sandbox 'http://localhost:8080/' When preparing the test case, make sure it begins 
its work immediately upon load, and close Chrome when the computation is done (hit Alt+F4), so that you only have the ticks you care about in the log file. Also note that web workers aren\u2019t yet profiled 
correctly with this technique. Then, process the v8.log file with the tick-processor script that ships with V8 (or the new practical web version): v8/tools/linux-tick-processor v8.log Here\u2019s an interesting 
snippet of the processed output that should catch your attention: Statistical profiling result from null, (14306 ticks, 0 unaccounted, 0 excluded). |Shared libraries]: ticks total nonlib name 6326 44.2% 0.0% 
/lib/x86_64-linux-gnu/libm-2.15.so 3258 22.8% 0.0% /.../chrome/src/out/Release/lib/libv8.so 1411 9.9% 0.0% /lib/x86_64-linux-gnu/libpthread-2.15.so 27 0.2% 0.0% /.../chrome/src/out/Release/lib/libwebkit.so The 
top section shows that V8 is spending more time inside an OS-specific system library than in its own code. Let\u2019s look at what\u2019s responsible for it by examining the \u201cbottom up\u201d output section,
where you can read indented lines as \u201cwas called by\u201d (and lines starting with a * mean that the function has been optimized by TurboFan): |Bottom up (heavy) profile]: Note: percentage shows a share of 
a particular caller in the total amount of its parent calls. Callers occupying less than 2.0% are not shown. ticks parent name 6326 44.2% /lib/x86_64-linux-gnu/libm-2.15.so 6325 100.0% LazyCompile: *exp native 
math.js:91 6314 99.8% LazyCompile: *calculateMandelbrot http://localhost:8080/Demo.js:215 More than 44% of the total time is spent executing the exp() function inside a system library ! Adding some overhead for 
calling system libraries, that means about two thirds of the overall time are spent evaluating Math.exp() . If you look at the JavaScript code, you\u2019ll see that exp() is used solely to produce a smooth 
grayscale palette. There are countless ways to produce a smooth grayscale palette, but let\u2019s suppose you really really like exponential gradients. Here is where algorithmic optimization comes into play. 
You\u2019ll notice that exp() is called with an argument in the range -4 < x < 0 , so we can safely replace it with its Taylor approximation for that range, which delivers the same smooth gradient with only a 
multiplication and a couple of divisions: exp(x) \u2248 1 / ( 1 - x + x * x / 2) for -4 < x < 0 Tweaking the algorithm this way boosts the performance by an extra 30% compared to latest Canary and 5\u00d7 to the
system library based Math.exp() on Chrome Canary. This example shows how V8\u2019s internal profiler can help you go deeper into understanding your code bottlenecks, and that a smarter algorithm can push 
performance even further. To find out more about how benchmark that represent today\u2019s complex and demanding web applications, read How V8 measures real-world performance . Branding Terms Privacy Twitter 
Edit this page on GitHub Except as otherwise noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons 
Attribution 3.0 License . For details, see our site policies ."
  },
  {
    "topic": "V8 JavaScript Engine",
    "file": "v8/v8.dev/blog/wasm-decompile.txt",
    "similarity": 0.404,
    "content_preview": "Topic: V8 JavaScript Engine\nFile: v8/v8.dev/blog/wasm-decompile.txt\n\nWhat\u2019s in that .wasm? Introducing: wasm-decompile \u00b7 V8 V8 Show navigation Home Blog Docs Tools JS/Wasm 
features Research What\u2019s in that .wasm ? Introducing: wasm-decompile Published 27 April 2020 \u00b7 Tagged with WebAssembly tooling We have a growing number of compilers and other tools that generate or 
manipulate .wasm files, and sometimes you might want to have a look inside. Maybe you\u2019re a developer of such a tool, or more directly, you\u2019re a programmer targeting Wasm, and wondering what the 
generated code looks like, for performance or other reasons. Problem is, Wasm is rather low-level, much like actual assembly code. In particular, unlike, say, the JVM, all data structures have been compiled down
to load/store operations, rather than conveniently named classes and fields. Compilers like LLVM can do an impressive amount of transformations that make the generated code look nothing like the code that went 
in. Disassemble or.. decompile? # You could use tools like wasm2wat (part of the WABT toolkit), to transform a .wasm into Wasm\u2019s standard text format, .wat , which is a very faithful but not particularly 
readable representation. For example, a simple C function like a dot product: typedef struct { float x , y , z ; } vec3 ; float dot ( const vec3 * a , const vec3 * b ) { return a -> x * b -> x + a -> y * b -> y 
+ a -> z * b -> z ; } We use clang dot.c -c -target wasm32 -O2 followed by wasm2wat -f dot.o to turn it into this .wat : ( func $dot ( type 0 ) ( param i32 i32 ) ( result f32 ) ( f32 . add ( f32 . add ( f32 . 
mul ( f32 . load ( local .get 0 ) ) ( f32 . load ( local .get 1 ) ) ) ( f32 . mul ( f32 . load offset = 4 ( local .get 0 ) ) ( f32 . load offset = 4 ( local .get 1 ) ) ) ) ( f32 . mul ( f32 . load offset = 8 ( 
local .get 0 ) ) ( f32 . load offset = 8 ( local .get 1 ) ) ) ) ) ) That is a tiny bit of code, but already not great to read for many reasons. Besides the lack of an expression based syntax and general 
verbosity, having to understand data structures as memory loads is not easy. Now imagine looking at the output of a large program, and things will get incomprehensible fast. Instead of wasm2wat , run 
wasm-decompile dot.o , and you get: function dot ( a : { a : float , b : float , c : float } , b : { a : float , b : float , c : float } ) : float { return a . a * b . a + a . b * b . b + a . c * b . c } This 
looks a lot more familiar. Besides an expression based syntax that mimics programming languages you may be familiar with, the decompiler looks at all loads and stores in a function, and tries to infer their 
structure. It then annotates each variable that is used as a pointer with an \"inline\" struct declaration. It does not create named struct declarations since it doesn\u2019t necessarily know which uses of 3 
floats represent the same concept. Decompile to what? # wasm-decompile produces output that tries to look like a \"very average programming language\" while still staying close to the Wasm it represents. Its #1 
goal is readability: help guide readers understand what is in a .wasm with as easy to follow code as possible. Its #2 goal is to still represent Wasm as 1:1 as possible, to not lose its utility as a 
disassembler. Obviously these two goals are not always unifiable. This output is not meant to be an actual programming language and there is currently no way to compile it back into Wasm. Loads and stores # As 
demonstrated above, wasm-decompile looks at all loads and stores over a particular pointer. If they form a continuous set of accesses, it will output one of these \"inline\" struct declarations. If not all 
\"fields\" are accessed, it can\u2019t tell for sure whether this is meant to be a struct, or some other form of unrelated memory access. In that case it falls back to simpler types like float_ptr (if the types 
are the same), or, in the worst case, will output an array access like o|2]:int , which says: o points to int values, and we\u2019re accessing the third one. That last case happens more often than you\u2019d 
think, since Wasm locals function more like registers than variables, so optimized code may share the same pointer for unrelated objects. The decompiler tries to be smart about indexing, and detects patterns 
like (base + (index << 2))|0]:int that result from regular C array indexing operations like base|index] where base points to a 4-byte type. These are very common in code since Wasm has only constant offsets on 
loads and stores. wasm-decompile output transforms them back into base|index]:int . Additionally it knows when absolute addresses refer to the data section. Control flow # Most familiar is Wasm\u2019s if-then 
construct, which translates to a familiar if (cond) { A } else { B } syntax, with the addition that in Wasm it can actually return a value, so it can also represent the ternary cond ? A : B syntax available in 
some languages. The rest of Wasm\u2019s control flow is based on the block and loop blocks, and the br , br_if and br_table jumps. The decompiler stays decently close to these constructs rather than trying to 
infer the while/for/switch constructs they may have come from, since this tends to work better with optimized output. For example, a typical loop in the wasm-decompile output may look like: loop A { // body of 
the loop here. if ( cond ) continue A ; } Here, A is a label that allows multiple of these to be nested. Having an if and continue to control the loop may look slightly foreign compared to a while loop, but it 
corresponds directly to Wasm\u2019s br_if . Blocks are similar, but instead of branching backwards, they branch forwards: block { if ( cond ) break ; // body goes here. } This actually implements an if-then. 
Future versions of the decompiler may translate these into actual if-thens when possible. Wasm\u2019s most surprising control construct is br_table , which implements something like a switch , except using 
nested block s, which tends to be hard to read. The decompiler flattens these to make them slightly easier to follow, for example: br_table | A , B , C , . . D ] ( a ) ; label A : return 0 ; label B : return 1 ;
label C : return 2 ; label D : This is similar to switch on a , with D being the default case. Other fun features # The decompiler: Can pull names from debug or linking information, or generate names itself. 
When using existing names, it has special code to simplify C++ name mangled symbols. Already supports the multi-value proposal, which makes turning things into expressions and statements a bit harder. Additional
variables are used when multiple values are returned. It can even generate names from the contents of data sections. Outputs nice declarations for all Wasm section types, not just code. For example, it tries to 
make data sections readable by outputting them as text when possible. Supports operator precedence (common to most C-style languages) to reduce the () on common expressions. Limitations # Decompiling Wasm is 
fundamentally harder than, say, JVM bytecode. The latter is un-optimized, so relatively faithful to the structure of the original code, and even though names may be missing, refers to unique classes rather than 
just memory locations. In contrast, most .wasm output has been heavily optimized by LLVM and thus has often lost most of its original structure. The output code is very unlike what a programmer would write. That
makes a decompiler for Wasm a bigger challenge to make useful, but that doesn\u2019t mean we shouldn\u2019t try! More # The best way to see more is of course to decompile your own Wasm project! Additionally, a 
more in-depth guide to wasm-decompile is here . Its implementation is in the source files starting with decompiler here (feel free to contribute a PR to make it better!). Some test cases that show further 
examples of differences between .wat and the decompiler are here . Posted by Wouter van Oortmerssen ( @wvo ). Retweet this article! Branding Terms Privacy Twitter Edit this page on GitHub Except as otherwise 
noted, any code samples from the V8 project are licensed under V8\u2019s BSD-style license . Other content on this page is licensed under the Creative Commons Attribution 3.0 License . For details, see our site 
policies ."
  }
]
[Step 19: Duration 9.55 seconds| Input tokens: 692,873 | Output tokens: 9,097]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 20 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'search_js_file_name_by_pattern' with arguments: {'pattern': 'OptimizeFunctionOnNextCall'}                                                                                                        │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: No results found
[Step 20: Duration 8.37 seconds| Input tokens: 789,560 | Output tokens: 9,512]
Reached max steps.
[Step 21: Duration 108.34 seconds| Input tokens: 885,679 | Output tokens: 15,708]
Observations: Here is the final answer from your managed agent 'PickSection':
### 1. Task outcome (short version):

Pick a focused fuzzing target: the *lazy-deoptimization / deoptimizer frame-materialization* path 
that bridges optimized TurboFan code and Ignition (baseline) code.  
Concrete starting code regions (high-value hotspots):

- src/deoptimizer.cc — class Deoptimizer, functions that build/translate FrameDescription(s) and 
compute output frames (e.g. FillInputFrameDescription / ComputeOutputFrames / DoComputeOutputFrames
/ TranslateFrameState).  
- src/builtins/builtins-deopt.* (or the builtins file that implements CompileLazyDeoptimizedCode) —
the builtin invoked by optimized-code prologues to lazily unlink deoptimized code and to trigger 
re-entry to interpreter.  
- src/compiler/backend/* (x64/arm64) — code generation for optimized-function prologues that emit 
the marked_for_deoptimization test and the deopt exit (search for code that references 
Code::kMarkedForDeoptimizationBit and CompileLazyDeoptimizedCode).

Why this is promising: very complex cross-component logic (register/stack -> heap/state 
transformations), many invariants (register mapping, object materialization, stack layout), 
frequent churn and past regressions, deopt is exercised heavily and is a known source of subtle 
bugs and crashes.

Three fuzzing entry points to generate JS programs/templates that exercise these paths:
1. Optimized function deopt trigger: create small hot functions and force optimization + 
type-change at runtime via --allow-natives-syntax helper calls (%PrepareFunctionForOptimization / 
%OptimizeFunctionOnNextCall) to cause deopts and lazy unlinking.  
2. Polymorphic call / map-mutation sequence: code sequences with repeated property access and shape
transitions (add/remove properties, change hidden classes, migrate elements) that cause 
checkmaps-with-migration deopts.  
3. Typed-array / numeric conversion edges: tight loops mixing Smi, HeapNumber, -0, NaN, BigInt and 
typed-array views (Float64Array, Int32Array) to trigger speculative arithmetic optimizations and 
frequent deopts.

Recommended flags to run V8 (d8) for effective fuzzing:
--turbo (enable TurboFan), --allow-natives-syntax (for optimization control), --trace-deopt or 
--trace-deopt-verbose (logging), --print-code / --trace-opt (optional). Consider disabling 
aggressive tiers/alternate backends to keep paths deterministic (e.g. disable maglev/concurrent 
tiers if your harness allows).

Artifacts to hand to the next agents:
- representative JS snippets (templates) that cause frequent deopts (see examples below),  
- small AST snippets showing polymorphic property sequences, array transitions, and call sites with
different receiver shapes,  
- IR/control-flow hotspots: samples of TurboFan graphs with Deopt nodes and the Liftoff deopt 
metadata mapping (what values map to which stack slots), and the prologue assembly snippet that 
tests marked_for_deoptimization and calls CompileLazyDeoptimizedCode.

---

### 2. Task outcome (extremely detailed version):

Goal: target and fuzz the Ignition ↔ TurboFan deoptimization/bailout machinery — specifically the 
lazy-deoptimization path where optimized code has a prologue/test that jumps into a builtin 
(CompileLazyDeoptimizedCode) to unlink deoptimized code, and the Deoptimizer machinery that 
captures optimized state, materializes FrameDescription(s), computes output (baseline) frames, and 
transfers control into Liftoff/Interpreter code. This area is highly promising because it is:

- Complex: it serializes CPU register state + stack, translates it to Liftoff’s calling/stack 
layout, materializes heap values when necessary, re-enters interpreter/baseline code in the middle 
of execution. That logic touches many subsystems: code generation, runtime builtins, object layout,
GC, debugger/stack-trace infrastructure.
- Speculative optimization heavy: TurboFan emits many speculative checks and inlined bodies; when 
those checks fail we hit deopt. Fuzzing the deopt path triggers conversions between many 
representations and materialization logic.
- Historically bug-prone: Deoptimizer code and deopt metadata have produced regressions. The lazy 
unlinking mechanism (CompileLazyDeoptimizedCode) is newer and changes where unlinking happens, 
increasing surface area.
- High-coverage payoff: exercising deopt exposes mismatches between TurboFan’s assumptions and 
runtime realities (register mapping, projection nodes, constants that must be materialized).
- Exposed to many JS language features: typed arrays, BigInt, toNumber coercions, property 
mutations, getters/setters, proxies — many of which create frequent deopt points.

Concrete starting code regions (files + functions) — what to look for and why (use these as 
CodeAnalyzer starting points):

1) Deoptimizer core: src/deoptimizer.cc / src/deoptimizer.h
- Key classes/functions to inspect:
  - class Deoptimizer — ctor, ComputeOutputFrames (or DoComputeOutputFrames), 
FillInputFrameDescription, TranslateState, MaterializeHeapObjects / MaterializeOutput or similar 
helpers.
  - FrameDescription class (often in src/frame-description.* or nested inside deoptimizer.cc) — 
methods: SetRegister, SetFrameSlot, GetFrameSize, Iterate or CopyToStack.
- Why: this is the heart of conversion from optimized to unoptimized frames. Bugs here cause 
crashes, incorrect values, or GC/heap corruption during deopt.

2) Lazy unlinking builtin + deopt entry: src/builtins/builtins-deopt.cc (or search for 
CompileLazyDeoptimizedCode in sources)
- Key symbols:
  - CompileLazyDeoptimizedCode builtin implementation — code that unlinks a code object from 
JSFunction (set code field to trampoline) and performs write barrier / record write.
  - DeoptimizationEntry builtin — the entry point used by TurboFan to transfer into the deoptimizer
runtime. (There may also be architecture-specific assembly sequences that call into this builtin.)
- Why: optimized functions’ prologues test Code::kMarkedForDeoptimizationBit and jump to this 
builtin. A malformed prologue, incorrect offset calculation, or buggy builtin logic will produce 
misbehavior.

3) Code generation and prologue emission (TurboFan backend)
- Files to scan:
  - src/compiler/backend/* (instruction-selector, code-generator-x64.cc / code-generator-arm64.cc) 
— search for references to Code::kMarkedForDeoptimizationBit, CompileLazyDeoptimizedCode, or 
“marked_for_deoptimization”.
  - src/codegen or architecture-specific assembler helpers that emit the prologue check.
- Why: the prologue must compute the address of the Code object at runtime (platform-specific 
sequence) and test the bit at the correct offset; small mistakes here yield misdirected calls or 
corrupted stack/register state.

4) Ignition bytecode handlers & interface: src/interpreter/* and src/ignition/* 
(bytecode-handler-assembler)
- Files to inspect:
  - src/interpreter/bytecodes.cc, src/interpreter/interpreter.cc, and the bytecode-handler 
assembler (names vary by V8 version, e.g., src/ignition/bytecode-handler-assembler.cc).
  - Functions/handlers: Call, Construct, CallProperty/CallIC, Ldar/Star/LoadNamedProperty handlers 
(exact names depend on repo).
- Why: deopt re-enters the interpreter at a particular bytecode offset. The deoptimizer must 
realign the stack/registers to what the interpreter expects for that bytecode. Bytecode handlers 
and their calling convention are crucial invariants.

Immediate dependencies to build/understand the region (files / headers / build/runtime flags):

- Source files:
  - src/deoptimizer.* (cc/h)
  - src/frame-description.* (if separate)
  - src/builtins/builtins-deopt.* (or whatever file contains 
CompileLazyDeoptimizedCode/DeoptimizationEntry)
  - src/compiler/deoptimization-data.* and src/compiler/deoptimizer-related data (if present)
  - src/interpreter/* (bytecode handlers)
  - src/compiler/backend/* (code-gen prologue emission)
- Headers:
  - include corresponding local headers: deoptimizer.h, objects.h, code.h (for Code:: offsets), 
frame-description.h
- Build/runtime flags you will want for fuzzing/analysis:
  - --turbo (ensure TurboFan is active / used)
  - --allow-natives-syntax (allows use of %PrepareFunctionForOptimization and optimization helpers 
in generated test drivers)
  - --trace-deopt or --trace-deopt-verbose (to get deopt logs)
  - --trace-opt and --print-opt-code / --print-opt-source (see how TurboFan compiled functions and 
where deopt nodes are)
  - --print-code (to inspect the generated machine code/prologues)
  - Consider: --no-concurrent-maglev or variables to make tiering deterministic — check V8 build 
flags to disable non-deterministic tiers if harness requires reproducible behavior.
- Build configuration:
  - GN args as usual; if working with builtins, ensure embedded builtins are enabled/disabled 
according to your platform needs; copying builtins to the code cage (discussed in blog posts) can 
change indirect branch behavior — but not required for deopt fuzzing.

Three high-value entry points for program generation (target when generating JS 
programs/templates):

1) "Hot function with forced optimization + type-change" template
- Mechanism: use --allow-natives-syntax and %PrepareFunctionForOptimization / 
%OptimizeFunctionOnNextCall.
- Goal: generate a small function that TurboFan optimizes under an assumption (e.g., parameter 
always a Smi), then change the type (pass HeapNumber / Object / BigInt / TypedArray / Proxy) to 
cause deopt.
- Example pattern (to pass to ProgramBuilder):
  function makeHot() {
    var v = 0;
    function f(x) { return x + v; }        // arithmetic + closure -> TurboFan optimizes
    %PrepareFunctionForOptimization(f);
    for (var i = 0; i < 2000; i++) f(1);   // warm up with Smi
    %OptimizeFunctionOnNextCall(f);
    f(1);
    // now call with HeapNumber/BigInt/object to cause deopt
    f(1.5);
  }
- Why effective: triggers deopt, lazy unlinking, frame materialization for arithmetic / ToNumber 
conversion cases.

2) "Polymorphic property access and map migration" template
- Mechanism: create objects with multiple shapes, repeatedly call a function that accesses or 
writes a property that changes hidden class; then mutate the object shape at runtime (add/remove 
properties, convert elements), forcing speculative checks to fail and cause deopt.
- Example pattern:
  function buildPolymorphicAccess() {
    function load(o) { return o.p + 1; }   // property access + arithmetic
    // warm-up with multiple shapes
    var o1 = { p: 1 };
    var o2 = { q: 2, p: 3 }; // different shape
    var o3 = Object.create(null); o3.p = 4;
    for (var i = 0; i < 5000; i++) {
      load(i % 3 === 0 ? o1 : (i % 3 === 1 ? o2 : o3));
    }
    // Now mutate shape aggressively: add property with accessor or convert elements
    o1.x = { nested: 1 };
    o2.p = 3.14;   // change value type
    // Call again with new types -> trigger deopt path
    load(o1);
  }
- Why effective: exercises checkmaps-with-migration and deopt nodes emitted by TurboFan around 
property loads/stores.

3) "Typed-array mixed-mode arithmetic / boundary conditions" template
- Mechanism: tight loops operating on typed arrays and mixing tagged and untagged values; includes 
-0, NaN, BigInt and numeric edge cases. Also use toLocaleString, Math.* conversions that TurboFan 
optimizes speculatively.
- Example pattern:
  function typedArrayLoop() {
    var a = new Float64Array(1024);
    for (var i = 0; i < a.length; i++) a|i] = i + 0.0;
    function work(i) {
      var v = a|i % a.length];
      return v * 2 + (i % 2 ? -0 : 1) + Number({valueOf: () => 3}); // mixing
    }
    for (var i = 0; i < 200000; i++) work(i);
    // Now inject a non-number in array or change backing store (detach) to trigger deopt:
    a|10] = {valueOf: () => 5};   // causes ToNumber coercion and deopt candidates
    work(10);
  }
- Why effective: many TurboFan optimizations assume numeric types; mixing in non-numeric values 
triggers deopts and materialization paths.

Recommended artifacts to pass to CodeAnalyzer and ProgramBuilder:

- Concrete JS templates (above three) expressed as AST fragments (for ProgramBuilder to mutate and 
expand). The templates should be parameterized:
  - hot loop count (so target functions become hot quickly),
  - types injected during deopt trigger (Smi, HeapNumber, BigInt, object, proxy),
  - property shapes sequences (for polymorphism).
- Representative small C++/IR snippets to verify with CodeAnalyzer:
  - A sample TurboFan graph excerpt with a Deopt node (graph-level) — program builder should aim to
produce JS that compiles to graphs containing Deopt nodes.
  - The prologue assembly snippet the code generator emits that computes the Code object address 
and tests KindSpecificFlags1; e.g. pseudo:
      leaq rcx, |rip + label_current]
      testl |rcx + offset], Immediate(1 << Code::kMarkedForDeoptimizationBit)
      jnz CompileLazyDeoptimizedCode
  - The Deoptimizer frame translation pseudo-IR: read registers -> FrameDescription inputs -> 
compute output frames using DeoptimizationData -> push outputs -> fill registers -> jump to 
baseline.
- A short list of "control-flow hotspots" that ProgramBuilder should try to generate via JS:
  - short loops with many iterations (for OSR/optimized compilation),
  - nested calls that may cause inlining (so deopt will need to reconstruct multiple output 
frames),
  - call sites with polymorphic feedback vectors (so TurboFan may generate polymorphic inlined 
bodies and multiple deopt exits),
  - operations that cause boxed/unboxed transitions (Smi <-> HeapNumber) including -0/NaN.

Constraints / Caveats to communicate to next-stage agents:

- You should run generated tests in d8 (V8 shell) built with the same V8 version used for analysis 
— deopt metadata and names change across versions.
- Make sure the testing harness uses --allow-natives-syntax when templates attempt to drive 
optimization (the % helpers). If you cannot use that, rely on running very long warm-ups to get 
functions hot and optimized naturally.
- To reproduce specific deopt traces, enable flags: --trace-deopt (or --trace-deopt-verbose) and 
--print-code; for TurboFan IR: --print-turbo-graph or --print-opt-code if available in your V8 
version.
- If your fuzzing agent mutates AST/JS code aggressively, prefer deterministic flags (disable 
concurrent tiers) to reduce non-determinism. Useful flags to consider: 
--no-concurrent-recompilation, --no-concurrent-maglev (or equivalent), though exact flag names vary
by V8 version.
- Some builtins rely on embedded builtins; on some platforms V8 copies builtins into the code cage 
(affects indirect-call behavior). That influences reproducibility for low-level assembly sequences.
If you need exact prologue offsets, build with stable GN args and one architecture (x64 
recommended).

Practical step-by-step plan for the CodeAnalyzer / ProgramBuilder agents:

1. CodeAnalyzer:
   - Index and open the files listed above; search for Code::kMarkedForDeoptimizationBit, 
CompileLazyDeoptimizedCode, Deoptimizer class, FrameDescription.
   - Extract the exact function names and signatures for your V8 checkout/version (line numbers 
vary; store exact file + function boundaries).
   - Extract the offset calculations used by the prologue (pc offset computation) and the builtin 
name used in the prologue (so ProgramBuilder can expect which builtin will be invoked).
   - Extract deopt metadata structures (DeoptimizationData layout) and the mapping format used to 
compute output frames (to know what the deoptimizer expects).

2. ProgramBuilder:
   - Use the three parameterized JS templates above, generating many variants by mutating:
     - types passed into hot function calls,
     - property shapes sequences,
     - mixing typed/boxed values,
     - number of inlined callees (create functions called repeatedly that are small so TurboFan 
will inline).
   - Add harness wrappers that use (when available) %PrepareFunctionForOptimization and 
%OptimizeFunctionOnNextCall to force optimization and cause deterministic deopt triggers.
   - For each generated program, run d8 with the recommended flags and collect:
     - deopt traces (--trace-deopt-verbose),
     - generated machine code (--print-code),
     - TurboFan IR/optimization logs (--trace-opt / --print-turbo-graph where available).
   - Score programs that reach Deoptimizer, CompileLazyDeoptimizedCode, or contain deopt traces, 
and minimize inputs that reliably trigger deopt logic.

Example short JS artifact for ProgramBuilder to start with (copyable template):

// Requires running with d8 --allow-natives-syntax --turbo
var global_tag = 0;
function makeHotAdd() {
  function f(x) { return x + global_tag + x; }     // arithmetic + closure
  %PrepareFunctionForOptimization(f);
  for (var i = 0; i < 2000; i++) f(1);               // warm-up with Smis
  %OptimizeFunctionOnNextCall(f);
  f(1);                                             // optimized run
  // Now trigger deopt by passing HeapNumber and changing global_tag
  global_tag = 3.14;
  f(1.5);                                           // expected deopt / lazy unlink
}
makeHotAdd();

(This example is intentionally small; ProgramBuilder should generate many variants, including 
nested inlining and property accesses.)

---

### 3. Additional context (if relevant):

- Why focus deopt/lazy-unlink rather than other places (ICs, codegen, register allocator)? 
Deoptimization code touches and integrates many subsystems and is often the last line of defense 
for correctness after aggressive speculative optimization — fuzzing it yields high-value bugs 
(crashes, miscompiled state, GC/heap corruption, wrong return values). It is also exercised by many
real-world behaviors (type changes, polymorphism, out-of-bounds, etc.), so fuzzed inputs are likely
to be realistic and high-impact.
- If you want to broaden coverage after hardening the deopt path, consider branching to:
  - TurboFan instruction selection backend (backend/x64 and backend/arm64) — generate IR patterns 
that force unusual register constraints and provoke backend corner cases.
  - Ignition bytecode handler generation (bytecode handler assembler) — fuzz interpreter handler 
code paths for bytecode miscompilation.
  - Wasm speculative inlining + deopts (wasm deopt tests) — for projects that also fuzz Wasm, 
call_indirect inlining + deopt support has similar complexity and high payoff.
- Useful V8 runtime flags for triage:
  - --trace-deopt-verbose (detailed deopt trace),
  - --trace-turbo, --trace-opt (TurboFan activity),
  - --print-turbo-graph (if available) to inspect TurboFan graphs and Deopt nodes,
  - --always-opt (or its inverse) to change compilation frequency; but prefer precise control via %
helpers for deterministic fuzz runs.
- Building a harness that can run thousands of generated programs and capture deopt/traces is the 
recommended next engineering step. Collect and triage failing runs with the minimal program that 
reproduces the deopt behavior (ideally a minimized AST).

If you want I can:
- produce a small corpus of parameterized JS templates (20–50 variants) ready for ProgramBuilder 
mutations, and  
- generate a checklist (commands + GN args + d8 flags) for the build/test harness for a specific V8
version you point me at.

Reblog: the three prioritized starting code areas to hand off now are:
1. src/deoptimizer.* (Deoptimizer::ComputeOutputFrames / FrameDescription methods).  
2. src/builtins/* (CompileLazyDeoptimizedCode / DeoptimizationEntry builtin).  
3. src/compiler/backend/* and src/interpreter/* (prologue emission and bytecode handlers) — locate 
the exact code that implements the prologue test and the interpreter entry calling convention.

These give the highest immediate yield for directed fuzzing aimed at TurboFan/Ignition interface, 
deoptimization, and lazy unlinking.
[Step 1: Duration 333.64 seconds| Input tokens: 1,417 | Output tokens: 616]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'CodeAnalyzer' with arguments: {'task': 'Analyze the following V8 code regions    │
│ and produce a structured, machine-readable summary that ProgramBuilder can use. Context: target │
│ the Ignition <-> TurboFan lazy-deoptimization path (lazy-unlinking builtin and                  │
│ Deoptimizer/frame materialization). Starting points (from PickSection):\n\n- src/deoptimizer.cc │
│ / src/deoptimizer.h — class Deoptimizer: methods like ComputeOutputFrames /                     │
│ DoComputeOutputFrames / FillInputFrameDescription / TranslateFrameState, FrameDescription class │
│ and materialization helpers.\n- src/builtins/* (file containing CompileLazyDeoptimizedCode and  │
│ DeoptimizationEntry builtin; search for CompileLazyDeoptimizedCode symbol or                    │
│ kMarkedForDeoptimizationBit references).\n- src/compiler/backend/* (code-generation for         │
│ prologues that reference Code::kMarkedForDeoptimizationBit and emit calls to                    │
│ CompileLazyDeoptimizedCode) — architecture-specific files (x64, arm64, maybe arm/ppc) that emit │
│ prologue test sequences.\n- src/interpreter/* and src/ignition/* — interpreter/bytecode handler │
│ functions and calling conventions that deoptimization must reconstruct.\n\nTasks (produce JSON  │
│ as output with the fields below):\n\n1) Files and exact function ranges: For the above files,   │
│ locate concrete file paths in the repo, and list exact function/class names and line ranges (or │
│ byte offsets) for the following targets if present in this V8 checkout: Deoptimizer class and   │
│ its ComputeOutputFrames/FillInputFrameDescription/Materialize helpers; FrameDescription         │
│ methods; builtin implementing CompileLazyDeoptimizedCode (or equivalent runtime builtin used by │
│ prologue), DeoptimizationEntry; architecture-specific prologue emission functions that          │
│ reference Code::kMarkedForDeoptimizationBit and emit jumps/calls to the builtin.\n\n2) Code     │
│ snippets: extract concise code snippets (up to ~200 lines each) of the most relevant functions: │
│ Deoptimizer::ComputeOutputFrames (or equivalent), FrameDescription class, the builtin           │
│ implementation, and the codegen prologue emission function(s). If exact function is large,      │
│ extract the top-level logic and any mapping/offset calculations.\n\n3) Deoptimization metadata  │
│ format: locate and summarize the DeoptimizationData/metadata format used to map input optimized │
│ frame slots/registers to output frame slots; include field names/types and an example mapping   │
│ for a simple inlined call (if available). Explain how FrameDescription is populated and how     │
│ materialization of heap objects is encoded.\n\n4) Prologue emission patterns: for x64 and arm64 │
│ (at least), extract the emitted prologue pattern that tests Code::kMarkedForDeoptimizationBit   │
│ and transfers control to the lazy-deopt builtin. Provide assembly or pseudo-assembly examples   │
│ and the offsets used (e.g., which field in Code object is tested and how the bit is masked).    │
│ Indicate any differences across architectures.\n\n5) Interpreter calling convention & bytecode  │
│ re-entry: list the interpreter/reg/stack conventions that the deoptimizer must satisfy to       │
│ re-enter at a bytecode. Extract the bytecode handler signatures and any assembler/handler       │
│ helpers used to re-enter from deopt.\n\n6) Immediate dependencies and build/runtime flags: list │
│ header files, helper classes, and recommended d8 flags to reproduce traces (--turbo,            │
│ --allow-natives-syntax, --trace-deopt[-verbose], --print-code, --trace-opt, any GN args to      │
│ disable tiers) and any GN build args if relevant.\n\n7) Entry points & suggested artifacts:     │
│ enumerate 3–5 concrete entry points (exported functions, builtins, bytecode handlers, runtime   │
│ functions) ProgramBuilder should target when generating programs, and list artifacts to hand to │
│ ProgramBuilder: AST templates, short IR patterns (Deopt node), prologue assembly snippets, and  │
│ deopt mapping examples.\n\n8) Constraints & caveats: list any version-specific caveats, flags   │
│ required for reproducibility, or known non-determinism (concurrent tiers, maglev), and whether  │
│ embedded builtins/code-cage behavior affects prologue offsets.\n\nOutput format: JSON object    │
│ with keys: files (array of {path, matches:[{symbol, start_line, end_line, snippet_path}]}),     │
│ deopt_metadata (object), prologue_patterns (array), interpreter_convention (object),            │
│ dependencies (array), suggested_flags (array), entry_points (array), artifacts (array),         │
│ constraints (array). If any item cannot be located in this checkout, indicate "not_found" with  │
│ search hints (e.g., search terms). Keep responses precise and actionable for ProgramBuilder to  │
│ synthesize JS templates that trigger deopts and for a human to quickly inspect the exact code   │
│ sites.\n\nPriority: focus first on accuracy of file/function locations and prologue emission    │
│ patterns, then the deopt metadata and FrameDescription mechanics.\n\nReturn only the structured │
│ JSON object as the tool response.'}                                                             │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭──────────────────────────────────── New run - CodeAnalyzer ─────────────────────────────────────╮
│                                                                                                 │
│ You're a helpful agent named 'CodeAnalyzer'.                                                    │
│ You have been submitted this task by your manager.                                              │
│ ---                                                                                             │
│ Task:                                                                                           │
│ Analyze the following V8 code regions and produce a structured, machine-readable summary that   │
│ ProgramBuilder can use. Context: target the Ignition <-> TurboFan lazy-deoptimization path      │
│ (lazy-unlinking builtin and Deoptimizer/frame materialization). Starting points (from           │
│ PickSection):                                                                                   │
│                                                                                                 │
│ - src/deoptimizer.cc / src/deoptimizer.h — class Deoptimizer: methods like ComputeOutputFrames  │
│ / DoComputeOutputFrames / FillInputFrameDescription / TranslateFrameState, FrameDescription     │
│ class and materialization helpers.                                                              │
│ - src/builtins/* (file containing CompileLazyDeoptimizedCode and DeoptimizationEntry builtin;   │
│ search for CompileLazyDeoptimizedCode symbol or kMarkedForDeoptimizationBit references).        │
│ - src/compiler/backend/* (code-generation for prologues that reference                          │
│ Code::kMarkedForDeoptimizationBit and emit calls to CompileLazyDeoptimizedCode) —               │
│ architecture-specific files (x64, arm64, maybe arm/ppc) that emit prologue test sequences.      │
│ - src/interpreter/* and src/ignition/* — interpreter/bytecode handler functions and calling     │
│ conventions that deoptimization must reconstruct.                                               │
│                                                                                                 │
│ Tasks (produce JSON as output with the fields below):                                           │
│                                                                                                 │
│ 1) Files and exact function ranges: For the above files, locate concrete file paths in the      │
│ repo, and list exact function/class names and line ranges (or byte offsets) for the following   │
│ targets if present in this V8 checkout: Deoptimizer class and its                               │
│ ComputeOutputFrames/FillInputFrameDescription/Materialize helpers; FrameDescription methods;    │
│ builtin implementing CompileLazyDeoptimizedCode (or equivalent runtime builtin used by          │
│ prologue), DeoptimizationEntry; architecture-specific prologue emission functions that          │
│ reference Code::kMarkedForDeoptimizationBit and emit jumps/calls to the builtin.                │
│                                                                                                 │
│ 2) Code snippets: extract concise code snippets (up to ~200 lines each) of the most relevant    │
│ functions: Deoptimizer::ComputeOutputFrames (or equivalent), FrameDescription class, the        │
│ builtin implementation, and the codegen prologue emission function(s). If exact function is     │
│ large, extract the top-level logic and any mapping/offset calculations.                         │
│                                                                                                 │
│ 3) Deoptimization metadata format: locate and summarize the DeoptimizationData/metadata format  │
│ used to map input optimized frame slots/registers to output frame slots; include field          │
│ names/types and an example mapping for a simple inlined call (if available). Explain how        │
│ FrameDescription is populated and how materialization of heap objects is encoded.               │
│                                                                                                 │
│ 4) Prologue emission patterns: for x64 and arm64 (at least), extract the emitted prologue       │
│ pattern that tests Code::kMarkedForDeoptimizationBit and transfers control to the lazy-deopt    │
│ builtin. Provide assembly or pseudo-assembly examples and the offsets used (e.g., which field   │
│ in Code object is tested and how the bit is masked). Indicate any differences across            │
│ architectures.                                                                                  │
│                                                                                                 │
│ 5) Interpreter calling convention & bytecode re-entry: list the interpreter/reg/stack           │
│ conventions that the deoptimizer must satisfy to re-enter at a bytecode. Extract the bytecode   │
│ handler signatures and any assembler/handler helpers used to re-enter from deopt.               │
│                                                                                                 │
│ 6) Immediate dependencies and build/runtime flags: list header files, helper classes, and       │
│ recommended d8 flags to reproduce traces (--turbo, --allow-natives-syntax,                      │
│ --trace-deopt[-verbose\], --print-code, --trace-opt, any GN args to disable tiers) and any GN   │
│ build args if relevant.                                                                         │
│                                                                                                 │
│ 7) Entry points & suggested artifacts: enumerate 3–5 concrete entry points (exported functions, │
│ builtins, bytecode handlers, runtime functions) ProgramBuilder should target when generating    │
│ programs, and list artifacts to hand to ProgramBuilder: AST templates, short IR patterns (Deopt │
│ node), prologue assembly snippets, and deopt mapping examples.                                  │
│                                                                                                 │
│ 8) Constraints & caveats: list any version-specific caveats, flags required for                 │
│ reproducibility, or known non-determinism (concurrent tiers, maglev), and whether embedded      │
│ builtins/code-cage behavior affects prologue offsets.                                           │
│                                                                                                 │
│ Output format: JSON object with keys: files (array of {path, matches:[{symbol, start_line,      │
│ end_line, snippet_path}\]}), deopt_metadata (object), prologue_patterns (array),                │
│ interpreter_convention (object), dependencies (array), suggested_flags (array), entry_points    │
│ (array), artifacts (array), constraints (array). If any item cannot be located in this          │
│ checkout, indicate "not_found" with search hints (e.g., search terms). Keep responses precise   │
│ and actionable for ProgramBuilder to synthesize JS templates that trigger deopts and for a      │
│ human to quickly inspect the exact code sites.                                                  │
│                                                                                                 │
│ Priority: focus first on accuracy of file/function locations and prologue emission patterns,    │
│ then the deopt metadata and FrameDescription mechanics.                                         │
│                                                                                                 │
│ Return only the structured JSON object as the tool response.                                    │
│ ---                                                                                             │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer,  │
│ but give as much information as possible to give them a clear understanding of the answer.      │
│                                                                                                 │
│ Your final_answer WILL HAVE to contain these parts:                                             │
│ ### 1. Task outcome (short version):                                                            │
│ ### 2. Task outcome (extremely detailed version):                                               │
│ ### 3. Additional context (if relevant):                                                        │
│                                                                                                 │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to      │
│ final_answer will be lost.                                                                      │
│ And even if your task resolution is not successful, please return as much context as possible,  │
│ so that your manager can act upon this feedback.                                                │
│                                                                                                 │
╰─ LiteLLMModel - gpt-5 ──────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'V8Search' with arguments: {'task': 'Initial survey (Stage 1). Please locate in   │
│ this V8 checkout the core files and symbols around Ignition <-> TurboFan lazy-deoptimization    │
│ path: src/deoptimizer.cc/.h (class Deoptimizer; ComputeOutputFrames, DoComputeOutputFrames,     │
│ FillInputFrameDescription, TranslateFrameState; FrameDescription class and materialization      │
│ helpers); src/builtins/* containing CompileLazyDeoptimizedCode and DeoptimizationEntry          │
│ builtins; src/compiler/backend/* architecture-specific prologues referencing                    │
│ Code::kMarkedForDeoptimizationBit and emitting calls/jumps to CompileLazyDeoptimizedCode;       │
│ src/interpreter/* or src/ignition/* for interpreter calling convention and re-entry handlers.   │
│ Return candidate file paths, symbol names, and IDs for follow-up retrieval.'}                   │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭────────────────────────────────────── New run - V8Search ───────────────────────────────────────╮
│                                                                                                 │
│ You're a helpful agent named 'V8Search'.                                                        │
│ You have been submitted this task by your manager.                                              │
│ ---                                                                                             │
│ Task:                                                                                           │
│ Initial survey (Stage 1). Please locate in this V8 checkout the core files and symbols around   │
│ Ignition <-> TurboFan lazy-deoptimization path: src/deoptimizer.cc/.h (class Deoptimizer;       │
│ ComputeOutputFrames, DoComputeOutputFrames, FillInputFrameDescription, TranslateFrameState;     │
│ FrameDescription class and materialization helpers); src/builtins/* containing                  │
│ CompileLazyDeoptimizedCode and DeoptimizationEntry builtins; src/compiler/backend/*             │
│ architecture-specific prologues referencing Code::kMarkedForDeoptimizationBit and emitting      │
│ calls/jumps to CompileLazyDeoptimizedCode; src/interpreter/* or src/ignition/* for interpreter  │
│ calling convention and re-entry handlers. Return candidate file paths, symbol names, and IDs    │
│ for follow-up retrieval.                                                                        │
│ ---                                                                                             │
│ You're helping your manager solve a wider task: so make sure to not provide a one-line answer,  │
│ but give as much information as possible to give them a clear understanding of the answer.      │
│                                                                                                 │
│ Your final_answer WILL HAVE to contain these parts:                                             │
│ ### 1. Task outcome (short version):                                                            │
│ ### 2. Task outcome (extremely detailed version):                                               │
│ ### 3. Additional context (if relevant):                                                        │
│                                                                                                 │
│ Put all these in your final_answer tool, everything that you do not pass as an argument to      │
│ final_answer will be lost.                                                                      │
│ And even if your task resolution is not successful, please return as much context as possible,  │
│ so that your manager can act upon this feedback.                                                │
│                                                                                                 │
╰─ LiteLLMModel - gpt-5 ──────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'directory': 'src', 'options': '-L 2'}                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'deoptimizer', 'options':              │
│ "--filter=''", 'directory': None}                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'CompileLazyDeoptimizedCode',          │
│ 'options': "--filter=''", 'directory': None}                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'DeoptimizationEntry', 'options':      │
│ "--filter=''", 'directory': None}                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: /bin/sh: line 0: cd: src: No such file or directory
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'kMarkedForDeoptimizationBit',         │
│ 'options': "--filter=''", 'directory': None}                                                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'directory': 'src/compiler/backend', 'options': '-L 2'}   │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'directory': 'src/builtins', 'options': '-L 2'}           │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: /bin/sh: fzf: command not found
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'directory': 'src/interpreter', 'options': '-L 2'}        │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: /bin/sh: fzf: command not found
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'ignition', 'options': "--filter=''",  │
│ 'directory': None}                                                                              │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: /bin/sh: fzf: command not found
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'FrameDescription', 'options':         │
│ "--filter=''", 'directory': None}                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'fuzzy_finder' with arguments: {'pattern': 'TranslateFrameState', 'options':      │
│ "--filter=''", 'directory': None}                                                               │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: /bin/sh: fzf: command not found
Observations: /bin/sh: line 0: cd: src/compiler/backend: No such file or directory
Observations: /bin/sh: line 0: cd: src/builtins: No such file or directory
Observations: /bin/sh: line 0: cd: src/interpreter: No such file or directory
Observations: /bin/sh: fzf: command not found
Observations: /bin/sh: fzf: command not found
Observations: /bin/sh: fzf: command not found
[Step 1: Duration 23.94 seconds| Input tokens: 1,777 | Output tokens: 765]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'class Deoptimizer', 'options': '-n         │
│ --line-number'}                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'class FrameDescription', 'options': '-n    │
│ --line-number'}                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'TranslateFrameState', 'options': '-n       │
│ --line-number'}                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'CompileLazyDeoptimizedCode', 'options':    │
│ '-n --line-number'}                                                                             │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'ComputeOutputFrames', 'options': '-n       │
│ --line-number'}                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'DeoptimizationEntry', 'options': '-n       │
│ --line-number'}                                                                                 │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments: {'pattern': 'kMarkedForDeoptimizationBit', 'options':   │
│ '-n --line-number'}                                                                             │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments: {'directory': 'src', 'options': '-L 2'}                    │
╰─────────────────────────────────────────────────────────────────────────────────────────────────╯
Observations: /bin/sh: line 0: cd: src: No such file or directory
Observations: v8/src/wasm/wasm-builtin-list.h:143:  V(DeoptimizationEntry_Eager)                   
\
v8/src/compiler/backend/code-generator.h:373:  DeoptimizationEntry const& 
GetDeoptimizationEntry(Instruction* instr,
v8/src/compiler/backend/arm64/code-generator-arm64.cc:3976:    __ 
LoadEntryFromBuiltin(Deoptimizer::GetDeoptimizationEntry(kind), scratch);
v8/src/compiler/backend/code-generator.cc:187:  Builtin target = 
Deoptimizer::GetDeoptimizationEntry(deopt_kind);
v8/src/compiler/backend/code-generator.cc:1159:      GetDeoptimizationEntry(instr, 
frame_state_offset).descriptor();
v8/src/compiler/backend/code-generator.cc:1193:DeoptimizationEntry const& 
CodeGenerator::GetDeoptimizationEntry(
v8/src/compiler/backend/code-generator.cc:1197:  return 
instructions()->GetDeoptimizationEntry(state_id);
v8/src/compiler/backend/code-generator.cc:1344:  DeoptimizationEntry const& entry =
v8/src/compiler/backend/code-generator.cc:1345:      GetDeoptimizationEntry(instr, 
frame_state_offset);
v8/src/compiler/backend/instruction.h:1644:class DeoptimizationEntry final {
v8/src/compiler/backend/instruction.h:1646:  DeoptimizationEntry(FrameStateDescriptor* descriptor, 
DeoptimizeKind kind,
v8/src/compiler/backend/instruction.h:1677:using DeoptimizationVector = 
ZoneVector<DeoptimizationEntry>;
v8/src/compiler/backend/instruction.h:1995:  int AddDeoptimizationEntry(FrameStateDescriptor* 
descriptor,
v8/src/compiler/backend/instruction.h:1998:  DeoptimizationEntry const& GetDeoptimizationEntry(int 
deoptimization_id);
v8/src/compiler/backend/instruction.h:1999:  int GetDeoptimizationEntryCount() const {
v8/src/compiler/backend/instruction-selector.cc:1443:  int const state_id = 
sequence()->AddDeoptimizationEntry(
v8/src/compiler/backend/instruction-selector.cc:1659:    int const state_id = 
sequence()->AddDeoptimizationEntry(
v8/src/maglev/maglev-code-generator.cc:1885:    __ 
CallForDeoptimization(Builtin::kDeoptimizationEntry_Eager, deopt_index,
v8/src/maglev/maglev-code-generator.cc:1906:    __ 
CallForDeoptimization(Builtin::kDeoptimizationEntry_Lazy, deopt_index,
v8/src/maglev/arm64/maglev-assembler-arm64.cc:245:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEntry_Eager, scratch);
v8/src/maglev/arm64/maglev-assembler-arm64.cc:250:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEntry_Lazy, scratch);
v8/src/maglev/riscv/maglev-assembler-riscv.cc:237:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEntry_Eager, scratch);
v8/src/maglev/riscv/maglev-assembler-riscv.cc:242:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEntry_Lazy, scratch);
v8/src/compiler/backend/instruction.cc:1092:int InstructionSequence::AddDeoptimizationEntry(
v8/src/compiler/backend/instruction.cc:1097:      DeoptimizationEntry(descriptor, kind, reason, 
node_id, feedback));
v8/src/compiler/backend/instruction.cc:1101:DeoptimizationEntry const& 
InstructionSequence::GetDeoptimizationEntry(
v8/src/deoptimizer/s390/deoptimizer-s390.cc:17:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
v8/src/deoptimizer/s390/deoptimizer-s390.cc:18:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy);
v8/test/cctest/test-macro-assembler-riscv32.cc:1308:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:17:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:18:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy);
v8/src/deoptimizer/frame-description.h:21:// the DeoptimizationEntry builtin (which fills in the 
input state of the
v8/src/deoptimizer/x64/deoptimizer-x64.cc:22:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
v8/src/deoptimizer/x64/deoptimizer-x64.cc:23:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy);
v8/src/deoptimizer/arm/deoptimizer-arm.cc:17:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
v8/src/deoptimizer/arm/deoptimizer-arm.cc:18:ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy);
v8/src/deoptimizer/deoptimizer.cc:647:    // generated by Generate_DeoptimizationEntry_Eager. These
calls have a fixed
v8/src/deoptimizer/deoptimizer.cc:770:Builtin Deoptimizer::GetDeoptimizationEntry(DeoptimizeKind 
kind) {
v8/src/deoptimizer/deoptimizer.cc:773:      return Builtin::kDeoptimizationEntry_Eager;
v8/src/deoptimizer/deoptimizer.cc:775:      return Builtin::kDeoptimizationEntry_Lazy;
v8/src/deoptimizer/deoptimizer.cc:1488:  // been removed from the stack by the DeoptimizationEntry 
builtin, so the
v8/src/deoptimizer/deoptimizer.cc:1651:    // Clear the continuation so that DeoptimizationEntry 
does not push the
v8/src/deoptimizer/deoptimizer.cc:2055:    // authenticated at the end of the DeoptimizationEntry 
builtin.
v8/src/deoptimizer/deoptimizer.cc:2273:    // authenticated at the end of the DeoptimizationEntry 
builtin.
v8/src/deoptimizer/deoptimizer.cc:2404:    // authenticated at the end of the DeoptimizationEntry 
builtin.
v8/src/deoptimizer/deoptimizer.cc:2877:    // authenticated at the end of the DeoptimizationEntry 
builtin.
v8/test/cctest/test-macro-assembler-riscv64.cc:1568:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-loong64.cc:2894:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-assembler-ia32.cc:1533:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-arm.cc:324:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-mips64.cc:1714:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/src/codegen/interface-descriptors.h:797:using DeoptimizationEntryDescriptor = VoidDescriptor;
v8/src/deoptimizer/deoptimizer.h:131:  V8_EXPORT_PRIVATE static Builtin 
GetDeoptimizationEntry(DeoptimizeKind kind);
v8/test/mjsunit/tools/tickprocessor-test-large.log:103:code-creation,Builtin,2,4945,0x557426328bc0,
788,DeoptimizationEntry_Eager
v8/test/mjsunit/tools/tickprocessor-test-large.log:104:code-creation,Builtin,2,4958,0x557426328ee0,
792,DeoptimizationEntry_Soft
v8/test/mjsunit/tools/tickprocessor-test-large.log:105:code-creation,Builtin,2,4970,0x557426329200,
792,DeoptimizationEntry_Bailout
v8/test/mjsunit/tools/tickprocessor-test-large.log:106:code-creation,Builtin,2,4983,0x557426329520,
792,DeoptimizationEntry_Lazy
v8/src/builtins/x64/builtins-x64.cc:3191:  // DeoptimizationEntry enters here.
v8/src/builtins/x64/builtins-x64.cc:4929:void Generate_DeoptimizationEntry(MacroAssembler* masm,
v8/src/builtins/x64/builtins-x64.cc:5152:  // DeoptimizationEntry.
v8/src/builtins/x64/builtins-x64.cc:5178:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/x64/builtins-x64.cc:5179:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/x64/builtins-x64.cc:5182:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/x64/builtins-x64.cc:5183:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/loong64/builtins-loong64.cc:4733:void Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/loong64/builtins-loong64.cc:4939:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/loong64/builtins-loong64.cc:4940:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/loong64/builtins-loong64.cc:4943:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/loong64/builtins-loong64.cc:4944:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/builtins-definitions.h:58:  ASM(DeoptimizationEntry_Eager, DeoptimizationEntry)    
\
v8/src/builtins/builtins-definitions.h:59:  ASM(DeoptimizationEntry_Lazy, DeoptimizationEntry)     
\
v8/src/builtins/ppc/builtins-ppc.cc:4870:void Generate_DeoptimizationEntry(MacroAssembler* masm,
v8/src/builtins/ppc/builtins-ppc.cc:5101:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/ppc/builtins-ppc.cc:5102:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/ppc/builtins-ppc.cc:5105:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/ppc/builtins-ppc.cc:5106:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/s390/builtins-s390.cc:3969:void Generate_DeoptimizationEntry(MacroAssembler* masm,
v8/src/builtins/s390/builtins-s390.cc:4193:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/s390/builtins-s390.cc:4194:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/s390/builtins-s390.cc:4197:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/s390/builtins-s390.cc:4198:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/ia32/builtins-ia32.cc:5194:void Generate_DeoptimizationEntry(MacroAssembler* masm,
v8/src/builtins/ia32/builtins-ia32.cc:5383:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/ia32/builtins-ia32.cc:5384:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/ia32/builtins-ia32.cc:5387:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/ia32/builtins-ia32.cc:5388:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/test/unittests/compiler/backend/turboshaft-instruction-selector-unittest.cc:126:  for (int i = 
0; i < sequence.GetDeoptimizationEntryCount(); i++) {
v8/test/unittests/compiler/backend/turboshaft-instruction-selector-unittest.cc:128:        
sequence.GetDeoptimizationEntry(i).descriptor());
v8/test/unittests/compiler/backend/instruction-selector-unittest.cc:112:  for (int i = 0; i < 
sequence.GetDeoptimizationEntryCount(); i++) {
v8/test/unittests/compiler/backend/instruction-selector-unittest.cc:114:        
sequence.GetDeoptimizationEntry(i).descriptor());
v8/src/builtins/mips64/builtins-mips64.cc:3584:void Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/mips64/builtins-mips64.cc:3868:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/mips64/builtins-mips64.cc:3869:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/mips64/builtins-mips64.cc:3872:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/mips64/builtins-mips64.cc:3873:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/arm/builtins-arm.cc:4679:void Generate_DeoptimizationEntry(MacroAssembler* masm,
v8/src/builtins/arm/builtins-arm.cc:4909:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/arm/builtins-arm.cc:4910:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/arm/builtins-arm.cc:4913:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/arm/builtins-arm.cc:4914:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/test/unittests/assembler/macro-assembler-x64-unittest.cc:1316:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/src/builtins/arm64/builtins-arm64.cc:5287:void Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/arm64/builtins-arm64.cc:5491:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/arm64/builtins-arm64.cc:5492:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/arm64/builtins-arm64.cc:5495:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/arm64/builtins-arm64.cc:5496:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/riscv/builtins-riscv.cc:4754:void Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/riscv/builtins-riscv.cc:4953:void 
Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {
v8/src/builtins/riscv/builtins-riscv.cc:4954:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/riscv/builtins-riscv.cc:4957:void 
Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
v8/src/builtins/riscv/builtins-riscv.cc:4958:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
Observations: v8/src/compiler/backend/code-generator.h:382:  void 
TranslateFrameStateDescriptorOperands(FrameStateDescriptor* desc,
v8/src/compiler/backend/code-generator.cc:1230:void 
CodeGenerator::TranslateFrameStateDescriptorOperands(
v8/src/compiler/backend/code-generator.cc:1338:  TranslateFrameStateDescriptorOperands(descriptor, 
iter);
Observations: v8/src/builtins/arm64/builtins-arm64.cc:5410:  __ Pop(x4, padreg);  // Restore 
deoptimizer object (class Deoptimizer).
v8/src/builtins/riscv/builtins-riscv.cc:4884:  __ pop(a0);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/builtins/arm/builtins-arm.cc:4817:  __ pop(r0);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/builtins/mips64/builtins-mips64.cc:3766:  __ pop(a0);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/builtins/s390/builtins-s390.cc:4106:  __ pop(r2);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/deoptimizer/translated-state.h:73:  friend class Deoptimizer;
v8/src/deoptimizer/translated-state.h:323:  friend class Deoptimizer;
v8/src/deoptimizer/deoptimized-frame-info.h:64:  friend class Deoptimizer;
v8/src/deoptimizer/deoptimizer.h:36:class Deoptimizer : public Malloced {
v8/src/execution/isolate.h:125:class Deoptimizer;
v8/src/builtins/ppc/builtins-ppc.cc:5006:  __ pop(r3);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/builtins/loong64/builtins-loong64.cc:4862:  __ Pop(a0);  // Restore deoptimizer object 
(class Deoptimizer).
Observations: v8/src/codegen/x64/macro-assembler-x64.cc:4699:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to do this we need
v8/src/codegen/x64/macro-assembler-x64.cc:4719:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, not_zero);
v8/src/codegen/loong64/macro-assembler-loong64.cc:4746:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/loong64/macro-assembler-loong64.cc:4774:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, scratch,
v8/src/codegen/s390/macro-assembler-s390.cc:5203:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/s390/macro-assembler-s390.cc:5223:  Jump(BUILTIN_CODE(isolate(), 
CompileLazyDeoptimizedCode),
v8/src/codegen/ppc/macro-assembler-ppc.cc:5370:// jumps to the CompileLazyDeoptimizedCode builtin. 
In order to do this we need
v8/src/codegen/ppc/macro-assembler-ppc.cc:5391:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, cr0);
v8/src/codegen/riscv/macro-assembler-riscv.cc:7235:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/riscv/macro-assembler-riscv.cc:7248:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, kScratchReg,
v8/src/codegen/arm/macro-assembler-arm.cc:2956:// jumps to the CompileLazyDeoptimizedCode builtin. 
In order to do this we need
v8/src/codegen/arm/macro-assembler-arm.cc:2978:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne);
v8/src/codegen/arm64/macro-assembler-arm64.cc:2712:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/arm64/macro-assembler-arm64.cc:2740:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode);
v8/src/compiler/backend/mips64/code-generator-mips64.cc:556:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to do this we need
v8/src/compiler/backend/mips64/code-generator-mips64.cc:568:  __ 
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, kScratchReg,
v8/src/compiler/backend/ia32/code-generator-ia32.cc:657:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/compiler/backend/ia32/code-generator-ia32.cc:681:  __ 
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode);
v8/src/compiler/backend/arm/code-generator-arm.cc:644:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/compiler/backend/riscv/code-generator-riscv.cc:710:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to do this we need
v8/src/compiler/backend/riscv/code-generator-riscv.cc:723:  __ 
TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, kScratchReg,
v8/src/builtins/builtins-lazy-gen.cc:276:TF_BUILTIN(CompileLazyDeoptimizedCode, 
LazyBuiltinsAssembler) {
v8/src/builtins/builtins-definitions.h:102:  TFC(CompileLazyDeoptimizedCode, JSTrampoline)
v8/test/mjsunit/tools/tickprocessor-test-large.log💯code-creation,Builtin,2,4908,0x557426328a00,88,
CompileLazyDeoptimizedCode
Observations: v8/src/deoptimizer/frame-description.h:67:class FrameDescription {
v8/src/deoptimizer/deoptimizer.h:320:  friend class FrameDescription;
Observations: 
v8/tools/testrunner/testproc/stack_utils/testdata/analyze_crash/maybe_is_cleared.txt:18:    
/b/s/w/ir/out/build/libv8.so(v8::internal::Deoptimizer::DoComputeOutputFrames()+0x8ab) 
|0x7f943bbde04b]
v8/src/builtins/arm64/builtins-arm64.cc:5406:    // Call Deoptimizer::ComputeOutputFrames().
v8/src/codegen/external-reference.cc:559:                   Deoptimizer::ComputeOutputFrames)
v8/src/codegen/external-reference.h:126:  V(compute_output_frames_function, 
"Deoptimizer::ComputeOutputFrames()")      \
v8/src/builtins/riscv/builtins-riscv.cc:4879:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/arm/builtins-arm.cc:4812:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/mips64/builtins-mips64.cc:3679:  // Move deopt-obj to a0 for call to 
Deoptimizer::ComputeOutputFrames() below.
v8/src/builtins/mips64/builtins-mips64.cc:3761:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/s390/builtins-s390.cc:4101:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/ppc/builtins-ppc.cc:5001:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/loong64/builtins-loong64.cc:4803:  // Move deopt-obj to a0 for call to 
Deoptimizer::ComputeOutputFrames() below.
v8/src/builtins/loong64/builtins-loong64.cc:4857:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/deoptimizer/deoptimizer.cc:573:void Deoptimizer::ComputeOutputFrames(Deoptimizer* 
deoptimizer) {
v8/src/deoptimizer/deoptimizer.cc:574:  deoptimizer->DoComputeOutputFrames();
v8/src/deoptimizer/deoptimizer.cc:1303:void Deoptimizer::DoComputeOutputFramesWasmImpl() {
v8/src/deoptimizer/deoptimizer.cc:1486:void Deoptimizer::DoComputeOutputFrames() {
v8/src/deoptimizer/deoptimizer.cc:1498:    DoComputeOutputFramesWasmImpl();
v8/src/deoptimizer/deoptimizer.h:129:  static void ComputeOutputFrames(Deoptimizer* deoptimizer);
v8/src/deoptimizer/deoptimizer.h:191:  void DoComputeOutputFrames();
v8/src/deoptimizer/deoptimizer.h:194:  void DoComputeOutputFramesWasmImpl();
Observations: v8/src/compiler/backend/riscv/code-generator-riscv.cc:714://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/compiler/backend/riscv/code-generator-riscv.cc:722:         Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/compiler/backend/arm/code-generator-arm.cc:648://    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/compiler/backend/mips64/code-generator-mips64.cc:560://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/compiler/backend/mips64/code-generator-mips64.cc:567:         Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/compiler/backend/ia32/code-generator-ia32.cc:661://    2. test kMarkedForDeoptimizationBit 
in those flags; and
v8/src/compiler/backend/ia32/code-generator-ia32.cc:671:            Immediate(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm64/macro-assembler-arm64.cc:2716://    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/codegen/arm64/macro-assembler-arm64.cc:2733:    Tbz(scratch.W(), 
Code::kMarkedForDeoptimizationBit, &not_deoptimized);
v8/src/codegen/arm64/macro-assembler-arm64.cc:2739:  Tbz(scratch.W(), 
Code::kMarkedForDeoptimizationBit, &not_deoptimized);
v8/src/codegen/arm64/macro-assembler-arm64.cc:3116:  Tbnz(scratch.W(), 
Code::kMarkedForDeoptimizationBit,
v8/src/codegen/riscv/macro-assembler-riscv.cc:6482:    And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/riscv/macro-assembler-riscv.cc:6720:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/riscv/macro-assembler-riscv.cc:7239://    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/codegen/riscv/macro-assembler-riscv.cc:7247:      Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm/macro-assembler-arm.cc:530:  tst(scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm/macro-assembler-arm.cc:2960://    2. test kMarkedForDeoptimizationBit in those 
flags; and
v8/src/codegen/arm/macro-assembler-arm.cc:2971:    tst(scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm/macro-assembler-arm.cc:3143:  tst(scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/mips64/macro-assembler-mips64.cc:4938:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/ia32/macro-assembler-ia32.cc:738:       Immediate(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/loong64/macro-assembler-loong64.cc:3407:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/loong64/macro-assembler-loong64.cc:4750://    2. test kMarkedForDeoptimizationBit in
those flags; and
v8/src/codegen/loong64/macro-assembler-loong64.cc:4767:    And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/loong64/macro-assembler-loong64.cc:4773:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/s390/macro-assembler-s390.cc:578:  TestBit(scratch, 
Code::kMarkedForDeoptimizationBit, scratch);
v8/src/codegen/s390/macro-assembler-s390.cc:5207://    2. test kMarkedForDeoptimizationBit in those
flags; and
v8/src/codegen/ppc/macro-assembler-ppc.cc:413:  TestBit(scratch1, 
Code::kMarkedForDeoptimizationBit, scratch2);
v8/src/codegen/ppc/macro-assembler-ppc.cc:5374://    2. test kMarkedForDeoptimizationBit in those 
flags; and
v8/src/codegen/ppc/macro-assembler-ppc.cc:5384:    TestBit(r11, Code::kMarkedForDeoptimizationBit);
v8/src/codegen/x64/macro-assembler-x64.cc:3745:  static_assert(Code::kMarkedForDeoptimizationBit >
v8/src/codegen/x64/macro-assembler-x64.cc:3748:        Immediate(1 << 
(Code::kMarkedForDeoptimizationBit -
v8/src/codegen/x64/macro-assembler-x64.cc:4703://    2. test kMarkedForDeoptimizationBit in those 
flags; and
v8/src/objects/code.h:456:  static const int kMarkedForDeoptimizationBit =
[Step 2: Duration 24.01 seconds| Input tokens: 4,028 | Output tokens: 1,758]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'directory': 'v8/src', 'options': '-L 2'}  │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'TranslateFrameState',          │
│ 'options': '-n --line-number', 'directory': │
│ 'v8/src'}                                   │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'FillInputFrameDescription',    │
│ 'options': '-n --line-number', 'directory': │
│ 'v8/src'}                                   │
╰─────────────────────────────────────────────╯
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'class FrameDescription',       │
│ 'options': '-n --line-number', 'directory': │
│ 'v8/src'}                                   │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'CompileLazyDeoptimizedCode',   │
│ 'options': '-n --line-number', 'directory': │
│ 'v8/src'}                                   │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'DeoptimizationEntry',          │
│ 'options': '-n --line-number', 'directory': │
│ 'v8/src'}                                   │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'class Deoptimizer', 'options': │
│ '-n --line-number', 'directory': 'v8/src'}  │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'Interpreter.*entry|Interpreter(Enter|Entry │
│ )|Bytecode.*Entry|Ignition', 'options': '-n │
│ --line-number', 'directory': 'v8/src'}      │
╰─────────────────────────────────────────────╯
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
[Step 3: Duration 20.50 seconds| Input tokens: 
13,158 | Output tokens: 2,743]
━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'options': '-L 2'}                         │
╰─────────────────────────────────────────────╯
Observations: .
└── v8
    ├── AUTHORS
    ├── bazel
    ├── build
    ├── build_overrides
    ├── BUILD.bazel
    ├── BUILD.gn
    ├── buildtools
    ├── CODE_OF_CONDUCT.md
    ├── codereview.settings
    ├── COMMON_OWNERS
    ├── custom_deps
    ├── DEPS
    ├── DIR_METADATA
    ├── docs
    ├── ENG_REVIEW_OWNERS
    ├── gni
    ├── include
    ├── infra
    ├── INFRA_OWNERS
    ├── INTL_OWNERS
    ├── LICENSE
    ├── LICENSE.fdlibm
    ├── LICENSE.strongtalk
    ├── LICENSE.v8
    ├── LOONG_OWNERS
    ├── MIPS_OWNERS
    ├── out
    ├── out.gn
    ├── OWNERS
    ├── PPC_OWNERS
    ├── PRESUBMIT.py
    ├── pyrightconfig.json
    ├── README.md
    ├── RISCV_OWNERS
    ├── S390_OWNERS
    ├── samples
    ├── src
    ├── test
    ├── testing
    ├── third_party
    ├── tools
    ├── v8 -> .
    ├── WATCHLISTS
    └── WORKSPACE

20 directories, 26 files
[Step 4: Duration 16.11 seconds| Input tokens: 
22,737 | Output tokens: 3,280]
━━━━━━━━━━━━━━━━━━━ Step 5 ━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'directory': 'v8/src/deoptimizer',         │
│ 'options': '-L 2'}                          │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'directory': 'v8/src/codegen', 'options':  │
│ '-L 2'}                                     │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'directory': 'v8/src/builtins', 'options': │
│ '-L 2'}                                     │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'directory': 'v8/src/compiler/backend',    │
│ 'options': '-L 2'}                          │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'tree' with arguments:        │
│ {'directory': 'v8/src/interpreter',         │
│ 'options': '-L 2'}                          │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'ComputeOutputFrames|DoComputeOutputFrames| │
│ FillInputFrameDescription|TranslateFrameSta │
│ te', 'options': '-n --line-number',         │
│ 'directory': 'v8/src'}                      │
╰─────────────────────────────────────────────╯
Observations: /bin/sh: line 0: cd: 
v8/src/deoptimizer: No such file or directory
Observations: /bin/sh: line 0: cd: 
v8/src/codegen: No such file or directory
Observations: /bin/sh: line 0: cd: 
v8/src/builtins: No such file or directory
Observations: /bin/sh: line 0: cd: 
v8/src/interpreter: No such file or directory
Observations: /bin/sh: line 0: cd: 
v8/src/compiler/backend: No such file or 
directory
Observations: /bin/sh: line 0: cd: v8/src: No 
such file or directory
[Step 5: Duration 13.77 seconds| Input tokens: 
32,701 | Output tokens: 4,111]
━━━━━━━━━━━━━━━━━━━ Step 6 ━━━━━━━━━━━━━━━━━━━━
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'FillInputFrameDescription|InputFrameDescri │
│ ption', 'options': '-n --line-number'}      │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'InterpreterEntry|InterpreterEntryTrampolin │
│ e|OnStackReplacement|Interpreter.*Trampolin │
│ e|Builtin::kInterpreter', 'options': '-n    │
│ --line-number'}                             │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'CompileLazyDeoptimizedCode|DeoptimizationE │
│ ntry', 'options': '-n --line-number'}       │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'kMarkedForDeoptimizationBit',  │
│ 'options': '-n --line-number'}              │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'class                          │
│ FrameDescription|FrameDescription::',       │
│ 'options': '-n --line-number'}              │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern': 'class                          │
│ Deoptimizer|ComputeOutputFrames\\(|DoComput │
│ eOutputFrames\\(|TranslateFrameState',      │
│ 'options': '-n --line-number'}              │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'Ignition|InterpreterDispatch|BytecodeDispa │
│ tch|Bytecode.*Dispatch|Interpreter.*Reentry │
│ ', 'options': '-n --line-number'}           │
╰─────────────────────────────────────────────╯
╭─────────────────────────────────────────────╮
│ Calling tool: 'ripgrep' with arguments:     │
│ {'pattern':                                 │
│ 'materializ|Materializ|MaterializedObjects| │
│ MaterializedObject|MaterializeObject',      │
│ 'options': '-n --line-number'}              │
╰─────────────────────────────────────────────╯
Observations: 
v8/tools/testrunner/testproc/stack_utils/testda
ta/analyze_crash/maybe_is_cleared.txt:18:    
/b/s/w/ir/out/build/libv8.so(v8::internal::Deop
timizer::DoComputeOutputFrames()+0x8ab) 
|0x7f943bbde04b]
v8/src/deoptimizer/translated-state.h:73:  
friend class Deoptimizer;
v8/src/deoptimizer/translated-state.h:323:  
friend class Deoptimizer;
v8/src/deoptimizer/deoptimizer.cc:573:void 
Deoptimizer::ComputeOutputFrames(Deoptimizer* 
deoptimizer) {
v8/src/deoptimizer/deoptimizer.cc:574:  
deoptimizer->DoComputeOutputFrames();
v8/src/deoptimizer/deoptimizer.cc:1486:void 
Deoptimizer::DoComputeOutputFrames() {
v8/src/deoptimizer/deoptimized-frame-info.h:64:
friend class Deoptimizer;
v8/src/deoptimizer/deoptimizer.h:36:class 
Deoptimizer : public Malloced {
v8/src/deoptimizer/deoptimizer.h:129:  static 
void ComputeOutputFrames(Deoptimizer* 
deoptimizer);
v8/src/deoptimizer/deoptimizer.h:191:  void 
DoComputeOutputFrames();
v8/src/builtins/arm64/builtins-arm64.cc:5406:  
// Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/arm64/builtins-arm64.cc:5410:  
__ Pop(x4, padreg);  // Restore deoptimizer 
object (class Deoptimizer).
v8/src/builtins/riscv/builtins-riscv.cc:4879:  
// Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/riscv/builtins-riscv.cc:4884:  
__ pop(a0);  // Restore deoptimizer object 
(class Deoptimizer).
v8/src/builtins/arm/builtins-arm.cc:4812:  // 
Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/arm/builtins-arm.cc:4817:  __ 
pop(r0);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/builtins/loong64/builtins-loong64.cc:480
3:  // Move deopt-obj to a0 for call to 
Deoptimizer::ComputeOutputFrames() below.
v8/src/builtins/loong64/builtins-loong64.cc:485
7:  // Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/loong64/builtins-loong64.cc:486
2:  __ Pop(a0);  // Restore deoptimizer object 
(class Deoptimizer).
v8/src/builtins/mips64/builtins-mips64.cc:3679:
// Move deopt-obj to a0 for call to 
Deoptimizer::ComputeOutputFrames() below.
v8/src/builtins/mips64/builtins-mips64.cc:3761:
// Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/mips64/builtins-mips64.cc:3766:
__ pop(a0);  // Restore deoptimizer object 
(class Deoptimizer).
v8/src/builtins/ppc/builtins-ppc.cc:5001:  // 
Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/ppc/builtins-ppc.cc:5006:  __ 
pop(r3);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/codegen/external-reference.h:126:  
V(compute_output_frames_function, 
"Deoptimizer::ComputeOutputFrames()")      \
v8/src/builtins/s390/builtins-s390.cc:4101:  //
Call Deoptimizer::ComputeOutputFrames().
v8/src/builtins/s390/builtins-s390.cc:4106:  __
pop(r2);  // Restore deoptimizer object (class 
Deoptimizer).
v8/src/compiler/backend/code-generator.h:382:  
void 
TranslateFrameStateDescriptorOperands(FrameStat
eDescriptor* desc,
v8/src/compiler/backend/code-generator.cc:1230:
void 
CodeGenerator::TranslateFrameStateDescriptorOpe
rands(
v8/src/compiler/backend/code-generator.cc:1338:
TranslateFrameStateDescriptorOperands(descripto
r, iter);
v8/src/execution/isolate.h:125:class 
Deoptimizer;
Observations: None
Observations: 
v8/src/codegen/arm64/macro-assembler-arm64.cc:2
716://    2. test kMarkedForDeoptimizationBit 
in those flags; and
v8/src/codegen/arm64/macro-assembler-arm64.cc:2
733:    Tbz(scratch.W(), 
Code::kMarkedForDeoptimizationBit, 
&not_deoptimized);
v8/src/codegen/arm64/macro-assembler-arm64.cc:2
739:  Tbz(scratch.W(), 
Code::kMarkedForDeoptimizationBit, 
&not_deoptimized);
v8/src/codegen/arm64/macro-assembler-arm64.cc:3
116:  Tbnz(scratch.W(), 
Code::kMarkedForDeoptimizationBit,
v8/src/codegen/riscv/macro-assembler-riscv.cc:6
482:    And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/riscv/macro-assembler-riscv.cc:6
720:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/riscv/macro-assembler-riscv.cc:7
239://    2. test kMarkedForDeoptimizationBit 
in those flags; and
v8/src/codegen/riscv/macro-assembler-riscv.cc:7
247:      Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/x64/macro-assembler-x64.cc:3745:
static_assert(Code::kMarkedForDeoptimizationBit
>
v8/src/codegen/x64/macro-assembler-x64.cc:3748:
Immediate(1 << 
(Code::kMarkedForDeoptimizationBit -
v8/src/codegen/x64/macro-assembler-x64.cc:4703:
//    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/codegen/loong64/macro-assembler-loong64.
cc:3407:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/loong64/macro-assembler-loong64.
cc:4750://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/codegen/loong64/macro-assembler-loong64.
cc:4767:    And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/loong64/macro-assembler-loong64.
cc:4773:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/ia32/macro-assembler-ia32.cc:738
:       Immediate(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/s390/macro-assembler-s390.cc:578
:  TestBit(scratch, 
Code::kMarkedForDeoptimizationBit, scratch);
v8/src/codegen/s390/macro-assembler-s390.cc:520
7://    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/codegen/ppc/macro-assembler-ppc.cc:413: 
TestBit(scratch1, 
Code::kMarkedForDeoptimizationBit, scratch2);
v8/src/codegen/ppc/macro-assembler-ppc.cc:5374:
//    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/codegen/ppc/macro-assembler-ppc.cc:5384:
TestBit(r11, 
Code::kMarkedForDeoptimizationBit);
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:714://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:722:         Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/compiler/backend/arm/code-generator-arm.
cc:648://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:560://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:567:         Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/compiler/backend/ia32/code-generator-ia3
2.cc:661://    2. test 
kMarkedForDeoptimizationBit in those flags; and
v8/src/compiler/backend/ia32/code-generator-ia3
2.cc:671:            Immediate(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/mips64/macro-assembler-mips64.cc
:4938:  And(scratch, scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm/macro-assembler-arm.cc:530: 
tst(scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm/macro-assembler-arm.cc:2960:
//    2. test kMarkedForDeoptimizationBit in 
those flags; and
v8/src/codegen/arm/macro-assembler-arm.cc:2971:
tst(scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/codegen/arm/macro-assembler-arm.cc:3143:
tst(scratch, Operand(1 << 
Code::kMarkedForDeoptimizationBit));
v8/src/objects/code.h:456:  static const int 
kMarkedForDeoptimizationBit =
Observations: 
v8/src/wasm/wasm-builtin-list.h:143:  
V(DeoptimizationEntry_Eager)                   
\
v8/src/compiler/backend/code-generator.h:373:  
DeoptimizationEntry const& 
GetDeoptimizationEntry(Instruction* instr,
v8/src/compiler/backend/instruction.cc:1092:int
InstructionSequence::AddDeoptimizationEntry(
v8/src/compiler/backend/instruction.cc:1097:   
DeoptimizationEntry(descriptor, kind, reason, 
node_id, feedback));
v8/src/compiler/backend/instruction.cc:1101:Deo
ptimizationEntry const& 
InstructionSequence::GetDeoptimizationEntry(
v8/src/compiler/backend/ia32/code-generator-ia3
2.cc:657:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to
do this we need
v8/src/compiler/backend/ia32/code-generator-ia3
2.cc:681:  __ 
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode);
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:3976:    __ 
LoadEntryFromBuiltin(Deoptimizer::GetDeoptimiza
tionEntry(kind), scratch);
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:556:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to
do this we need
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:568:  __ 
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, ne, kScratchReg,
v8/src/compiler/backend/instruction-selector.cc
:1443:  int const state_id = 
sequence()->AddDeoptimizationEntry(
v8/src/compiler/backend/instruction-selector.cc
:1659:    int const state_id = 
sequence()->AddDeoptimizationEntry(
v8/src/compiler/backend/arm/code-generator-arm.
cc:644:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to
do this we need
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:710:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to
do this we need
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:723:  __ 
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, ne, kScratchReg,
v8/src/compiler/backend/instruction.h:1644:clas
s DeoptimizationEntry final {
v8/src/compiler/backend/instruction.h:1646:  
DeoptimizationEntry(FrameStateDescriptor* 
descriptor, DeoptimizeKind kind,
v8/src/compiler/backend/instruction.h:1677:usin
g DeoptimizationVector = 
ZoneVector<DeoptimizationEntry>;
v8/src/compiler/backend/instruction.h:1995:  
int 
AddDeoptimizationEntry(FrameStateDescriptor* 
descriptor,
v8/src/compiler/backend/instruction.h:1998:  
DeoptimizationEntry const& 
GetDeoptimizationEntry(int deoptimization_id);
v8/src/compiler/backend/instruction.h:1999:  
int GetDeoptimizationEntryCount() const {
v8/src/compiler/backend/code-generator.cc:187: 
Builtin target = 
Deoptimizer::GetDeoptimizationEntry(deopt_kind)
;
v8/src/compiler/backend/code-generator.cc:1159:
GetDeoptimizationEntry(instr, 
frame_state_offset).descriptor();
v8/src/compiler/backend/code-generator.cc:1193:
DeoptimizationEntry const& 
CodeGenerator::GetDeoptimizationEntry(
v8/src/compiler/backend/code-generator.cc:1197:
return 
instructions()->GetDeoptimizationEntry(state_id
);
v8/src/compiler/backend/code-generator.cc:1344:
DeoptimizationEntry const& entry =
v8/src/compiler/backend/code-generator.cc:1345:
GetDeoptimizationEntry(instr, 
frame_state_offset);
v8/src/maglev/maglev-code-generator.cc:1885:   
__ 
CallForDeoptimization(Builtin::kDeoptimizationE
ntry_Eager, deopt_index,
v8/src/maglev/maglev-code-generator.cc:1906:   
__ 
CallForDeoptimization(Builtin::kDeoptimizationE
ntry_Lazy, deopt_index,
v8/src/maglev/arm64/maglev-assembler-arm64.cc:2
45:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEn
try_Eager, scratch);
v8/src/maglev/arm64/maglev-assembler-arm64.cc:2
50:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEn
try_Lazy, scratch);
v8/src/maglev/riscv/maglev-assembler-riscv.cc:2
37:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEn
try_Eager, scratch);
v8/src/maglev/riscv/maglev-assembler-riscv.cc:2
42:    
LoadEntryFromBuiltin(Builtin::kDeoptimizationEn
try_Lazy, scratch);
v8/test/cctest/test-macro-assembler-riscv32.cc:
1308:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-arm.cc:324:
Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-mips64.cc:1
714:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-loong64.cc:
2894:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-macro-assembler-riscv64.cc:
1568:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/test/cctest/test-assembler-ia32.cc:1533:    
Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
v8/src/codegen/x64/macro-assembler-x64.cc:4699:
// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/x64/macro-assembler-x64.cc:4719:
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, not_zero);
v8/src/codegen/loong64/macro-assembler-loong64.
cc:4746:// jumps to the 
CompileLazyDeoptimizedCode builtin. In order to
do this we need
v8/src/codegen/loong64/macro-assembler-loong64.
cc:4774:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, ne, scratch,
v8/src/builtins/x64/builtins-x64.cc:3191:  // 
DeoptimizationEntry enters here.
v8/src/builtins/x64/builtins-x64.cc:4929:void 
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/x64/builtins-x64.cc:5152:  // 
DeoptimizationEntry.
v8/src/builtins/x64/builtins-x64.cc:5178:void 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/x64/builtins-x64.cc:5179:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/x64/builtins-x64.cc:5182:void 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/x64/builtins-x64.cc:5183:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/loong64/builtins-loong64.cc:473
3:void 
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/loong64/builtins-loong64.cc:493
9:void 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/loong64/builtins-loong64.cc:494
0:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/loong64/builtins-loong64.cc:494
3:void 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/loong64/builtins-loong64.cc:494
4:  Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/builtins-definitions.h:58:  
ASM(DeoptimizationEntry_Eager, 
DeoptimizationEntry)                       \
v8/src/builtins/builtins-definitions.h:59:  
ASM(DeoptimizationEntry_Lazy, 
DeoptimizationEntry)                        \
v8/src/builtins/builtins-definitions.h:102:  
TFC(CompileLazyDeoptimizedCode, JSTrampoline)
v8/src/codegen/arm64/macro-assembler-arm64.cc:2
712:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/arm64/macro-assembler-arm64.cc:2
740:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode);
v8/src/codegen/interface-descriptors.h:797:usin
g DeoptimizationEntryDescriptor = 
VoidDescriptor;
v8/test/mjsunit/tools/tickprocessor-test-large.
log💯code-creation,Builtin,2,4908,0x557426328a0
0,88,CompileLazyDeoptimizedCode
v8/test/mjsunit/tools/tickprocessor-test-large.
log:103:code-creation,Builtin,2,4945,0x55742632
8bc0,788,DeoptimizationEntry_Eager
v8/test/mjsunit/tools/tickprocessor-test-large.
log:104:code-creation,Builtin,2,4958,0x55742632
8ee0,792,DeoptimizationEntry_Soft
v8/test/mjsunit/tools/tickprocessor-test-large.
log:105:code-creation,Builtin,2,4970,0x55742632
9200,792,DeoptimizationEntry_Bailout
v8/test/mjsunit/tools/tickprocessor-test-large.
log:106:code-creation,Builtin,2,4983,0x55742632
9520,792,DeoptimizationEntry_Lazy
v8/src/builtins/arm64/builtins-arm64.cc:5287:vo
id Generate_DeoptimizationEntry(MacroAssembler*
masm,
v8/src/builtins/arm64/builtins-arm64.cc:5491:vo
id 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/arm64/builtins-arm64.cc:5492:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/arm64/builtins-arm64.cc:5495:vo
id 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/arm64/builtins-arm64.cc:5496:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/deoptimizer/arm/deoptimizer-arm.cc:17:AS
SERT_OFFSET(Builtin::kDeoptimizationEntry_Eager
);
v8/src/deoptimizer/arm/deoptimizer-arm.cc:18:AS
SERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy)
;
v8/src/deoptimizer/deoptimizer.cc:647:    // 
generated by 
Generate_DeoptimizationEntry_Eager. These calls
have a fixed
v8/src/deoptimizer/deoptimizer.cc:770:Builtin 
Deoptimizer::GetDeoptimizationEntry(DeoptimizeK
ind kind) {
v8/src/deoptimizer/deoptimizer.cc:773:      
return Builtin::kDeoptimizationEntry_Eager;
v8/src/deoptimizer/deoptimizer.cc:775:      
return Builtin::kDeoptimizationEntry_Lazy;
v8/src/deoptimizer/deoptimizer.cc:1488:  // 
been removed from the stack by the 
DeoptimizationEntry builtin, so the
v8/src/deoptimizer/deoptimizer.cc:1651:    // 
Clear the continuation so that 
DeoptimizationEntry does not push the
v8/src/deoptimizer/deoptimizer.cc:2055:    // 
authenticated at the end of the 
DeoptimizationEntry builtin.
v8/src/deoptimizer/deoptimizer.cc:2273:    // 
authenticated at the end of the 
DeoptimizationEntry builtin.
v8/src/deoptimizer/deoptimizer.cc:2404:    // 
authenticated at the end of the 
DeoptimizationEntry builtin.
v8/src/deoptimizer/deoptimizer.cc:2877:    // 
authenticated at the end of the 
DeoptimizationEntry builtin.
v8/src/deoptimizer/s390/deoptimizer-s390.cc:17:
ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eag
er);
v8/src/deoptimizer/s390/deoptimizer-s390.cc:18:
ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Laz
y);
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:17:AS
SERT_OFFSET(Builtin::kDeoptimizationEntry_Eager
);
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:18:AS
SERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy)
;
v8/src/deoptimizer/frame-description.h:21:// 
the DeoptimizationEntry builtin (which fills in
the input state of the
v8/src/deoptimizer/x64/deoptimizer-x64.cc:22:AS
SERT_OFFSET(Builtin::kDeoptimizationEntry_Eager
);
v8/src/deoptimizer/x64/deoptimizer-x64.cc:23:AS
SERT_OFFSET(Builtin::kDeoptimizationEntry_Lazy)
;
v8/src/deoptimizer/deoptimizer.h:131:  
V8_EXPORT_PRIVATE static Builtin 
GetDeoptimizationEntry(DeoptimizeKind kind);
v8/src/builtins/riscv/builtins-riscv.cc:4754:vo
id Generate_DeoptimizationEntry(MacroAssembler*
masm,
v8/src/builtins/riscv/builtins-riscv.cc:4953:vo
id 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/riscv/builtins-riscv.cc:4954:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/riscv/builtins-riscv.cc:4957:vo
id 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/riscv/builtins-riscv.cc:4958:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/ppc/builtins-ppc.cc:4870:void 
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/ppc/builtins-ppc.cc:5101:void 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/ppc/builtins-ppc.cc:5102:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/ppc/builtins-ppc.cc:5105:void 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/ppc/builtins-ppc.cc:5106:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/s390/builtins-s390.cc:3969:void
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/s390/builtins-s390.cc:4193:void
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/s390/builtins-s390.cc:4194:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/s390/builtins-s390.cc:4197:void
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/s390/builtins-s390.cc:4198:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/arm/builtins-arm.cc:4679:void 
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/arm/builtins-arm.cc:4909:void 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/arm/builtins-arm.cc:4910:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/arm/builtins-arm.cc:4913:void 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/arm/builtins-arm.cc:4914:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/builtins/builtins-lazy-gen.cc:276:TF_BUI
LTIN(CompileLazyDeoptimizedCode, 
LazyBuiltinsAssembler) {
v8/src/codegen/s390/macro-assembler-s390.cc:520
3:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/s390/macro-assembler-s390.cc:522
3:  Jump(BUILTIN_CODE(isolate(), 
CompileLazyDeoptimizedCode),
v8/src/codegen/ppc/macro-assembler-ppc.cc:5370:
// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/ppc/macro-assembler-ppc.cc:5391:
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, ne, cr0);
v8/src/builtins/ia32/builtins-ia32.cc:5194:void
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/ia32/builtins-ia32.cc:5383:void
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/ia32/builtins-ia32.cc:5384:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/ia32/builtins-ia32.cc:5387:void
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/ia32/builtins-ia32.cc:5388:  
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/src/codegen/arm/macro-assembler-arm.cc:2956:
// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/arm/macro-assembler-arm.cc:2978:
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, ne);
v8/src/codegen/riscv/macro-assembler-riscv.cc:7
235:// jumps to the CompileLazyDeoptimizedCode 
builtin. In order to do this we need
v8/src/codegen/riscv/macro-assembler-riscv.cc:7
248:  
TailCallBuiltin(Builtin::kCompileLazyDeoptimize
dCode, ne, kScratchReg,
v8/src/builtins/mips64/builtins-mips64.cc:3584:
void 
Generate_DeoptimizationEntry(MacroAssembler* 
masm,
v8/src/builtins/mips64/builtins-mips64.cc:3868:
void 
Builtins::Generate_DeoptimizationEntry_Eager(Ma
croAssembler* masm) {
v8/src/builtins/mips64/builtins-mips64.cc:3869:
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kEager);
v8/src/builtins/mips64/builtins-mips64.cc:3872:
void 
Builtins::Generate_DeoptimizationEntry_Lazy(Mac
roAssembler* masm) {
v8/src/builtins/mips64/builtins-mips64.cc:3873:
Generate_DeoptimizationEntry(masm, 
DeoptimizeKind::kLazy);
v8/test/unittests/compiler/backend/turboshaft-i
nstruction-selector-unittest.cc:126:  for (int 
i = 0; i < 
sequence.GetDeoptimizationEntryCount(); i++) {
v8/test/unittests/compiler/backend/turboshaft-i
nstruction-selector-unittest.cc:128:        
sequence.GetDeoptimizationEntry(i).descriptor()
);
v8/test/unittests/compiler/backend/instruction-
selector-unittest.cc:112:  for (int i = 0; i < 
sequence.GetDeoptimizationEntryCount(); i++) {
v8/test/unittests/compiler/backend/instruction-
selector-unittest.cc:114:        
sequence.GetDeoptimizationEntry(i).descriptor()
);
v8/test/unittests/assembler/macro-assembler-x64
-unittest.cc:1316:    Builtin target = 
Deoptimizer::GetDeoptimizationEntry(kind);
Observations: 
v8/src/builtins/arm64/builtins-arm64.cc:5364:  
CopyRegListToFrame(masm, x1, 
FrameDescription::registers_offset(),
v8/src/builtins/arm64/builtins-arm64.cc:5368:  
CopyRegListToFrame(masm, x1, 
FrameDescription::simd128_registers_offset(),
v8/src/builtins/arm64/builtins-arm64.cc:5387:  
__ Ldr(unwind_limit, MemOperand(x1, 
FrameDescription::frame_size_offset()));
v8/src/builtins/arm64/builtins-arm64.cc:5392:  
__ Add(x3, x1, 
FrameDescription::frame_content_offset());
v8/src/builtins/arm64/builtins-arm64.cc:5430:  
__ Ldr(x3, MemOperand(current_frame, 
FrameDescription::frame_size_offset()));
v8/src/builtins/arm64/builtins-arm64.cc:5434:  
__ Add(x7, current_frame, 
FrameDescription::frame_content_offset());
v8/src/builtins/arm64/builtins-arm64.cc:5443:  
FrameDescription::simd128_registers_offset());
v8/src/builtins/arm64/builtins-arm64.cc:5468:  
FrameDescription::registers_offset());
v8/src/builtins/arm64/builtins-arm64.cc:5474:  
FrameDescription::continuation_offset()));
v8/src/builtins/arm64/builtins-arm64.cc:5475:  
__ Ldr(lr, MemOperand(last_output_frame, 
FrameDescription::pc_offset()));
v8/src/builtins/riscv/builtins-riscv.cc:4777:  
// Push saved_regs (needed to populate 
FrameDescription::registers_).
v8/src/builtins/riscv/builtins-riscv.cc:4826:  
// Copy core registers into 
FrameDescription::registers_|kNumRegisters].
v8/src/builtins/riscv/builtins-riscv.cc:4830:  
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/riscv/builtins-riscv.cc:4840:  
int double_regs_offset = 
FrameDescription::double_registers_offset();
v8/src/builtins/riscv/builtins-riscv.cc:4841:  
// int simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/riscv/builtins-riscv.cc:4859:  
__ LoadWord(a2, MemOperand(a1, 
FrameDescription::frame_size_offset()));
v8/src/builtins/riscv/builtins-riscv.cc:4865:  
__ AddWord(a3, a1, 
Operand(FrameDescription::frame_content_offset(
)));
v8/src/builtins/riscv/builtins-riscv.cc:4900:  
__ LoadWord(a3, MemOperand(a2, 
FrameDescription::frame_size_offset()));
v8/src/builtins/riscv/builtins-riscv.cc:4905:  
__ LoadWord(a7, MemOperand(a6, 
FrameDescription::frame_content_offset()));
v8/src/builtins/riscv/builtins-riscv.cc:4923:  
__ LoadWord(a6, MemOperand(a2, 
FrameDescription::pc_offset()));
v8/src/builtins/riscv/builtins-riscv.cc:4925:  
__ LoadWord(a6, MemOperand(a2, 
FrameDescription::continuation_offset()));
v8/src/builtins/riscv/builtins-riscv.cc:4935:  
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/arm/builtins-arm.cc:4704:  // 
Push all 16 registers (needed to populate 
FrameDescription::registers_).
v8/src/builtins/arm/builtins-arm.cc:4751:  // 
Copy core registers into 
FrameDescription::registers_.
v8/src/builtins/arm/builtins-arm.cc:4754:    
int offset = (i * kPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/arm/builtins-arm.cc:4761:      
FrameDescription::simd128_registers_offset();
v8/src/builtins/arm/builtins-arm.cc:4790:  __ 
ldr(r2, MemOperand(r1, 
FrameDescription::frame_size_offset()));
v8/src/builtins/arm/builtins-arm.cc:4796:  __ 
add(r3, r1, 
Operand(FrameDescription::frame_content_offset(
)));
v8/src/builtins/arm/builtins-arm.cc:4832:  __ 
ldr(r3, MemOperand(r2, 
FrameDescription::frame_size_offset()));
v8/src/builtins/arm/builtins-arm.cc:4837:  __ 
ldr(r6, MemOperand(r6, 
FrameDescription::frame_content_offset()));
v8/src/builtins/arm/builtins-arm.cc:4863:  __ 
ldr(r6, MemOperand(r2, 
FrameDescription::pc_offset()));
v8/src/builtins/arm/builtins-arm.cc:4865:  __ 
ldr(r6, MemOperand(r2, 
FrameDescription::continuation_offset()));
v8/src/builtins/arm/builtins-arm.cc:4870:    
int offset = (i * kPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/mips64/builtins-mips64.cc:3633:
// Push saved_regs (needed to populate 
FrameDescription::registers_).
v8/src/builtins/mips64/builtins-mips64.cc:3683:
// Copy core registers into 
FrameDescription::registers_|kNumRegisters].
v8/src/builtins/mips64/builtins-mips64.cc:3687:
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/mips64/builtins-mips64.cc:3698:
int simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/mips64/builtins-mips64.cc:3741:
__ Ld(a2, MemOperand(a1, 
FrameDescription::frame_size_offset()));
v8/src/builtins/mips64/builtins-mips64.cc:3747:
__ Daddu(a3, a1, 
Operand(FrameDescription::frame_content_offset(
)));
v8/src/builtins/mips64/builtins-mips64.cc:3784:
MemOperand(current_frame, 
FrameDescription::frame_size_offset()));
v8/src/builtins/mips64/builtins-mips64.cc:3790:
__ Ld(a7, MemOperand(a6, 
FrameDescription::frame_content_offset()));
v8/src/builtins/mips64/builtins-mips64.cc:3835:
__ Ld(a6, MemOperand(current_frame, 
FrameDescription::pc_offset()));
v8/src/builtins/mips64/builtins-mips64.cc:3837:
__ Ld(a6, MemOperand(current_frame, 
FrameDescription::continuation_offset()));
v8/src/builtins/mips64/builtins-mips64.cc:3847:
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/ia32/builtins-ia32.cc:5260:    
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/ia32/builtins-ia32.cc:5264:  
int simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/ia32/builtins-ia32.cc:5289:  __
mov(ecx, Operand(esi, 
FrameDescription::frame_size_offset()));
v8/src/builtins/ia32/builtins-ia32.cc:5295:  __
lea(edx, Operand(esi, 
FrameDescription::frame_content_offset()));
v8/src/builtins/ia32/builtins-ia32.cc:5330:  __
mov(ecx, Operand(esi, 
FrameDescription::frame_size_offset()));
v8/src/builtins/ia32/builtins-ia32.cc:5334:  __
push(Operand(esi, ecx, times_1, 
FrameDescription::frame_content_offset()));
v8/src/builtins/ia32/builtins-ia32.cc:5352:  __
push(Operand(esi, 
FrameDescription::pc_offset()));
v8/src/builtins/ia32/builtins-ia32.cc:5353:  __
mov(eax, Operand(esi, 
FrameDescription::continuation_offset()));
v8/src/builtins/ia32/builtins-ia32.cc:5365:    
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/s390/builtins-s390.cc:4036:  //
Copy core registers into 
FrameDescription::registers_|kNumRegisters].
v8/src/builtins/s390/builtins-s390.cc:4038:  //
__ mvc(MemOperand(r3, 
FrameDescription::registers_offset()),
v8/src/builtins/s390/builtins-s390.cc:4040:  //
Copy core registers into 
FrameDescription::registers_|kNumRegisters].
v8/src/builtins/s390/builtins-s390.cc:4045:    
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/s390/builtins-s390.cc:4050:  
int simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/s390/builtins-s390.cc:4079:  __
LoadU64(r4, MemOperand(r3, 
FrameDescription::frame_size_offset()));
v8/src/builtins/s390/builtins-s390.cc:4085:  __
la(r5, MemOperand(r3, 
FrameDescription::frame_content_offset()));
v8/src/builtins/s390/builtins-s390.cc:4124:  __
LoadU64(r5, MemOperand(r4, 
FrameDescription::frame_size_offset()));
v8/src/builtins/s390/builtins-s390.cc:4130:  __
LoadU64(r8, MemOperand(r8, 
FrameDescription::frame_content_offset()));
v8/src/builtins/s390/builtins-s390.cc:4151:  __
LoadU64(r8, MemOperand(r4, 
FrameDescription::pc_offset()));
v8/src/builtins/s390/builtins-s390.cc:4153:  __
LoadU64(r8, MemOperand(r4, 
FrameDescription::continuation_offset()));
v8/src/builtins/s390/builtins-s390.cc:4160:    
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/ppc/builtins-ppc.cc:4893:  // 
Push saved_regs (needed to populate 
FrameDescription::registers_).
v8/src/builtins/ppc/builtins-ppc.cc:4942:  // 
Copy core registers into 
FrameDescription::registers_|kNumRegisters].
v8/src/builtins/ppc/builtins-ppc.cc:4946:      
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/ppc/builtins-ppc.cc:4951:  int 
simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/ppc/builtins-ppc.cc:4979:  __ 
LoadU64(r5, MemOperand(r4, 
FrameDescription::frame_size_offset()));
v8/src/builtins/ppc/builtins-ppc.cc:4985:  __ 
addi(r6, r4, 
Operand(FrameDescription::frame_content_offset(
)));
v8/src/builtins/ppc/builtins-ppc.cc:5024:  __ 
LoadU64(r6, MemOperand(r5, 
FrameDescription::frame_size_offset()));
v8/src/builtins/ppc/builtins-ppc.cc:5030:  __ 
LoadU64(r9, MemOperand(r9, 
FrameDescription::frame_content_offset()));
v8/src/builtins/ppc/builtins-ppc.cc:5051:  __ 
LoadU64(r9, MemOperand(r5, 
FrameDescription::pc_offset()));
v8/src/builtins/ppc/builtins-ppc.cc:5053:  __ 
LoadU64(r9, MemOperand(r5, 
FrameDescription::continuation_offset()));
v8/src/builtins/ppc/builtins-ppc.cc:5064:      
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/deoptimizer/arm64/deoptimizer-arm64.cc:4
4:void FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/arm64/deoptimizer-arm64.cc:5
1:void FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/arm64/deoptimizer-arm64.cc:5
5:void 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/arm64/deoptimizer-arm64.cc:6
0:void FrameDescription::SetPc(intptr_t pc) {
v8/src/deoptimizer/riscv/deoptimizer-riscv.cc:3
4:void FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/riscv/deoptimizer-riscv.cc:3
8:void FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/riscv/deoptimizer-riscv.cc:4
2:void 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/riscv/deoptimizer-riscv.cc:4
7:void FrameDescription::SetPc(intptr_t pc) { 
pc_ = pc; }
v8/src/deoptimizer/arm/deoptimizer-arm.cc:50:vo
id FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/arm/deoptimizer-arm.cc:54:vo
id FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/arm/deoptimizer-arm.cc:58:vo
id 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/arm/deoptimizer-arm.cc:63:vo
id FrameDescription::SetPc(intptr_t pc) { pc_ =
pc; }
v8/src/deoptimizer/deoptimizer.cc:673:    
input_ = 
FrameDescription::Create(input_frame_size, 
parameter_stack_slots,
v8/src/deoptimizer/deoptimizer.cc:695:  input_ 
= FrameDescription::Create(size, 
parameter_count, isolate_);
v8/src/deoptimizer/deoptimizer.cc:995:  
FrameDescription* output_frame = 
FrameDescription::Create(
v8/src/deoptimizer/deoptimizer.cc:1819:      
FrameDescription::Create(output_frame_size, 
parameters_count, isolate());
v8/src/deoptimizer/deoptimizer.cc:2124:  
FrameDescription* output_frame = 
FrameDescription::Create(
v8/src/deoptimizer/deoptimizer.cc:2182:      
FrameDescription::Create(output_frame_size, 
parameters_count, isolate());
v8/src/deoptimizer/deoptimizer.cc:2333:      
FrameDescription::Create(output_frame_size, 0, 
isolate());
v8/src/deoptimizer/deoptimizer.cc:2656:  
FrameDescription* output_frame = 
FrameDescription::Create(
v8/src/deoptimizer/mips64/deoptimizer-mips64.cc
:38:void FrameDescription::SetCallerPc(unsigned
offset, intptr_t value) {
v8/src/deoptimizer/mips64/deoptimizer-mips64.cc
:42:void FrameDescription::SetCallerFp(unsigned
offset, intptr_t value) {
v8/src/deoptimizer/mips64/deoptimizer-mips64.cc
:46:void 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/mips64/deoptimizer-mips64.cc
:51:void FrameDescription::SetPc(intptr_t pc) {
pc_ = pc; }
v8/src/builtins/loong64/builtins-loong64.cc:475
7:  // Push saved_regs (needed to populate 
FrameDescription::registers_).
v8/src/builtins/loong64/builtins-loong64.cc:480
6:  // Copy core registers into 
FrameDescription::registers_|kNumRegisters].
v8/src/builtins/loong64/builtins-loong64.cc:481
0:        (i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/loong64/builtins-loong64.cc:482
2:  int simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/loong64/builtins-loong64.cc:483
7:  __ Ld_d(a2, MemOperand(a1, 
FrameDescription::frame_size_offset()));
v8/src/builtins/loong64/builtins-loong64.cc:484
3:  __ Add_d(a3, a1, 
Operand(FrameDescription::frame_content_offset(
)));
v8/src/builtins/loong64/builtins-loong64.cc:488
0:          MemOperand(current_frame, 
FrameDescription::frame_size_offset()));
v8/src/builtins/loong64/builtins-loong64.cc:488
6:  __ Ld_d(a7, MemOperand(a6, 
FrameDescription::frame_content_offset()));
v8/src/builtins/loong64/builtins-loong64.cc:490
6:  __ Ld_d(a6, MemOperand(current_frame, 
FrameDescription::pc_offset()));
v8/src/builtins/loong64/builtins-loong64.cc:490
9:          MemOperand(current_frame, 
FrameDescription::continuation_offset()));
v8/src/builtins/loong64/builtins-loong64.cc:491
9:        (i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/deoptimizer/deoptimizer.h:320:  friend 
class FrameDescription;
v8/src/deoptimizer/frame-description.h:67:class
FrameDescription {
v8/src/deoptimizer/x64/deoptimizer-x64.cc:76:vo
id FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/x64/deoptimizer-x64.cc:81:vo
id FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/x64/deoptimizer-x64.cc:85:vo
id 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/x64/deoptimizer-x64.cc:90:vo
id FrameDescription::SetPc(intptr_t pc) { pc_ =
pc; }
v8/src/deoptimizer/loong64/deoptimizer-loong64.
cc:38:void 
FrameDescription::SetCallerPc(unsigned offset, 
intptr_t value) {
v8/src/deoptimizer/loong64/deoptimizer-loong64.
cc:42:void 
FrameDescription::SetCallerFp(unsigned offset, 
intptr_t value) {
v8/src/deoptimizer/loong64/deoptimizer-loong64.
cc:46:void 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/loong64/deoptimizer-loong64.
cc:51:void FrameDescription::SetPc(intptr_t pc)
{ pc_ = pc; }
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:49:vo
id FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:53:vo
id FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:57:vo
id 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/ppc/deoptimizer-ppc.cc:62:vo
id FrameDescription::SetPc(intptr_t pc) { pc_ =
pc; }
v8/src/deoptimizer/s390/deoptimizer-s390.cc:47:
void FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/s390/deoptimizer-s390.cc:51:
void FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/s390/deoptimizer-s390.cc:55:
void 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/s390/deoptimizer-s390.cc:60:
void FrameDescription::SetPc(intptr_t pc) { pc_
= pc; }
v8/src/deoptimizer/ia32/deoptimizer-ia32.cc:39:
void FrameDescription::SetCallerPc(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/ia32/deoptimizer-ia32.cc:43:
void FrameDescription::SetCallerFp(unsigned 
offset, intptr_t value) {
v8/src/deoptimizer/ia32/deoptimizer-ia32.cc:47:
void 
FrameDescription::SetCallerConstantPool(unsigne
d offset, intptr_t value) {
v8/src/deoptimizer/ia32/deoptimizer-ia32.cc:52:
void FrameDescription::SetPc(intptr_t pc) { pc_
= pc; }
v8/src/builtins/x64/builtins-x64.cc:3122:  int 
simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/x64/builtins-x64.cc:5009:      
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
v8/src/builtins/x64/builtins-x64.cc:5014:  int 
simd128_regs_offset = 
FrameDescription::simd128_registers_offset();
v8/src/builtins/x64/builtins-x64.cc:5032:  __ 
movq(rcx, Operand(rbx, 
FrameDescription::frame_size_offset()));
v8/src/builtins/x64/builtins-x64.cc:5038:  __ 
leaq(rdx, Operand(rbx, 
FrameDescription::frame_content_offset()));
v8/src/builtins/x64/builtins-x64.cc:5076:  __ 
movq(rcx, Operand(rbx, 
FrameDescription::frame_size_offset()));
v8/src/builtins/x64/builtins-x64.cc:5080:  __ 
Push(Operand(rbx, rcx, times_1, 
FrameDescription::frame_content_offset()));
v8/src/builtins/x64/builtins-x64.cc:5090:  __ 
PushQuad(Operand(rbx, 
FrameDescription::pc_offset()));
v8/src/builtins/x64/builtins-x64.cc:5091:  __ 
movq(rax, Operand(rbx, 
FrameDescription::continuation_offset()));
v8/src/builtins/x64/builtins-x64.cc:5105:      
(i * kSystemPointerSize) + 
FrameDescription::registers_offset();
Observations: 
v8/src/ic/accessor-assembler.h:344:  // logic 
not inlined into Ignition bytecode handlers.
v8/BUILD.gn:179:  # extension function 
getIgnitionDispatchCounters().
v8/BUILD.gn:598:       "Ignition tracing 
requires unoptimized tracing to be enabled.")
v8/src/runtime/runtime-compiler.cc:721:    //  
deoptimizing to Ignition under the hood.
v8/src/runtime/runtime-compiler.cc:725:    // 
We solve this synchronous OSR case by bailing 
out early to Ignition, and
v8/src/runtime/runtime.h:282:  
F(BytecodeBudgetInterrupt_Ignition, 1, 1)      
\
v8/src/runtime/runtime.h:283:  
F(BytecodeBudgetInterruptWithStackCheck_Ignitio
n, 1, 1)  \
v8/src/runtime/runtime.h:529:  
F(ActiveTierIsIgnition, 1, 1)               \
v8/src/runtime/runtime-internal.cc:411:RUNTIME_
FUNCTION(Runtime_BytecodeBudgetInterruptWithSta
ckCheck_Ignition) {
v8/src/runtime/runtime-internal.cc:416:RUNTIME_
FUNCTION(Runtime_BytecodeBudgetInterrupt_Igniti
on) {
v8/src/runtime/runtime-test.cc:517:RUNTIME_FUNC
TION(Runtime_ActiveTierIsIgnition) {
v8/src/runtime/runtime-test.cc:523:  return 
isolate->heap()->ToBoolean(function->ActiveTier
IsIgnition(isolate));
v8/src/runtime/runtime-test.cc:968:  if 
(function->ActiveTierIsIgnition(isolate)) {
v8/src/ast/scopes.h:1012:  // Ignition without 
ScopeInfo.
v8/src/maglev/maglev-ir.cc:6840:  //    value 
means we should deopt into Ignition which will 
handle all further
v8/src/logging/log.h:389:  V(CompileIgnition, 
true)       \
v8/src/logging/runtime-call-stats.h:327:  
ADD_THREAD_SPECIFIC_COUNTER(V, Compile, 
Ignition)                           \
v8/src/logging/runtime-call-stats.h:328:  
ADD_THREAD_SPECIFIC_COUNTER(V, Compile, 
IgnitionFinalization)               \
v8/src/diagnostics/objects-printer.cc:2258:  } 
else if (ActiveTierIsIgnition(isolate)) {
v8/src/execution/tiering-manager.cc:165:       
(function->ActiveTierIsIgnition(isolate) &&
v8/src/execution/tiering-manager.cc:534:  // 
relevant later since the configuration 
'Ignition without a vector' can be
v8/src/execution/tiering-manager.cc:547:      
function->ActiveTierIsIgnition(isolate_) && 
!maybe_had_optimized_osr_code;
v8/src/execution/tiering-manager.cc:574:  // 
Ignition-Sparkplug-Turbofan, and only a single 
tierup is requested at
v8/src/compiler/js-call-reducer.cc:5302:      
// implementation in Ignition.
v8/src/compiler/linkage.cc:677:CallDescriptor* 
Linkage::GetBytecodeDispatchCallDescriptor(
v8/src/flags/flag-definitions.h:787:// 
JavaScript code can execute either in Ignition 
or Sparkplug.
v8/src/flags/flag-definitions.h:978:// Flags 
for Ignition.
v8/src/init/bootstrapper.cc:165:  
v8::RegisterExtension(std::make_unique<Ignition
StatisticsExtension>());
v8/src/compiler/pipeline.cc:3154:        
InterpreterDispatchDescriptor descriptor;
v8/test/cctest/test-cpu-profiler.cc:4286:  if 
(function->ActiveTierIsIgnition(isolate)) 
return -1;
v8/src/d8/d8.h:812:  static void 
WriteIgnitionDispatchCountersFile(v8::Isolate* 
isolate);
v8/src/d8/d8.cc:4217:void 
Shell::WriteIgnitionDispatchCountersFile(v8::Is
olate* isolate) {
v8/src/d8/d8.cc:6659:        
WriteIgnitionDispatchCountersFile(isolate);
v8/src/compiler/js-operator.h:1092:  // Used to
implement Ignition's SuspendGenerator bytecode.
v8/src/compiler/js-operator.h:1095:  // Used to
implement Ignition's SwitchOnGeneratorState 
bytecode.
v8/src/compiler/js-operator.h:1099:  // Used to
implement Ignition's ResumeGenerator bytecode.
v8/src/compiler/code-assembler.cc:54:    // 
TODO(rmcilroy): Should we use 
Linkage::GetBytecodeDispatchDescriptor for
v8/src/compiler/code-assembler.cc:1444:void 
CodeAssembler::TailCallStubThenBytecodeDispatch
Impl(
v8/src/compiler/code-assembler.cc:1468:void 
CodeAssembler::TailCallBytecodeDispatch(
v8/src/compiler/code-assembler.cc:1472:  auto 
call_descriptor = 
Linkage::GetBytecodeDispatchCallDescriptor(
v8/src/compiler/code-assembler.cc:1480:// 
Instantiate TailCallBytecodeDispatch() for 
argument counts used by
v8/src/compiler/code-assembler.cc:1482:template
V8_EXPORT_PRIVATE void 
CodeAssembler::TailCallBytecodeDispatch(
v8/src/compiler/code-assembler.h:1379:  void 
TailCallBytecodeDispatch(const 
CallInterfaceDescriptor& descriptor,
v8/src/compiler/code-assembler.h:1383:  void 
TailCallBuiltinThenBytecodeDispatch(Builtin 
builtin, Node* context,
v8/src/compiler/code-assembler.h:1387:    
TailCallStubThenBytecodeDispatchImpl(callable.d
escriptor(), target, context,
v8/src/compiler/code-assembler.h:1565:  void 
TailCallStubThenBytecodeDispatchImpl(
v8/src/parsing/expression-scope.h:416:  // in 
Ignition is 2^31-1 due to the size of register 
operands. We limit it to
v8/src/interpreter/interpreter-generator-tsa.cc
:99:    TraceBytecodeDispatch(target_bytecode);
v8/src/interpreter/interpreter-generator-tsa.cc
:113:    TailCallBytecodeDispatch(
v8/src/interpreter/interpreter-generator-tsa.cc
:114:        InterpreterDispatchDescriptor{}, 
handler_entry, accumulator_.Get(),
v8/src/interpreter/interpreter-generator-tsa.cc
:115:        bytecode_offset, 
BytecodeArrayTaggedPointer(), 
DispatchTablePointer());
v8/src/interpreter/interpreter-generator-tsa.cc
:121:  void TailCallBytecodeDispatch(const 
CallInterfaceDescriptor& descriptor,
v8/src/interpreter/interpreter-generator-tsa.cc
:124:    auto call_descriptor = 
compiler::Linkage::GetBytecodeDispatchCallDescr
iptor(
v8/src/interpreter/interpreter-generator-tsa.cc
:318:        
InterpreterDispatchDescriptor::kAccumulator);
v8/src/interpreter/interpreter-generator-tsa.cc
:320:        
InterpreterDispatchDescriptor::kBytecodeOffset)
;
v8/src/interpreter/interpreter-generator-tsa.cc
:322:        
InterpreterDispatchDescriptor::kBytecodeArray);
v8/src/interpreter/interpreter-generator-tsa.cc
:324:        
InterpreterDispatchDescriptor::kDispatchTable);
v8/src/interpreter/interpreter-assembler.h:365:
void TraceBytecodeDispatch(TNode<WordT> 
target_bytecode);
v8/src/interpreter/interpreter.h:25:class 
IgnitionStatisticsTester;
v8/src/interpreter/interpreter.h:96:  friend 
class v8::internal::IgnitionStatisticsTester;
v8/src/interpreter/interpreter.cc:186:         
RuntimeCallCounterId::kCompileIgnition,
v8/src/interpreter/interpreter.cc:189:  
TRACE_EVENT0(TRACE_DISABLED_BY_DEFAULT("v8.comp
ile"), "V8.CompileIgnition");
v8/src/interpreter/interpreter.cc:250:         
RuntimeCallCounterId::kCompileIgnitionFinalizat
ion);
v8/src/interpreter/interpreter.cc:252:         
"V8.CompileIgnitionFinalization");
v8/src/interpreter/interpreter.cc:258:  
RCS_SCOPE(isolate, 
RuntimeCallCounterId::kCompileIgnitionFinalizat
ion,
v8/src/interpreter/interpreter.cc:261:         
"V8.CompileIgnitionFinalization");
v8/src/interpreter/interpreter-assembler.cc:35:
InterpreterDispatchDescriptor::kBytecodeArray))
,
v8/src/interpreter/interpreter-assembler.cc:39:
InterpreterDispatchDescriptor::kBytecodeOffset)
),
v8/src/interpreter/interpreter-assembler.cc:42:
InterpreterDispatchDescriptor::kDispatchTable))
,
v8/src/interpreter/interpreter-assembler.cc:45:
Parameter<Object>(InterpreterDispatchDescriptor
::kAccumulator)),
v8/src/interpreter/interpreter-assembler.cc:88:
InterpreterDispatchDescriptor::kBytecodeOffset)
)) {
v8/src/interpreter/interpreter-assembler.cc:146
:           
InterpreterDispatchDescriptor::kDispatchTable))
) {
v8/src/interpreter/interpreter-assembler.cc:745
:  TailCallBuiltinThenBytecodeDispatch(builtin,
context, args_count,
v8/src/interpreter/interpreter-assembler.cc:767
:    
TailCallBuiltinThenBytecodeDispatch(builtin, 
context, function, arg_count,
v8/src/interpreter/interpreter-assembler.cc:770
:    
TailCallBuiltinThenBytecodeDispatch(builtin, 
context, function, arg_count,
v8/src/interpreter/interpreter-assembler.cc:814
:  TailCallBuiltinThenBytecodeDispatch(builtin,
context, args_count,
v8/src/interpreter/interpreter-assembler.cc:865
:    // constructor feedback collection inside 
of Ignition.
v8/src/interpreter/interpreter-assembler.cc:109
7:                  ? 
Runtime::kBytecodeBudgetInterruptWithStackCheck
_Ignition
v8/src/interpreter/interpreter-assembler.cc:109
8:                  : 
Runtime::kBytecodeBudgetInterrupt_Ignition,
v8/src/interpreter/interpreter-assembler.cc:130
1:    TraceBytecodeDispatch(target_bytecode);
v8/src/interpreter/interpreter-assembler.cc:131
2:  TailCallBytecodeDispatch(
v8/src/interpreter/interpreter-assembler.cc:131
3:      InterpreterDispatchDescriptor{}, 
handler_entry, GetAccumulatorUnchecked(),
v8/src/interpreter/interpreter-assembler.cc:131
4:      bytecode_offset, 
BytecodeArrayTaggedPointer(), 
DispatchTablePointer());
v8/src/interpreter/interpreter-assembler.cc:133
0:    TraceBytecodeDispatch(next_bytecode);
v8/src/interpreter/interpreter-assembler.cc:147
6:    // compilation as in the Ignition -> 
Turbofan case. Therefore we can just
v8/src/interpreter/interpreter-assembler.cc:148
9:void 
InterpreterAssembler::TraceBytecodeDispatch(TNo
de<WordT> target_bytecode) {
v8/src/compiler/linkage.h:392:// 
Call|BytecodeDispatch] address,    arg 1, arg 
2, |...]
v8/src/compiler/linkage.h:429:  static 
CallDescriptor* 
GetBytecodeDispatchCallDescriptor(
v8/tools/zone-stats/categories.js:102:  
|'interpreter', 'Ignition'],
v8/test/cctest/test-ignition-statistics-extensi
on.cc:13:class IgnitionStatisticsTester {
v8/test/cctest/test-ignition-statistics-extensi
on.cc:15:  explicit 
IgnitionStatisticsTester(Isolate* isolate) : 
isolate_(isolate) {
v8/test/cctest/test-ignition-statistics-extensi
on.cc:45:TEST(IgnitionStatisticsExtension) {
v8/test/cctest/test-ignition-statistics-extensi
on.cc:50:  IgnitionStatisticsTester 
tester(CcTest::i_isolate());
v8/test/cctest/test-ignition-statistics-extensi
on.cc:53:      CompileRun("typeof 
getIgnitionDispatchCounters === 'function'");
v8/test/cctest/test-ignition-statistics-extensi
on.cc:67:    var emptyCounters = 
getIgnitionDispatchCounters();
v8/test/cctest/test-ignition-statistics-extensi
on.cc:98:    var nonEmptyCounters = 
getIgnitionDispatchCounters();
v8/test/cctest/test-api.cc:23644:  // When run 
with Ignition, tests that the streaming parser 
canonicalizes
v8/test/cctest/test-api.cc:23656:  // When run 
with Ignition, tests that the streaming parser 
canonicalizes
v8/tools/profview/profile-utils.js:60:    if 
(kind === "Builtin" || kind == "Ignition" || 
kind === "Unopt") {
v8/tools/profview/profile-utils.js:275:  
addCategory("JS Ignition", | "JS_IGNITION", 
"BC" ]);
v8/tools/profview/profview.js:306:    text: "JS
Ignition"
v8/tools/profview/profview.js:447:      return 
"JS Ignition";
v8/test/cctest/test-debug.cc:4755:  // With 
Ignition we generate one return location per 
return statement,
v8/test/cctest/test-api-incumbent.cc:73://    
Ignition, Sparkplug, Maglev, TurboFan.
v8/test/cctest/test-api-incumbent.cc:84:  const
int n = 4;  // Ignition, Sparkplug, Maglev, 
TurboFan.
v8/src/heap/marking-visitor-inl.h:492:      // 
Ignition/Sparkplug. But that might have been 
some time after the last
v8/src/extensions/ignition-statistics-extension
.h:17:class IgnitionStatisticsExtension : 
public v8::Extension {
v8/src/extensions/ignition-statistics-extension
.h:19:  IgnitionStatisticsExtension()
v8/src/extensions/ignition-statistics-extension
.h:25:  static void 
GetIgnitionDispatchCounters(
v8/src/codegen/arm64/register-arm64.h:605:const
expr Register kInterpreterDispatchTableRegister
= x21;
v8/src/codegen/arm64/interface-descriptors-arm6
4-inl.h:375:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/arm64/interface-descriptors-arm6
4-inl.h:378:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/interface-descriptors.h:96:  
V(InterpreterDispatch)                         
\
v8/src/codegen/interface-descriptors.h:2249:cla
ss V8_EXPORT_PRIVATE 
InterpreterDispatchDescriptor
v8/src/codegen/interface-descriptors.h:2250:   
: public 
StaticCallInterfaceDescriptor<InterpreterDispat
chDescriptor> {
v8/src/codegen/interface-descriptors.h:2259:  
DECLARE_DESCRIPTOR(InterpreterDispatchDescripto
r)
v8/src/extensions/ignition-statistics-extension
.cc:18:IgnitionStatisticsExtension::GetNativeFu
nctionTemplate(
v8/src/extensions/ignition-statistics-extension
.cc:21:                   
"getIgnitionDispatchCounters"),
v8/src/extensions/ignition-statistics-extension
.cc:24:      isolate, 
IgnitionStatisticsExtension::GetIgnitionDispatc
hCounters);
v8/src/extensions/ignition-statistics-extension
.cc:27:const char* const 
IgnitionStatisticsExtension::kSource =
v8/src/extensions/ignition-statistics-extension
.cc:28:    "native function 
getIgnitionDispatchCounters();";
v8/src/extensions/ignition-statistics-extension
.cc:30:void 
IgnitionStatisticsExtension::GetIgnitionDispatc
hCounters(
v8/src/codegen/riscv/register-riscv.h:297:const
expr Register kInterpreterDispatchTableRegister
= t2;
v8/src/codegen/x64/register-x64.h:280:constexpr
Register kInterpreterDispatchTableRegister = 
r15;
v8/src/codegen/riscv/interface-descriptors-risc
v-inl.h:377:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/riscv/interface-descriptors-risc
v-inl.h:380:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/arm/interface-descriptors-arm-in
l.h:376:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/arm/interface-descriptors-arm-in
l.h:379:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/arm/register-arm.h:312:constexpr
Register kInterpreterDispatchTableRegister = 
r8;
v8/src/codegen/x64/interface-descriptors-x64-in
l.h:376:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/x64/interface-descriptors-x64-in
l.h:379:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/test/mjsunit/tools/tickprocessor-test-large.
log:94:code-creation,Builtin,2,4832,0x557426328
1a0,92,InterpreterEnterBytecodeDispatch
v8/src/codegen/mips64/interface-descriptors-mip
s64-inl.h:352:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/mips64/interface-descriptors-mip
s64-inl.h:355:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/mips64/register-mips64.h:300:con
stexpr Register 
kInterpreterDispatchTableRegister = t2;
v8/src/codegen/ia32/interface-descriptors-ia32-
inl.h:345:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/ia32/interface-descriptors-ia32-
inl.h:348:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/ia32/register-ia32.h:135:constex
pr Register kInterpreterDispatchTableRegister =
esi;
v8/src/objects/js-function.h:201:  
V8_EXPORT_PRIVATE bool 
ActiveTierIsIgnition(IsolateForSandbox isolate)
const;
v8/src/codegen/loong64/interface-descriptors-lo
ong64-inl.h:352:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/loong64/interface-descriptors-lo
ong64-inl.h:355:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/loong64/register-loong64.h:220:c
onstexpr Register 
kInterpreterDispatchTableRegister = t2;
v8/src/codegen/s390/register-s390.h:254:constex
pr Register kInterpreterDispatchTableRegister =
r8;
v8/src/codegen/s390/interface-descriptors-s390-
inl.h:362:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/s390/interface-descriptors-s390-
inl.h:365:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/ppc/interface-descriptors-ppc-in
l.h:342:constexpr auto 
InterpreterDispatchDescriptor::registers() {
v8/src/codegen/ppc/interface-descriptors-ppc-in
l.h:345:      
kInterpreterBytecodeArrayRegister, 
kInterpreterDispatchTableRegister);
v8/src/codegen/ppc/register-ppc.h:314:constexpr
Register kInterpreterDispatchTableRegister = 
r17;
v8/src/objects/js-function.cc:161:bool 
JSFunction::ActiveTierIsIgnition(IsolateForSand
box isolate) const {
v8/src/objects/js-function.cc:218:  
DCHECK(!is_compiled(isolate) || 
ActiveTierIsIgnition(isolate) ||
v8/src/objects/js-function.cc:744:      
function->ActiveTierIsIgnition(isolate)) {
v8/src/builtins/arm64/builtins-arm64.cc:601:  
// Resume (Ignition/TurboFan) generator object.
v8/src/builtins/arm64/builtins-arm64.cc:1491:  
kInterpreterDispatchTableRegister,
v8/src/builtins/arm64/builtins-arm64.cc:1497:  
MemOperand(kInterpreterDispatchTableRegister, 
x1));
v8/src/builtins/arm64/builtins-arm64.cc:1988:  
kInterpreterDispatchTableRegister,
v8/src/builtins/arm64/builtins-arm64.cc:2024:  
MemOperand(kInterpreterDispatchTableRegister, 
x1));
v8/src/builtins/arm64/builtins-arm64.cc:2050:  
__ IsObjectType(x1, 
kInterpreterDispatchTableRegister,
v8/src/builtins/arm64/builtins-arm64.cc:2051:  
kInterpreterDispatchTableRegister, 
INTERPRETER_DATA_TYPE);
v8/src/builtins/riscv/builtins-riscv.cc:449:  
// Resume (Ignition/TurboFan) generator object.
v8/src/builtins/riscv/builtins-riscv.cc:1251:  
__ li(kInterpreterDispatchTableRegister,
v8/src/builtins/riscv/builtins-riscv.cc:1256:  
__ CalcScaledAddress(kScratchReg, 
kInterpreterDispatchTableRegister, a7,
v8/src/builtins/riscv/builtins-riscv.cc:1734:  
__ GetObjectType(t0, 
kInterpreterDispatchTableRegister,
v8/src/builtins/riscv/builtins-riscv.cc:1735:  
kInterpreterDispatchTableRegister);
v8/src/builtins/riscv/builtins-riscv.cc:1736:  
__ Branch(&builtin_trampoline, ne, 
kInterpreterDispatchTableRegister,
v8/src/builtins/riscv/builtins-riscv.cc:1754:  
__ li(kInterpreterDispatchTableRegister,
v8/src/builtins/riscv/builtins-riscv.cc:1791:  
__ CalcScaledAddress(a1, 
kInterpreterDispatchTableRegister, a7,
v8/src/builtins/arm/builtins-arm.cc:441:  // 
Resume (Ignition/TurboFan) generator object.
v8/src/builtins/arm/builtins-arm.cc:1244:      
kInterpreterDispatchTableRegister,
v8/src/builtins/arm/builtins-arm.cc:1250:      
MemOperand(kInterpreterDispatchTableRegister, 
r4, LSL, kPointerSizeLog2));
v8/src/builtins/arm/builtins-arm.cc:1729:  __ 
CompareObjectType(r2, 
kInterpreterDispatchTableRegister,
v8/src/builtins/arm/builtins-arm.cc:1730:      
kInterpreterDispatchTableRegister,
v8/src/builtins/arm/builtins-arm.cc:1750:      
kInterpreterDispatchTableRegister,
v8/src/builtins/arm/builtins-arm.cc:1788:      
MemOperand(kInterpreterDispatchTableRegister, 
scratch, LSL,
v8/src/objects/js-function-inl.h:69:  if 
(ActiveTierIsIgnition(isolate)) {
v8/src/builtins/mips64/builtins-mips64.cc:420: 
// Resume (Ignition/TurboFan) generator object.
v8/src/builtins/mips64/builtins-mips64.cc:1202:
__ li(kInterpreterDispatchTableRegister,
v8/src/builtins/mips64/builtins-mips64.cc:1207:
__ Dlsa(kScratchReg, 
kInterpreterDispatchTableRegister, a7,
v8/src/builtins/mips64/builtins-mips64.cc:1678:
__ GetObjectType(t0, 
kInterpreterDispatchTableRegister,
v8/src/builtins/mips64/builtins-mips64.cc:1679:
kInterpreterDispatchTableRegister);
v8/src/builtins/mips64/builtins-mips64.cc:1680:
__ Branch(&builtin_trampoline, ne, 
kInterpreterDispatchTableRegister,
v8/src/builtins/mips64/builtins-mips64.cc:1697:
__ li(kInterpreterDispatchTableRegister,
v8/src/builtins/mips64/builtins-mips64.cc:1733:
__ Dlsa(a1, kInterpreterDispatchTableRegister, 
a7, kSystemPointerSizeLog2);
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:30:          test->isolate(), 
test->zone(), InterpreterDispatchDescriptor{},
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:97:      
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:99:          
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:108:      
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:110:          
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:120:        
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:122:            
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:138:          
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:140:              
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:154:        
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:156:            
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:172:          
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:174:              
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:188:        
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:190:            
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:206:          
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:208:              
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:227:        
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:229:            
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:245:          
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeArray),
v8/test/unittests/interpreter/interpreter-assem
bler-unittest.cc:247:              
c::IsParameter(InterpreterDispatchDescriptor::k
BytecodeOffset),
v8/src/builtins/ia32/builtins-ia32.cc:699:  // 
Resume (Ignition/TurboFan) generator object.
v8/src/builtins/ia32/builtins-ia32.cc:1031:  __
Move(kInterpreterDispatchTableRegister,
v8/src/builtins/ia32/builtins-ia32.cc:1037:    
Operand(kInterpreterDispatchTableRegister, ecx,
v8/src/builtins/ia32/builtins-ia32.cc:1069:    
kInterpreterDispatchTableRegister, eax,
v8/src/builtins/ia32/builtins-ia32.cc:1756:  __
Move(kInterpreterDispatchTableRegister,
v8/src/builtins/ia32/builtins-ia32.cc:1792:    
Operand(kInterpreterDispatchTableRegister, 
scratch,
v8/src/builtins/s390/builtins-s390.cc:681:  // 
Resume (Ignition/TurboFan) generator object.
v8/src/builtins/s390/builtins-s390.cc:1517:    
kInterpreterDispatchTableRegister,
v8/src/builtins/s390/builtins-s390.cc:1524:    
MemOperand(kInterpreterDispatchTableRegister, 
r5));
v8/src/builtins/s390/builtins-s390.cc:2004:  __
CompareObjectType(r4, 
kInterpreterDispatchTableRegister,
v8/src/builtins/s390/builtins-s390.cc:2005:    
kInterpreterDispatchTableRegister,
v8/src/builtins/s390/builtins-s390.cc:2025:    
kInterpreterDispatchTableRegister,
v8/src/builtins/s390/builtins-s390.cc:2064:    
MemOperand(kInterpreterDispatchTableRegister, 
scratch));
v8/src/builtins/ppc/builtins-ppc.cc:733:  // 
Resume (Ignition/TurboFan) generator object.
v8/src/builtins/ppc/builtins-ppc.cc:1494:      
kInterpreterDispatchTableRegister,
v8/src/builtins/ppc/builtins-ppc.cc:1500:      
MemOperand(kInterpreterDispatchTableRegister, 
r6));
v8/src/builtins/ppc/builtins-ppc.cc:1988:  __ 
IsObjectType(r5, 
kInterpreterDispatchTableRegister,
v8/src/builtins/ppc/builtins-ppc.cc:1989:      
kInterpreterDispatchTableRegister, 
INTERPRETER_DATA_TYPE);
v8/src/builtins/ppc/builtins-ppc.cc:2010:      
kInterpreterDispatchTableRegister,
v8/src/builtins/ppc/builtins-ppc.cc:2051:      
MemOperand(kInterpreterDispatchTableRegister, 
scratch));
v8/test/mjsunit/regress/regress-crbug-658691.js
:15:// The {g} function is compiled using 
Ignition.
v8/src/objects/feedback-vector.tq:33:  // In 
order to have fast OSR checks in Ignition and 
Sparkplug, these bits
v8/src/builtins/x64/builtins-x64.cc:907:  // 
Resume (Ignition/TurboFan) generator object.
v8/src/builtins/x64/builtins-x64.cc:1244:      
kInterpreterDispatchTableRegister,
v8/src/builtins/x64/builtins-x64.cc:1250:      
Operand(kInterpreterDispatchTableRegister, 
kScratchRegister,
v8/src/builtins/x64/builtins-x64.cc:1799:      
kInterpreterDispatchTableRegister,
v8/src/builtins/x64/builtins-x64.cc:1835:      
Operand(kInterpreterDispatchTableRegister, 
kScratchRegister,
v8/src/builtins/x64/builtins-x64.cc:3002:    //
unoptimized (~= Ignition) stack frame layout. 
Instead, return to Maglev
v8/src/builtins/loong64/builtins-loong64.cc:435
:  // Resume (Ignition/TurboFan) generator 
object.
v8/src/builtins/loong64/builtins-loong64.cc:124
4:  __ li(kInterpreterDispatchTableRegister,
v8/src/builtins/loong64/builtins-loong64.cc:124
9:  __ Alsl_d(kScratchReg, a7, 
kInterpreterDispatchTableRegister,
v8/src/builtins/loong64/builtins-loong64.cc:173
4:                      
kInterpreterDispatchTableRegister);
v8/src/builtins/loong64/builtins-loong64.cc:175
1:  __ li(kInterpreterDispatchTableRegister,
v8/src/builtins/loong64/builtins-loong64.cc:178
7:  __ Alsl_d(a1, a7, 
kInterpreterDispatchTableRegister, 
kSystemPointerSizeLog2,
v8/test/mjsunit/harmony/weakrefs/finalizationre
gistry-keeps-holdings-alive.js:21:  // Ignition
holds references to objects in temporary 
registers. These will be
v8/test/mjsunit/harmony/weakrefs/weak-cell-basi
cs.js:20:  // Ignition holds references to 
objects in temporary registers. These will be
v8/test/mjsunit/harmony/weakrefs/cleanup.js:20:
// Ignition holds references to objects in 
temporary registers. These will be
v8/test/mjsunit/baseline/flush-baseline-code.js
:70:    // Also, the active tier is Sparkplug 
and not Ignition.
v8/test/mjsunit/baseline/flush-baseline-code.js
:71:    // 
assertTrue(ActiveTierIsIgnition(f1));
v8/test/mjsunit/maglev/tier-to-ml-to-tf.js:20: 
// We test the entire pipeline, i.e. 
Ignition-SP-ML-TF.
Observations: 
v8/src/debug/debug-wasm-objects.cc:760:  
explicit 
DebugWasmInterpreterScopeIterator(WasmInterpret
erEntryFrame* frame)
v8/src/debug/debug-wasm-objects.cc:843:  
WasmInterpreterEntryFrame* const frame_;
v8/src/debug/debug-wasm-objects.cc:1149:    
WasmInterpreterEntryFrame* frame) {
v8/src/debug/debug-wasm-objects.h:35:class 
WasmInterpreterEntryFrame;
v8/src/debug/debug-wasm-objects.h:77:    
WasmInterpreterEntryFrame* frame);
v8/src/debug/debug-frames.cc:82:      auto 
wasm_frame = 
WasmInterpreterEntryFrame::cast(frame_);
v8/src/debug/debug-stack-trace-iterator.cc:164:
auto frame = 
WasmInterpreterEntryFrame::cast(iterator_.frame
());
v8/src/debug/debug-stack-trace-iterator.cc:201:
WasmInterpreterEntryFrame::cast(iterator_.frame
()));
v8/src/debug/debug.cc:1736:  auto trampoline = 
BUILTIN_CODE(isolate_, 
InterpreterEntryTrampoline);
v8/src/debug/debug.cc:1754:  auto trampoline = 
BUILTIN_CODE(isolate_, 
InterpreterEntryTrampoline);
v8/src/baseline/baseline-compiler.cc:2013:  
using D = OnStackReplacementDescriptor;
v8/src/baseline/baseline-compiler.cc:2081:    
CallBuiltin<Builtin::kBaselineOnStackReplacemen
t>(maybe_target_code,
v8/src/regexp/regexp.cc:659:        
BUILTIN_CODE(isolate, 
RegExpInterpreterTrampoline);
v8/src/snapshot/code-serializer.cc:284:// to 
create duplicates of InterpreterEntryTrampoline
for the deserialized
v8/src/snapshot/code-serializer.cc:310:        
Builtins::CreateInterpreterEntryTrampolineForPr
ofiling(isolate);
v8/src/tracing/perfetto-logger.cc:166:      if 
(code->builtin_id() == 
Builtin::kInterpreterEntryTrampoline) {
v8/src/snapshot/embedded/embedded-data.cc:411: 
// Ensure that 
InterpreterEntryTrampolineForProfiling is 
relocatable.
v8/src/snapshot/embedded/embedded-data.cc:414: 
builtins->code(Builtin::kInterpreterEntryTrampo
lineForProfiling));
v8/src/runtime/runtime-test.cc:390:    
Tagged<Code> code = *BUILTIN_CODE(isolate, 
InterpreterEntryTrampoline);
v8/src/roots/roots.h:434:  V(Smi, 
interpreter_entry_return_pc_offset, 
InterpreterEntryReturnPCOffset)
v8/src/interpreter/interpreter-generator.cc:243
0:    OnStackReplacement(context, 
feedback_vector, relative_jump, loop_depth,
v8/src/interpreter/interpreter-generator.cc:243
2:                       
OnStackReplacementParams::kBaselineCodeIsCached
);
v8/src/interpreter/interpreter-generator.cc:243
9:    OnStackReplacement(context, 
feedback_vector, relative_jump, loop_depth,
v8/src/interpreter/interpreter-generator.cc:244
1:                       
OnStackReplacementParams::kDefault);
v8/src/interpreter/interpreter-assembler.h:302:
enum OnStackReplacementParams {
v8/src/interpreter/interpreter-assembler.h:306:
void OnStackReplacement(TNode<Context> context,
v8/src/interpreter/interpreter-assembler.h:311:
OnStackReplacementParams params);
v8/src/interpreter/constant-array-builder.h:32:
V(InterpreterTrampolineSymbol, 
interpreter_trampoline_symbol)              \
v8/src/interpreter/interpreter.cc:349:  
DirectHandle<Code> code = 
BUILTIN_CODE(isolate_, 
InterpreterEntryTrampoline);
v8/src/interpreter/interpreter-assembler.cc:844
:        
CallBuiltin(Builtin::kInterpreterPushArgsThenFa
stConstructFunction,
v8/src/interpreter/interpreter-assembler.cc:102
7:  return 
CallBuiltin(Builtin::kInterpreterForwardAllArgs
ThenConstruct, context,
v8/src/interpreter/interpreter-assembler.cc:140
2:void 
InterpreterAssembler::OnStackReplacement(
v8/src/interpreter/interpreter-assembler.cc:140
6:    OnStackReplacementParams params) {
v8/src/interpreter/interpreter-assembler.cc:144
0:  if (params == 
OnStackReplacementParams::kBaselineCodeIsCached
) {
v8/src/interpreter/interpreter-assembler.cc:144
3:    DCHECK_EQ(params, 
OnStackReplacementParams::kDefault);
v8/src/interpreter/interpreter-assembler.cc:146
7:    
CallBuiltin(Builtin::kInterpreterOnStackReplace
ment, context,
v8/src/interpreter/interpreter-assembler.cc:147
9:    
TailCallBuiltin(Builtin::kInterpreterOnStackRep
lacement_ToBaseline,
v8/src/wasm/wasm-code-manager.h:97:    
kInterpreterEntry,
v8/src/wasm/interpreter/arm64/interpreter-built
ins-arm64.cc:151:void 
Builtins::Generate_WasmInterpreterEntry(MacroAs
sembler* masm) {
v8/src/wasm/interpreter/arm64/interpreter-built
ins-arm64.cc:727:  __ 
Call(BUILTIN_CODE(masm->isolate(), 
WasmInterpreterEntry),
v8/src/wasm/interpreter/arm64/interpreter-built
ins-arm64.cc:1034:    
masm->isolate()->builtins()->SetCWasmInterprete
rEntryHandlerOffset(
v8/src/wasm/interpreter/wasm-interpreter-runtim
e.cc:52:Address 
FindInterpreterEntryFramePointer(Isolate* 
isolate) {
v8/src/wasm/interpreter/wasm-interpreter-runtim
e.cc:110:  Address frame_pointer = 
FindInterpreterEntryFramePointer(isolate);
v8/src/wasm/interpreter/wasm-interpreter-runtim
e.cc:1893:      Address frame_pointer = 
FindInterpreterEntryFramePointer(isolate_);
v8/src/wasm/interpreter/wasm-interpreter-runtim
e.cc:2061:    Address frame_pointer = 
FindInterpreterEntryFramePointer(isolate_);
v8/src/wasm/interpreter/wasm-interpreter-runtim
e.cc:2961:  WasmInterpreterEntryFrame* frame =
v8/src/wasm/interpreter/wasm-interpreter-runtim
e.cc:2962:      
WasmInterpreterEntryFrame::cast(it.frame());
v8/src/wasm/interpreter/wasm-interpreter.cc:414
:  // It is possible to have a 
WasmInterpreterEntryFrame without having a Wasm
v8/src/wasm/interpreter/x64/interpreter-builtin
s-x64.cc:164:void 
Builtins::Generate_WasmInterpreterEntry(MacroAs
sembler* masm) {
v8/src/wasm/interpreter/x64/interpreter-builtin
s-x64.cc:523:  __ 
Call(BUILTIN_CODE(masm->isolate(), 
WasmInterpreterEntry),
v8/src/wasm/interpreter/x64/interpreter-builtin
s-x64.cc:973:  
masm->isolate()->builtins()->SetCWasmInterprete
rEntryHandlerOffset(
v8/src/wasm/function-compiler.h:70:    
kInterpreterEntry,
v8/src/wasm/wasm-code-manager.cc:252:    case 
kInterpreterEntry:
v8/src/wasm/wasm-code-manager.cc:547:    case 
WasmCode::kInterpreterEntry:
v8/src/wasm/wasm-code-manager.cc:1436:    case 
WasmCompilationResult::kInterpreterEntry:
v8/src/wasm/wasm-code-manager.cc:1437:      
return WasmCode::Kind::kInterpreterEntry;
v8/test/js-perf-test/InterpreterEntryTrampoline
/run.js:11:  print(name + 
'-InterpreterEntryTrampoline(Score): ' + 
result);
v8/src/maglev/maglev-graph-builder.cc:13048:   
AddNewNode<TryOnStackReplacement>(
v8/src/maglev/maglev-ir.cc:6829:void 
AttemptOnStackReplacement(MaglevAssembler* 
masm,
v8/src/maglev/maglev-ir.cc:6831:               
TryOnStackReplacement* node, Register scratch0,
v8/src/maglev/maglev-ir.cc:6843:  // See also: 
InterpreterAssembler::OnStackReplacement.
v8/src/maglev/maglev-ir.cc:6896:    __ 
EmitEagerDeopt(node, 
DeoptimizeReason::kPrepareForOnStackReplacement
);
v8/src/maglev/maglev-ir.cc:6908:int 
TryOnStackReplacement::MaxCallStackArgs() const
{
v8/src/maglev/maglev-ir.cc:6913:void 
TryOnStackReplacement::SetValueLocationConstrai
nts() {
v8/src/maglev/maglev-ir.cc:6917:void 
TryOnStackReplacement::GenerateCode(MaglevAssem
bler* masm,
v8/src/maglev/maglev-ir.cc:6946:      __ 
MakeDeferredCode(AttemptOnStackReplacement, 
no_code_for_osr, this,
v8/src/maglev/maglev-ir.h:336:  
V(TryOnStackReplacement)                    \
v8/src/maglev/maglev-ir.h:4578:class 
TryOnStackReplacement : public 
FixedInputNodeT<1, TryOnStackReplacement> {
v8/src/maglev/maglev-ir.h:4579:  using Base = 
FixedInputNodeT<1, TryOnStackReplacement>;
v8/src/maglev/maglev-ir.h:4582:  explicit 
TryOnStackReplacement(uint64_t bitfield, 
int32_t loop_depth,
v8/src/diagnostics/etw-jit-win.cc:276:      // 
InterpreterEntryTrampoline builtin and only 
when the
v8/src/diagnostics/etw-jit-win.cc:280:         
code.value()->builtin_id() == 
Builtin::kInterpreterEntryTrampoline &&
v8/src/logging/log.cc:137:      
code->builtin_id(cage_base) == 
Builtin::kInterpreterEntryTrampoline) {
v8/src/logging/log.cc:2183:    
Compiler::InstallInterpreterTrampolineCopy(isol
ate_, sfi, log_tag);
v8/src/logging/log.cc:2515:                  
Builtin::kInterpreterEntryTrampoline);
v8/src/logging/log.cc:2598:              
Cast<AbstractCode>(shared->InterpreterTrampolin
e(isolate_)),
v8/src/execution/frames.h:55://       - 
WasmInterpreterEntryFrame (#if 
V8_ENABLE_DRUMBRAKE)
v8/src/execution/frames.h:124:  
IF_WASM_DRUMBRAKE(V, WASM_INTERPRETER_ENTRY, 
WasmInterpreterEntryFrame) \
v8/src/execution/frames.h:1342:class 
WasmInterpreterEntryFrame final : public 
WasmFrame {
v8/src/execution/frames.h:1368:  static 
WasmInterpreterEntryFrame* cast(StackFrame* 
frame) {
v8/src/execution/frames.h:1370:    return 
static_cast<WasmInterpreterEntryFrame*>(frame);
v8/src/execution/frames.h:1374:  inline 
explicit 
WasmInterpreterEntryFrame(StackFrameIteratorBas
e* iterator);
v8/src/logging/counters-definitions.h:262:  
HT(turbofan_osr_prepare, 
V8.TurboFanOptimizeForOnStackReplacementPrepare
,    \
v8/src/logging/counters-definitions.h:264:  
HT(turbofan_osr_execute, 
V8.TurboFanOptimizeForOnStackReplacementExecute
,    \
v8/src/logging/counters-definitions.h:266:  
HT(turbofan_osr_finalize, 
V8.TurboFanOptimizeForOnStackReplacementFinaliz
e,  \
v8/src/logging/counters-definitions.h:269:     
V8.TurboFanOptimizeForOnStackReplacementTotalTi
me, 10000000, MICROSECOND) \
v8/src/execution/frames-inl.h:334:inline 
WasmInterpreterEntryFrame::WasmInterpreterEntry
Frame(
v8/test/js-perf-test/JSTests3.json:462:      
"name": "InterpreterEntryTrampoline",
v8/test/js-perf-test/JSTests3.json:463:      
"path": |"InterpreterEntryTrampoline"],
v8/test/js-perf-test/JSTests3.json:471:        
"results_regexp": 
"^%s\\-InterpreterEntryTrampoline\\(Score\\): 
(.+)$",
v8/test/js-perf-test/JSTests3.json:484:        
"results_regexp": 
"^%s\\-InterpreterEntryTrampoline\\(Score\\): 
(.+)$",
v8/test/unittests/logging/log-unittest.cc:851: 
code_event_handler.CountLines("Builtin", 
"InterpreterEntryTrampoline"),
v8/src/execution/frames.cc:467:      (builtin 
== Builtin::kInterpreterEntryTrampoline ||
v8/src/execution/frames.cc:468:       builtin 
== Builtin::kInterpreterEnterAtBytecode ||
v8/src/execution/frames.cc:469:       builtin 
== Builtin::kInterpreterEnterAtNextBytecode)) {
v8/src/execution/frames.cc:980:      case 
wasm::WasmCode::kInterpreterEntry:
v8/src/execution/frames.cc:1723:    return 
wasm_code_ ? wasm_code_->kind() : 
wasm::WasmCode::kInterpreterEntry;
v8/src/execution/frames.cc:3814:void 
WasmInterpreterEntryFrame::Iterate(RootVisitor*
v) const {
v8/src/execution/frames.cc:3815:  //  
WasmInterpreterEntryFrame stack layout
v8/src/execution/frames.cc:3837:void 
WasmInterpreterEntryFrame::Print(StringStream* 
accumulator, PrintMode mode,
v8/src/execution/frames.cc:3847:FrameSummaries 
WasmInterpreterEntryFrame::Summarize() const {
v8/src/execution/frames.cc:3862:Tagged<HeapObje
ct> WasmInterpreterEntryFrame::unchecked_code()
const {
v8/src/execution/frames.cc:3866:Tagged<WasmInst
anceObject> 
WasmInterpreterEntryFrame::wasm_instance() 
const {
v8/src/execution/frames.cc:3873:WasmInterpreter
EntryFrame::trusted_instance_data() const {
v8/src/execution/frames.cc:3877:Tagged<Tuple2> 
WasmInterpreterEntryFrame::interpreter_object()
const {
v8/src/execution/frames.cc:3881:Tagged<WasmModu
leObject> 
WasmInterpreterEntryFrame::module_object() 
const {
v8/src/execution/frames.cc:3885:int 
WasmInterpreterEntryFrame::function_index(
v8/src/execution/frames.cc:3892:int 
WasmInterpreterEntryFrame::position() const {
v8/src/execution/frames.cc:3896:Tagged<Object> 
WasmInterpreterEntryFrame::context() const {
v8/src/execution/frames.cc:3900:Address 
WasmInterpreterEntryFrame::GetCallerStackPointe
r() const {
v8/src/compiler/code-assembler.h:655:  // 
include the CompileLazy builtin and the 
InterpreterEntryTrampoline, or the
v8/src/sandbox/js-dispatch-table-inl.h:326:  //
include builtins like the 
InterpreterEntryTrampoline or the JSToWasm and
v8/src/sandbox/js-dispatch-table-inl.h:341:    
case Builtin::kInterpreterEntryTrampoline:
v8/test/mjsunit/tools/tickprocessor-test-large.
default:33:      4    4.0%    1.3%  
v8::internal::Runtime_CompileForOnStackReplacem
ent(int, unsigned long*, 
v8::internal::Isolate*)
v8/test/mjsunit/tools/tickprocessor-test-large.
default:56:      3   12.5%    
v8::internal::Runtime_CompileForOnStackReplacem
ent(int, unsigned long*, 
v8::internal::Isolate*)
v8/test/mjsunit/tools/tickprocessor-test-large.
log:86:code-creation,Builtin,2,4725,0x557426327
980,1240,InterpreterEntryTrampoline
v8/test/mjsunit/tools/tickprocessor-test-large.
log:95:code-creation,Builtin,2,4845,0x557426328
200,56,InterpreterOnStackReplacement
v8/test/mjsunit/tools/tickprocessor-test-large.
log:97:code-creation,Builtin,2,4871,0x557426328
4c0,56,BaselineOnStackReplacement
v8/test/mjsunit/tools/tickprocessor-test-large.
log:538:code-creation,Builtin,2,10344,0x5574263
7d2a0,8,RegExpInterpreterTrampoline
v8/test/mjsunit/tools/tickprocessor-test-large.
log.symbols.json:8:    
"v8::internal::Runtime_CompileForOnStackReplace
ment(int, unsigned long*, 
v8::internal::Isolate*)",
v8/src/heap/mark-compact.cc:3586:    // 
InterpreterEntryTrampoline or baseline code.
v8/src/heap/heap.cc:158:void 
Heap::SetInterpreterEntryReturnPCOffset(int 
pc_offset) {
v8/src/heap/heap.h:1087:  void 
SetInterpreterEntryReturnPCOffset(int 
pc_offset);
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:4414:  maglev::ProcessResult 
Process(maglev::TryOnStackReplacement*,
v8/test/unittests/interpreter/interpreter-teste
r.h:166:      
function->UpdateCode(*BUILTIN_CODE(isolate_, 
InterpreterEntryTrampoline));
v8/test/unittests/interpreter/interpreter-unitt
est.cc:4767:      BUILTIN_CODE(i_isolate(), 
InterpreterEntryTrampoline);
v8/src/codegen/compiler.cc:660:void 
Compiler::InstallInterpreterTrampolineCopy(
v8/src/codegen/compiler.cc:672:      
Builtins::CreateInterpreterEntryTrampolineForPr
ofiling(isolate);
v8/src/codegen/compiler.cc:1500:      
Compiler::InstallInterpreterTrampolineCopy(isol
ate, shared_info, log_tag);
v8/src/codegen/arm64/macro-assembler-arm64.cc:4
389:      // InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/interface-descriptors.h:123:  
V(OnStackReplacement)                          
\
v8/src/codegen/interface-descriptors.h:2233:cla
ss OnStackReplacementDescriptor
v8/src/codegen/interface-descriptors.h:2234:   
: public 
StaticCallInterfaceDescriptor<OnStackReplacemen
tDescriptor> {
v8/src/codegen/interface-descriptors.h:2241:  
DECLARE_DESCRIPTOR(OnStackReplacementDescriptor
)
v8/src/codegen/bailout-reason.h:31:  
V(kFunctionDataShouldBeBytecodeArrayOnInterpret
erEntry,                      \
v8/src/codegen/external-reference.h:32:    
"Address of the InterpreterEntryTrampoline 
instruction start")             \
v8/src/codegen/x64/macro-assembler-x64.cc:1070:
// InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/code-stub-assembler.cc:17244:   
HeapConstantNoHole(BUILTIN_CODE(isolate(), 
InterpreterEntryTrampoline));
v8/src/codegen/code-stub-assembler.cc:17259:   
CAST(sfi_data), 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/codegen/loong64/macro-assembler-loong64.
cc:3981:      // InterpreterEntryTrampoline and
InterpreterEntryTrampolineForProfiling
v8/src/codegen/compiler.h:279:  static void 
InstallInterpreterTrampolineCopy(
v8/src/builtins/arm64/builtins-arm64.cc:1328:  
__ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/arm64/builtins-arm64.cc:1346:vo
id 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/arm64/builtins-arm64.cc:1347:  
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/arm64/builtins-arm64.cc:1500:  
__ RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/arm64/builtins-arm64.cc:1501:  
if (mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/arm64/builtins-arm64.cc:1502:  
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/arm64/builtins-arm64.cc:1505:  
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/arm64/builtins-arm64.cc:1999:  
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/arm64/builtins-arm64.cc:2003:  
eq, 
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/arm64/builtins-arm64.cc:2055:  
x1, FieldMemOperand(x1, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/arm64/builtins-arm64.cc:2259:vo
id OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/arm64/builtins-arm64.cc:2348:vo
id 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/arm64/builtins-arm64.cc:2349:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/arm64/builtins-arm64.cc:2351:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/arm64/builtins-arm64.cc:2356:vo
id 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/arm64/builtins-arm64.cc:2357:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/arm64/builtins-arm64.cc:2362:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/arm64/builtins-arm64.cc:5502:vo
id 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/riscv/builtins-riscv.cc:995:  
__ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/riscv/builtins-riscv.cc:1138:vo
id 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/riscv/builtins-riscv.cc:1139:  
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/riscv/builtins-riscv.cc:1261:  
__ RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/riscv/builtins-riscv.cc:1262:  
if (mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/riscv/builtins-riscv.cc:1263:  
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/riscv/builtins-riscv.cc:1266:  
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/riscv/builtins-riscv.cc:1740:  
t0, FieldMemOperand(t0, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/riscv/builtins-riscv.cc:1765:  
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/riscv/builtins-riscv.cc:1769:  
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/riscv/builtins-riscv.cc:1950:vo
id OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/riscv/builtins-riscv.cc:2035:vo
id 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/riscv/builtins-riscv.cc:2036:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/riscv/builtins-riscv.cc:2038:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/riscv/builtins-riscv.cc:2043:vo
id 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/riscv/builtins-riscv.cc:2044:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/riscv/builtins-riscv.cc:2049:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/riscv/builtins-riscv.cc:4964:vo
id 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/arm/builtins-arm.cc:1114:  __ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/arm/builtins-arm.cc:1132:void 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/arm/builtins-arm.cc:1133:    
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/arm/builtins-arm.cc:1253:  __ 
RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/arm/builtins-arm.cc:1254:  if 
(mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/arm/builtins-arm.cc:1255:    
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/arm/builtins-arm.cc:1258:    
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/arm/builtins-arm.cc:1735:      
FieldMemOperand(r2, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/arm/builtins-arm.cc:1761:      
ne, 
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/arm/builtins-arm.cc:1765:      
eq, 
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/arm/builtins-arm.cc:1945:void 
OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/arm/builtins-arm.cc:2019:void 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/arm/builtins-arm.cc:2020:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/arm/builtins-arm.cc:2022:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/arm/builtins-arm.cc:2027:void 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/arm/builtins-arm.cc:2028:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/arm/builtins-arm.cc:2033:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/arm/builtins-arm.cc:4920:void 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/builtins-lazy-gen.cc:150:  // A
usual case would be the 
InterpreterEntryTrampoline to start executing
v8/src/builtins/builtins-interpreter-gen.cc:12:
void 
Builtins::Generate_InterpreterEntryTrampoline(M
acroAssembler* masm) {
v8/src/builtins/builtins-interpreter-gen.cc:13:
Generate_InterpreterEntryTrampoline(masm,
v8/src/builtins/builtins-interpreter-gen.cc:14:
InterpreterEntryTrampolineMode::kDefault);
v8/src/builtins/builtins-interpreter-gen.cc:17:
void 
Builtins::Generate_InterpreterEntryTrampolineFo
rProfiling(
v8/src/builtins/builtins-interpreter-gen.cc:19:
Generate_InterpreterEntryTrampoline(
v8/src/builtins/builtins-interpreter-gen.cc:20:
masm, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/mips64/builtins-mips64.cc:1069:
__ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/mips64/builtins-mips64.cc:1087:
void 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/mips64/builtins-mips64.cc:1088:
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/mips64/builtins-mips64.cc:1212:
__ RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/mips64/builtins-mips64.cc:1213:
if (mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/mips64/builtins-mips64.cc:1214:
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/mips64/builtins-mips64.cc:1217:
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/mips64/builtins-mips64.cc:1683:
__ Ld(t0, FieldMemOperand(t0, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/mips64/builtins-mips64.cc:1708:
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/mips64/builtins-mips64.cc:1712:
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/mips64/builtins-mips64.cc:1891:
void OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/mips64/builtins-mips64.cc:1971:
void 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/mips64/builtins-mips64.cc:1972:
using D = OnStackReplacementDescriptor;
v8/src/builtins/mips64/builtins-mips64.cc:1974:
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/mips64/builtins-mips64.cc:1979:
void 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/mips64/builtins-mips64.cc:1980:
using D = OnStackReplacementDescriptor;
v8/src/builtins/mips64/builtins-mips64.cc:1985:
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/mips64/builtins-mips64.cc:3879:
void 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/builtins.cc:430:Handle<Code> 
Builtins::CreateInterpreterEntryTrampolineForPr
ofiling(
v8/src/builtins/builtins.cc:436:      
Builtin::kInterpreterEntryTrampolineForProfilin
g);
v8/src/builtins/builtins.cc:466:      // Mimic 
the InterpreterEntryTrampoline.
v8/src/builtins/builtins.cc:467:      
.set_builtin(Builtin::kInterpreterEntryTrampoli
ne)
v8/src/builtins/ia32/builtins-ia32.cc:893:void 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/ia32/builtins-ia32.cc:894:    
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/ia32/builtins-ia32.cc:967:     
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/ia32/builtins-ia32.cc:1041:  __
RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/ia32/builtins-ia32.cc:1042:  if
(mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/ia32/builtins-ia32.cc:1043:    
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/ia32/builtins-ia32.cc:1046:    
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/ia32/builtins-ia32.cc:1738:    
FieldOperand(scratch, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/ia32/builtins-ia32.cc:1771:    
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/ia32/builtins-ia32.cc:1941:    
// InterpreterEntryTrampoline.
v8/src/builtins/ia32/builtins-ia32.cc:2017:  __
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/ia32/builtins-ia32.cc:2985:void
OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/ia32/builtins-ia32.cc:3059:void
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/ia32/builtins-ia32.cc:3060:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/ia32/builtins-ia32.cc:3062:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/ia32/builtins-ia32.cc:3067:void
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/ia32/builtins-ia32.cc:3068:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/ia32/builtins-ia32.cc:3073:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/ia32/builtins-ia32.cc:5394:void
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/builtins.h:305:  // Creates a 
copy of InterpreterEntryTrampolineForProfiling 
in the code space.
v8/src/builtins/builtins.h:306:  static 
Handle<Code> 
CreateInterpreterEntryTrampolineForProfiling(
v8/src/builtins/builtins.h:340:  void 
SetCWasmInterpreterEntryHandlerOffset(int 
offset) {
v8/src/builtins/builtins.h:376:  enum class 
InterpreterEntryTrampolineMode {
v8/src/builtins/builtins.h:377:    // The 
version of InterpreterEntryTrampoline used by 
default.
v8/src/builtins/builtins.h:379:    // The 
position independent version of 
InterpreterEntryTrampoline used as
v8/src/builtins/builtins.h:385:  static void 
Generate_InterpreterEntryTrampoline(
v8/src/builtins/builtins.h:386:      
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode);
v8/src/builtins/builtins.h:426:  // label) in 
CWasmInterpreterEntry. It's used to generate 
the handler table
v8/src/builtins/builtins.h:438:V8_INLINE 
constexpr bool 
IsInterpreterTrampolineBuiltin(Builtin 
builtin_id) {
v8/src/builtins/builtins.h:442:         
(builtin_id == 
Builtin::kInterpreterEntryTrampoline ||
v8/src/builtins/builtins.h:443:          
builtin_id == 
Builtin::kInterpreterEnterAtBytecode ||
v8/src/builtins/builtins.h:444:          
builtin_id == 
Builtin::kInterpreterEnterAtNextBytecode);
v8/src/builtins/s390/builtins-s390.cc:133:void 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/s390/builtins-s390.cc:241:void 
OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/s390/builtins-s390.cc:1377:  __
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/s390/builtins-s390.cc:1396:void
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/s390/builtins-s390.cc:1397:    
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/s390/builtins-s390.cc:1527:  __
RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/s390/builtins-s390.cc:1528:  if
(mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/s390/builtins-s390.cc:1529:    
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/s390/builtins-s390.cc:1532:    
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/s390/builtins-s390.cc:2010:    
r4, FieldMemOperand(r4, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/s390/builtins-s390.cc:2036:    
ne, 
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/s390/builtins-s390.cc:2040:    
eq, 
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/s390/builtins-s390.cc:4201:void
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/s390/builtins-s390.cc:4202:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/s390/builtins-s390.cc:4204:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/s390/builtins-s390.cc:4209:void
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/s390/builtins-s390.cc:4210:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/s390/builtins-s390.cc:4215:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/builtins-definitions.h:243:  /*
InterpreterEntryTrampoline dispatches to the 
interpreter to run a */      \
v8/src/builtins/builtins-definitions.h:245:  
ASM(InterpreterEntryTrampoline, JSTrampoline)  
\
v8/src/builtins/builtins-definitions.h:246:  
ASM(InterpreterEntryTrampolineForProfiling, 
JSTrampoline)                    \
v8/src/builtins/builtins-definitions.h:260:  
ASM(InterpreterOnStackReplacement, 
OnStackReplacement)                       \
v8/src/builtins/builtins-definitions.h:265:  
ASM(BaselineOnStackReplacement, 
OnStackReplacement)                          \
v8/src/builtins/builtins-definitions.h:267:  
ASM(InterpreterOnStackReplacement_ToBaseline, 
Void)                          \
v8/src/builtins/builtins-definitions.h:989:  
ASM(RegExpInterpreterTrampoline, 
RegExpTrampoline)                           \
v8/src/builtins/builtins-definitions.h:1102:  
IF_WASM_DRUMBRAKE(ASM, WasmInterpreterEntry, 
WasmDummy)                      \
v8/src/builtins/ppc/builtins-ppc.cc:160:void 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/ppc/builtins-ppc.cc:375:void 
OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/ppc/builtins-ppc.cc:1350:  __ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/ppc/builtins-ppc.cc:1369:void 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/ppc/builtins-ppc.cc:1370:    
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/ppc/builtins-ppc.cc:1503:  __ 
RecordComment("--- InterpreterEntryReturnPC 
point ---");
v8/src/builtins/ppc/builtins-ppc.cc:1504:  if 
(mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/ppc/builtins-ppc.cc:1505:    
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/ppc/builtins-ppc.cc:1508:    
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/ppc/builtins-ppc.cc:1993:      
r5, FieldMemOperand(r5, 
InterpreterData::kInterpreterTrampolineOffset),
v8/src/builtins/ppc/builtins-ppc.cc:2021:      
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/ppc/builtins-ppc.cc:2026:      
eq, 
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/ppc/builtins-ppc.cc:2192:void 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/ppc/builtins-ppc.cc:2193:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/ppc/builtins-ppc.cc:2195:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/ppc/builtins-ppc.cc:2200:void 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/ppc/builtins-ppc.cc:2201:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/ppc/builtins-ppc.cc:2206:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/builtins-inl.h:211:      return
Builtin::kInterpreterPushArgsThenCallWithFinalS
pread;
v8/src/builtins/builtins-inl.h:215:          
return 
Builtin::kInterpreterPushUndefinedAndArgsThenCa
ll;
v8/src/builtins/builtins-inl.h:218:          
return Builtin::kInterpreterPushArgsThenCall;
v8/src/builtins/builtins-inl.h:229:      return
Builtin::kInterpreterPushArgsThenConstructArray
Function;
v8/src/builtins/builtins-inl.h:231:      return
Builtin::kInterpreterPushArgsThenConstructWithF
inalSpread;
v8/src/builtins/builtins-inl.h:233:      return
Builtin::kInterpreterPushArgsThenConstruct;
v8/src/builtins/x64/builtins-x64.cc:1106:void 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/x64/builtins-x64.cc:1107:    
MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/x64/builtins-x64.cc:1257:  __ 
RecordComment("--- InterpreterEntryPC point 
---");
v8/src/builtins/x64/builtins-x64.cc:1258:  if 
(mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/x64/builtins-x64.cc:1259:    
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/x64/builtins-x64.cc:1262:    
DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/x64/builtins-x64.cc:1780:      
rbx, FieldOperand(rbx, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/x64/builtins-x64.cc:1813:      
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry);
v8/src/builtins/x64/builtins-x64.cc:1981:    //
InterpreterEntryTrampoline.
v8/src/builtins/x64/builtins-x64.cc:2065:  __ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/x64/builtins-x64.cc:2971:void 
OnStackReplacement(MacroAssembler* masm, 
OsrSourceTier source,
v8/src/builtins/x64/builtins-x64.cc:3068:void 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/x64/builtins-x64.cc:3069:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/x64/builtins-x64.cc:3071:  
OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/x64/builtins-x64.cc:3076:void 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/x64/builtins-x64.cc:3077:  
using D = OnStackReplacementDescriptor;
v8/src/builtins/x64/builtins-x64.cc:3081:  
OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/x64/builtins-x64.cc:5189:void 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/loong64/builtins-loong64.cc:110
6:  __ 
TailCallBuiltin(Builtin::kInterpreterEntryTramp
oline);
v8/src/builtins/loong64/builtins-loong64.cc:112
5:void 
Builtins::Generate_InterpreterEntryTrampoline(
v8/src/builtins/loong64/builtins-loong64.cc:112
6:    MacroAssembler* masm, 
InterpreterEntryTrampolineMode mode) {
v8/src/builtins/loong64/builtins-loong64.cc:125
4:  __ RecordComment("--- 
InterpreterEntryReturnPC point ---");
v8/src/builtins/loong64/builtins-loong64.cc:125
5:  if (mode == 
InterpreterEntryTrampolineMode::kDefault) {
v8/src/builtins/loong64/builtins-loong64.cc:125
6:    
masm->isolate()->heap()->SetInterpreterEntryRet
urnPCOffset(
v8/src/builtins/loong64/builtins-loong64.cc:125
9:    DCHECK_EQ(mode, 
InterpreterEntryTrampolineMode::kForProfiling);
v8/src/builtins/loong64/builtins-loong64.cc:173
7:      t0, FieldMemOperand(t0, 
InterpreterData::kInterpreterTrampolineOffset))
;
v8/src/builtins/loong64/builtins-loong64.cc:176
2:              
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/loong64/builtins-loong64.cc:176
6:              
AbortReason::kFunctionDataShouldBeBytecodeArray
OnInterpreterEntry,
v8/src/builtins/loong64/builtins-loong64.cc:194
5:void OnStackReplacement(MacroAssembler* masm,
OsrSourceTier source,
v8/src/builtins/loong64/builtins-loong64.cc:203
1:void 
Builtins::Generate_InterpreterOnStackReplacemen
t(MacroAssembler* masm) {
v8/src/builtins/loong64/builtins-loong64.cc:203
2:  using D = OnStackReplacementDescriptor;
v8/src/builtins/loong64/builtins-loong64.cc:203
4:  OnStackReplacement(masm, 
OsrSourceTier::kInterpreter,
v8/src/builtins/loong64/builtins-loong64.cc:203
9:void 
Builtins::Generate_BaselineOnStackReplacement(M
acroAssembler* masm) {
v8/src/builtins/loong64/builtins-loong64.cc:204
0:  using D = OnStackReplacementDescriptor;
v8/src/builtins/loong64/builtins-loong64.cc:204
5:  OnStackReplacement(masm, 
OsrSourceTier::kBaseline,
v8/src/builtins/loong64/builtins-loong64.cc:495
0:void 
Builtins::Generate_InterpreterOnStackReplacemen
t_ToBaseline(
v8/src/builtins/builtins-regexp-gen.cc:32:void 
Builtins::Generate_RegExpInterpreterTrampoline(
MacroAssembler* masm) {
v8/src/builtins/setup-builtins-internal.cc:86: 
if (builtin == 
Builtin::kInterpreterEntryTrampolineForProfilin
g) {
v8/src/builtins/setup-builtins-internal.cc:87: 
// InterpreterEntryTrampolineForProfiling must 
be generated in a position
v8/src/deoptimizer/deoptimizer.cc:521:  
V(Builtin::kInterpreterEnterAtBytecode,        
\
v8/src/deoptimizer/deoptimizer.cc:523:  
V(Builtin::kInterpreterEnterAtNextBytecode,    
\
v8/src/deoptimizer/deoptimizer.cc:536:  
V(Builtin::kInterpreterPushArgsThenFastConstruc
tFunction,                  \
v8/src/deoptimizer/deoptimizer.cc:1775:  return
advance_bc ? 
Builtin::kInterpreterEnterAtNextBytecode
v8/src/deoptimizer/deoptimizer.cc:1776:        
: Builtin::kInterpreterEnterAtBytecode;
v8/src/deoptimizer/deoptimizer.cc:2397:      
Builtin::kInterpreterPushArgsThenFastConstructF
unction);
v8/src/deoptimizer/deoptimize-reason.h:19:  
V(PrepareForOnStackReplacement, "prepare for on
stack replacement (OSR)")    \
v8/src/deoptimizer/deoptimize-reason.h:150:  
return reason == 
DeoptimizeReason::kPrepareForOnStackReplacement
||
v8/src/objects/shared-function-info.h:406:  
inline Tagged<Code> 
InterpreterTrampoline(IsolateForSandbox 
isolate) const;
v8/src/codegen/riscv/macro-assembler-riscv.cc:6
389:      // InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/interface-descriptors-inl.h:496:
constexpr auto 
OnStackReplacementDescriptor::registers() {
v8/src/codegen/interface-descriptors-inl.h:519:
constexpr Register 
OnStackReplacementDescriptor::MaybeTargetCodeRe
gister() {
v8/src/codegen/interface-descriptors-inl.h:527:
OnStackReplacementDescriptor::ExpectedParameter
CountRegister() {
v8/src/codegen/arm/macro-assembler-arm.cc:2484:
// InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/mips64/macro-assembler-mips64.cc
:5397:      // InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/ia32/macro-assembler-ia32.cc:192
7:      // InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/s390/macro-assembler-s390.cc:232
0:      // InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/src/codegen/ppc/macro-assembler-ppc.cc:2482:
// InterpreterEntryTrampoline and 
InterpreterEntryTrampolineForProfiling
v8/tools/v8windbg/test/v8windbg-test.cc:234:  
//     
"f.ToDisplayString().Contains(\"InterpreterEntr
yTrampoline\")).Skip(1)."
v8/test/mjsunit/regress/regress-718891.js:66://
the InterpreterEntryTrampoline respectively.
v8/src/objects/objects-body-descriptors-inl.h:8
24:    IterateProtectedPointer(obj, 
kInterpreterTrampolineOffset, v);
v8/test/mjsunit/baseline/test-baseline.js:301:/
/ Tier-up via InterpreterEntryTrampoline
v8/src/objects/code-inl.h:351:  // 
Generate_BaselineOrInterpreterEntry with the 
bytecode that was used to
v8/src/objects/code-inl.h:411:                 
builtin_id() == 
Builtin::kInterpreterEntryTrampoline ||
v8/src/objects/code-inl.h:528:            
handle, *BUILTIN_CODE(isolate, 
InterpreterEntryTrampoline));
v8/src/objects/code-inl.h:925:  return 
IsInterpreterTrampolineBuiltin(builtin_id());
v8/src/objects/js-function-inl.h:564:    
UpdateCode(*BUILTIN_CODE(isolate, 
InterpreterEntryTrampoline));
v8/src/objects/shared-function-info.cc:88:     
return 
isolate->builtins()->code(Builtin::kInterpreter
EntryTrampoline);
v8/src/objects/shared-function-info.cc:96:     
Tagged<Code> code = 
InterpreterTrampoline(isolate);
v8/src/objects/shared-function-info-inl.h:110: 
kInterpreterTrampolineOffset)
v8/src/objects/shared-function-info-inl.h:774:T
agged<Code> 
SharedFunctionInfo::InterpreterTrampoline(
Observations: v8/BUILD.bazel:1505:        
"src/deoptimizer/materialized-object-store.cc",
v8/BUILD.bazel:1506:        
"src/deoptimizer/materialized-object-store.h",
v8/src/debug/debug-evaluate.cc:113:  // 
materialized and included on top of the native 
context. Changes to
v8/src/debug/debug-evaluate.cc:114:  // the 
materialized object are written back 
afterwards.
v8/src/debug/debug-evaluate.cc:140:  // 
Materialize arguments as property on an 
extension object.
v8/src/debug/debug-evaluate.cc:141:  
DirectHandle<JSObject> materialized = 
factory->NewSlowJSObjectWithNullProto();
v8/src/debug/debug-evaluate.cc:144:      
materialized, arguments_str,
v8/src/debug/debug-evaluate.cc:148:  // 
Materialize receiver.
v8/src/debug/debug-evaluate.cc:153:    
JSObject::SetOwnPropertyIgnoreAttributes(materi
alized, this_str, this_value,
v8/src/debug/debug-evaluate.cc:163:      
native_context, scope_info, materialized, 
DirectHandle<Context>());
v8/src/debug/debug-evaluate.cc:216:  //  - To 
make stack-allocated variables visible, we 
materialize them and
v8/src/debug/debug-evaluate.cc:217:  //    use 
a debug-evaluate context to wrap both the 
materialized object and
v8/src/debug/debug-evaluate.cc:228:  //  - Look
up in the materialized stack variables.
v8/src/debug/debug-evaluate.cc:238:      
context_chain_element.materialized_object =
v8/src/debug/debug-evaluate.cc:277:        
evaluation_context_, scope_info, 
element.materialized_object,
v8/src/debug/debug-evaluate.cc:285:    if 
(!element.materialized_object.is_null()) {
v8/src/debug/debug-evaluate.cc:287:          
KeyAccumulator::GetKeys(isolate_, 
element.materialized_object,
v8/src/debug/debug-evaluate.cc:296:            
isolate_, element.materialized_object, key);
v8/src/debug/debug-evaluate.h:33:  // - 
Parameters and stack-allocated locals need to 
be materialized.  Altered
v8/src/debug/debug-evaluate.h:35:  // - The 
arguments object needs to materialized.
v8/src/debug/debug-evaluate.h:47:  // 
materialized arguments object and receiver of 
the current call.
v8/src/debug/debug-evaluate.h:68:  // The 
builder materializes all stack variables into 
properties of objects;
v8/src/debug/debug-evaluate.h:92:      
Handle<JSObject> materialized_object;
v8/src/debug/debug.h:685:    // deoptimizer 
uses the info to materialize and drop execution
into the
v8/src/debug/debug-scopes.h:58:  
Handle<JSObject> MaterializeScopeDetails();
v8/src/debug/debug-scopes.cc:32:    // 
Optimized frame, context or function cannot be 
materialized. Give up.
v8/src/debug/debug-scopes.cc:377:Handle<JSObjec
t> ScopeIterator::MaterializeScopeDetails() {
v8/src/debug/debug-scopes.cc:890:      // We 
want to materialize "new.target" for 
debug-evaluate.
v8/src/debug/debug-scopes.cc:938:            //
We'll rematerialize this later.
v8/src/debug/debug-scopes.cc:997:// Create a 
plain JSObject which materializes the block 
scope for the specified
v8/src/debug/debug-scopes.cc:1014:      // 
Currently we don't yet support materializing 
the arguments object of
v8/include/cppgc/internal/name-trait.h:47:  // 
substring of __PRETTY_FUNCTION__ gets 
materialized in the binary.
v8/third_party/inspector_protocol/crdtp/error_s
upport.h:24:// stack is materialized and 
prefixes the error message. E.g.,
v8/third_party/inspector_protocol/crdtp/error_s
upport.h:40:  // Materializes the error 
internally. |error| must be a C++ string 
literal
v8/tools/gcmole/gcmole.cc:716:    
VISIT(MaterializeTemporaryExpr);
v8/tools/gcmole/gcmole.cc:826:  
DECL_VISIT_EXPR(MaterializeTemporaryExpr) {
v8/src/builtins/accessors.cc:422:  // 
Materialize the function.
v8/src/builtins/accessors.cc:423:  bool 
should_deoptimize = 
iter->IsMaterializedObject();
v8/src/builtins/accessors.cc:435:    // If we 
materialize any object, we should deoptimize 
the frame because we
v8/src/builtins/accessors.cc:437:    
should_deoptimize = should_deoptimize || 
iter->IsMaterializedObject();
v8/src/builtins/accessors.cc:445:    
translated_values.StoreMaterializedValuesAndDeo
pt(frame);
v8/src/builtins/accessors.cc:603:  // In case 
of inlined frames the function could have been 
materialized from
v8/src/builtins/accessors.cc:606:  // the value
to JavaScript. Make sure to store the 
materialized value and
v8/src/builtins/accessors.cc:608:  
DirectHandle<JSFunction> MaterializeFunction() 
{
v8/src/builtins/accessors.cc:620:    bool 
should_deoptimize = 
iter->IsMaterializedObject();
v8/src/builtins/accessors.cc:623:      
translated_values.StoreMaterializedValuesAndDeo
pt(frame);
v8/src/builtins/accessors.cc:686:  // 
Materialize the function that the iterator is 
currently sitting on. Note
v8/src/builtins/accessors.cc:688:  // 
materialized. Identity of the function must be 
preserved because we are
v8/src/builtins/accessors.cc:690:  
DirectHandle<JSFunction> caller = 
it.MaterializeFunction();
v8/src/codegen/code-stub-assembler.h:2126:     
const Function& materialize_target,
v8/src/codegen/code-stub-assembler-inl.h:93:   
const Function& materialize_target, bool 
target_is_new) {
v8/src/codegen/code-stub-assembler-inl.h:146:  
TNode<JSReceiver> target = materialize_target(
v8/src/deoptimizer/translated-state.h:62:  // 
Returns the value, possibly materializing it 
first (and the whole subgraph
v8/src/deoptimizer/translated-state.h:67:  bool
IsMaterializedObject() const;
v8/src/deoptimizer/translated-state.h:68:  bool
IsMaterializableByDebugger() const;
v8/src/deoptimizer/translated-state.h:99:  enum
MaterializationState : uint8_t {
v8/src/deoptimizer/translated-state.h:110:  
MaterializationState materialization_state() 
const {
v8/src/deoptimizer/translated-state.h:111:    
return materialization_state_;
v8/src/deoptimizer/translated-state.h:142:  
void mark_finished() { materialization_state_ =
kFinished; }
v8/src/deoptimizer/translated-state.h:143:  
void mark_allocated() { materialization_state_ 
= kAllocated; }
v8/src/deoptimizer/translated-state.h:146:    
DCHECK_NE(materialization_state(), 
kUninitialized);
v8/src/deoptimizer/translated-state.h:153:  
MaterializationState materialization_state_ = 
kUninitialized;
v8/src/deoptimizer/translated-state.h:154:  
TranslatedState* container_;  // This is only 
needed for materialization of
v8/src/deoptimizer/translated-state.h:158:  
Handle<HeapObject> storage_;  // Contains the 
materialized value or the
v8/src/deoptimizer/translated-state.h:160:     
// the materialized object.
v8/src/deoptimizer/translated-state.h:162:  
struct MaterializedObjectInfo {
v8/src/deoptimizer/translated-state.h:183:    
MaterializedObjectInfo materialization_info_;
v8/src/deoptimizer/translated-state.h:469:  // 
Store newly materialized values into the 
isolate.
v8/src/deoptimizer/translated-state.h:470:  
void 
StoreMaterializedValuesAndDeopt(JavaScriptFrame
* frame);
v8/src/deoptimizer/translated-state.h:495:  
void VerifyMaterializedObjects();
v8/src/deoptimizer/translated-state.h:522:  
void UpdateFromPreviouslyMaterializedObjects();
v8/src/deoptimizer/translated-state.h:523:  
void 
MaterializeFixedDoubleArray(TranslatedFrame* 
frame, int* value_index,
v8/src/deoptimizer/translated-state.h:526:  
void MaterializeHeapNumber(TranslatedFrame* 
frame, int* value_index,
v8/src/deoptimizer/deoptimized-frame-info.cc:18
:      !it->IsMaterializableByDebugger()) {
v8/src/deoptimizer/deoptimized-frame-info.cc:34
:  // Get the function. Note that this might 
materialize the function.
v8/src/deoptimizer/deoptimized-frame-info.cc:36
:  // the function and remember the value in 
the materialized value store.
v8/src/deoptimizer/materialized-object-store.h:
18:class MaterializedObjectStore {
v8/src/deoptimizer/materialized-object-store.h:
20:  explicit MaterializedObjectStore(Isolate* 
isolate) : isolate_(isolate) {}
v8/src/deoptimizer/materialized-object-store.h:
23:  void Set(Address fp, 
DirectHandle<FixedArray> materialized_objects);
v8/src/deoptimizer/deoptimizer.cc:15:#include 
"src/deoptimizer/materialized-object-store.h"
v8/src/deoptimizer/deoptimizer.cc:180:    
deoptimizer_->QueueValueForMaterialization(outp
ut_address(top_offset_), obj,
v8/src/deoptimizer/deoptimizer.cc:184:  void 
PushFeedbackVectorForMaterialization(
v8/src/deoptimizer/deoptimizer.cc:189:    
deoptimizer_->QueueFeedbackVectorForMaterializa
tion(
v8/src/deoptimizer/deoptimizer.cc:1301:// 
FrameDescription objects representing the 
output frames to be "materialized"
v8/src/deoptimizer/deoptimizer.cc:1563:    // 
If the debugger requested to restart a 
particular frame, only materialize
v8/src/deoptimizer/deoptimizer.cc:1965:  // We 
need to materialize the closure before getting 
the feedback vector.
v8/src/deoptimizer/deoptimizer.cc:1966:  
frame_writer.PushFeedbackVectorForMaterializati
on(function_iterator);
v8/src/deoptimizer/deoptimizer.cc:2020:    // 
after materialization).
v8/src/deoptimizer/deoptimizer.cc:2075:  // 
Clear the context register. The context might 
be a de-materialized object
v8/src/deoptimizer/deoptimizer.cc:2076:  // and
will be materialized by 
{Runtime_NotifyDeoptimized}. For additional
v8/src/deoptimizer/deoptimizer.cc:2292:  // 
Clear the context register. The context might 
be a de-materialized object
v8/src/deoptimizer/deoptimizer.cc:2293:  // and
will be materialized by 
{Runtime_NotifyDeoptimized}. For additional
v8/src/deoptimizer/deoptimizer.cc:2423:  // 
Clear the context register. The context might 
be a de-materialized object
v8/src/deoptimizer/deoptimizer.cc:2424:  // and
will be materialized by 
{Runtime_NotifyDeoptimized}. For additional
v8/src/deoptimizer/deoptimizer.cc:2851:  // 
Clear the context register. The context might 
be a de-materialized object
v8/src/deoptimizer/deoptimizer.cc:2852:  // and
will be materialized by 
{Runtime_NotifyDeoptimized}. For additional
v8/src/deoptimizer/deoptimizer.cc:2891:void 
Deoptimizer::MaterializeHeapObjects() {
v8/src/deoptimizer/deoptimizer.cc:2899:  for 
(auto& materialization : 
values_to_materialize_) {
v8/src/deoptimizer/deoptimizer.cc:2900:    
DirectHandle<Object> value = 
materialization.value_->GetValue();
v8/src/deoptimizer/deoptimizer.cc:2904:        
"Materialization |" V8PRIxPTR_FMT "] <- " 
V8PRIxPTR_FMT " ;  ",
v8/src/deoptimizer/deoptimizer.cc:2905:        
static_cast<intptr_t>(materialization.output_sl
ot_address_),
v8/src/deoptimizer/deoptimizer.cc:2911:    
*(reinterpret_cast<Address*>(materialization.ou
tput_slot_address_)) =
v8/src/deoptimizer/deoptimizer.cc:2915:  for 
(auto& fbv_materialization : 
feedback_vector_to_materialize_) {
v8/src/deoptimizer/deoptimizer.cc:2916:    
DirectHandle<Object> closure = 
fbv_materialization.value_->GetValue();
v8/src/deoptimizer/deoptimizer.cc:2921:    
*(reinterpret_cast<Address*>(fbv_materializatio
n.output_slot_address_)) =
v8/src/deoptimizer/deoptimizer.cc:2925:  
translated_state_.VerifyMaterializedObjects();
v8/src/deoptimizer/deoptimizer.cc:2937:  
isolate_->materialized_object_store()->Remove(
v8/src/deoptimizer/deoptimizer.cc:2941:void 
Deoptimizer::QueueValueForMaterialization(
v8/src/deoptimizer/deoptimizer.cc:2945:    
values_to_materialize_.push_back({output_addres
s, iterator});
v8/src/deoptimizer/deoptimizer.cc:2949:void 
Deoptimizer::QueueFeedbackVectorForMaterializat
ion(
v8/src/deoptimizer/deoptimizer.cc:2951:  
feedback_vector_to_materialize_.push_back({outp
ut_address, iterator});
v8/src/deoptimizer/translated-state.cc:15:#incl
ude 
"src/deoptimizer/materialized-object-store.h"
v8/src/deoptimizer/translated-state.cc:381:  
slot.materialization_info_ = {object_index, 
length};
v8/src/deoptimizer/translated-state.cc:389:  
slot.materialization_info_ = {id, -1};
v8/src/deoptimizer/translated-state.cc:397:  
slot.materialization_info_ = {id, -1};
v8/src/deoptimizer/translated-state.cc:545:  
return materialization_info_.length_;
v8/src/deoptimizer/translated-state.cc:551:  
return materialization_info_.id_;
v8/src/deoptimizer/translated-state.cc:556:  if
(materialization_state() == kFinished) {
v8/src/deoptimizer/translated-state.cc:690:  
DCHECK_EQ(kUninitialized, 
materialization_state());
v8/src/deoptimizer/translated-state.cc:692:  
materialization_state_ = kFinished;
v8/src/deoptimizer/translated-state.cc:697:  if
(materialization_state() == kFinished) return 
value;
v8/src/deoptimizer/translated-state.cc:701:    
// HeapNumber when materializing a JSObject 
with a field of HeapObject
v8/src/deoptimizer/translated-state.cc:715:  //
Otherwise we have to materialize.
v8/src/deoptimizer/translated-state.cc:719:    
// We need to materialize the object (or 
possibly even object graphs).
v8/src/deoptimizer/translated-state.cc:720:    
// To make the object verifier happy, we 
materialize in two steps.
v8/src/deoptimizer/translated-state.cc:737:    
// We need to materialize the string 
concatenation.
v8/src/deoptimizer/translated-state.cc:782:bool
TranslatedValue::IsMaterializedObject() const {
v8/src/deoptimizer/translated-state.cc:793:bool
TranslatedValue::IsMaterializableByDebugger() 
const {
v8/src/deoptimizer/translated-state.cc:794:  //
At the moment, we only allow materialization of
doubles.
v8/src/deoptimizer/translated-state.cc:1993:  
UpdateFromPreviouslyMaterializedObjects();
v8/src/deoptimizer/translated-state.cc:2003:  
if (slot->materialization_state() == 
TranslatedValue::kFinished) {
v8/src/deoptimizer/translated-state.cc:2007:  
CHECK_EQ(TranslatedValue::kUninitialized, 
slot->materialization_state());
v8/src/deoptimizer/translated-state.cc:2022:  
// do `value_index++`, because the left input 
could itself be a dematerialized
v8/src/deoptimizer/translated-state.cc:2043:  
if (slot->materialization_state() != 
TranslatedValue::kFinished) {
v8/src/deoptimizer/translated-state.cc:2071:  
CHECK_EQ(TranslatedValue::kFinished, 
slot->materialization_state());
v8/src/deoptimizer/translated-state.cc:2083:   
if (child_slot->materialization_state() != 
TranslatedValue::kFinished) {
v8/src/deoptimizer/translated-state.cc:2085:   
child_slot->materialization_state());
v8/src/deoptimizer/translated-state.cc:2094:  
// The map should never be materialized, so let
us check we already have
v8/src/deoptimizer/translated-state.cc:2148:  
if (slot->materialization_state() == 
TranslatedValue::kUninitialized) {
v8/src/deoptimizer/translated-state.cc:2167:voi
d 
TranslatedState::MaterializeFixedDoubleArray(Tr
anslatedFrame* frame,
v8/src/deoptimizer/translated-state.cc:2191:voi
d 
TranslatedState::MaterializeHeapNumber(Translat
edFrame* frame,
v8/src/deoptimizer/translated-state.cc:2234:  
CHECK_EQ(TranslatedValue::kAllocated, 
slot->materialization_state());
v8/src/deoptimizer/translated-state.cc:2238:  
// The map should never be materialized, so let
us check we already have
v8/src/deoptimizer/translated-state.cc:2248:   
// Materialize (i.e. allocate&initialize) the 
array and return since
v8/src/deoptimizer/translated-state.cc:2250:   
return MaterializeFixedDoubleArray(frame, 
&value_index, slot, map);
v8/src/deoptimizer/translated-state.cc:2253:   
// Materialize (i.e. allocate&initialize) the 
heap number and return.
v8/src/deoptimizer/translated-state.cc:2255:   
return MaterializeHeapNumber(frame, 
&value_index, slot);
v8/src/deoptimizer/translated-state.cc:2328:   
// We are materializing the property array, so 
make sure we put the
v8/src/deoptimizer/translated-state.cc:2347:   
// We are materializing a JSArray for the 
purpose of frame inspection.
v8/src/deoptimizer/translated-state.cc:2366:  
DCHECK_EQ(materialization_state(), 
TranslatedValue::kFinished);
v8/src/deoptimizer/translated-state.cc:2393:   
} else if (child_slot->materialization_state() 
==
v8/src/deoptimizer/translated-state.cc:2410:   
properties_slot->materialization_state());
v8/src/deoptimizer/translated-state.cc:2483:  
CHECK_NE(slot->materialization_state(), 
TranslatedValue::kUninitialized);
v8/src/deoptimizer/translated-state.cc:2553:   
// The JSDispatchHandle will be materialized as
a number, but we need
v8/src/deoptimizer/translated-state.cc:2729:voi
d 
TranslatedState::StoreMaterializedValuesAndDeop
t(JavaScriptFrame* frame) {
v8/src/deoptimizer/translated-state.cc:2730:  
MaterializedObjectStore* materialized_store =
v8/src/deoptimizer/translated-state.cc:2731:   
isolate_->materialized_object_store();
v8/src/deoptimizer/translated-state.cc:2732:  
DirectHandle<FixedArray> 
previously_materialized_objects =
v8/src/deoptimizer/translated-state.cc:2733:   
materialized_store->Get(stack_frame_pointer_);
v8/src/deoptimizer/translated-state.cc:2739:  
if (previously_materialized_objects.is_null()) 
{
v8/src/deoptimizer/translated-state.cc:2740:   
previously_materialized_objects =
v8/src/deoptimizer/translated-state.cc:2743:   
previously_materialized_objects->set(i, 
*marker);
v8/src/deoptimizer/translated-state.cc:2748:  
CHECK_EQ(length, 
previously_materialized_objects->length());
v8/src/deoptimizer/translated-state.cc:2756:   
CHECK(value_info->IsMaterializedObject());
v8/src/deoptimizer/translated-state.cc:2761:   
DirectHandle<Object> 
previous_value(previously_materialized_objects-
>get(i),
v8/src/deoptimizer/translated-state.cc:2773:   
previously_materialized_objects->set(i, 
*value);
v8/src/deoptimizer/translated-state.cc:2785:   
materialized_store->Set(stack_frame_pointer_,
v8/src/deoptimizer/translated-state.cc:2786:   
previously_materialized_objects);
v8/src/deoptimizer/translated-state.cc:2790:   
frame->function(), 
LazyDeoptimizeReason::kFrameValueMaterialized,
v8/src/deoptimizer/translated-state.cc:2795:voi
d 
TranslatedState::UpdateFromPreviouslyMaterializ
edObjects() {
v8/src/deoptimizer/translated-state.cc:2796:  
MaterializedObjectStore* materialized_store =
v8/src/deoptimizer/translated-state.cc:2797:   
isolate_->materialized_object_store();
v8/src/deoptimizer/translated-state.cc:2798:  
DirectHandle<FixedArray> 
previously_materialized_objects =
v8/src/deoptimizer/translated-state.cc:2799:   
materialized_store->Get(stack_frame_pointer_);
v8/src/deoptimizer/translated-state.cc:2801:  
// If we have no previously materialized 
objects, there is nothing to do.
v8/src/deoptimizer/translated-state.cc:2802:  
if (previously_materialized_objects.is_null()) 
return;
v8/src/deoptimizer/translated-state.cc:2807:  
CHECK_EQ(length, 
previously_materialized_objects->length());
v8/src/deoptimizer/translated-state.cc:2810:   
// For a previously materialized objects, 
inject their value into the
v8/src/deoptimizer/translated-state.cc:2812:   
if (previously_materialized_objects->get(i) != 
*marker) {
v8/src/deoptimizer/translated-state.cc:2816:   
CHECK(value_info->IsMaterializedObject());
v8/src/deoptimizer/translated-state.cc:2820:   
Handle<Object> 
object(previously_materialized_objects->get(i),
v8/src/deoptimizer/translated-state.cc:2829:voi
d TranslatedState::VerifyMaterializedObjects() 
{
v8/src/deoptimizer/translated-state.cc:2838:   
if (slot->materialization_state() == 
TranslatedValue::kFinished) {
v8/src/deoptimizer/translated-state.cc:2841:   
CHECK_EQ(slot->materialization_state(),
v8/src/codegen/s390/macro-assembler-s390.cc:408
4:    // Materialize offset into scratch 
register.
v8/src/codegen/s390/macro-assembler-s390.cc:461
4:    // Materialize offset into scratch 
register.
v8/src/deoptimizer/deoptimize-reason.h:120:  
V(FrameValueMaterialized, "value in stack frame
was materialized")
v8/src/deoptimizer/materialized-object-store.cc
:5:#include 
"src/deoptimizer/materialized-object-store.h"
v8/src/deoptimizer/materialized-object-store.cc
:15:Handle<FixedArray> 
MaterializedObjectStore::Get(Address fp) {
v8/src/deoptimizer/materialized-object-store.cc
:25:void MaterializedObjectStore::Set(
v8/src/deoptimizer/materialized-object-store.cc
:26:    Address fp, DirectHandle<FixedArray> 
materialized_objects) {
v8/src/deoptimizer/materialized-object-store.cc
:34:  array->set(index, *materialized_objects);
v8/src/deoptimizer/materialized-object-store.cc
:37:bool 
MaterializedObjectStore::Remove(Address fp) {
v8/src/deoptimizer/materialized-object-store.cc
:43:  Tagged<FixedArray> array = 
isolate()->heap()->materialized_objects();
v8/src/deoptimizer/materialized-object-store.cc
:54:int 
MaterializedObjectStore::StackIdToIndex(Address
fp) {
v8/src/deoptimizer/materialized-object-store.cc
:61:Handle<FixedArray> 
MaterializedObjectStore::GetStackEntries() {
v8/src/deoptimizer/materialized-object-store.cc
:62:  return 
Handle<FixedArray>(isolate()->heap()->materiali
zed_objects(),
v8/src/deoptimizer/materialized-object-store.cc
:66:Handle<FixedArray> 
MaterializedObjectStore::EnsureStackEntries(int
length) {
v8/src/deoptimizer/materialized-object-store.cc
:87:  
isolate()->heap()->SetRootMaterializedObjects(*
new_array);
v8/src/deoptimizer/deoptimizer.h:127:  void 
MaterializeHeapObjects();
v8/src/deoptimizer/deoptimizer.h:182:  void 
QueueValueForMaterialization(Address 
output_address, Tagged<Object> obj,
v8/src/deoptimizer/deoptimizer.h:184:  void 
QueueFeedbackVectorForMaterialization(
v8/src/deoptimizer/deoptimizer.h:282:  // Key 
for lookup of previously materialized objects.
v8/src/deoptimizer/deoptimizer.h:286:  struct 
ValueToMaterialize {
v8/src/deoptimizer/deoptimizer.h:290:  
std::vector<ValueToMaterialize> 
values_to_materialize_;
v8/src/deoptimizer/deoptimizer.h:291:  
std::vector<ValueToMaterialize> 
feedback_vector_to_materialize_;
v8/src/objects/templates.h:55:  // Whether or 
not to cache every instance: when we 
materialize a getter or
v8/src/compiler/escape-analysis.h:150:  // is 
used in an operation that requires 
materialization.
v8/src/compiler/js-create-lowering.cc:158:  // 
Use the ArgumentsAccessStub for materializing 
both mapped and unmapped
v8/src/compiler/type-cache.h:180:  // 
materialize more than the max size of a fixed 
array, because we require a
v8/src/compiler/type-cache.h:185:  // 
materialize more than the max size of a fixed 
array, because we require a
v8/src/compiler/representation-change.cc:172:  
// Rematerialize any truncated BigInt if user 
is not expecting a BigInt.
v8/src/compiler/js-native-context-specializatio
n.cc:3501:        // Materialize undefined for 
out-of-bounds loads.
v8/src/compiler/js-native-context-specializatio
n.cc:3921:          // Materialize undefined 
for out-of-bounds loads.
v8/src/compiler/turboshaft/recreate-schedule.cc
:1398:    case Instr::kDematerializedObject: {
v8/src/compiler/turboshaft/recreate-schedule.cc
:1401:      
it->ConsumeDematerializedObject(&obj_id, 
&field_count);
v8/src/compiler/turboshaft/recreate-schedule.cc
:1415:    case 
Instr::kDematerializedObjectReference: {
v8/src/compiler/turboshaft/recreate-schedule.cc
:1417:      
it->ConsumeDematerializedObjectReference(&obj_i
d);
v8/src/compiler/turboshaft/recreate-schedule.cc
:1435:    case 
Instr::kDematerializedStringConcat:
v8/src/compiler/turboshaft/recreate-schedule.cc
:1436:    case 
Instr::kDematerializedStringConcatReference:
v8/tools/clusterfuzz/js_fuzzer/generated/except
ions.js:1556:  
"WebKit/JSTests/stress/lazy-global-object-prope
rty-materialization-should-not-putDirectWithout
Transition.js",
v8/src/compiler/simplified-lowering.cc:1398:   
// BigInt64s are rematerialized in 
deoptimization. The other BigInts
v8/src/compiler/simplified-lowering.cc:1399:   
// must be rematerialized before 
deoptimization. By propagating an
v8/src/compiler/escape-analysis.cc:651:      //
BoundedSize fields cannot currently be 
materialized by the deoptimizer,
v8/src/compiler/turboshaft/duplication-optimiza
tion-reducer.h:45:// As you can see, TurboFan 
materialized the == bit into a temporary 
register.
v8/src/compiler/turboshaft/assembler.h:446:    
return std::tuple_cat(std::tuple{true}, 
MaterializePhis(assembler));
v8/src/compiler/turboshaft/assembler.h:499:  
values_t MaterializePhis(A& assembler) {
v8/src/compiler/turboshaft/assembler.h:500:    
return MaterializePhisImpl(assembler, data_,
v8/src/compiler/turboshaft/assembler.h:505:  
static values_t MaterializePhisImpl(A& 
assembler, BlockData& data,
v8/src/compiler/turboshaft/assembler.h:639:    
MaterializeLoopPhis(assembler, 
loop_header_data_);
v8/src/compiler/turboshaft/assembler.h:666:  
static values_t MaterializeLoopPhis(A& 
assembler, BlockData& data) {
v8/src/compiler/turboshaft/assembler.h:667:    
return MaterializeLoopPhisImpl(assembler, data,
v8/src/compiler/turboshaft/assembler.h:672:  
static values_t MaterializeLoopPhisImpl(A& 
assembler, BlockData& data,
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:4853:      // We need to add 
HeapNumbers as dematerialized HeapNumbers 
(rather than
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:4857:      
builder.AddDematerializedObject(deduplicator_.C
reateFreshId().id,
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:4868:      
builder.AddDematerializedObjectReference(dup_id
.id);
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:4875:      
builder.AddDematerializedObject(dup_id.id, 
field_count);
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:4899:    
builder.AddDematerializedObject(dup_id.id, 
field_count);
v8/src/compiler/turboshaft/maglev-graph-buildin
g-phase.cc:5353:  // Maglev should avoid 
materializing JS booleans, so there is a good 
chance
v8/test/debugger/debug/debug-scopes.js:155:  //
Temporary variables introduced by the parser 
have not been materialized.
v8/src/compiler/turboshaft/deopt-data.h:21:    
kDematerializedObject,           // 2 Operands:
id, field_count
v8/src/compiler/turboshaft/deopt-data.h:22:    
kDematerializedObjectReference,  // 1 Operand: 
id
v8/src/compiler/turboshaft/deopt-data.h:26:    
kDematerializedStringConcat,  // 1 Operand: id
v8/src/compiler/turboshaft/deopt-data.h:28:    
// single pass, and always use 
kDematerializedObjectReference rather than
v8/src/compiler/turboshaft/deopt-data.h:29:    
// kDematerializedStringConcatReference (and 
thus remove
v8/src/compiler/turboshaft/deopt-data.h:30:    
// kDematerializedStringConcatReference).
v8/src/compiler/turboshaft/deopt-data.h:31:    
kDematerializedStringConcatReference  // 1 
Operand: id
v8/src/compiler/turboshaft/deopt-data.h:51:    
void AddDematerializedObjectReference(uint32_t 
id) {
v8/src/compiler/turboshaft/deopt-data.h:52:    
instructions_.push_back(Instr::kDematerializedO
bjectReference);
v8/src/compiler/turboshaft/deopt-data.h:56:    
void AddDematerializedObject(uint32_t id, 
uint32_t field_count) {
v8/src/compiler/turboshaft/deopt-data.h:57:    
instructions_.push_back(Instr::kDematerializedO
bject);
v8/src/compiler/turboshaft/deopt-data.h:62:    
void AddDematerializedStringConcat(uint32_t id)
{
v8/src/compiler/turboshaft/deopt-data.h:63:    
instructions_.push_back(Instr::kDematerializedS
tringConcat);
v8/src/compiler/turboshaft/deopt-data.h:67:    
void 
AddDematerializedStringConcatReference(uint32_t
id) {
v8/src/compiler/turboshaft/deopt-data.h:68:    
instructions_.push_back(Instr::kDematerializedS
tringConcatReference);
v8/src/compiler/turboshaft/deopt-data.h:130:   
void ConsumeDematerializedObject(uint32_t* id, 
uint32_t* field_count) {
v8/src/compiler/turboshaft/deopt-data.h:131:   
DCHECK_EQ(instructions|0], 
Instr::kDematerializedObject);
v8/src/compiler/turboshaft/deopt-data.h:137:   
void 
ConsumeDematerializedObjectReference(uint32_t* 
id) {
v8/src/compiler/turboshaft/deopt-data.h:138:   
DCHECK_EQ(instructions|0], 
Instr::kDematerializedObjectReference);
v8/src/compiler/turboshaft/deopt-data.h:143:   
void 
ConsumeDematerializedStringConcat(uint32_t* id)
{
v8/src/compiler/turboshaft/deopt-data.h:144:   
DCHECK_EQ(instructions|0], 
Instr::kDematerializedStringConcat);
v8/src/compiler/turboshaft/deopt-data.h:149:   
void 
ConsumeDematerializedStringConcatReference(uint
32_t* id) {
v8/src/compiler/turboshaft/deopt-data.h:150:   
DCHECK_EQ(instructions|0], 
Instr::kDematerializedStringConcatReference);
v8/test/debugger/debug/es6/debug-blockscopes.js
:123:  // Temporary variables introduced by the
parser have not been materialized.
v8/src/compiler/backend/code-generator.h:169:  
// Check if a heap object can be materialized 
by loading from a heap root,
v8/src/compiler/backend/code-generator.h:170:  
// which is cheaper on some platforms than 
materializing the actual heap
v8/src/compiler/backend/code-generator.h:172:  
bool 
IsMaterializableFromRoot(Handle<HeapObject> 
object,
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:3395:// Assemble boolean 
materializations after this instruction.
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:3400:  // Materialize a full 64-bit 1 or
0 value. The result register is always the
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:3494:// Assemble a conditional compare 
and boolean materializations after this
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:3497:  // Materialize a full 64-bit 1 or
0 value. The result register is always the
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:4172:      if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/arm64/code-generator-ar
m64.cc:4180:      if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/code-generator.cc:593:b
ool 
CodeGenerator::IsMaterializableFromRoot(Handle<
HeapObject> object,
v8/src/compiler/backend/code-generator.cc:857: 
// Assemble a boolean materialization after 
this instruction.
v8/src/compiler/backend/code-generator.cc:862: 
// Assemble a conditional boolean 
materialization after this instruction.
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:4008:// Assembles boolean 
materializations after an instruction.
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:4013:  // Materialize a full 32-bit 1 or
0 value. The result register is always the
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:4792:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/riscv/code-generator-ri
scv.cc:4802:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/branch-condition-duplicator.h:4
2:// As you can see, TurboFan materialized the 
== bit into a temporary register.
v8/src/compiler/backend/instruction.h:76:  // 
value must be rematerialized, it is loaded from
an immediate constant
v8/src/compiler/backend/arm/code-generator-arm.
cc:3213:      // The shuffle lane mask is a 
byte mask, materialize in scratch.
v8/src/compiler/backend/arm/code-generator-arm.
cc:3683:// Assembles boolean materializations 
after an instruction.
v8/src/compiler/backend/arm/code-generator-arm.
cc:3688:  // Materialize a full 32-bit 1 or 0 
value. The result register is always the
v8/src/compiler/backend/arm/code-generator-arm.
cc:4053:      if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/instruction-selector.h:
140:// The flags continuation is a way to 
combine a branch or a materialization
v8/src/builtins/typed-array-filter.tq:76:    //
TypedArrayCopyElements() to avoid JSArray 
materialization. Or collect
v8/src/compiler/backend/x64/code-generator-x64.
cc:7480:// Assembles boolean materializations 
after this instruction.
v8/src/compiler/backend/x64/code-generator-x64.
cc:7486:  // Materialize a full 64-bit 1 or 0 
value. The result register is always the
v8/src/compiler/backend/x64/code-generator-x64.
cc:8157:        if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/x64/code-generator-x64.
cc:8167:        if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:3989:// Assembles boolean 
materializations after an instruction.
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:3994:  // Materialize a full 32-bit 1 
or 0 value. The result register is always the
v8/src/compiler/backend/mips64/code-generator-m
ips64.cc:4568:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/ia32/code-generator-ia3
2.cc:3828:// Assembles boolean materializations
after an instruction.
v8/src/compiler/backend/ia32/code-generator-ia3
2.cc:3834:  // Materialize a full 32-bit 1 or 0
value. The result register is always the
v8/src/compiler/backend/instruction-selector.cc
:1117:    case 
FrameStateData::Instr::kDematerializedObject: {
v8/src/compiler/backend/instruction-selector.cc
:1120:      
it->ConsumeDematerializedObject(&obj_id, 
&field_count);
v8/src/compiler/backend/instruction-selector.cc
:1139:    case 
FrameStateData::Instr::kDematerializedObjectRef
erence: {
v8/src/compiler/backend/instruction-selector.cc
:1141:      
it->ConsumeDematerializedObjectReference(&obj_i
d);
v8/src/compiler/backend/instruction-selector.cc
:1150:    case 
FrameStateData::Instr::kDematerializedStringCon
cat: {
v8/src/compiler/backend/instruction-selector.cc
:1153:      
it->ConsumeDematerializedStringConcat(&obj_id);
v8/src/compiler/backend/instruction-selector.cc
:1174:    case 
FrameStateData::Instr::kDematerializedStringCon
catReference: {
v8/src/compiler/backend/instruction-selector.cc
:1177:      
it->ConsumeDematerializedStringConcatReference(
&obj_id);
v8/src/builtins/accessors.h:112:  // newly 
materialized arguments object for the given 
{frame}. Note that for
v8/src/compiler/backend/s390/code-generator-s39
0.cc:3284:// Assembles boolean materializations
after an instruction.
v8/src/compiler/backend/s390/code-generator-s39
0.cc:3297:  // Materialize a full 32-bit 1 or 0
value. The result register is always the
v8/src/compiler/backend/s390/code-generator-s39
0.cc:3748:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/s390/code-generator-s39
0.cc:3758:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/builtins/builtins-collections-gen.cc:151
:      // In case exception is thrown during 
collection population, materialize
v8/src/compiler/backend/ppc/code-generator-ppc.
cc:2950:// Assembles boolean materializations 
after an instruction.
v8/src/compiler/backend/ppc/code-generator-ppc.
cc:2959:  // Materialize a full 32-bit 1 or 0 
value. The result register is always the
v8/src/compiler/backend/ppc/code-generator-ppc.
cc:3465:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/ppc/code-generator-ppc.
cc:3475:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/loong64/code-generator-
loong64.cc:2300:// Assembles boolean 
materializations after an instruction.
v8/src/compiler/backend/loong64/code-generator-
loong64.cc:2305:  // Materialize a full 32-bit 
1 or 0 value. The result register is always the
v8/src/compiler/backend/loong64/code-generator-
loong64.cc:2982:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/backend/loong64/code-generator-
loong64.cc:2992:          if 
(IsMaterializableFromRoot(src_object, &index)) 
{
v8/src/compiler/turboshaft/assert-types-reducer
.h:49:      // LoadRootRegister is a bit 
special and should never be materialized,
v8/src/compiler/turboshaft/operations.cc:749:  
case 
FrameStateData::Instr::kDematerializedObject: {
v8/src/compiler/turboshaft/operations.cc:752:  
it.ConsumeDematerializedObject(&id, 
&field_count);
v8/src/compiler/turboshaft/operations.cc:756:  
case 
FrameStateData::Instr::kDematerializedObjectRef
erence: {
v8/src/compiler/turboshaft/operations.cc:758:  
it.ConsumeDematerializedObjectReference(&id);
v8/src/compiler/turboshaft/operations.cc:762:  
case 
FrameStateData::Instr::kDematerializedStringCon
cat: {
v8/src/compiler/turboshaft/operations.cc:764:  
it.ConsumeDematerializedStringConcat(&id);
v8/src/compiler/turboshaft/operations.cc:765:  
os << "£" << id << 
"DematerializedStringConcat";
v8/src/compiler/turboshaft/operations.cc:768:  
case 
FrameStateData::Instr::kDematerializedStringCon
catReference: {
v8/src/compiler/turboshaft/operations.cc:770:  
it.ConsumeDematerializedStringConcatReference(&
id);
v8/src/compiler/turboshaft/operations.cc:819:  
case 
FrameStateData::Instr::kDematerializedObject: {
v8/src/compiler/turboshaft/operations.cc:822:  
it.ConsumeDematerializedObject(&id, 
&field_count);
v8/src/compiler/turboshaft/operations.cc:825:  
case 
FrameStateData::Instr::kDematerializedObjectRef
erence: {
v8/src/compiler/turboshaft/operations.cc:827:  
it.ConsumeDematerializedObjectReference(&id);
v8/src/compiler/turboshaft/operations.cc:830:  
case 
FrameStateData::Instr::kDematerializedStringCon
cat: {
v8/src/compiler/turboshaft/operations.cc:832:  
it.ConsumeDematerializedStringConcat(&id);
v8/src/compiler/turboshaft/operations.cc:835:  
case 
FrameStateData::Instr::kDematerializedStringCon
catReference: {
v8/src/compiler/turboshaft/operations.cc:837:  
it.ConsumeDematerializedStringConcatReference(&
id);
v8/src/compiler/turboshaft/graph-builder.cc:135
:      
builder->AddDematerializedObjectReference(Objec
tIdOf(input->op()));
v8/src/compiler/turboshaft/graph-builder.cc:140
:      
builder->AddDematerializedObject(info.object_id
(),
v8/test/debugger/debug/debug-scopes-suspended-g
enerators.js:90:  // Temporary variables 
introduced by the parser have not been 
materialized.
v8/test/debugger/debug/harmony/modules-debug-sc
opes1.mjs:126:  // Temporary variables 
introduced by the parser have not been 
materialized.
v8/test/debugger/debug/harmony/modules-debug-sc
opes2.mjs:104:  // Temporary variables 
introduced by the parser have not been 
materialized.
v8/src/profiler/sampling-heap-profiler.cc:162: 
// If we are materializing objects during 
deoptimization, inlined
v8/src/profiler/sampling-heap-profiler.cc:163: 
// closures may not yet be materialized, and 
this includes the
v8/test/debugger/debug/regress/regress-crbug-22
2893.js:51:  debugger;  // Arguments object is 
already materialized.
v8/test/debugger/debug/regress/regress-crbug-22
2893.js:63:  debugger;  // Arguments object is 
not yet materialized.
v8/test/debugger/debug/debug-evaluate-shadowed-
context-reuse.js:237:// materialized 
stack-local.
v8/src/ast/ast.h:135:class MaterializedLiteral;
v8/src/ast/ast.h:171:  MaterializedLiteral* 
AsMaterializedLiteral();
v8/src/ast/ast.h:1081:class MaterializedLiteral
: public Expression {
v8/src/ast/ast.h:1083:  // A 
Materializedliteral is simple if the values 
consist of only
v8/src/ast/ast.h:1088:  MaterializedLiteral(int
pos, NodeType type) : Expression(pos, type) {}
v8/src/ast/ast.h:1100:class RegExpLiteral final
: public MaterializedLiteral {
v8/src/ast/ast.h:1111:      : 
MaterializedLiteral(pos, kRegExpLiteral),
v8/src/ast/ast.h:1120:class AggregateLiteral : 
public MaterializedLiteral {
v8/src/ast/ast.h:1131:  AggregateLiteral(int 
pos, NodeType type) : MaterializedLiteral(pos, 
type) {}
v8/src/ast/ast.h:1152:  // if the expression is
a materialized literal and is_simple
v8/src/ast/ast.h:1223:  static void 
InitDepthAndFlags(MaterializedLiteral* expr);
v8/src/ast/ast.h:1227:  void 
BuildConstants(IsolateT* isolate, 
MaterializedLiteral* expr);
v8/src/ast/ast.h:1260:    MATERIALIZED_LITERAL,
// Property value is a materialized literal.
v8/src/ast/ast.cc:57:MaterializedLiteral* 
AstNode::AsMaterializedLiteral() {
v8/src/ast/ast.cc:101:  MaterializedLiteral* 
literal = AsMaterializedLiteral();
v8/src/ast/ast.cc:289:  } else if 
(value_->AsMaterializedLiteral() != nullptr) {
v8/src/ast/ast.cc:441:    MaterializedLiteral* 
literal = 
property->value()->AsMaterializedLiteral();
v8/src/ast/ast.cc:512:    MaterializedLiteral* 
m_literal = 
property->value()->AsMaterializedLiteral();
v8/src/ast/ast.cc:595:    MaterializedLiteral* 
materialized_literal =
v8/src/ast/ast.cc:596:        
element->AsMaterializedLiteral();
v8/src/ast/ast.cc:597:    if 
(materialized_literal != nullptr) {
v8/src/ast/ast.cc:598:      
LiteralBoilerplateBuilder::InitDepthAndFlags(ma
terialized_literal);
v8/src/ast/ast.cc:703:      
MaterializedLiteral* m_literal = 
element->AsMaterializedLiteral();
v8/src/ast/ast.cc:759:bool 
MaterializedLiteral::IsSimple() const {
v8/src/ast/ast.cc:767:void 
LiteralBoilerplateBuilder::InitDepthAndFlags(Ma
terializedLiteral* expr) {
v8/src/ast/ast.cc:777:bool 
MaterializedLiteral::NeedsInitialAllocationSite
(
v8/src/ast/ast.cc:792:                         
MaterializedLiteral* expr) {
v8/src/ast/ast.cc:804:    
BuildConstants(Isolate* isolate, 
MaterializedLiteral* expr);
v8/src/ast/ast.cc:806:    
BuildConstants(LocalIsolate* isolate, 
MaterializedLiteral* expr);
v8/src/maglev/maglev-code-generator.cc:132:    
ValueNode* materializing_register_move =
v8/src/maglev/maglev-code-generator.cc:133:    
materializing_register_moves_|reg.code()];
v8/src/maglev/maglev-code-generator.cc:134:    
if (materializing_register_move) {
v8/src/maglev/maglev-code-generator.cc:135:    
materializing_register_move->LoadToRegister(mas
m_, reg);
v8/src/maglev/maglev-code-generator.cc:144:    
for (auto |stack_slot, node] : 
materializing_stack_slot_moves_) {
v8/src/maglev/maglev-code-generator.cc:201:    
if 
(materializing_register_moves_|target_reg.code(
)] != nullptr) {
v8/src/maglev/maglev-code-generator.cc:202:    
FATAL("Existing materialization of %p to %s",
v8/src/maglev/maglev-code-generator.cc:203:    
materializing_register_moves_|target_reg.code()
],
v8/src/maglev/maglev-code-generator.cc:225:    
for (auto& |stack_slot, node] : 
materializing_stack_slot_moves_) {
v8/src/maglev/maglev-code-generator.cc:227:    
FATAL("Existing materialization of %p to stack 
slot %d", node,
v8/src/maglev/maglev-code-generator.cc:273:    
materializing_register_moves_|target_reg.code()
] = node;
v8/src/maglev/maglev-code-generator.cc:274:    
// No need to update 
`targets.needs_decompression`, materialization 
is
v8/src/maglev/maglev-code-generator.cc:318:    
materializing_stack_slot_moves_.emplace_back(ta
rget_slot, node);
v8/src/maglev/maglev-code-generator.cc:319:    
// No need to update 
`targets.needs_decompression`, materialization 
is
v8/src/maglev/maglev-code-generator.cc:508:  //
materializing_register_moves|target] = node.
v8/src/maglev/maglev-code-generator.cc:510:    
materializing_register_moves_ = {};
v8/src/maglev/maglev-code-generator.cc:512:  //
materializing_stack_slot_moves = 
{(node,target), ... }.
v8/src/maglev/maglev-code-generator.cc:513:  
std::vector<std::pair<int32_t, ValueNode*>> 
materializing_stack_slot_moves_;
v8/BUILD.gn:3726:    
"src/deoptimizer/materialized-object-store.h",
v8/BUILD.gn:5496:    
"src/deoptimizer/materialized-object-store.cc",
v8/src/objects/contexts.cc:427:      // Check 
materialized locals.
v8/src/runtime/runtime-scopes.cc:500:      // 
If we materialize any object, we should 
deoptimize the frame because we
v8/src/runtime/runtime-scopes.cc:502:      
should_deoptimize = should_deoptimize || 
iter->IsMaterializedObject();
v8/src/runtime/runtime-scopes.cc:509:      
translated_values.StoreMaterializedValuesAndDeo
pt(frame);
v8/src/runtime/runtime-compiler.cc:535:  // 
TODO(turbofan): We currently need the native 
context to materialize
v8/src/runtime/runtime-compiler.cc:544:  // 
Make sure to materialize objects before causing
any allocation.
v8/src/runtime/runtime-compiler.cc:545:  
deoptimizer->MaterializeHeapObjects();
v8/src/runtime/runtime-compiler.cc:550:  // 
Ensure the context register is updated for 
materialized objects.
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:258:      // The parent FrameState
could contain dematerialized objects, and the
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:314:      case 
Instr::kDematerializedObject: {
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:317:        
it->ConsumeDematerializedObject(&old_id, 
&field_count);
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:318:        
builder->AddDematerializedObject(old_id, 
field_count);
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:324:      case 
Instr::kDematerializedObjectReference: {
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:326:        
it->ConsumeDematerializedObjectReference(&old_i
d);
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:327:        
builder->AddDematerializedObjectReference(old_i
d);
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:348:      case 
FrameStateData::Instr::kDematerializedStringCon
cat:
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:349:      case 
FrameStateData::Instr::kDematerializedStringCon
catReference:
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:365:        
builder->AddDematerializedStringConcatReference
(dup_id.id);
v8/src/compiler/turboshaft/string-escape-analys
is-reducer.h:368:      
builder->AddDematerializedStringConcat(dup_id.i
d);
v8/src/maglev/maglev-graph-builder.cc:1406:    
// Receiver and closure values have to be 
materialized, even if
v8/src/maglev/maglev-graph-builder.cc:4972:    
// re-materialize objects differently, 
depending on whether the throw
v8/src/maglev/maglev-graph-builder.cc:7767:  //
receiver needs to have been materialized.
v8/src/runtime/runtime-debug.cc:436:  return 
*it.MaterializeScopeDetails();
v8/src/objects/code-inl.h:542:          case 
LazyDeoptimizeReason::kFrameValueMaterialized:
v8/src/objects/code-inl.h:699:  // how to 
materialize an object upon deoptimization, in 
which case it is
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:276:// Materializes a boolean (1 or 
0) from a comparison.
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:277:class CmpMaterializeBoolGen : 
public BinopGen<int32_t> {
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:282:  
CmpMaterializeBoolGen(TurboshaftComparison op, 
bool i) : w(op), invert(i) {}
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:332:TEST(BranchCombineInt32CmpAllInp
utShapes_materialized) {
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:334:    CmpMaterializeBoolGen 
gen(int32cmp_opcodes|i], false);
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:340:TEST(BranchCombineInt32CmpAllInp
utShapes_inverted_materialized) {
v8/test/cctest/compiler/turboshaft-test-branch-
combine.cc:342:    CmpMaterializeBoolGen 
gen(int32cmp_opcodes|i], true);
v8/src/objects/value-serializer.cc:492:      //
TODO(jbroman): It may be possible to avoid 
materializing a typed
v8/test/cctest/compiler/test-branch-combine.cc:
289:// Materializes a boolean (1 or 0) from a 
comparison.
v8/test/cctest/compiler/test-branch-combine.cc:
290:class CmpMaterializeBoolGen : public 
BinopGen<int32_t> {
v8/test/cctest/compiler/test-branch-combine.cc:
295:  CmpMaterializeBoolGen(IrOpcode::Value 
opcode, bool i)
v8/test/cctest/compiler/test-branch-combine.cc:
346:TEST(BranchCombineInt32CmpAllInputShapes_ma
terialized) {
v8/test/cctest/compiler/test-branch-combine.cc:
348:    CmpMaterializeBoolGen 
gen(int32cmp_opcodes|i], false);
v8/test/cctest/compiler/test-branch-combine.cc:
355:TEST(BranchCombineInt32CmpAllInputShapes_in
verted_materialized) {
v8/test/cctest/compiler/test-branch-combine.cc:
357:    CmpMaterializeBoolGen 
gen(int32cmp_opcodes|i], true);
v8/src/execution/isolate.h:138:class 
MaterializedObjectStore;
v8/src/execution/isolate.h:1374:  
MaterializedObjectStore* 
materialized_object_store() const {
v8/src/execution/isolate.h:1375:    return 
materialized_object_store_;
v8/src/execution/isolate.h:2500:  
MaterializedObjectStore* 
materialized_object_store_ = nullptr;
v8/test/mjsunit/compiler/escape-analysis-materi
alize.js:7:(function TestMaterializeArray() {
v8/test/mjsunit/compiler/escape-analysis-materi
alize.js:20:(function TestMaterializeFunction()
{
v8/src/execution/frames.cc:2548:  // 
materialize some closures on the stack. The 
arguments marker object
v8/src/execution/frames.cc:3117:  // Prepare 
iteration over translation. We must not 
materialize values here
v8/src/execution/frames.cc:3137:      
CHECK(!translated_values->IsMaterializedObject(
));
v8/src/execution/frames.cc:3143:      
CHECK(!translated_values->IsMaterializedObject(
));
v8/test/mjsunit/compiler/escape-analysis.js:413
:// Test materialization of a field that 
requires a Smi value.
v8/test/mjsunit/compiler/escape-analysis.js:434
:// Test materialization of a field that 
requires a heap object value.
v8/test/mjsunit/compiler/inline-literals.js:30:
// Test that we can inline functions containing
materialized literals.
v8/src/sandbox/testing.cc:641:  // (caused e.g.
by unbounded recursion) materialize: for (1) 
the kernel will
v8/test/mjsunit/compiler/escape-analysis-repres
entation.js:30:// This tests that captured 
objects materialized through the deoptimizer
v8/test/mjsunit/compiler/deopt-closure.js:7:(fu
nction 
TestMaterializeTargetOfInterpretedFrame() {
v8/test/mjsunit/compiler/deopt-closure.js:22:(f
unction 
TestMaterializeTargetOfArgumentsAdaptorFrame() 
{
v8/test/mjsunit/compiler/deopt-closure.js:37:(f
unction 
TestMaterializeTargetOfConstructStubFrame() {
v8/test/mjsunit/compiler/deopt-materialize-accu
mulator.js:30:// Tests that Turbofan correctly 
materializes values which are in the
v8/test/mjsunit/compiler/deopt-materialize-accu
mulator.js:36:  // accumulator holding an 
unboxed double which needs materialized.
v8/src/d8/d8-platforms.cc:152:    // Check if 
we can re-materialize the weak ptr in our map.
v8/src/heap/cppgc-js/unified-heap-marking-state
-inl.h:46:    // The embedder is not aware of 
whether numbers are materialized as heap
v8/src/heap/setup-heap-internal.cc:1369:  
set_materialized_objects(*factory->NewFixedArra
y(0, AllocationType::kOld));
v8/src/heap/heap-inl.h:120:void 
Heap::SetRootMaterializedObjects(Tagged<FixedAr
ray> objects) {
v8/src/heap/heap-inl.h:121:  
roots_table()|RootIndex::kMaterializedObjects] 
= objects.ptr();
v8/src/heap/heap.h:851:  V8_INLINE void 
SetRootMaterializedObjects(Tagged<FixedArray> 
objects);
v8/src/heap/traced-handles-marking-visitor.cc:4
4:      // The embedder is not aware of whether
numbers are materialized as heap
v8/src/roots/roots.h:392:  V(FixedArray, 
materialized_objects, MaterializedObjects)     
\
v8/test/mjsunit/compiler/inline-arguments.js:28
0:// Test materialization of arguments object 
with values in registers.
v8/test/mjsunit/compiler/bigint-rematerialize.j
s:24:  // Should be rematerialized to 2n ** 64n
- 1n in code generation.
v8/test/mjsunit/compiler/bigint-rematerialize.j
s:49:  // Should be rematerialized to 2n ** 64n
- 1n in deoptimization.
v8/test/mjsunit/compiler/bigint-rematerialize.j
s:75:  // Should be rematerialized to 2n ** 64n
- 1n in deoptimization.
v8/test/mjsunit/compiler/bigint-rematerialize.j
s:99:  // Should be rematerialized to -1n in 
code generation.
v8/test/mjsunit/compiler/bigint-rematerialize.j
s:124:  // Should be rematerialized to -1n in 
deoptimization.
v8/test/mjsunit/compiler/bigint-rematerialize.j
s:151:  // Should be rematerialized to -1n in 
deoptimization.
v8/test/mjsunit/compiler/uint32.js:186:// that 
arguments object with uint32 values inside is 
correctly materialized.
v8/test/mjsunit/compiler/literals.js:78:// Test
some materialized array literals.
v8/test/cctest/test-api-array-buffer.cc:334:  
// Calling Buffer() will materialize the 
ArrayBuffer (transitioning it from
v8/test/cctest/wasm/test-run-wasm-simd-liftoff.
cc:55:  // We use a temp local to materialize a
SIMD value, since at this point
v8/src/interpreter/bytecode-register-optimizer.
h:55:    // accumulator until the value is 
actually materialized in the accumulator.
v8/src/interpreter/bytecode-register-optimizer.
h:69:  // Materialize all live registers and 
flush equivalence sets.
v8/src/interpreter/bytecode-register-optimizer.
h:91:    // Materialize the accumulator if it 
is read by the bytecode. The
v8/src/interpreter/bytecode-register-optimizer.
h:92:    // accumulator is special and no other
register can be materialized
v8/src/interpreter/bytecode-register-optimizer.
h:95:      Materialize(accumulator_info_);
v8/src/interpreter/bytecode-register-optimizer.
h:98:    // Materialize an equivalent to the 
accumulator if it will be
v8/src/interpreter/bytecode-register-optimizer.
h:159:  void 
CreateMaterializedEquivalent(RegisterInfo* 
info);
v8/src/interpreter/bytecode-register-optimizer.
h:160:  RegisterInfo* 
GetMaterializedEquivalentNotAccumulator(Registe
rInfo* info);
v8/src/interpreter/bytecode-register-optimizer.
h:161:  void Materialize(RegisterInfo* info);
v8/src/interpreter/interpreter-generator.cc:245
:  // To avoid special logic in the deoptimizer
to re-materialize the value in
v8/src/interpreter/interpreter-generator.cc:708
:    // To avoid special logic in the 
deoptimizer to re-materialize the value in
v8/src/interpreter/interpreter-generator.cc:761
:  // To avoid special logic in the deoptimizer
to re-materialize the value in
v8/src/interpreter/interpreter-generator.cc:792
:  // To avoid special logic in the deoptimizer
to re-materialize the value in
v8/src/interpreter/interpreter-generator.cc:816
:  // To avoid special logic in the deoptimizer
to re-materialize the value in
v8/src/interpreter/bytecode-generator.cc:3904: 
// Materialize a regular expression literal.
v8/src/interpreter/bytecode-generator.cc:8444: 
// Dead registers might be materialized 
otherwise.
v8/src/interpreter/bytecode-register-optimizer.
cc:26:enum class MaterializedInfo { 
kNotMaterialized, kMaterialized };
v8/src/interpreter/bytecode-register-optimizer.
cc:32:// register is materialized in the 
bytecode stream.
v8/src/interpreter/bytecode-register-optimizer.
cc:35:  RegisterInfo(Register reg, uint32_t 
equivalence_id, bool materialized,
v8/src/interpreter/bytecode-register-optimizer.
cc:39:        materialized_(materialized),
v8/src/interpreter/bytecode-register-optimizer.
cc:51:      uint32_t equivalence_id, 
MaterializedInfo materialized,
v8/src/interpreter/bytecode-register-optimizer.
cc:62:  // materialized. The materialized 
equivalent will be this register
v8/src/interpreter/bytecode-register-optimizer.
cc:63:  // if it is materialized. Returns 
nullptr if no materialized
v8/src/interpreter/bytecode-register-optimizer.
cc:65:  RegisterInfo* 
GetMaterializedEquivalent();
v8/src/interpreter/bytecode-register-optimizer.
cc:68:  // materialized and not register |reg|.
The materialized equivalent
v8/src/interpreter/bytecode-register-optimizer.
cc:69:  // will be this register if it is 
materialized. Returns nullptr if
v8/src/interpreter/bytecode-register-optimizer.
cc:70:  // no materialized equivalent exists.
v8/src/interpreter/bytecode-register-optimizer.
cc:71:  RegisterInfo* 
GetMaterializedEquivalentOtherThan(Register 
reg);
v8/src/interpreter/bytecode-register-optimizer.
cc:74:  // to be materialized in place of this 
register (which is currently
v8/src/interpreter/bytecode-register-optimizer.
cc:75:  // materialized). The best candidate is
deemed to be the register
v8/src/interpreter/bytecode-register-optimizer.
cc:79:  RegisterInfo* 
GetEquivalentToMaterialize();
v8/src/interpreter/bytecode-register-optimizer.
cc:81:  // Marks all temporary registers of the
equivalence set as unmaterialized.
v8/src/interpreter/bytecode-register-optimizer.
cc:82:  void 
MarkTemporariesAsUnmaterialized(Register 
temporary_base);
v8/src/interpreter/bytecode-register-optimizer.
cc:88:  bool materialized() const { return 
materialized_; }
v8/src/interpreter/bytecode-register-optimizer.
cc:89:  void set_materialized(bool 
materialized) { materialized_ = materialized; }
v8/src/interpreter/bytecode-register-optimizer.
cc:117:  bool materialized_;
v8/src/interpreter/bytecode-register-optimizer.
cc:140:  set_materialized(false);
v8/src/interpreter/bytecode-register-optimizer.
cc:146:    uint32_t equivalence_id, 
MaterializedInfo materialized,
v8/src/interpreter/bytecode-register-optimizer.
cc:152:  materialized_ = materialized == 
MaterializedInfo::kMaterialized;
v8/src/interpreter/bytecode-register-optimizer.
cc:228:BytecodeRegisterOptimizer::RegisterInfo:
:GetMaterializedEquivalent() {
v8/src/interpreter/bytecode-register-optimizer.
cc:231:    if (visitor->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:241:BytecodeRegisterOptimizer::RegisterInfo:
:GetMaterializedEquivalentOtherThan(
v8/src/interpreter/bytecode-register-optimizer.
cc:245:    if (visitor->materialized() && 
visitor->register_value() != reg) {
v8/src/interpreter/bytecode-register-optimizer.
cc:255:BytecodeRegisterOptimizer::RegisterInfo:
:GetEquivalentToMaterialize() {
v8/src/interpreter/bytecode-register-optimizer.
cc:256:  DCHECK(this->materialized());
v8/src/interpreter/bytecode-register-optimizer.
cc:260:    if (visitor->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:273:void 
BytecodeRegisterOptimizer::RegisterInfo::MarkTe
mporariesAsUnmaterialized(
v8/src/interpreter/bytecode-register-optimizer.
cc:276:  DCHECK(this->materialized());
v8/src/interpreter/bytecode-register-optimizer.
cc:280:      visitor->set_materialized(false);
v8/src/interpreter/bytecode-register-optimizer.
cc:345:    } else if (reg_info->allocated() && 
!reg_info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:357:  // Materialize all live registers and 
break equivalences.
v8/src/interpreter/bytecode-register-optimizer.
cc:364:    RegisterInfo* materialized = 
reg_info->materialized()
v8/src/interpreter/bytecode-register-optimizer.
cc:366:                                     : 
reg_info->GetMaterializedEquivalent();
v8/src/interpreter/bytecode-register-optimizer.
cc:368:    if (materialized != nullptr) {
v8/src/interpreter/bytecode-register-optimizer.
cc:369:      // Walk equivalents of 
materialized registers, materializing
v8/src/interpreter/bytecode-register-optimizer.
cc:373:      while ((equivalent = 
materialized->GetEquivalent()) != materialized)
{
v8/src/interpreter/bytecode-register-optimizer.
cc:374:        if (equivalent->allocated() && 
!equivalent->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:375:          
OutputRegisterTransfer(materialized, 
equivalent);
v8/src/interpreter/bytecode-register-optimizer.
cc:378:                                        
    MaterializedInfo::kMaterialized,
v8/src/interpreter/bytecode-register-optimizer.
cc:386:                                        
MaterializedInfo::kNotMaterialized,
v8/src/interpreter/bytecode-register-optimizer.
cc:413:  output_info->set_materialized(true);
v8/src/interpreter/bytecode-register-optimizer.
cc:416:void 
BytecodeRegisterOptimizer::CreateMaterializedEq
uivalent(
v8/src/interpreter/bytecode-register-optimizer.
cc:418:  DCHECK(info->materialized());
v8/src/interpreter/bytecode-register-optimizer.
cc:419:  RegisterInfo* unmaterialized = 
info->GetEquivalentToMaterialize();
v8/src/interpreter/bytecode-register-optimizer.
cc:420:  if (unmaterialized) {
v8/src/interpreter/bytecode-register-optimizer.
cc:421:    OutputRegisterTransfer(info, 
unmaterialized);
v8/src/interpreter/bytecode-register-optimizer.
cc:426:BytecodeRegisterOptimizer::GetMaterializ
edEquivalentNotAccumulator(
v8/src/interpreter/bytecode-register-optimizer.
cc:428:  if (info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:432:  RegisterInfo* result = 
info->GetMaterializedEquivalentOtherThan(accumu
lator_);
v8/src/interpreter/bytecode-register-optimizer.
cc:434:    Materialize(info);
v8/src/interpreter/bytecode-register-optimizer.
cc:441:void 
BytecodeRegisterOptimizer::Materialize(Register
Info* info) {
v8/src/interpreter/bytecode-register-optimizer.
cc:442:  if (!info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:443:    RegisterInfo* materialized = 
info->GetMaterializedEquivalent();
v8/src/interpreter/bytecode-register-optimizer.
cc:444:    DCHECK_NOT_NULL(materialized);
v8/src/interpreter/bytecode-register-optimizer.
cc:445:    OutputRegisterTransfer(materialized,
info);
v8/src/interpreter/bytecode-register-optimizer.
cc:463:      (!output_is_observable || 
output_info->materialized())) {
v8/src/interpreter/bytecode-register-optimizer.
cc:467:  // Materialize an alternate in the 
equivalence set that
v8/src/interpreter/bytecode-register-optimizer.
cc:469:  if (output_info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:470:    
CreateMaterializedEquivalent(output_info);
v8/src/interpreter/bytecode-register-optimizer.
cc:480:    
output_info->set_materialized(false);
v8/src/interpreter/bytecode-register-optimizer.
cc:481:    RegisterInfo* materialized_info = 
input_info->GetMaterializedEquivalent();
v8/src/interpreter/bytecode-register-optimizer.
cc:482:    
OutputRegisterTransfer(materialized_info, 
output_info);
v8/src/interpreter/bytecode-register-optimizer.
cc:488:    // registers as unmaterialized so 
that this register is used in preference.
v8/src/interpreter/bytecode-register-optimizer.
cc:489:    
input_info->MarkTemporariesAsUnmaterialized(tem
porary_base_);
v8/src/interpreter/bytecode-register-optimizer.
cc:495:  if (reg_info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:496:    
CreateMaterializedEquivalent(reg_info);
v8/src/interpreter/bytecode-register-optimizer.
cc:499:                                    
MaterializedInfo::kMaterialized);
v8/src/interpreter/bytecode-register-optimizer.
cc:515:  if (reg_info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:519:        
GetMaterializedEquivalentNotAccumulator(reg_inf
o);
v8/src/interpreter/bytecode-register-optimizer.
cc:535:      Materialize(input_info);
v8/src/interpreter/bytecode-register-optimizer.
cc:558:  if (!info->materialized()) {
v8/src/interpreter/bytecode-register-optimizer.
cc:560:                                  
MaterializedInfo::kMaterialized);
v8/src/interpreter/bytecode-array-builder.cc:12
29:  // materialized.
v8/src/wasm/baseline/arm64/liftoff-assembler-ar
m64-inl.h:305:  // a temp register to 
materialize the offset.
v8/src/bigint/bitwise.cc:265:    // 
materialized minuend_msd again.
v8/src/wasm/baseline/riscv/liftoff-assembler-ri
scv32-inl.h:1503:  // materializing?
v8/src/wasm/baseline/liftoff-assembler.cc:459: 
// Materialize constants.
v8/src/wasm/fuzzing/random-module-generation.cc
:3690:        //   ((ref extern) gets 
materialized through (ref any)).
v8/src/heap/factory-base-inl.h:46:  // 
Materialize as a SMI if possible.
v8/test/mjsunit/arguments-deopt.js:7:(function 
MaterializeStrictArguments() {
v8/test/mjsunit/arguments-deopt.js:30:(function
MaterializeSloppyArguments() {
v8/test/mjsunit/arguments-deopt.js:51:(function
MaterializeStrictOverwrittenArguments() {
v8/test/mjsunit/arguments-deopt.js:75:(function
MaterializeSloppyOverwrittenArguments() {
v8/test/unittests/heap/cppgc/stack-unittest.cc:
107:// must not actually be materialized.
v8/src/execution/isolate.cc:49:#include 
"src/deoptimizer/materialized-object-store.h"
v8/src/execution/isolate.cc:2371:        // 
Note: Needed by the deoptimizer to 
rematerialize frames.
v8/src/execution/isolate.cc:2586:        //    
materialized by the deoptimizer. If there is a 
handler frame
v8/src/execution/isolate.cc:2658:      // 
Remove per-frame stored materialized objects.
v8/src/execution/isolate.cc:2659:      bool 
removed = 
materialized_object_store_->Remove(frame->fp())
;
v8/src/execution/isolate.cc:2661:      // If 
there were any materialized objects, the code 
should be
v8/src/execution/isolate.cc:4656:  delete 
materialized_object_store_;
v8/src/execution/isolate.cc:4657:  
materialized_object_store_ = nullptr;
v8/src/execution/isolate.cc:5459:  
materialized_object_store_ = new 
MaterializedObjectStore(this);
v8/test/unittests/parser/parsing-unittest.cc:78
33:TEST_F(ParsingTest, 
TemplateMaterializedLiterals) {
v8/test/unittests/objects/roots-unittest.cc:60:
V(materialized_objects)                 \
v8/test/unittests/interpreter/bytecode-register
-optimizer-unittest.cc:68:TEST_F(BytecodeRegist
erOptimizerTest, TemporaryMaterializedForFlush)
{
v8/test/unittests/interpreter/bytecode-register
-optimizer-unittest.cc:79:TEST_F(BytecodeRegist
erOptimizerTest, TemporaryMaterializedForJump) 
{
v8/test/unittests/interpreter/bytecode-register
-optimizer-unittest.cc:176:TEST_F(BytecodeRegis
terOptimizerTest, 
SingleTemporaryNotMaterializedForInput) {
v8/test/unittests/interpreter/bytecode-register
-optimizer-unittest.cc:194:TEST_F(BytecodeRegis
terOptimizerTest, 
RangeOfTemporariesMaterializedForInput) {
v8/test/mjsunit/regress/regress-6907.js:7:(func
tion TestDematerializedContextInBuiltin() {
v8/test/mjsunit/regress/regress-crbug-163530.js
:30:// Test materialization of an arguments 
object with unknown argument
v8/test/mjsunit/regress/regress-crbug-163530.js
:57:// Test materialization of an arguments 
object with unknown argument
v8/test/mjsunit/regress/regress-crbug-732169.js
:7:(function TestGeneratorMaterialization() {
v8/test/mjsunit/regress/regress-crbug-732169.js
:18:(function 
TestGeneratorMaterializationWithProperties() {
v8/test/mjsunit/regress/regress-1364400.js:21:/
/ access. The HeapNumber should be correctly 
rematerialized in deoptimzer.
v8/test/mjsunit/regress/regress-2261.js:30:// 
Test materialization of the arguments object 
when deoptimizing a
v8/test/mjsunit/regress/regress-2261.js:54:// 
Test materialization of the arguments object 
when deoptimizing an
v8/test/mjsunit/regress/regress-2261.js:82:// 
Test materialization of the multiple arguments 
objects when
v8/test/mjsunit/wasm/bigint-rematerialize.js:38
:// passed to StateValues without conversion. 
Rematerialization will happen
v8/test/mjsunit/bugs/bug-941049.js:46:// Test 
function with a materialized arguments array.
v8/test/mjsunit/bugs/bug-941049.js:56:// Test 
function without a materialized arguments 
array.
v8/test/mjsunit/regress/wasm/regress-381458039.
js:7:// materializing the heap objects.
v8/test/mjsunit/turboshaft/maglev-frontend/hole
y-float64-deopt.js:20:  // materialize a NaN 
HeapNumber for it, which is wrong. It should 
instead have
v8/test/mjsunit/turboshaft/maglev-frontend/hole
y-float64-deopt.js:21:  // HoleyFloat64 
MachineRepresentation, and be materialized by 
the deoptimizer
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:7:function 
dematerialized_duplicated(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:21:%PrepareF
unctionForOptimization(dematerialized_duplicate
d);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:22:assertEqu
als(7, dematerialized_duplicated(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:23:assertEqu
als(7, dematerialized_duplicated(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:25:%Optimize
FunctionOnNextCall(dematerialized_duplicated);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:26:assertEqu
als(7, dematerialized_duplicated(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated-recursive.js:28:let |o1, 
o2] = dematerialized_duplicated(-1);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:7:function 
dematerialized_object(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:25:%PrepareFunctionF
orOptimization(dematerialized_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:26:assertEquals(7, 
dematerialized_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:27:assertEquals(7, 
dematerialized_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:29:%OptimizeFunction
OnNextCall(dematerialized_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:30:assertEquals(7, 
dematerialized_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:32:assertEquals({x :
42}, dematerialized_object(-1));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:34:%ClearFunctionFee
dback(dematerialized_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:35:%PrepareFunctionF
orOptimization(dematerialized_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:36:assertEquals(7, 
dematerialized_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:37:assertEquals(7, 
dematerialized_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:39:%OptimizeFunction
OnNextCall(dematerialized_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:40:assertEquals(7, 
dematerialized_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-multi-escape.js:42:assertEquals({x :
17}, dematerialized_object(1));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:7:function 
dematerialized_simple_object(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:19:%PrepareFunction
ForOptimization(dematerialized_simple_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:20:assertEquals(7, 
dematerialized_simple_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:21:assertEquals(7, 
dematerialized_simple_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:23:%OptimizeFunctio
nOnNextCall(dematerialized_simple_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:24:assertEquals(7, 
dematerialized_simple_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-simple-object.js:25:assertEquals({ x
: 42, y : -1 }, 
dematerialized_simple_object(-1));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-regexp.js:37:// Triggering a deopt, 
which will materialize the regex object.
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:7:function 
dematerialized_duplicated(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:20:%PrepareFunctionFor
Optimization(dematerialized_duplicated);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:21:assertEquals(7, 
dematerialized_duplicated(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:22:assertEquals(7, 
dematerialized_duplicated(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:24:%OptimizeFunctionOn
NextCall(dematerialized_duplicated);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:25:assertEquals(7, 
dematerialized_duplicated(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-duplicated.js:27:let o = 
dematerialized_duplicated(-1);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:7:function 
dematerialized_nested_object(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:20:%PrepareFunctio
nForOptimization(dematerialized_nested_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:21:assertEquals(7,
 dematerialized_nested_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:22:assertEquals(7,
 dematerialized_nested_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:24:%OptimizeFuncti
onOnNextCall(dematerialized_nested_object);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:25:assertEquals(7,
 dematerialized_nested_object(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-nested-objects.js:27:             
dematerialized_nested_object(-1));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:7:function 
dematerialized_double_array(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:19:%PrepareFunctionF
orOptimization(dematerialized_double_array);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:20:assertEquals(7, 
dematerialized_double_array(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:21:assertEquals(7, 
dematerialized_double_array(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:23:%OptimizeFunction
OnNextCall(dematerialized_double_array);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:24:assertEquals(7, 
dematerialized_double_array(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-double-array.js:25:assertEquals(|1.1
, 2.2, 3.3], dematerialized_double_array(-1));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:7:function 
dematerialized_rest(n, ...rest) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:18:%PrepareFunctionForOptimi
zation(dematerialized_rest);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:19:assertEquals(7, 
dematerialized_rest(5, 2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:20:assertEquals(7, 
dematerialized_rest(5, 2, 3, 4));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:22:%OptimizeFunctionOnNextCa
ll(dematerialized_rest);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:23:assertEquals(7, 
dematerialized_rest(5, 2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest.js:25:assertEquals(|2, 3, 4], 
dematerialized_rest(-1, 2, 3, 4));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-heap-number.js:7:function 
dematerialized_heap_number(n) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-heap-number.js:18:%PrepareFunctionFo
rOptimization(dematerialized_heap_number);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-heap-number.js:19:assertEquals(7, 
dematerialized_heap_number(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-heap-number.js:21:%OptimizeFunctionO
nNextCall(dematerialized_heap_number);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-heap-number.js:22:assertEquals(7, 
dematerialized_heap_number(5));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-heap-number.js:23:assertEquals(22.72
, dematerialized_heap_number(-1));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:7:function 
dematerialized_rest(n, ...rest) {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:19:%PrepareFunctionFo
rOptimization(dematerialized_rest);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:20:assertEquals(7, 
dematerialized_rest(5, 2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:21:assertEquals(7, 
dematerialized_rest(5, 2, 3, 4));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:23:%OptimizeFunctionO
nNextCall(dematerialized_rest);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:24:assertEquals(7, 
dematerialized_rest(5, 2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-rest-length.js:26:assertEquals(3, 
dematerialized_rest(-1, 2, 3, 4));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:10:function 
dematerialized_arguments_length() {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:22:%PrepareFunct
ionForOptimization(dematerialized_arguments_len
gth);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:24:assertEquals(
7, dematerialized_arguments_length(2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:25:assertEquals(
7, dematerialized_arguments_length(2, 3, 4, 
9));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:27:%OptimizeFunc
tionOnNextCall(dematerialized_arguments_length)
;
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:28:assertEquals(
7, dematerialized_arguments_length(2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments-length.js:31:assertEquals(
4, dematerialized_arguments_length(2, 3, 4, 
9));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:10:function 
dematerialized_arguments() {
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:22:%PrepareFunctionForO
ptimization(dematerialized_arguments);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:24:assertEquals(7, 
dematerialized_arguments(2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:25:assertEquals(7, 
dematerialized_arguments(2, 3, 4, 9));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:27:%OptimizeFunctionOnN
extCall(dematerialized_arguments);
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:28:assertEquals(7, 
dematerialized_arguments(2, 3));
v8/test/mjsunit/turboshaft/maglev-frontend/dema
terialized-arguments.js:31:assertEquals(|2, 3, 
4, 9], |... dematerialized_arguments(2, 3, 4, 
9)]);
v8/test/mjsunit/turboshaft/maglev-frontend/hole
y-float64-phi-deopt.js:20:  // materialize a 
NaN HeapNumber for it, which is wrong. It 
should instead have
v8/test/mjsunit/turboshaft/maglev-frontend/hole
y-float64-phi-deopt.js:21:  // HoleyFloat64 
MachineRepresentation, and be materialized by 
the deoptimizer
v8/test/mjsunit/object-literal-multiple-fields.
js:14:(function TestMaterialized() {
v8/test/mjsunit/object-literal-multiple-fields.
js:23:(function TestMaterialize2() {
v8/test/mjsunit/maglev/regress-362784006.js:11:
// --deopt-every-n-times > 0 forces 
MaterializeHeapObjects to call GC.
v8/test/mjsunit/maglev/spill-double.js:22:    
g(x); // When we lazy deopt, we call g with the
materialized
v8/test/mjsunit/maglev/regress-328281224.js:13:
f(); // f can throw, so we will need to 
re-materialize x.
v8/test/mjsunit/maglev/escape-analysis-context.
js:31:// This will deopt and the context will 
be materialized during
v8/test/mjsunit/maglev/regress-328868985.js:20:
// but if materialization is wrong, it can
v8/test/mjsunit/regress/wasm/regress-341947685.
js:53:  // the Liftoff register allocator 
should consider "in use"; materialize
[Step 6: Duration 25.44 seconds| Input tokens: 
42,995 | Output tokens: 5,441]
━━━━━━━━━━━━━━━━━━━ Step 7 ━━━━━━━━━━━━━━━━━━━━
[Step 7: Duration 12.06 seconds]
[Step 1: Duration 154.83 seconds| Input tokens:
2,946 | Output tokens: 621]
[Step 2: Duration 183.73 seconds| Input tokens:
7,478 | Output tokens: 2,519]
Traceback (most recent call last):
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rises-the-fog.py", line 101, in <module>
    sys.exit(main())
             ~~~~^^
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/rises-the-fog.py", line 95, in main
    a.system.start_system()
    ~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/FoG.py", line 221, in start_system
    result = self.run_task(
        task_description="Initialize FoG orchestration across PickSection, FatherOfGeorge, CodeAnalyzer, and ProgramBuilder",
    ...<5 lines>...
        }
    )
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 323, in run_task
    agent_output = manager_agent.run(prompt)
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 292, in _wrapped_run
    result = orig_run(prompt, *args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1322, in _step_stream
    for output in self.process_tool_calls(chat_message, memory_step):
                  ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1407, in process_tool_calls
    tool_output = process_single_tool_call(tool_call)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1376, in process_single_tool_call
    tool_call_result = self.execute_tool_call(tool_name, tool_arguments)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1474, in execute_tool_call
    return tool(**arguments) if is_managed_agent else tool(**arguments, sanitize_inputs_outputs=True)
           ~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 877, in __call__
    result = self.run(full_task, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1322, in _step_stream
    for output in self.process_tool_calls(chat_message, memory_step):
                  ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1407, in process_tool_calls
    tool_output = process_single_tool_call(tool_call)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1376, in process_single_tool_call
    tool_call_result = self.execute_tool_call(tool_name, tool_arguments)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1474, in execute_tool_call
    return tool(**arguments) if is_managed_agent else tool(**arguments, sanitize_inputs_outputs=True)
           ~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 877, in __call__
    result = self.run(full_task, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 503, in run
    steps = list(self._run_stream(task=self.task, max_steps=max_steps, images=images))
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 582, in _run_stream
    for output in self._step_stream(action_step):
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/agents.py", line 1290, in _step_stream
    chat_message: ChatMessage = self.model.generate(
                                ~~~~~~~~~~~~~~~~~~~^
        input_messages,
        ^^^^^^^^^^^^^^^
        stop_sequences=["Observation:", "Calling tools:"],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        tools_to_call_from=self.tools_and_managed_agents,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/smolagents/models.py", line 1243, in generate
    response = self.client.completion(**completion_kwargs)
  File "/Users/tarun/vrig/fuzzillai/Sources/Agentic_System/agents/BaseAgent.py", line 164, in wrapped_completion
    result = original_completion(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/utils.py", line 1244, in wrapper
    result = original_function(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/main.py", line 2125, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<16 lines>...
        shared_session=shared_session,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/llms/openai/openai.py", line 673, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/litellm_core_utils/logging_utils.py", line 237, in sync_wrapper
    result = func(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/litellm/llms/openai/openai.py", line 471, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
        request,
        stream=stream or self._should_stream_response_body(request=request),
        **kwargs,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
        request,
    ...<2 lines>...
        history=[],
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
        request,
        follow_redirects=follow_redirects,
        history=history,
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
        pool_request.request
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
        self.READ_NUM_BYTES, timeout=timeout
    )
  File "/Users/tarun/vrig/fuzzillai/.venv/lib/python3.14/site-packages/httpcore/_backends/sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py", line 1285, in recv
    return self.read(buflen)
           ~~~~~~~~~^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py", line 1140, in read
    return self._sslobj.read(len)
           ~~~~~~~~~~~~~~~~~^^^^^
KeyboardInterrupt
